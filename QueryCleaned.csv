Id,PostTypeId,AcceptedAnswerId,CreationDate,Score,ViewCount,Body,OwnerUserId,LastEditorUserId,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,ClosedDate,ContentLicense
71911980,1,-1.0,2022-04-18 12:44:51,0,368,"<p>Everything in my code is working well, until I try to use the KNN algorithm to predict the quality of wine using its attributes. This is my first time trying this code for KNN.</p>
<p>this part is giving me errors</p>
<pre><code>from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=0)
classifier.fit(wine_train[X], y_train)
</code></pre>
<p>Error location:</p>
<pre><code>      1 from sklearn.neighbors import KNeighborsClassifier
      2 classifier = KNeighborsClassifier(n_neighbors=0)
----&gt; 3 classifier.fit(wine_train[X], y_train)


File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:979, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
    962     raise ValueError(&quot;y cannot be None&quot;)
    964 X = check_array(
    965     X,
    966     accept_sparse=accept_sparse,
   (...)
    976     estimator=estimator,
    977 )
--&gt; 979 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric)
    981 check_consistent_length(X, y)
    983 return X, y

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:989, in _check_y(y, multi_output, y_numeric)
    987 &quot;&quot;&quot;Isolated part of check_X_y dedicated to y validation&quot;&quot;&quot;
    988 if multi_output:
--&gt; 989     y = check_array(
    990         y, accept_sparse=&quot;csr&quot;, force_all_finite=True, ensure_2d=False, dtype=None
    991     )
    992 else:
    993     y = column_or_1d(y, warn=True)

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:800, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    794         raise ValueError(
    795             &quot;Found array with dim %d. %s expected &lt;= 2.&quot;
    796             % (array.ndim, estimator_name)
    797         )
    799     if force_all_finite:
--&gt; 800         _assert_all_finite(array, allow_nan=force_all_finite == &quot;allow-nan&quot;)
    802 if ensure_min_samples &gt; 0:
    803     n_samples = _num_samples(array)

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:122, in _assert_all_finite(X, allow_nan, msg_dtype)
    120 elif X.dtype == np.dtype(&quot;object&quot;) and not allow_nan:
    121     if _object_dtype_isnan(X).any():
--&gt; 122         raise ValueError(&quot;Input contains NaN&quot;)

ValueError: Input contains NaN
</code></pre>
",6542807.0,6542807.0,2022-04-18 17:16:12,2022-04-18 17:16:12,ValueError on KNN in Python,<python><data-science><knn>,0,6,N/A,CC BY-SA 4.0
71482445,1,-1.0,2022-03-15 12:42:56,0,512,"<p>For example:</p>
<h2>dataframe A</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>zip</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>4</td>
<td>7</td>
</tr>
<tr>
<td>3</td>
<td>2</td>
<td>5</td>
<td>8</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>6</td>
<td>9</td>
</tr>
</tbody>
</table>
</div><h2>dataframe B</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>zip</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>4</td>
<td>6</td>
</tr>
</tbody>
</table>
</div><h2>Desired result</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>zip</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>8</td>
<td>7</td>
</tr>
<tr>
<td>3</td>
<td>6</td>
<td>10</td>
<td>8</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>24</td>
<td>54</td>
</tr>
</tbody>
</table>
</div>",18472617.0,9840637.0,2022-03-15 12:51:33,2022-03-15 16:10:09,How to multiply two pyspark dataframes element wise?,<python><pyspark><data-science>,2,2,N/A,CC BY-SA 4.0
73047143,1,-1.0,2022-07-20 06:53:51,1,45,"<p>My requirement is that I have an xlsx workbook and I want to pass the values of specific columns in the below form to another code.
ex. (col1,col2,col3)
(col1,col2,col3)
(col1,col2,col3)
Can someone please help me out.</p>
",19452662.0,19230181.0,2022-07-20 07:07:46,2022-07-20 07:07:46,How to pass data from an xlsx to another code row by row with python,<python><excel><data-science>,0,1,N/A,CC BY-SA 4.0
72243463,1,-1.0,2022-05-14 19:55:19,0,251,"<p>I have two datasets that are collected at different frequencies at the same time. One is recorded at 128Hz and another one is recorded at 512 Hz. I am trying to extract some features using the moving window technique but I have some problems.</p>
<ol>
<li>Frequencies of both datasets are different.</li>
<li>the timestamp is in unix format and changes in nanoseconds. hence there won't be any match at the start and end of each second or minute.</li>
<li>one of the datasets is actually a little longer than the other by a few seconds.</li>
</ol>
<p>is there any way I can align my window properly with respect to timestamp and calculate features? or if there is another way please suggest me.</p>
",8753176.0,-1.0,N/A,2022-09-16 21:15:32,how to align sliding window to extract features from multi modal timeseries data?,<python><machine-learning><statistics><data-science><feature-extraction>,1,1,N/A,CC BY-SA 4.0
72243721,1,72244080.0,2022-05-14 20:41:51,0,26,"<p>I am learning pandas and Data Science and am a beginner.
I have a data as following</p>
<pre><code>Rahul
1
2
5
Suresh
4
2
1
Dharm
1
3
4
</code></pre>
<p>I would like it in my dataframe as</p>
<pre><code>Rahul   1
        2
        5
Suresh  4
        2
        1
Dharm   1
        3
        4
</code></pre>
<p>How can I achieve this without iterating over every row, as I have data in hundreds of thousand. I have searched a lot but cannot find anything other than iteration yet. Is there a better way.</p>
<p>Thank you for your kindness and patience</p>
",9918947.0,11865956.0,2022-05-14 20:48:32,2022-05-14 21:46:21,How to get this single column data into data frame with appropriate columns,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
72251266,1,72253428.0,2022-05-15 18:48:32,0,383,"<p>I have a question related to plotting the moving average by doing group by. I have taken the dataset from Kaggle <a href=""https://www.kaggle.com/code/kp4920/s-p-500-stock-data-time-series-analysis/comments"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/kp4920/s-p-500-stock-data-time-series-analysis/comments</a>. I have extracted the few rows by applying the below condition.</p>
<pre><code>new_df_A = new_df[(new_df.Name == 'A')]
new_df_A.sort_values(by=['Name', 'Date'])
</code></pre>
<p>And I tried to calculate the moving average for 30 days by implementing this code</p>
<pre><code>for cols in new_df_A.columns:
    if cols not in ['Name', 'Date',]:
        new_df_A['ma_'+cols]=new_df_A.groupby('Name').rolling(30)[cols].mean().reset_index(drop=True)
</code></pre>
<p>And I got this warning error</p>
<pre><code>/var/folders/6j/0bj57ss10ggbdk87dtdkbgyw0000gn/T/ipykernel_130/1482748670.py:3: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  new_df_A['ma_'+cols]=new_df_A.groupby('Name').rolling(30)[cols].mean().reset_index(drop=True)
/var/folders/6j/0bj57ss10ggbdk87dtdkbgyw0000gn/T/ipykernel_130/1482748670.py:3: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  new_df_A['ma_'+cols]=new_df_A.groupby('Name').rolling(30)[cols].mean().reset_index(drop=True)
/var/folders/6j/0bj57ss10ggbdk87dtdkbgyw0000gn/T/ipykernel_130/1482748670.py:3: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
</code></pre>
<p>And When I tried to plot the figure, it is blank. Can somebody help me with this?</p>
<p>Thank You</p>
",16875907.0,16875907.0,2022-05-16 00:13:19,2022-05-16 03:56:14,How to plot moving average by groupby in python?,<python><pandas><seaborn><moving-average><data-science-experience>,1,0,N/A,CC BY-SA 4.0
72252860,1,-1.0,2022-05-15 23:23:41,0,33,"<p>I have a dataset in which out of 3600 data lines, one feature &quot;Test_8&quot; is missing in 1000 of these. The values currently in my dataset are 2018.0 (2329 times) and 2019.0 (242 times), possibly indicating a year (though the type is a bit weird). I wanted to create a predictor to get its prediction for the 1000 missing values, but I have 2 problems:</p>
<ol>
<li>I have a few tens of places where there are other missing values (only 2 other features has missing lines - test_1 and test_8).</li>
<li>I didn't manage to encode this kind of data so that the random forrest classifier will be able to work with it.</li>
</ol>
<p>Here is the head of my data:</p>
<pre><code>Test_0  Test_1  Test_2  Test_3  Test_4  Test_5  Test_6  Test_7  Test_8  Test_9  Test_10 Test_11 Test_12 Test_13 Test_14 Test_15 RMA
0   261.36  E   Low OP-1    True    102.14  False   True    2018.0  58.9    nov 136 1   120 1   TP-1549 False
1   268.62  H   Mid OP-17   True    655.42  False   True    2018.0  62.0    apr 114 1   152 2   TP-1549 False
2   297.66  F   Low OP-1    True    605.50  False   False   2018.0  65.1    nov 285 3   116 4   TP-1549 False
3   515.46  NaN Low OP-17   True    144.78  True    True    2018.0  83.7    jan 208 1   93  2   TP-1549 False
4   290.40  D   Low OP-4    True    416.22  False   True    2019.0  37.2    may 58  2   334 1   TP-1549 False
</code></pre>
<p>Here is my code:</p>
<pre><code>df_with_Test_8_encoded = pd.factorize(df_with_Test_8)
df_with_Test_8_encoded.head()
</code></pre>
<p>The error I get:</p>
<pre><code>ValueError: could not broadcast input array from shape (2571,17) into shape (2571,)
</code></pre>
",19118083.0,19118083.0,2022-05-15 23:25:59,2022-05-15 23:25:59,how to encode my data for it to fit a random forrest regressor?,<python><pandas><encoding><data-science>,0,2,N/A,CC BY-SA 4.0
72252897,1,-1.0,2022-05-15 23:32:07,1,114,"<p>I have used the code below in the Anaconda/pandas environment to create dataframes from multiple xml files and then began analyzing the data:</p>
<pre><code> &quot;&quot;&quot;Thanks to Roberto Preste Author From XML to Pandas dataframes,
    from xtree ... to return out_df
    https://medium.com/@robertopreste/from-xml-to-pandas-dataframes-9292980b1c1c
    &quot;&quot;&quot;
    def parse_XML(xml_file, df_cols): 
    xtree = et.parse(xml_file)
    xroot = xtree.getroot()
    rows = []
    
    for node in xroot: 
        res = []
        res.append(node.attrib.get(df_cols[0]))
        for el in df_cols[1:]: 
            if node is not None and node.find(el) is not None:
                res.append(node.find(el).text)
            else: 
                res.append(None)
        rows.append({df_cols[i]: res[i] 
                     for i, _ in enumerate(df_cols)})
    
    out_df = pd.DataFrame(rows, columns=df_cols)
    
    
    return out_df
</code></pre>
<p>As a means of keeping the data organized by year I want to add a column to the resulting individual dfs that gives the name of the file from which each year's data was parsed.  I have seen many examples of how to add a column for filename when reading data from a csv file- but almost none with regard to how to add a filename column when parsing data from xml.  Using some of the other examples I've found I have tried what I have shown below, replacing the csv with xml in the appropriate places in the code.</p>
<pre><code># Make the names of xml files read in a column in the resulting dataframe
import glob
import os.path

# Create a list of all XML files
files = glob.glob(&quot;*.xml&quot;)

# Create an empty list to append the df
filenames = []

for xml in files:
    df = pd.read_xml(xml)
    df['file name'] = os.path.basename(xml)
    filenames.append(df)    

path = r'xml_in'
allFiles = glob.glob(path + '/*.xml')

for file_ in allFiles:   
    df = pd.read_xml(file_, header=0)
    df.name = file_
    print(df.name)
</code></pre>
<p>This results in a variable called &quot;files&quot; of type list with all the correct filenames from the directory in the list but doesn't make all the desired dataframes as I had hoped.  The only dataframe it creates correctly is the one for the filename in the directory that falls last alphabetically and that variable is called &quot;df&quot; which I realize is part of the problem from using an example meant to read csv and not xml- but the other dataframes I attempt to create are all filled with &quot;None&quot; in each cell even though the number of rows and columns I expect to be parsed in are correct based on previous running of the code I had prior to trying to add a filename column.</p>
<p>How can I parse xml and add a column for a filename for all the files in the directory?</p>
",13351519.0,-1.0,N/A,2022-05-15 23:32:07,Add a column for filename when data is parsed from multiple xml files to individual pandas Data Frames,<python><pandas><xml><data-science><filenames>,0,0,N/A,CC BY-SA 4.0
72259212,1,-1.0,2022-05-16 12:28:08,0,120,"<p>guys im currently working on dataframe with pandas and for data preparation i was use get_dummies and dummies has a keyword argument (drop_first)</p>
<p>I knew drop first is removing first level of categorical but why we should remove that ?</p>
<pre><code>pd.get_dummies(df[SigCat], drop_first = True)
</code></pre>
",19127333.0,19127333.0,2022-05-16 14:01:38,2022-05-16 14:01:38,why we use drop_first in dummies?,<pandas><dataframe><data-science>,0,3,N/A,CC BY-SA 4.0
72259217,1,-1.0,2022-05-16 12:28:38,-1,861,"<p>I have to read jsonl file in python and normalize column consist of json.
I succesfully read with pandas</p>
<pre><code>    df = pd.read_json('file.jsonl', lines=True)

</code></pre>
<p>and I have information in one of my columns like</p>
<p><strong>column2</strong></p>
<pre><code>{&quot;key1&quot; : &quot;kvalue1&quot;, &quot;key2&quot; : &quot;kvalue2&quot; }
</code></pre>
<p>I can normalize them seperately but I don't know why it doesn't work for all the data.
I do normalization for this column like this</p>
<pre><code>normdata = pd.json_normalize(df['column1'])

</code></pre>
<p>it works like this but when I want to do for all the dataset it doesn't work. I thought maybe problem lies in the type of data so I checked them. <strong>df['column2']</strong> is <strong>series</strong> and <strong>df</strong> itself is <strong>DataFrame</strong>. I converted DataFrame to series to check but it didn't work.</p>
<p>The data example :</p>
<pre><code>{&quot;column1&quot; : &quot;value1&quot;, &quot;column2&quot; : {&quot;key1&quot; : &quot;kvalue1&quot;, &quot;key2&quot; : &quot;kvalue2&quot; } }
{&quot;column1&quot; : &quot;value2&quot;, &quot;column2&quot; : {&quot;key1&quot; : &quot;kvalue3&quot;, &quot;key2&quot; : &quot;kvalue4&quot; } }
{&quot;column1&quot; : &quot;value3&quot;, &quot;column2&quot; : {&quot;key1&quot; : &quot;kvalue5&quot;, &quot;key2&quot; : &quot;kvalue6&quot; } }
</code></pre>
<p>I want to seperate 'column2' informations like 'key1', 'key2' in seperate columns using pandas</p>
<p>Do you have nice idea how to solve it ?</p>
",11042420.0,11042420.0,2022-05-16 13:08:05,2022-05-16 15:09:52,How to read jsonl file and normalize json column,<python><json><pandas><data-science>,1,5,N/A,CC BY-SA 4.0
72263282,1,72263391.0,2022-05-16 17:28:17,1,1722,"<p>I have a dataframe with columns that has list of numbers as strings:</p>
<pre><code>C1 C2 l
1   3 ['5','9','1']
7   1 ['7','1','6']
</code></pre>
<p>What is the best way to convert it to list of ints?</p>
<pre><code>C1 C2 l
1   3 [5,9,1]
7   1 [7,1,6]
</code></pre>
<p>Thanks</p>
",6057371.0,-1.0,N/A,2022-05-16 17:37:18,Pandas convert column where every cell is list of strings to list of integers,<python-3.x><pandas><dataframe><data-science><data-munging>,2,0,N/A,CC BY-SA 4.0
72263932,1,-1.0,2022-05-16 18:25:40,1,43,"<p>I have a dataframe:</p>
<pre><code>df = 
A  B  C
1 [2,3] [4,5]
</code></pre>
<p>And I want to explode it element-wise based on [B,C] to get:</p>
<pre><code>df = 
A B  C
1 2  4
1 3  5
</code></pre>
<p>What is the best way to do so?
B and C are always at the same length.</p>
<p>Thanks</p>
",6057371.0,-1.0,N/A,2022-05-16 18:33:32,pandas how to explode from two cells element-wise,<pandas><dataframe><data-science><data-munging><pandas-explode>,1,1,2022-05-16 18:43:36,CC BY-SA 4.0
72267417,1,72282625.0,2022-05-17 02:04:59,0,272,"<p>I'm trying to use gds in neo4j do calculate similarities. I understand how to get gds to calculate all the similarities in the in memory graph, but really this answer the question &quot;Tell me, over the whole graph, the similarity of each pair of nodes.&quot;
Now my question is different, my question is &quot;Given this node N, give me the similarity of N with every other node&quot;. Obviously the performance of the latter would be much faster..
I tried to express this with a query of this type:</p>
<pre><code>CALL gds.nodeSimilarity.stream('test', { relationshipWeightProperty: 'strength', similarityCutoff: 0.1 })
YIELD node1, node2, similarity
WITH gds.util.asNode(node1) AS n1, gds.util.asNode(node2)AS n2, similarity
WHERE n1.name = &quot;Chair1&quot;
RETURN n1.name, n2.name, similarity
ORDER BY n1.name
</code></pre>
<p>But what is really happening under the hood?
Is gds :
A) calculating ALL the similarities between every node1 and node2 and then filtering the results only for Chair1?
OR
B) Is gds ONLY calculating the results between Chair1 and every other node?
I'd need behaviour B to happen for me, but after some testing with the airport databases it seems that the execution time is shorter without the WHERE clause than with, so my nose tells me that it may be behaviour A. Is there a way to force behaviour B?</p>
",15233037.0,-1.0,N/A,2022-05-18 02:36:16,Neo4j similarity of single node with entire graph,<neo4j><cypher><similarity><graph-data-science>,1,0,N/A,CC BY-SA 4.0
73046568,1,-1.0,2022-07-20 05:54:29,0,67,"<p>My code is:</p>
<pre><code>escolaridade = dados4[dados4[&quot;DMDEDUC&quot;] == 5]
sentimento = dados4[dados4[&quot;DPQ020&quot;] != 0].head(875)
plt.bar(escolaridade,sentimento, color=&quot;blue&quot;)
plt.xlabel(&quot;Superior Completo&quot;)
plt.ylabel(&quot;Deprimido 1x na semana&quot;)
plt.tittle(&quot;Relação da tristeza com um adulto graduado&quot;)
plt.show()
</code></pre>
<p>I defined the <code>.head(875)</code> because the length of <code>escolaridade</code> and <code>sentimento</code> are distinct.</p>
<p>&quot;DMDEDUC&quot; and &quot;DPQ020&quot; are numerical columns of a dataframe and for this I didn't understand the error.</p>
<blockquote>
<p>&quot;TypeError: only size-1 arrays can be converted to Python scalars&quot;</p>
</blockquote>
<p>Thanks!</p>
",19584522.0,12370687.0,2022-08-26 08:52:48,2022-08-26 08:52:48,"I need help in this error ""TypeError: only size-1 arrays can be converted to Python scalars""",<python><dataframe><matplotlib><scikit-learn><data-science>,1,2,N/A,CC BY-SA 4.0
72248377,1,72248408.0,2022-05-15 12:44:29,0,41,"<p>Suppose I have two lists of height and age for <strong>K</strong> people.</p>
<p>For instance,</p>
<pre><code>height = [180, 190, 185, 150, 180, 185]
age = [23, 28, 29, 30, 40, 20]
</code></pre>
<p>First person is 23 years old, and is 180cm tall, second one is 28 years old and 190cm tall, and so on.</p>
<p>I would like to sort these two lists first for height order and then by age, as follows:</p>
<pre><code>result = [[150,30], [180,23], [180,40], [185,20], [185,29], [190,28]]
</code></pre>
<p>Would it be possible? What kind of data structure should I search for?</p>
",11381650.0,-1.0,N/A,2022-05-15 13:02:52,How to sort lists of two data fields using two filters,<python><data-structures><data-science>,1,0,2022-05-15 12:50:39,CC BY-SA 4.0
72278557,1,72278922.0,2022-05-17 17:49:13,0,120,"<p>I have a dataset with a column labeled 'antecedents', that involves a list of elements in it, that are of type 'object', listed like shown below.  How can I filter my dataset to return only rows where that specific antecedent column's element number length is 1? The values</p>
<p>antecedent column example:</p>
<p>(APPLE, BANANA)</p>
<p>(APPLE, PEAR)</p>
<p>(APPLE, BANANA, PEAR)</p>
<p>(APPLE)</p>
<p>I am doing association rule analysis and would only like the rows with 1 antecedent (so APPLE in the case above), and so far I have:</p>
<p>df[df['antecedents'] <em>now check the length of this object list of elements in the antecedents column and only return rows where this antecedent column's length is 1</em></p>
<p>I am unsure what to do to access inside this antecedent column to only return rows whose length is 1 (only one element in the antecedent column).</p>
",19138733.0,-1.0,N/A,2022-05-17 18:32:52,Filtering a dataframe (where 1 specific column is type 'object') based on the columns element number length (Association Rule Analysis),<python><pandas><dataframe><return><data-science>,1,2,N/A,CC BY-SA 4.0
72282419,1,72282528.0,2022-05-18 01:53:15,0,73,"<p>I have a dataframe which needs to be consolidated for same ids and split other columns to different columns as such. I have presented the example input dataframe and required output dataframe.</p>
<p><strong>Input dataframe example</strong></p>
<pre><code>data = {'id':[1, 1, 1, 2, 2, 2], 
    'status':[3, 3, 3, 4, 4, 4],
    'amount':[30, 40, 50, 60, 70, 80],
    'paid':[100, 200, 300, 400, 500, 600]}

dataframe = pd.DataFrame(data)


   id status amount paid
0   1   3    30     100
1   1   3    40     200
2   1   3    50     300
3   2   4    60     400
4   2   4    70     500
5   2   4    80     600
</code></pre>
<p>Required output dataframe</p>
<pre><code>   id   status  amount_1 amount_2 amount_3 paid_1 paid_2 paid_3
0   1    3       30       40       50       100    200    300
1   2    4       60       70       80       300    400    600
</code></pre>
",18341075.0,-1.0,N/A,2022-05-18 02:14:58,How to consolidate the dataframe of matching indices and splitting other columns?,<python><pandas><dataframe><dictionary><data-science>,1,0,2022-05-18 05:00:02,CC BY-SA 4.0
72286445,1,-1.0,2022-05-18 09:21:45,1,180,"<p>after doing web scraping for a project while merging two datasets I realized that some data were not matched because the strings are not exactly the same ( example: Usop = Usopp), so to overcome this problem I am using FuzzyWuzzy library, the problem is that the datasets to be merged have different sizes</p>
<pre><code>list1 = df1['Name'].tolist()
list2 = df1['name'].tolist()
mat1 = []
mat2 = []
for i in list1:
    mat1.append(process.extract(i, list2, limit=2))
df1['matches'] = mat1
</code></pre>
<p>ERROR: Length of values (2416) does not match length of index (190)
the length problem cannot be solved, how could I solve it?</p>
",19143503.0,-1.0,N/A,2022-05-18 09:21:45,Fuzzy Matching beetwen two list with different lenght,<pandas><match><data-science><fuzzywuzzy>,0,0,N/A,CC BY-SA 4.0
72261434,1,-1.0,2022-05-16 15:08:00,0,1016,"<p>I want to know how to handle the skewed data which contains a particular column that has  multiple categorical values. Some of these values have more <code>value_counts()</code> than others.<a href=""https://i.stack.imgur.com/ywxd7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ywxd7.png"" alt=""y is the count and x is the unique value"" /></a>
As you can see in this data the values greater than 7 have value counts lot less than others. How to handle this kind of skewed data? (This is not the target variable. I want to know about skewed independent variable)</p>
<p>I tried changing ' these smaller count values to a particular value (<code>-1</code>). That way I got count of <code>-1</code> comparable to other values. But training classification model on this data will affect the accuracy. <a href=""https://i.stack.imgur.com/c5QeZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c5QeZ.png"" alt=""converted smaller count values to -1"" /></a></p>
",9011045.0,9011045.0,2022-05-19 17:38:48,2022-05-19 17:38:48,How to handle skewed categorical data for multiclass-classification task?,<pandas><machine-learning><data-science><multiclass-classification><data-preprocessing>,1,0,N/A,CC BY-SA 4.0
72268164,1,72269601.0,2022-05-17 04:19:55,0,148,"<p>I am doing a machine learning (value prediction) task. While I am preprocessing data, it takes a very long time. I have a csv file with around 640000 rows, and I am trying to subtract the dates of consecutive rows and calculate the time duration. The csv file looks as attached. For example, 2011-08-17 to 2011-08-19 takes 2 days, and I would like to write 2 to the &quot;time duration&quot; column. I've used the python datetime function to do this. And it costs a lot of time.</p>
<pre><code>data = pd.read_csv(f'{proj_dir}/raw data/measures.csv', encoding=&quot;cp1252&quot;) 

file = data[['ID', 'date', 'value1', 'value2', 'duration']]

def time_subtraction(date, prev_date):
  diff = datetime.strptime(date, '%Y-%m-%d') - datetime.strptime(prev_date, '%Y-%m-%d')
  diff_days = diff.days
  return diff_days

def calculate_time_duration(dataframe, set_0_indices):
  for i in range(dataframe.shape[0]):
    # For each patient, sets &quot;Time Duration&quot; at the first measurement to be 0
    if i in set_time_0_indices.values:
      dataframe.iloc[i, 4] = 0 # set time duration to 0 (beginning of this patient)
    else: # time subtraction
      dataframe.iloc[i, 4] = time_subtraction(date=dataframe.iloc[i, 1], prev_date=dataframe.iloc[i-1, 1])
  return dataframe

# I am running on Google Colab. This line takes very long.
result = calculate_time_duration(dataframe = file, set_0_indices = set_time_0_indices)
</code></pre>
<p>I wonder if there are any ways to accelerate this process. Does using a GPU help? I have access to a remote GPU, but I don't know if using a GPU helps with data preprocessing. By the way, under what scenario can GPUs really make things faster? Thanks in advance!</p>
<p><a href=""https://i.stack.imgur.com/hNbsL.png"" rel=""nofollow noreferrer"">what my data looks like</a></p>
",17203565.0,17203565.0,2022-05-17 07:10:38,2022-05-17 07:12:09,Does GPU accelerate data preprocessing in ML tasks?,<machine-learning><gpu><data-science>,1,1,N/A,CC BY-SA 4.0
72276575,1,-1.0,2022-05-17 15:15:16,1,109,"<p>I am doing sentence comparison between actual text and speech to text.
on this scenario I need to convert the number names one point ninety nine percent to 1.99%.</p>
<p>input : you will get discount of one point ninety nine percent.</p>
<p>output :  you will get discount of 1.99%.</p>
<p>any suggestions or any libraries which will help to get rid of this situations.
Thanks for your input.</p>
",17273467.0,17273467.0,2022-05-17 15:17:37,2022-05-17 15:17:37,how to convert number names to numbers like one point ninety nine percent to 1.99% for nlp task,<python-3.x><nlp><data-science>,0,5,N/A,CC BY-SA 4.0
72296566,1,72296617.0,2022-05-18 22:11:51,0,25,"<p>I want to extract some data from each row, and make that new columns of existing or new dataframe, without repeatedly doing the same operation of re. match.</p>
<p>Here's how one entry of the dataframe looks:</p>
<pre><code>00:00 Someones_name: some text goes here
</code></pre>
<p>And i have a regex that successfully takes 3 groups that I need:</p>
<pre><code>re.match(r&quot;^(\d{2}:\d{2}) (.*): (.*)$&quot;, x)
</code></pre>
<p>The problem I have is, how to take matched_part[1], [2], and [3] without actually matching for every new column again.</p>
<p>The solution that I don't want is:</p>
<pre><code>new_df['time'] = old_df['text'].apply(function1)`
new_df['name'] = old_df['text'].apply(function2)`
new_df['text'] = old_df['text'].apply(function3)`

def function1(x):
  return re.match(r&quot;^(\d{2}:\d{2}) (.*): (.*)$&quot;, x)[1]
</code></pre>
",15223794.0,-1.0,N/A,2022-05-18 22:19:19,Generating 3 columns from one with .apply on dataframe,<python><pandas><dataframe><data-science><series>,1,0,N/A,CC BY-SA 4.0
72249726,1,72250918.0,2022-05-15 15:35:24,0,107,"<p><strong>I have a time series data like below where the data consists of year and week. So, the data is from 2014 1st week to 2015 52 weeks.</strong></p>
<p><a href=""https://i.stack.imgur.com/SGiOm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SGiOm.png"" alt=""Time series data"" /></a></p>
<p><strong>Now, below is the line plot of the above mentioned data</strong></p>
<p><a href=""https://i.stack.imgur.com/yEw6Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yEw6Y.png"" alt=""sns line plot"" /></a></p>
<p><strong>As you can see the x axis labelling is not quite what I was trying to achieve since the point after 201453 should be 201501 and there should not be any straight line and it should not be up to 201499. How can I rescale the xaxis exactly according to Due_date column? Below is the code</strong></p>
<pre><code>rand_products = np.random.choice(Op_2['Sp_number'].unique(), 3)
selected_products = Op_2[Op_2['Sp_number'].isin(rand_products)][['Due_date', 'Sp_number', 'Billing']]

plt.figure(figsize=(20,10))
plt.grid(True)

g = sns.lineplot(data=selected_products, x='Due_date', y='Billing', hue='Sp_number', ci=False, legend='full', palette='Set1');
</code></pre>
",13829846.0,-1.0,N/A,2022-05-15 18:01:42,Time series data visualization issue,<python><matplotlib><seaborn><data-science><line-plot>,1,1,N/A,CC BY-SA 4.0
72269625,1,72270392.0,2022-05-17 07:13:52,0,131,"<p>I have a csv file with many patients' health measurement data. Each patient has a different number of measurements. (Some patients come frequently, some don't.) I am trying to do a next value prediction model to predict the patients' risk of specific incidences.
Since the values are all in time sequence, I've tried to use LSTM to make predictions. Also, I am concatenating all the patients' health data together into a long column. (Please see attachment)</p>
<p><a href=""https://i.stack.imgur.com/IFnMY.png"" rel=""nofollow noreferrer"">what I am feeding into the LSTM</a></p>
<p>And my LSTM model generates results like stock price prediction.</p>
<p><a href=""https://i.stack.imgur.com/qeZzG.png"" rel=""nofollow noreferrer"">kind of like my result</a></p>
<p>But I wonder if there are better ways. I think my current method of concatenating all my patients' data is strange. Since all the patients have a different number of measurements, I am not sure if can feed them to the LSTM model in parallel. Or maybe I should use random forest because each patient's data has unique distribution? Thank you!</p>
",17203565.0,-1.0,N/A,2022-05-17 08:12:31,Next value prediction using LSTM or other methods?,<machine-learning><deep-learning><data-science><lstm><random-forest>,1,0,N/A,CC BY-SA 4.0
72286029,1,-1.0,2022-05-18 08:53:20,0,929,"<p>I'm working on a multi class classification problem using onevsrest model with Python, and I want to get the accuracy score for each class, for example, class1 : accuracy 80%, class2 : accuracy 88%, class3 : accuracy 90%, I only get the accuracy of the entire model, how can I get it for each class ?</p>
",18784307.0,-1.0,N/A,2022-05-18 08:59:54,Can we get Accuracy score for each class in multi class in multi classification problem?,<python><machine-learning><data-science><prediction><metrics>,0,1,N/A,CC BY-SA 4.0
72301839,1,-1.0,2022-05-19 09:16:46,1,33,"<pre><code>    ctr &lt;- 1
while(ctr &lt;=7){
  if(ctr%%5==0){
    break
  }
  print(paste(&quot;ctr is set to&quot;, ctr))
  ctr &lt;- ctr+1
}
</code></pre>
<p>the output I get is</p>
<pre><code>[1] &quot;ctr is set to 1&quot;
[1] &quot;ctr is set to 2&quot;
[1] &quot;ctr is set to 3&quot;
[1] &quot;ctr is set to 4&quot;
</code></pre>
<p>but when I try to <em>replace break with next</em>, it doesn't work .</p>
",19152122.0,-1.0,N/A,2022-05-19 09:28:39,Why can't I use a next statement in a while loop?,<r><data-science>,1,3,N/A,CC BY-SA 4.0
72305273,1,-1.0,2022-05-19 13:16:55,0,77,"<p>How do you measure with features of your dataframe are important for your Kmeans model?
I'm working with a dataframe that has 37 columns of which 33 columns are of categorical data.
These 33 data columns go through one-hot-encoding and now I have 400 columns.</p>
<p>I want to see which columns have an impact on my model and which don't.</p>
<p>Is there a method for this or do I loop this?</p>
",6188007.0,6446053.0,2022-05-19 13:22:31,2022-05-19 13:28:14,Kmeans clustering measuring important features,<python><data-science><cluster-analysis><k-means><one-hot-encoding>,1,1,N/A,CC BY-SA 4.0
72283924,1,-1.0,2022-05-18 06:04:52,0,1247,"<p>CODE:-</p>
<pre><code>import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


class Vizualizer:

    def __init__(self,data,cols):
        self.data=data
        self.cols=cols

    def box_plot(self):
            for col in self.cols:
                sns.boxplot(x=col,y='Ex-Showroom_Price',data=self.data[self.data['Ex-Showroom_Price']])


    

df=pd.read_csv('cars_engage_2022.csv')
cols=['Make','Model','City_Mileage','Highway_Mileage','ARAI_Certified_Mileage','Type','Fuel_Type','Body_Type']
d=Vizualizer(df,cols)
df[&quot;Ex-Showroom_Price&quot;] = df[&quot;Ex-Showroom_Price&quot;].str.replace(r'.* ([\d,]+)+$', r'\1',regex=True).str.replace(',', '',regex=True).astype('int32')
d.box_plot()
</code></pre>
<p><strong>ERROR</strong></p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\shweta\OneDrive\Desktop\DEMO\demo\src\carsengage2022\preprocessing\roughVIZ.py&quot;, line 36, in &lt;module&gt;
    d.box_plot()
  File &quot;c:\Users\shweta\OneDrive\Desktop\DEMO\demo\src\carsengage2022\preprocessing\roughVIZ.py&quot;, line 25, in box_plot
    sns.boxplot(x=col,y='Ex-Showroom_Price',data=self.data[self.data['Ex-Showroom_Price']])
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\frame.py&quot;, line 3511, in __getitem__
    indexer = self.columns._get_indexer_strict(key, &quot;columns&quot;)[1]
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\indexes\base.py&quot;, line 5782, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\indexes\base.py&quot;, line 5842, in _raise_if_missing
    raise KeyError(f&quot;None of [{key}] are in the [{axis_name}]&quot;)
KeyError: &quot;None of [Int64Index([ 292667,  236447,  296661,  334768,  272223,  314815,  279650,\n             351832,  333419,  362000,\n            ...\n    
        1065900, 1182000, 1312000, 1111000, 1191000, 1302000, 1421000,\n            1431000, 1201000, 6862560],\n           dtype='int64', length=1276)] are 
in the [columns]&quot;
</code></pre>
<p>I'm trying to plot a boxplot between the columns in cols and their Ex-Showroom-Price.
The values of the Ex-Showroom-Price are categorical, therefore, I first converted them to integer and then I'm trying to do the boxplot ,but it's throwing the error that,Key Error: &quot;None of [Int64Index...] dtype='int64] are in the columns&quot;.</p>
",16854609.0,16854609.0,2022-05-18 06:05:25,2022-05-18 09:04:27,"Key Error: ""None of [Int64Index...] dtype='int64] are in the columns""",<python><pandas><data-science><data-analysis><boxplot>,1,0,N/A,CC BY-SA 4.0
72310049,1,-1.0,2022-05-19 19:21:58,0,77,"<p>I want to create ML Model to identify if the Transaction is fraud or not.
Each row represents one Transaction. I understand that this ML Model can be built. What the model will be missing is the behaviour when Multiple Transactions are done within short duration. How do I capture that behaviour? If 1st transaction for a card happens at 10 am and other transaction happens at 10.01 am then that Transaction is generally Fraud. But my model is missing that. Please help</p>
",16116272.0,-1.0,N/A,2022-05-20 08:45:03,How to create Credit Card Fraud Detection model so that it captures dependency in the observations,<python><machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
72310137,1,72310256.0,2022-05-19 19:29:21,1,39,"<p>I have a list of tuples, which looks like below:</p>
<pre><code>tuplelist = [
    (datetime.date(2020, 4, 20), 4.23, 'EUR'),
    (datetime.date(2020, 4, 20), 3.76, 'USD'),
    (datetime.date(2020, 4, 20), 4.21, 'EUR'),
    (datetime.date(2020, 4, 20), 5.20, 'GPB'),
    (datetime.date(2020, 4, 20), 3.77, 'USD'),
    (datetime.date(2020, 4, 20), 4.27, 'EUR'),
    (datetime.date(2020, 4, 20), 3.79, 'USD'),
    (datetime.date(2020, 4, 20), 4.30, 'EUR'),
    (datetime.date(2020, 4, 20), 5.14, 'GPB'),
    (datetime.date(2020, 4, 20), 3.77, 'USD'),
    (datetime.date(2020, 4, 25), 4.23, 'EUR'),
    (datetime.date(2020, 4, 25), 3.76, 'USD'),
    (datetime.date(2020, 4, 25), 4.21, 'EUR'),
    (datetime.date(2020, 4, 25), 5.20, 'GPB'),
    (datetime.date(2020, 4, 25), 3.77, 'USD'),
    (datetime.date(2020, 4, 27), 4.27, 'EUR'),
    (datetime.date(2020, 4, 27), 3.79, 'USD'),
    (datetime.date(2020, 4, 27), 4.30, 'EUR'),
    (datetime.date(2020, 4, 27), 5.14, 'GPB'),
    (datetime.date(2020, 4, 28), 3.77, 'USD'),
    (datetime.date(2020, 4, 28), 4.23, 'EUR'),
    (datetime.date(2020, 5, 2), 3.76, 'USD'),
    (datetime.date(2020, 5, 2), 4.21, 'EUR'),
    (datetime.date(2020, 5, 2), 5.20, 'GPB'),
    (datetime.date(2020, 5, 2), 3.77, 'USD'),
    (datetime.date(2020, 5, 2), 4.27, 'EUR'),
    (datetime.date(2020, 5, 5), 3.79, 'USD'),
    (datetime.date(2020, 5, 5), 4.30, 'EUR'),
    (datetime.date(2020, 5, 5), 5.14, 'GPB'),
    (datetime.date(2020, 5, 5), 3.77, 'USD')
]
</code></pre>
<p>and I'd like to group it by a date and currency symbol. It should look like this (for each day):</p>
<pre><code>(datetime.date(2020, 4, 20), [{'EUR': [4.23, 4.21, 4.27, 4.3]}, {'USD': [3.76, 3.77, 3.79, 3.77]}, {'GPB': [5.2, 5.14]}])
</code></pre>
<p>I managed to group this by data, using this line of code:</p>
<pre><code>tuplelist2dict = {key: [*map(lambda v: {v[2]:v[1]}, values)] for key, values in groupby(tuplelist, lambda x: x[0])} 
</code></pre>
<p>and I get this output:</p>
<pre><code>(datetime.date(2020, 4, 20), [{'EUR': 4.23}, {'USD': 3.76}, {'EUR': 4.21}, {'GPB': 5.2}, {'USD': 3.77}, {'EUR': 4.27}, {'USD': 3.79}, {'EUR': 4.3}, {'GPB': 5.14}, {'USD': 3.77}])
(datetime.date(2020, 4, 25), [{'EUR': 4.23}, {'USD': 3.76}, {'EUR': 4.21}, {'GPB': 5.2}, {'USD': 3.77}])
(datetime.date(2020, 4, 27), [{'EUR': 4.27}, {'USD': 3.79}, {'EUR': 4.3}, {'GPB': 5.14}])
(datetime.date(2020, 4, 28), [{'USD': 3.77}, {'EUR': 4.23}])
(datetime.date(2020, 5, 2), [{'USD': 3.76}, {'EUR': 4.21}, {'GPB': 5.2}, {'USD': 3.77}, {'EUR': 4.27}])
(datetime.date(2020, 5, 5), [{'USD': 3.79}, {'EUR': 4.3}, {'GPB': 5.14}, {'USD': 3.77}])
</code></pre>
<p>However I'm struggling with merging the values of different currencies to get the format of this data I'd shown.</p>
<p>I'd appreciate any hints.</p>
",18582965.0,-1.0,N/A,2022-05-19 19:39:32,Problem with processing data from a list of tuples / groupby / lambda,<python><dictionary><lambda><data-science>,1,0,N/A,CC BY-SA 4.0
72308480,1,-1.0,2022-05-19 17:02:20,1,248,"<p><em>Thank you in advance for any assistance.</em></p>
<p><strong>Aim:</strong> I have a 5-day food intake survey dataset that I am trying to analyse in R. I am interested in calculating the mean, se, min and max intake for the weight of a specific food consumed per day.
I would more easily complete this in excel, but due to the scale of data, I require R to complete this.</p>
<p><strong>Example question:</strong> What is a person's daily intake (g) of lettuce? [mean, standard deviation, standard error, min, and max]</p>
<p><strong>Example extraction dataset:</strong> please note the actual dataset includes a number of foods and a large no. of participants.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">participant</th>
<th style=""text-align: center;"">day</th>
<th style=""text-align: center;"">code</th>
<th style=""text-align: center;"">foodname</th>
<th style=""text-align: center;"">weight</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">132</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">62</td>
<td style=""text-align: center;"">lettuce</td>
<td style=""text-align: center;"">53</td>
</tr>
<tr>
<td style=""text-align: left;"">84</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">62</td>
<td style=""text-align: center;"">lettuce</td>
<td style=""text-align: center;"">23</td>
</tr>
<tr>
<td style=""text-align: left;"">132</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">62</td>
<td style=""text-align: center;"">lettuce</td>
<td style=""text-align: center;"">32</td>
</tr>
<tr>
<td style=""text-align: left;"">153</td>
<td style=""text-align: center;"">4</td>
<td style=""text-align: center;"">62</td>
<td style=""text-align: center;"">lettuce</td>
<td style=""text-align: center;"">26</td>
</tr>
<tr>
<td style=""text-align: left;"">142</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">62</td>
<td style=""text-align: center;"">lettuce</td>
<td style=""text-align: center;"">23</td>
</tr>
<tr>
<td style=""text-align: left;"">123</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">62</td>
<td style=""text-align: center;"">lettuce</td>
<td style=""text-align: center;"">23</td>
</tr>
<tr>
<td style=""text-align: left;"">131</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">62</td>
<td style=""text-align: center;"">lettuce</td>
<td style=""text-align: center;"">30</td>
</tr>
<tr>
<td style=""text-align: left;"">153</td>
<td style=""text-align: center;"">5</td>
<td style=""text-align: center;"">62</td>
<td style=""text-align: center;"">lettuce</td>
<td style=""text-align: center;"">16</td>
</tr>
</tbody>
</table>
</div>
<p><strong>At present:</strong></p>
<pre><code># import dataset
foodsurvey&lt;-read.spss(&quot;foodsurvey.sav&quot;,to.data.frame=T,use.value.labels=T)
summary(foodsurvey)

# keep my relevant columns
myvariables = subset(food survey, select = c(1,2,3,4,5) )

# rename columns
colnames(myvariables)&lt;-c('participant','day','code','foodname','foodweight')

# create values
day&lt;-myvariables$day
participant&lt;-myvariables$participant
foodcode&lt;-myvariables$foodcode
foodname&lt;-myvariables$foodname
foodweight&lt;-myvariables$foodweight

# extract lettuce by ID code to be analysed
lettuce&lt;- filter(myvariables, foodcode == &quot;62&quot;)
dim(lettuce)
str(lettuce)

# errors arise attempting to analyse consumption (weight) of lettuce per day using ops.factor function

# to analyse the outputs
summary(lettuce/days)
quantile(lettuce/foodweight)
max(lettuce)
min(lettuce)
median(lettuce)
mean(lettuce)

</code></pre>
",18244488.0,3358272.0,2022-05-19 18:30:26,2022-05-19 18:36:30,Looking for advice to analyse this particular objective and data in R,<r><data-science>,2,2,N/A,CC BY-SA 4.0
72316924,1,-1.0,2022-05-20 09:54:21,0,135,"<p><a href=""https://i.stack.imgur.com/5VfzR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5VfzR.png"" alt=""enter image description here"" /></a></p>
<p>Please take a look at the picture.</p>
<p>I wanted to use Yen's Algorithm Shortest Path from Neo4j Graph Data Science to dig the Shortest Path between A and F, but nothing came back.</p>
<p>The following code:</p>
<pre><code>MATCH (source:Location{name: ‘A’}), (target:Location{name: ‘F’})
CALL gds.shortestPath.dijkstra.stream(‘myGraph’, {
sourceNode: source,
targetNode: target
})
YIELD index, sourceNode, targetNode,nodeIds,path
RETURN
index,
gds.util.asNode(sourceNode).name AS sourceNodeName,
gds.util.asNode(targetNode).name AS targetNodeName,
[nodeId IN nodeIds | gds.util.asNode(nodeId).name] AS nodeNames,
nodes(path) as path
ORDER BY index
</code></pre>
<p>Does the relation between A and F need to be in the same direction.</p>
<p>If I just want to look at the shortest distance between A and F, regardless of the direction and weight of relation, what should I do.</p>
<p>Thank you for any help</p>
",10492865.0,-1.0,N/A,2022-05-21 08:27:20,Does the shortest path mining algorithm of Neo4j require the relationship between nodes to be in the same direction,<python-3.x><neo4j><py2neo><graph-data-science>,1,0,N/A,CC BY-SA 4.0
72301027,1,-1.0,2022-05-19 08:21:00,2,734,"<p>I am trying to analyze the output of running xgb classifier. I haven't been able to find a proper explanation of the difference between the feature weights and the features importance chart.
Here is a sample screenshot (not from my dataset but the same analysis I am running).
I will appreciate explanations or references to where I can get any.
Thanks in advance
<a href=""https://i.stack.imgur.com/CjVh8.png"" rel=""nofollow noreferrer"">Screenshot</a></p>
",18526703.0,18526703.0,2022-05-19 11:39:11,2022-05-19 11:39:11,The difference between feature importance and feature weights in XGBoost,<python><machine-learning><data-science><xgboost><xgbclassifier>,0,3,N/A,CC BY-SA 4.0
72304256,1,72304445.0,2022-05-19 12:06:53,0,45,"<p>I have a data of conversations between the phone numbers</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>from phone</th>
<th>to phone</th>
<th>message</th>
</tr>
</thead>
<tbody>
<tr>
<td>7788994455</td>
<td>2233665588</td>
<td>hi</td>
</tr>
<tr>
<td>2233665588</td>
<td>7788994455</td>
<td>hello</td>
</tr>
<tr>
<td>1122335566</td>
<td>4455117766</td>
<td>where are you</td>
</tr>
<tr>
<td>4455117766</td>
<td>1122335566</td>
<td>I am home</td>
</tr>
<tr>
<td>2233665588</td>
<td>7788994455</td>
<td>wassup?</td>
</tr>
</tbody>
</table>
</div>
<p>I am looking to segregate all the messages of each number (two way).</p>
<p>Should be as:-</p>
<p><strong>Example from the above table:</strong></p>
<p>7788994455,2233665588:- hi|hello|wassup?</p>
<p>I am looking for the whole conversation should be grouped accordingly(from phone and to phone).</p>
",19153418.0,-1.0,N/A,2022-05-19 12:56:23,Segregate (Group) all the messages of each number(two way),<python><pandas><dataframe><pandas-groupby><data-science>,2,0,N/A,CC BY-SA 4.0
72307807,1,-1.0,2022-05-19 16:09:09,1,31,"<p>I have a df:</p>
<pre><code>  gender    category    subcategory  item_brand   item_NWT  item_price
   Women    Outerwear     Jacket      J. Crew        NWT       22.0
   Women    Outerwear     Jacket      Talbots        NWT       50.0
   Women    Outerwear     Jacket      J. Crew        NWT       100.0
</code></pre>
<p>I have some features with similar values and different target price, I get the feeling the my model will perform poorly by have this type of data right?. Also some similar features have the same target price, the thing is: it's my first time dealing with an issue like this one.</p>
",13335589.0,-1.0,N/A,2022-05-19 16:09:09,"Dataset with similar data points, but some of the targets are different",<python><pandas><dataframe><data-science><feature-engineering>,0,2,N/A,CC BY-SA 4.0
72323179,1,-1.0,2022-05-20 18:09:50,1,34,"<p>I have 2 tables from 2 different systems. The identifiers (column ID) of these systems are not equal. I have a mapping file which says, which ID from system 1 matches to which ID from system 2.</p>
<p>These three files are given as xlsx files and are imported into data frames (pandas). The task is to write something from one cell in system 1 to the correct cell in system 2. I have to implement this task in Python. Maybe you can help me!</p>
<p>The following code is simplified to the core of my question.. the <strong>performance</strong>! I have looked for other solutions (<code>np.where</code> &amp; binary search), but my skills are to weak and I`m getting more and more exhausted. Maybe you can help me!</p>
<pre class=""lang-py prettyprint-override""><code># Iterate through every row in df_system1
for a in tqdm(df_system1.index, total=len(df_system1)):
    # Search for the matching key in the df_keyTable
    for b in df_keyTable.index: 
        # Search for the ID in df_system2, which matches to the key and do...
        for c in df_system1:
</code></pre>
<p>Each of these tables cointains more than 25000 rows, which is the reason why I can't implement the  task with for loops.</p>
",19163184.0,1422451.0,2022-05-20 18:52:10,2022-05-20 18:52:10,Options for better performance instead of using for loops | Python,<python-3.x><pandas><dataframe><performance><data-science>,0,2,N/A,CC BY-SA 4.0
72324363,1,72324534.0,2022-05-20 20:13:34,0,332,"<p>I did check for possible solutions, but the most common solutions didn't work.</p>
<pre><code>df_woningen.groupby(['postcode'], dropna=True)['energy_ranking'].agg(pd.Series.mode)
</code></pre>
<p>Gives me multiple arrays in this format:</p>
<pre><code>2611BA            []
2611BB           4.0
2611BC    [3.0, 6.0]
</code></pre>
<p>QUESTION: How to select the last item to use as value for a new column?</p>
<p>Background: one column has rankings. Per group I want to take the mode() and put it as imputed value for NaN's in that group.</p>
<p>In case of multiple modes I want to take the highest. Sometimes a group has only NaN, in that case it should or could stay like that. If a group has 8 NaN's and 1 ranking '8', that de mode should be 8, disregarding the NaN's.</p>
<p>I am trying to create a new column by using codes like this:</p>
<pre><code>df_woningen.groupby(['postcode'], dropna=True)['energy_ranking'].agg(
    lambda x: pd.Series.mode(x)[0])
</code></pre>
<p>Or</p>
<pre><code>df_woningen.groupby(['postcode'], dropna=True)['energy_ranking'].agg(lambda x:x.value_counts(dropna=True).index[0])
</code></pre>
<p>But I get errors and I believe it's because of the different lengths of the arrays.</p>
<p><code>TypeError: 'function' object is not subscriptable</code></p>
<p><code>index 0 is out of bounds for axis 0 with size 0</code></p>
<p>Anyone an idea how to solve this?</p>
",18676995.0,-1.0,N/A,2022-05-20 20:36:10,"Python groupby mode() pick last item when having empty, single and multiple array lengths",<python><pandas><dataframe><numpy><data-science>,2,0,N/A,CC BY-SA 4.0
72325388,1,72326405.0,2022-05-20 22:45:02,0,85,"<p>I'm trying to use fillna() and transform() to impute some missing values in a column with respect to the 'release_year' and 'brand_name' of the phone, but after running my code I still have the same missing value counts.</p>
<p>Here are my missing value counts &amp; percentages prior to running the code:</p>
<p><a href=""https://i.stack.imgur.com/hhb3e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hhb3e.png"" alt=""The column I'm imputing on is 'main_camera_mp"" /></a></p>
<p>Here is the code I ran to impute 'main_camera_mp' and the result (just an FYI that I copied the above dataframe into df2):</p>
<pre><code>df2['main_camera_mp'] = df2['main_camera_mp'].fillna(value = df2.groupby(['release_year','brand_name'])['main_camera_mp'].transform('mean'))
</code></pre>
<p><a href=""https://i.stack.imgur.com/ucQ7s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ucQ7s.png"" alt=""Missing value counts &amp; percentages after running the above line"" /></a></p>
",16071188.0,14909621.0,2022-05-21 17:04:06,2022-05-21 17:04:06,Fillna() not imputing values with respect to groupby(),<python><data-science><imputation><fillna>,1,0,N/A,CC BY-SA 4.0
72325479,1,72325661.0,2022-05-20 23:05:41,0,93,"<p>Have a question about pandas:</p>
<p>I have two dataframes:</p>
<pre><code>df1 = pd.DataFrame({'user_id': ['12', '22', '33', '44'],
                    'time': ['t1', 't2', 't3', 't4'],
                    'data': [{'av': '8.0', 'si': 3, 'am' : 2}, {'av': '8.0', 'si': 44}, {'av': '8.0', 'si': 1}, {'av': '8.0', 'si': 22}]})

df2 = pd.DataFrame({'user_id': ['11', '22', '33', '44'],
                    'time': ['t1', 't2', 't3', 't4'],
                    'data': [{'cv': 'ff', 'si': 3}, {'cv': 'ff', 'si': 44}, {'cv': 'fa', 'si': 2}, {'cv': 'ff', 'si': 21}]})
</code></pre>
<p>And I need to filter df1 to reject rows contains values of 'user_id' and ['data'].'si' the same as 'user_id' and ['data'].'si' from df2 rows. If I'll do:</p>
<pre><code>filter1 = df1['data'].str['si'].isin(df2['data'].str['si'])
filter2 = df1['user_id'].isin(df2['user_id'])
df3= df1[filter1 &amp; filter2]
</code></pre>
<p>Result won't be valid, cause I need to reject exactly rows, where values satisfices both conditions:
As example row 2 from df1</p>
<pre><code>user_id       time        data
  22           t2    'av': '8.0', 'si': 44
</code></pre>
<p>and from df2:</p>
<pre><code>user_id       time        data
  22           t2    'cv': 'ff', 'si': 44
</code></pre>
<p>Thanks a lot for any help!</p>
",16843398.0,-1.0,N/A,2022-05-20 23:49:08,Select rows from pandas dataframe by two values at the same time from rows in another dataframe,<python><pandas><dataframe><numpy><data-science>,1,2,N/A,CC BY-SA 4.0
72332762,1,72334563.0,2022-05-21 19:47:25,1,132,"<p>i'm pretty new to d3. Could you help me, please, why doesn't my graph redraw after I generate new data on button press? It draws once, but after I push button it only logs new data array to console. Do I remove old data wrong?</p>
<p>Just in case I import d3 functions separately this is why it's written <code>select</code> instead of <code>d3.select</code>. Everything works besides updating graph.</p>
<p>Thank you in advance. There is a similar question here, and I tried to do it in a similar way, but I don't get what am I doing wrong.</p>
<pre><code>const randomValue = randomInt(0, 100);
const months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sen', 'Oct', 'Nov', 'Dec'];

let data = months.map(m =&gt; ({
    label: m,
    value: randomValue()
}))

let values = data.map(m =&gt; m.value)

const yScale = scaleLinear().domain([0, 100]).range([0, height]);
const colorScale = scaleLinear().domain([0, 100]).range(['#d22626', '#086cdc']);
const $chartContainer = $root.append('g')
    .classed('.mz-chart__container', true)
    .attr('transform', `translate(${padding}, ${100})`);    

const xScale = scaleBand().domain(months).range([0, width]).paddingInner(0.2);
const hAxis = axisBottom().scale(xScale);
$root.append(&quot;g&quot;)
    .attr('transform', `translate(${padding}, ${height+100})`)
    .call(hAxis);


const vAxisScale = scaleLinear().domain([0, 100]).range([height, 0]);
const vAxis = axisLeft()
    .scale(vAxisScale)
    .tickSize(-width, 0, 0)
    .tickFormat((y, x) =&gt; y);

$root.append(&quot;g&quot;)
    .attr(&quot;transform&quot;, &quot;translate(40, 100)&quot;)
    .call(vAxis);

function updateChart() {
    data = months.map(m =&gt; ({
        label: m,
        value: randomValue()
    }))

    console.log(data);
    let chartbars = $chartContainer.selectAll('.mz-chart__bar').data(data)

    chartbars
        .exit()
        .remove()

    chartbars
        .enter()
        .append('g')
        .classed('mz-chart__bar', true)
        .append('rect')
        .attr('x', (d, i) =&gt; xScale(d.label))
        .attr('y', (d, i) =&gt; height - yScale(d.value))
        .attr('height', (d, i) =&gt; yScale(d.value))
        .attr('width', (d, i) =&gt; xScale.bandwidth())
        .attr('fill', d =&gt; colorScale(d.value));
}

select(&quot;.mz-button&quot;).on(&quot;click&quot;, updateChart)
updateChart()
</code></pre>
",17206578.0,-1.0,N/A,2022-05-22 03:16:46,Redraw d3 graph with a new data,<javascript><d3.js><graph><data-science>,1,0,N/A,CC BY-SA 4.0
72298797,1,-1.0,2022-05-19 04:52:55,0,418,"<p>CODE:-</p>
<p>I'm trying to convert the values of ARAI_Certified_Mileage column to float but it is throwing an error that string values like '9.8-10.0' can't be converted to float.
So I'm thinking of removing 10.0 and converting 9.8 to float, but I'm not getting how to do it.</p>
<pre><code>import os
import pandas as pd

df = pd.read_csv('cars_engage_2022.csv')

df[&quot;ARAI_Certified_Mileage&quot;] = df[&quot;ARAI_Certified_Mileage&quot;].str.replace(r'.* ([\d,]+)+$', r'\1',regex=True).str.replace('km/litre', '',regex=True).astype('float')
print(df.head())
</code></pre>
<p>ERROR:-</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\shweta\OneDrive\Desktop\DEMO\demo\src\carsengage2022\preprocessing\roughKMC.py&quot;, line 9, in &lt;module&gt;
    df[&quot;ARAI_Certified_Mileage&quot;] = df[&quot;ARAI_Certified_Mileage&quot;].str.replace(r'.* ([\d,]+)+$', r'\1',regex=True).str.replace('km/litre', '',regex=True).astype('float')
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\generic.py&quot;, line 5912, in astype
    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\internals\managers.py&quot;, line 419, in astype
    return self.apply(&quot;astype&quot;, dtype=dtype, copy=copy, errors=errors)
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\internals\managers.py&quot;, line 304, in apply
    applied = getattr(b, f)(**kwargs)
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\internals\blocks.py&quot;, line 580, in astype
    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\dtypes\cast.py&quot;, line 1292, in astype_array_safe
    new_values = astype_array(values, dtype, copy=copy)
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\dtypes\cast.py&quot;, line 1237, in astype_array
    values = astype_nansafe(values, dtype, copy=copy)
  File &quot;C:\Users\shweta\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\dtypes\cast.py&quot;, line 1181, in astype_nansafe
    return arr.astype(dtype, copy=True)
ValueError: could not convert string to float: '9.8-10.0 '
</code></pre>
<p>values of ARAI_Certified_Mileage column:-</p>
<pre><code>ARAI_Certified_Mileage
22.4-21.9 km/litre
9.8-10.0 km/litre
20.6-22.8 km/litre
and so on...
</code></pre>
",16854609.0,16854609.0,2022-05-19 04:56:47,2022-05-19 05:12:54,ValueError: could not convert string to float: '9.8-10.0 ',<python><pandas><machine-learning><data-science>,1,1,N/A,CC BY-SA 4.0
72341470,1,72342167.0,2022-05-22 21:40:47,-3,1286,"<p>I am doing a project based on Machine learning (Python) and trying all models on my data.
Really confused in</p>
<p>For Classification and For Regression</p>
<ol>
<li>If I have to apply normalization, Z Score or Standard deviation on whole data set and then set the values of Features(X) and output(y).</li>
</ol>
<pre><code>    def normalize(df):
        from sklearn.preprocessing import MaxAbsScaler
        scaler = MaxAbsScaler()
        scaler.fit(df)
        scaled = scaler.transform(df)
        scaled_df = pd.DataFrame(scaled, columns=df.columns)
        return scaled_df
    
data=normalize(data)
X=data.drop['col']
y=data['col']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

</code></pre>
<ol start=""2"">
<li>Or only have to apply on features(X)</li>
</ol>
<pre><code>X=data.drop['col']
y=data['col']

def normalize(df):
    from sklearn.preprocessing import MaxAbsScaler
    scaler = MaxAbsScaler()
    scaler.fit(df)
    scaled = scaler.transform(df)
    scaled_df = pd.DataFrame(scaled, columns=df.columns)
    return scaled_df

X=normalize(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

</code></pre>
",13214782.0,13214782.0,2022-05-22 22:15:29,2022-05-23 00:36:50,Should we apply normalization on whole data set or only X,<python><machine-learning><jupyter-notebook><data-science><normalization>,1,2,N/A,CC BY-SA 4.0
72355804,1,72359611.0,2022-05-23 23:37:03,3,318,"<p><strong>Issue</strong>: Getting r2 near to 0.64. Want to improve my results more. Don't know what's the issue of these results. Have done Removing outliers, Converting String -&gt; Numerical, normalization. Wanna know is there any issue with my output? Please ask me anything if I didn't ask the question correctly. It's just my starting on Stack overflow.</p>
<pre><code>y.value_counts()
</code></pre>
<pre><code>3.3    215
3.0    185
2.7    154
3.7    134
2.3     96
4.0     54
2.0     31
1.7     21
1.3     20
</code></pre>
<p>This is histogram of my outputs. I am not professional in Regression need super help from your side.<br></p>
<p><a href=""https://i.stack.imgur.com/NFTzy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NFTzy.png"" alt=""Histogram of my Outputs"" /></a></p>
<p><strong>Removing Collinearity  in my inputs</strong></p>
<pre><code>
import seaborn as sns
# data=z_scores(df)
data=df
correlation=data.corr()

k=22
cols=correlation.nlargest(k,'Please enter your Subjects GPA which you have studied? (CS) [Introduction to ICT]')['Please enter your Subjects GPA which you have studied? (CS) [Introduction to ICT]'].index
cm=np.corrcoef(data[cols].values.T)
f,ax=plt.subplots(figsize=(15,15))
sns.heatmap(cm,vmax=.8,linewidths=0.01,square=True,annot=True,cmap='viridis',
            linecolor=&quot;white&quot;,xticklabels=cols.values,annot_kws={'size':12},yticklabels=cols.values)
</code></pre>
<p><a href=""https://i.stack.imgur.com/3zrAs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3zrAs.png"" alt=""enter image description here"" /></a></p>
<pre><code>cols=pd.DataFrame(cols)
cols=cols.set_axis([&quot;Selected Features&quot;], axis=1)
cols=cols[cols['Selected Features'] != 'Please enter your Subjects GPA which you have studied? (CS) [Introduction to ICT]']
cols=cols[cols['Selected Features'] != 'Your Fsc/Ics marks percentage?']
X=df[cols['Selected Features'].tolist()]
X

</code></pre>
<p><strong>Then applied Random Forest Regressor and got these results</strong></p>
<pre><code>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)
model=regressor.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(&quot;MAE Score: &quot;, mean_absolute_error(y_test, y_pred))
print(&quot;MSE Score: &quot;, mean_squared_error(y_test, y_pred))
print(&quot;RMSE Score: &quot;, math.sqrt(mean_squared_error(y_test, y_pred)))
print(&quot;R2 score : %.2f&quot; %r2_score(y_test,y_pred))

</code></pre>
<p>Got these Results.</p>
<pre><code>MAE Score:  0.252967032967033
MSE Score:  0.13469450549450546
RMSE Score:  0.36700750059706605
R2 score : 0.64
</code></pre>
",13214782.0,13214782.0,2022-05-23 23:50:51,2022-05-24 08:28:34,How to improve my regression models results more accurate in random forest regression,<python><pandas><machine-learning><data-science><random-forest>,1,0,N/A,CC BY-SA 4.0
72356040,1,72356079.0,2022-05-24 00:27:57,0,101,"<p>Perhaps a very basic syntactical question here but I haven’t been able to find an answer:</p>
<p>why, when you take a basic series (series_sample_name) and write df_sample_name = pd.DataFrame(series_sample_name) do you get basically the same output as the series but in df form, whereas if you write pd.DataFrame([series_sample_name]) you would get the series values displayed horizontally and the index labels become columns? I wasn’t aware that adding single brackets to a single object, when you only pass that one object, had meaning.</p>
",19176762.0,19176762.0,2022-05-24 00:28:25,2022-05-24 00:35:36,What do brackets (instead of no brackets) mean when you pass a series in pd.DataFrame?,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
72359861,1,-1.0,2022-05-24 08:48:54,0,173,"<p>Using Neo4j v4.4 and GDS 2.0. I'm trying to train a model. When I type:</p>
<pre><code>CALL gds.beta.pipeline.nodeClassification.train('individual-graph', {
   pipeline: 'pipe',
   nodeLabels: ['PERSON'],
   modelName: 'xmen-model-fastRP',
   targetProperty: 'is_risky',
   metrics: ['F1_WEIGHTED','ACCURACY'],
   randomSeed: 2
   }) YIELD modelInfo
  RETURN
  modelInfo.bestParameters AS winningModel,
  modelInfo.metrics.F1_WEIGHTED.outerTrain AS trainGraphScore,
  modelInfo.metrics.F1_WEIGHTED.test AS testGraphScore
</code></pre>
<p>I get the following error message:
Failed to invoke procedure <code>gds.beta.pipeline.nodeClassification.train</code>: Caused by: java.lang.IllegalArgumentException: Target property <code>is_risky</code> not found in graph with node properties: [[embedding]]</p>
<p>What am I doing wrong? Can you help please?</p>
",1692562.0,-1.0,N/A,2022-05-24 15:15:26,gds.beta.pipeline.nodeClassification.train: Target property not found,<neo4j><graph-data-science>,1,0,N/A,CC BY-SA 4.0
72379755,1,-1.0,2022-05-25 14:44:48,0,1644,"<p>I am performing a sagemaker batch transform using a transformer created out of an xgboost estimator. The csv input for prediction/batch transform has both, an ID column and a header (with names of columns). For example, something like this:</p>
<p>Name |Age |Height|Weight</p>
<p>Sam  |10  |2     |3</p>
<p>John |20  |3     |4</p>
<p>Jane |30  |4     |5</p>
<p>Of course, what needs to be passed is just the model inputs without the index (in this case, Name) or header (first row)</p>
<p>We can exclude the index (i.e. 0th) column by using the InputFilter argument when creating the job as follows:</p>
<pre><code>DataProcessing = { 
      &quot;InputFilter&quot;: &quot;$[1:]&quot;}
</code></pre>
<p>My question is how do we exclude the header? What JSONPath can be used for that?</p>
",19199141.0,-1.0,N/A,2022-06-03 09:38:28,How to use a csv with header for sagemaker batch transform?,<amazon-web-services><data-science><amazon-sagemaker><jsonpath><json-path-expression>,1,0,N/A,CC BY-SA 4.0
72384390,1,72384624.0,2022-05-25 21:31:37,0,103,"<p>I have a dataframe with longitude and latitude columns. I need to get the county name for the location based on long and lat values with the help of the geoPy package.</p>
<pre><code> longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \
0    -114.31     34.19                15.0       5612.0          1283.0   
1    -114.47     34.40                19.0       7650.0          1901.0   
2    -114.56     33.69                17.0        720.0           174.0   
3    -114.57     33.64                14.0       1501.0           337.0   
4    -114.57     33.57                20.0       1454.0           326.0   

   population  households  median_income  median_house_value  
0      1015.0       472.0         1.4936             66900.0  
1      1129.0       463.0         1.8200             80100.0  
2       333.0       117.0         1.6509             85700.0  
3       515.0       226.0         3.1917             73400.0  
4       624.0       262.0         1.9250             65500.0

</code></pre>
<p>I had success with a for loop:</p>
<pre><code>geolocator = geopy.Nominatim(user_agent='1234')

for index, row in df.iloc[:10, :].iterrows():
    location = geolocator.reverse([row[&quot;latitude&quot;], row[&quot;longitude&quot;]])
    county = location.raw['address']['county']
    print(county)
</code></pre>
<p>The dataset has 17,000 rows, so that should be a problem, right?</p>
<p>So I've been trying to figure out how to build a function which I could use in pandas.apply() in order to get quicker results.</p>
<pre><code>def get_zipcodes():
    location = geolocator.reverse([row[&quot;latitude&quot;], row[&quot;longitude&quot;]])
    county = location.raw['address']['county']
    print(county)

counties = get_zipcodes()
</code></pre>
<p>I'm stuck and don't know how to use apply (or any other clever method) in here. Help is much appreciated.</p>
",9798874.0,9798874.0,2022-05-25 23:16:05,2022-05-25 23:16:05,How can I transfer my working for loop over to pandas much quicker apply functionality?,<python><data-science><geopy><data-preprocessing>,1,1,N/A,CC BY-SA 4.0
72399746,1,72399875.0,2022-05-27 02:09:03,0,428,"<p>I have trained my model using machine learning and want to check with original value. Is i am doing it right?
As whenever I change the numbers in 'value' getting the same result.</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(normalize(df4), y, test_size=0.2, random_state=0)

rfc1=RandomForestClassifier( random_state=0, max_features='auto', n_estimators= 90, max_depth=8, criterion='gini' )
rfc1.fit(X_train, y_train)

value=[2.60,1.0,3.0,19.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0]
print(rfc1.predict([value]))

y_pred=rfc1.predict(X_test)

print(metrics.classification_report(y_test,y_pred))
df_cm = pd.DataFrame(confusion_matrix(y_test,y_pred), index = [i for i in ['Avg Marks','Good Marks','Bad Marks']],
                  columns = [i for i in ['Avg Marks','Good Marks','Bad Marks']])
plt.figure(figsize = (4,2))
sn.heatmap(df_cm, annot=True, fmt='g')
</code></pre>
<p>Model Accuracy is good but still getting always &quot;Hight chances of Good Marks&quot;</p>
<pre><code>['High Chances of Good Marks']
                            precision    recall  f1-score   support

             Average Marks       0.80      0.83      0.82        59
 High Chances of Bad Marks       0.81      0.72      0.76        18
High Chances of Good Marks       0.86      0.86      0.86        50

                  accuracy                           0.83       127
                 macro avg       0.83      0.80      0.81       127
              weighted avg       0.83      0.83      0.83       127

</code></pre>
<p>Original data looks like this
<a href=""https://i.stack.imgur.com/i0f6x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i0f6x.png"" alt=""enter image description here"" /></a></p>
",13214782.0,13214782.0,2022-05-27 02:17:23,2022-05-27 12:37:42,How to check the model prediction with original values in machine learning,<python><pandas><machine-learning><data-science><random-forest>,2,1,N/A,CC BY-SA 4.0
72338056,1,-1.0,2022-05-22 13:36:19,0,253,"<p>I hope you are doing well. I am dealing with a steel dataset having different attributes. I wanted to fill their missing values with their mean. However, rather than doing it separately I want to do it with in two or three lines of codes in a single go. I am trying the following code:</p>
<pre><code>empty_list = []
empty_list.append(df.columns[df.isnull().any()])
for names in empty_list:
    df[names].fillna(df[names].mean(),axis=1,inplace=True)
</code></pre>
<p>First, I append the names of the columns having missing values into a list and try to iterate over that list with filling their missing values in the dataframe with their mean. But it is giving me the following error:</p>
<pre><code>NotImplementedError: Currently only can fill with dict/Series column by column
</code></pre>
<p>Can someone have any other suggestions of how to do it. Thanks!!!!</p>
",16879380.0,16879380.0,2022-05-22 13:44:16,2022-05-22 13:44:16,Can we fill missing values of different columns in a pandas dataframe in two/three lines of code,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
72404872,1,72405429.0,2022-05-27 11:40:01,0,2697,"<p>I have a matrix as 2d-np.array and I would like to remove all rows that contain an element x in a specific column. My goal is to return a matrix without these rows, so it should be smaller.
My function looks like this:</p>
<pre><code>def delete_rows(matrix, x, col):
    for i in range(matrix.shape[0]-1):
        if(matrix[i,col] == x):
            np.delete(matrix, i, axis = 0)
    return matrix
</code></pre>
<p>Sadly in my test the shape of the matrix stayed the same after removing rows. I think the deleted rows were substituted by rows with 0s.
Any advice on how I can achieve my goal?</p>
",9758325.0,-1.0,N/A,2022-05-27 12:57:09,Remove rows in a 2d-numpy array if they contain a specific element,<python><arrays><numpy><matrix><data-science>,3,1,N/A,CC BY-SA 4.0
72407412,1,-1.0,2022-05-27 15:03:31,0,104,"<p>I have a data in my hand, I increase the sample size by increasing the sampling frequency of the data while the variance is fixed. As the sample size increases, the mean square error decreases.</p>
<p>What could be the reason for this? Why is it decreasing?</p>
",17606385.0,13302.0,2022-05-27 15:07:15,2022-05-27 20:42:22,Why error decreases while increasing sample size?,<error-handling><statistics><data-science><signal-processing><mean-square-error>,1,1,N/A,CC BY-SA 4.0
72416845,1,-1.0,2022-05-28 15:16:34,0,29,"<p>a coin is tossed n times and if during those n times heads occurs consecutively k times then one had to pay a dollar , the problem is in last result e[n] in the list and it cant work as e[n+1] doesnt exist
code</p>
<pre><code>def coinflip2(n,k):
ϵ=[]
h=1
c=0
for i in range(n):
    e= np.random.uniform(0,1)
    if(e&gt;0.5):
        e=1
    else:
        e=0
    ϵ.append(e)
print(ϵ)    
for i in range (len(ϵ)):
    if(ϵ[i]==ϵ[i+1]):
        c=c+1
    else:
        c=0
    print(c)
    if(c&gt;=k):  print('pay 1 Dallor')
    else: print('pay nothing')
</code></pre>
<p>coinflip2(10,2)</p>
",19221001.0,-1.0,N/A,2022-05-28 15:16:34,i want find if an element occurs consecutively a certain no. of times in a list H occurs k times consecutively in n experiments,<python><list><numpy><data-science>,0,2,N/A,CC BY-SA 4.0
72419624,1,-1.0,2022-05-28 22:18:43,0,88,"<p>i want to map the the AQI column with AQI_Bucket column in pandas dataframe
i tried it using for loop but couldnt get it</p>
<pre><code>for aqi in df['AQI']:
    col1,col2 = df['AQI'],df['AQI_Bucket']
    _col1,_col2 = col1[0],col2[0]
    if df[aqi] == df['AQI_Bucket']:
        
        if pd.isnull(_col2):
            if _col1 in range(51):
                _col2 = &quot;Good&quot;
            elif _col1 in range(51, 101):
                _col2 = &quot;Satisfactory&quot;
            elif _col1 in range(101,201):
                _col2 = &quot;Moderate&quot;
            elif _col1 in range(201, 301):
                _col2 = &quot;Poor&quot;
            elif _col1 in range(301, 401):
                _col2 == &quot;Very Poor&quot;
            elif _col1 in range(401, 500):
                _col2 == &quot;Severe&quot;

</code></pre>
",16429304.0,-1.0,N/A,2022-05-29 00:07:44,How to change categorical data in any column in pandas dataframe with respect to numeric data in other column (mapping)?,<python><pandas><dataframe><data-science><exploratory-data-analysis>,1,2,N/A,CC BY-SA 4.0
72332785,1,-1.0,2022-05-21 19:51:21,1,36,"<p>I would like to extract tweets with R for any determined account. Is that possible? I have tried with:</p>
<pre><code>library(rtweet)

api_key &lt;- &quot;xxxxxxxxxxxxxxxxxxxxxxxxxx&quot;
api_secret_key &lt;- &quot;xxxxxxxxxxxxxxxxxxxxxxxxxx&quot;
access_token = &quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;
access_secret =&quot;xxxxxxxxxxxxxxxxxxxxxxxxxx&quot;

token &lt;- create_token(
  app = &quot;example coding&quot;,
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_secret)

search_tweets(&quot;@elonmusk&quot;, n = 10, lang ='en') 
</code></pre>
<p>I will really appreciate any help, thank you in advance.</p>
",13452992.0,15293191.0,2022-05-22 04:56:57,2022-05-22 04:56:57,"Filter tweets for a given account or ID, but not having to account for tweets published from this account in R?",<r><twitter><data-science><rtweet>,1,0,N/A,CC BY-SA 4.0
72358154,1,72358455.0,2022-05-24 06:31:23,-1,34,"<p>Hi I am trying to analyse descriptions of around 30000 requests to identify common requests as the data has no tags or titles.</p>
<p>I’ve looked at a lot of content on sentiment analysis and I’m currently thinking I need to train a model from a small random sample to better classify the data.</p>
<p>Is there a better approach I should be following?</p>
",6623826.0,-1.0,N/A,2022-05-24 06:59:48,NLP to analyse requests,<nlp><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
72368195,1,72368366.0,2022-05-24 19:03:31,0,717,"<p>I am using smote to balanced the output (y) only for Model train but want to test the model with original data as it makes logic how we can test the model with smote created outputs. Please ask anything for clarification if I didn't explained it well. It's my starting on Stack overflow.</p>
<pre><code>from imblearn.over_sampling import SMOTE
oversample = SMOTE()
X_sm, y_sm = oversample.fit_resample(X, y)

# Splitting Dataset into Train and Test (Smote)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm,test_size=0.2,random_state=42)
</code></pre>
<p>Here i applied the Random Forest Classifier on my data</p>
<pre><code>import math
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sn
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

# RF = RandomForestClassifier(n_estimators=100)
# RF.fit(X_train, y_train.values.ravel())
# y_pred = RF.predict(X)
# print(metrics.classification_report(y,y_pred))

RF = RandomForestClassifier(n_estimators=10)
RF.fit(X_train, y_train.values.ravel())
</code></pre>
<p>If i applied this but X also contains the data which we used for train. how we can remove the data which we already used for training the data.</p>
<pre><code>y_pred = RF.predict(X)
print(metrics.classification_report(y,y_pred))
</code></pre>
",13214782.0,-1.0,N/A,2022-05-25 04:15:26,How to properly use Smote in Classification models,<python><machine-learning><scikit-learn><jupyter-notebook><data-science>,2,0,N/A,CC BY-SA 4.0
72387804,1,72387928.0,2022-05-26 06:45:28,0,318,"<p>it displays the following error
<a href=""https://i.stack.imgur.com/vX0wG.png"" rel=""nofollow noreferrer"">this is the error</a>
well i have already tried with diffrent file but it doesnt work</p>
<pre><code>data1=pd.read_excel('second.xlsx',sheet_name=&quot;Sheet1&quot;)
</code></pre>
",19204198.0,-1.0,N/A,2022-05-26 07:00:47,"I am unable to open the excel file with pandas,",<excel><pandas><data-science>,1,0,2022-05-26 07:12:45,CC BY-SA 4.0
72400727,1,-1.0,2022-05-27 05:13:54,0,24,"<p>For example, if I have a Project_ID = 123456 in one column and Project_Name = R123456. I want to check if Project_Name was correctly input with prefix R in every elements. What query can I use?</p>
",13232521.0,9952196.0,2022-05-27 07:28:26,2022-05-27 07:28:26,How can I check the validity of a column elements in MySQL that were derived from another column of the same table?,<mysql><database><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
72422066,1,-1.0,2022-05-29 08:26:59,1,450,"<p>I am practicing using the fft function in scipy with the following code below in which I define a sinusoidal wave of frequency Fr and then calculate the fft. If I reduce the sample time, t, then the resulting fft plot has a much larger maximum frequency but a lower resolution. I understand that this is caused by the fact that frequency is 1 over time and that the fft range must encompass all possible frequencies. However, is there anyway to increase the number of datapoints in ‘yf’ so that I get a higher resolution output. This is important to me because in my application I will have a wide range of frequencies (about 50 – 10,000 Hz).</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq

N=1000
Fr=3
t=0.1 #sample time

x=np.linspace(0,N*t,N)
y=np.sin(2*np.pi*Fr*x)

yf=fft(y)
xf = fftfreq(n=N, d=t)[:N//2]
plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))
</code></pre>
",19144913.0,-1.0,N/A,2022-05-29 08:26:59,How to increase FFT frequency bin resolution without reducing frequency range in scipy,<scipy><data-science><fft>,0,1,N/A,CC BY-SA 4.0
72423769,1,-1.0,2022-05-29 12:45:08,0,85,"<p>If i am training my model and attempt to minimise MSE, if over a few epochs the MSE increases,
say</p>
<pre><code>epoch 100, mse = 0.0992
epoch 101, mse = 0.0992
epoch 102, mse = 0.0994
epoch 103, mse = 0.0993
epoch 104, mse = 0.0992
epoch 105, mse = 0.0992
epoch 106, mse = 0.0991
...
...
</code></pre>
<p>if this happens at different stages does this imply over fitting?</p>
<p>Background:
I am using early stopping and if at any point the model accuracy goes up it will terminate, I want to know if this means that I should not use early stopping as it does not show overfitting?</p>
<p>Thank you</p>
",17321543.0,-1.0,N/A,2022-05-29 12:45:08,Overfitting with epochs Mean squared error,<data-science><artificial-intelligence><cross-validation><theory><overfitting-underfitting>,0,2,N/A,CC BY-SA 4.0
72430118,1,-1.0,2022-05-30 06:44:06,0,1138,"<p><strong>Please help me out</strong></p>
<p>I am unable to install Turicreate
I have referred to the website <a href=""https://github.com/apple/turicreate"" rel=""nofollow noreferrer"">https://github.com/apple/turicreate</a></p>
<p>I have Python 3.8.0 installed currently which suits the system requirements.</p>
<p>I have followed the steps:</p>
<pre><code>pip install virtualenv
</code></pre>
<p>then</p>
<pre><code># Create a Python virtual environment
cd ~
virtualenv venv

# Activate your virtual environment
source ~/venv/bin/activate
</code></pre>
<p>and finally</p>
<pre><code>(venv) pip install -U turicreate
</code></pre>
<p>and i'm getting the error message like this:</p>
<pre><code>              ==================================================================================
          TURICREATE ERROR
  
          If you see this message, pip install did not find an available binary package
          for your system.
  
          Supported Platforms:
              * macOS 10.12+ x86_64.
              * Linux x86_64 (including WSL on Windows 10).
  
          Support Python Versions:
              * 2.7
              * 3.5
              * 3.6
              * 3.7
              * 3.8
  
          Another possible cause of this error is an outdated pip version. Try:
              `pip install -U pip`
  
          ==================================================================================
  
  
  
  [end of output]
</code></pre>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
ERROR: Failed building wheel for turicreate
Running setup.py clean for turicreate
Failed to build turicreate
Installing collected packages: turicreate
Running setup.py install for turicreate ... error
error: subprocess-exited-with-error</p>
<p>× Running setup.py install for turicreate did not run successfully.
│ exit code: 1
╰─&gt; [30 lines of output]
running install
/Users/samyuktag/venv/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
warnings.warn(</p>
<pre><code>          ==================================================================================
          TURICREATE ERROR
  
          If you see this message, pip install did not find an available binary package
          for your system.
  
          Supported Platforms:
              * macOS 10.12+ x86_64.
              * Linux x86_64 (including WSL on Windows 10).
  
          Support Python Versions:
              * 2.7
              * 3.5
              * 3.6
              * 3.7
              * 3.8
  
          Another possible cause of this error is an outdated pip version. Try:
              `pip install -U pip`
  
          ==================================================================================
  
  
  
  [end of output]
</code></pre>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure</p>
<p>× Encountered error while trying to install package.
╰─&gt; turicreate</p>
<p>note: This is an issue with the package mentioned above, not pip.
hint: See above for output from the failure.
WARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.
You should consider upgrading via the '/Users/samyuktag/venv/bin/python -m pip install --upgrade pip' command.
(virtual_environment_name) (venv) samyuktag@Samyuktas-MacBook-Air ~ %</p>
<p><strong>Please help me out</strong></p>
",19229519.0,-1.0,N/A,2022-06-05 09:58:36,I am unable to install Turi Create on Mac M1,<python-3.x><macos><data-science><apple-m1><turi-create>,1,1,N/A,CC BY-SA 4.0
72427016,1,-1.0,2022-05-29 20:21:51,0,118,"<p>Hi how's it going? I have a giant dataframe and am trying to do a groupby, filter, then count within each group the occurrence of a particular event. The code I have works but doesn't scale well at all, it takes forever to run. Can someone help me with a fast way to perform the same computation? Below is what I have so far reproduced in a dummy example:</p>
<pre><code>dates = ['2012-03-30','2012-03-30','2012-03-30','2012-03-30','2012-03-30','2012-03-31','2012-03-31','2012-03-31','2012-03-31','2012-03-31']
person = ['dave','mike','mike','dave','mike','dave','dave','dave','mike','mike']
weather = ['rainy','sunny','cloudy','cloudy','rainy','sunny','cloudy','sunny','cloudy','rainy']
events = ['sneeze','cough','sneeze','sneeze','cough','cough','sneeze','cough','sneeze','sneeze']

df = pd.DataFrame({'date':dates,'person':person,'weather':weather,'event':events}) 

def sneeze_by_weather(df):
    num_sneeze = df[df['event']=='sneeze'].shape[0] 
    if num_sneeze==0:
        return 0
    else:
        return num_sneeze

df_transformed = df.groupby(['date','person','weather']).apply(lambda x: sneeze_by_weather(x)).reset_index()
</code></pre>
<p><a href=""https://i.stack.imgur.com/ZJcLQ.png"" rel=""nofollow noreferrer""><strong>Link to resulting dataframe</strong></a></p>
<p>Is there any way to perform this computation much faster so that it scales when I have millions of rows?</p>
",17974581.0,-1.0,N/A,2022-05-29 21:24:00,"What's an efficient way to groupby, filter and count the occurrence of a particular value within each group in Pandas?",<python><pandas><dataframe><data-science>,3,0,N/A,CC BY-SA 4.0
72431096,1,72434213.0,2022-05-30 08:12:46,0,326,"<p>Here i have tried power transfer technique to detect outliers and to remove them but its not working, i dont know why and if any one has any new suggestion then please give me.</p>
<p>suppose i have a dataset and in that dataset skewness is present so, i need to define a function that could detect skewness at a certain threshold from every column of the dataset and remove the skewness and return back the data after removing skewness.</p>
<pre><code># Removing outliers
from sklearn.preprocessing import PowerTransformer
def remove_skewness(x):
    value = x.skew().values
    for skew in value:
        if skew &gt; 4.0:
            #skewness removal
            pt=PowerTransformer(method='yeo-johnson') 
            X_power=pt.fit_transform(x)
            df1=pd.DataFrame(X_power,columns=X.columns)
            print(&quot;Skewness is Detected and will be Removed:&quot;)
            return df1
        else:
            print(&quot;Skewness not Detected:&quot;)
            return x
        
df2 = remove_skewness(df_new)
df2.head()
</code></pre>
",19225087.0,19225087.0,2022-05-30 12:22:13,2022-05-31 07:50:34,how can we automatically detect skewnes in the data and skewness is present then how we can remove it?,<python-3.x><machine-learning><data-science>,1,1,N/A,CC BY-SA 4.0
72433368,1,72434141.0,2022-05-30 11:11:16,1,319,"<p>I am performing below steps using cypher query. I am getting error in step 3. I have listed all steps below. Please help me to achieve expected output.</p>
<p><strong>Step 1- Load data and define properties of nodes relationship</strong></p>
<pre><code>

LOAD CSV WITH HEADERS FROM 'file://nodes_1Jan22_full_v2.csv' AS row
CREATE (n: Organisation {id: row.organisation, esg_index: toFloat(row.tone)});


LOAD CSV WITH HEADERS FROM 'file://edges_1Jan22_full_v2.csv' AS row
MERGE (src: Organisation {id: row.src})
MERGE (dst: Organisation {id: row.dst})
MERGE (src)-[:RELATE {freq: toInteger(row.relationship), sentiment: toFloat(row.avg_tone)}]-&gt;(dst);
</code></pre>
<p><strong>Sample query and table structure</strong></p>
<pre><code>MATCH p=()-[r:RELATE]-&gt;() RETURN p LIMIT 1
</code></pre>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>{
  ""start"": {
""identity"": 18862,
""labels"": [
      ""Organisation""
    ],
""properties"": {
""id"": ""american university"",
""esg_index"": -3.005288932058546
    }
  },
  ""end"": {
""identity"": 20048,
""labels"": [
      ""Organisation""
    ],
""properties"": {
""id"": ""duke university"",
""esg_index"": -1.6810932825414502
    }
  },
  ""segments"": [
    {
      ""start"": {
""identity"": 18862,
""labels"": [
          ""Organisation""
        ],
""properties"": {
""id"": ""american university"",
""esg_index"": -3.005288932058546
        }
      },
      ""relationship"": {
""identity"": 0,
""start"": 18862,
""end"": 20048,
""type"": ""RELATE"",
""properties"": {
""sentiment"": -4.367701625823974,
""freq"": 250
        }
      },
      ""end"": {
""identity"": 20048,
""labels"": [
          ""Organisation""
        ],
""properties"": {
""id"": ""duke university"",
""esg_index"": -1.6810932825414502
        }
      }
    }
  ],
  ""length"": 1.0
}</code></pre>
</div>
</div>
</p>
<p><strong>Step 2- Create graph projection</strong></p>
<pre><code>CALL gds.graph.project(
    'gdelt-analytics',
    'Organisation',
    'RELATE',
    {
        relationshipProperties: 'freq'
    }
)

MATCH (org:Organisation {id: 'public health'})
CALL gds.pageRank.stream('gdelt-analytics', {
    maxIterations: 100,
    dampingFactor: 0.85,
    sourceNodes: [org],
    relationshipWeightProperty: 'freq'
})
YIELD nodeId, score
RETURN *

</code></pre>
<p><strong>Current Output</strong></p>
<p><a href=""https://i.stack.imgur.com/tHLmm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tHLmm.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step 3- Attempt to color node based on property &quot;esg_index&quot; and edges based on property &quot;sentiment&quot; (Query that is throwing error)</strong></p>
<pre><code>CALL apoc.create.addLabels(n.esg_index, [apoc.text.upperCamelCase(n.id)]) YIELD node
RETURN *
</code></pre>
<blockquote>
<p>Neo.ClientError.Procedure.ProcedureCallFailed
Failed to invoke procedure <code>apoc.create.addLabels</code>: Caused by: org.neo4j.internal.kernel.api.exceptions.EntityNotFoundException: Unable to load NODE with id -2.</p>
</blockquote>
<p><strong>Expected Output</strong></p>
<p>Graph with nodes and edges colored. Nodes colored based on esg_index and edges colored based on sentiment</p>
",1344006.0,1344006.0,2022-05-30 11:33:33,2022-05-30 16:23:51,Issue in Cypher Query - org.neo4j.internal.kernel.api.exceptions.EntityNotFoundException,<neo4j><cypher><graph-databases><graph-data-science>,1,0,N/A,CC BY-SA 4.0
72430664,1,-1.0,2022-05-30 07:36:00,0,469,"<p>Failed to create table: Error while reading data, error message: Could not parse '5150.595542' as INT64 for field fuel_consumption (position 2) starting at location 249269356 with message 'Unable to parse'</p>
",19229938.0,-1.0,N/A,2022-05-31 11:01:02,How to solve this error in Bigquery while i try to store the dataset to GCS,<python><sql><google-bigquery><append><data-science>,1,2,N/A,CC BY-SA 4.0
72434568,1,-1.0,2022-05-30 12:46:14,0,25,"<p>So I have a dataframe of 168 rows and 121 columns. There are some columns that I want to compare and display the different values.</p>
<p>The code is:</p>
<pre><code>comparison_column = np.where((df_full[&quot;Name&quot;] == df_full[&quot;Displ_name&quot;]), True, False)
print(comparison_column)
</code></pre>
<p>output is a list with True and False.</p>
<p>I used apply method with lambda, but it shows not only rows with different values but also all columns from the dataframe. Is there a method which can display rows with different value but only from compared columns?</p>
<pre><code>df_full[df_full.apply(lambda x: x[&quot;Name&quot;] != x[&quot;Displ_name&quot;], axis=1)]
</code></pre>
",19135855.0,19135855.0,2022-06-01 08:25:47,2022-06-01 08:25:47,I compared the value of two columns for equality and I want to see in the output only different values from two compaired columns of dataframe,<python><pandas><dataframe><data-science><exploratory-data-analysis>,0,2,2022-05-30 12:51:27,CC BY-SA 4.0
72434724,1,72434809.0,2022-05-30 12:57:46,1,668,"<p>Here is my code:
So I wanted to extract all the bollywood movies, and the project requires, movie titles, cast, crew, IMDB id etc.... I am not able to get all the IMDb IDs with the error nonetype. When I used it on one page only it was working quite well, however, when I use it on multiple pages it shows an error. Please help</p>
<pre><code>#importing the libraries needed 
import pandas as pd
import numpy as np
import requests
import re
from bs4 import BeautifulSoup
from time import sleep
from random import randint

#declaring the list of empty variables, So that we can append the data overall

movie_name = []
year = []
time=[]
rating=[]
votes = []
description = []
director_s = []
starList= []
imdb_id = []

#the whole core of the script
url = &quot;https://www.imdb.com/search/title/?title_type=feature&amp;primary_language=hi&amp;sort=num_votes,desc&amp;start=1&amp;ref_=adv_nxt&quot;
page = requests.get(url)
soup = BeautifulSoup(page.text, 'html.parser')
movie_data = soup.findAll('div', attrs = {'class': 'lister-item mode-advanced'})

for store in movie_data:
    name = store.h3.a.text
    movie_name.append(name)
    
    year_of_release = store.h3.find('span', class_ = &quot;lister-item-year text-muted unbold&quot;).text
    year.append(year_of_release)
        
    runtime = store.p.find(&quot;span&quot;, class_ = 'runtime').text if store.p.find(&quot;span&quot;, class_ = 'runtime') else &quot; &quot;
    time.append(runtime)
        
    rate = store.find('div', class_ = &quot;inline-block ratings-imdb-rating&quot;).text.replace('\n', '') if store.find('div', class_ = &quot;inline-block ratings-imdb-rating&quot;) else &quot; &quot;
    rating.append(rate)
        
    value = store.find_all('span', attrs = {'name': &quot;nv&quot;})
        
    vote = value[0].text if store.find_all('span', attrs = {'name': &quot;nv&quot;}) else &quot; &quot;
    votes.append(vote)
        
    # Description of the Movies 
    describe = store.find_all('p', class_ = 'text-muted')
    description_ = describe[1].text.replace('\n', '') if len(describe) &gt; 1 else ' '
    description.append(description_)
        
    ## Director  
    ps = store.find_all('p')
    for p in ps:
        if 'Director'in p.text:
            director =p.find('a').text
    
    director_s.append(director)
    
    ## ID
    imdbID = store.find('span','rating-cancel').a['href'].split('/')[2]
    imdb_id.append(imdbID)

    ## actors
    star = store.find(&quot;p&quot;, attrs={&quot;class&quot;:&quot;&quot;}).text.replace(&quot;Stars:&quot;, &quot;&quot;).replace(&quot;\n&quot;, &quot;&quot;).replace(&quot;Director:&quot;, &quot;&quot;).strip()
    starList.append(star)


Error:
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_17576/2711511120.py in &lt;module&gt;
     63 
     64         ## IDs
---&gt; 65         imdbID = store.find('span','rating-cancel').a['href'].split('/')[2] if store.find('span','rating-cancel').a['href'].split('/')[2] else ' '
     66         imdb_id.append(imdbID)
     67 

AttributeError: 'NoneType' object has no attribute 'a'
</code></pre>
",10078688.0,-1.0,N/A,2022-05-30 14:19:34,I want to extract IMDb movie IDs using python,<python><data-science>,1,2,N/A,CC BY-SA 4.0
72420958,1,72421102.0,2022-05-29 04:39:02,1,84,"<pre><code>tweets = [
    &quot;Wow, what a great day today!! #sunshine&quot;,
    &quot;I feel sad about the things going on around us. #covid19&quot;,
    &quot;I'm really excited to learn Python with @JovianML #zerotopandas&quot;,
    &quot;This is a really nice song. #linkinpark&quot;,
    &quot;The python programming language is useful for data science&quot;,
    &quot;Why do bad things happen to me?&quot;,
    &quot;Apple announces the release of the new iPhone 12. Fans are excited.&quot;,
    &quot;Spent my day with family!! #happy&quot;,
    &quot;Check out my blog post on common string operations in Python. #zerotopandas&quot;,
    &quot;Freecodecamp has great coding tutorials. #skillup&quot;
]


happy_words = ['great', 'excited', 'happy', 'nice', 'wonderful', 'amazing', 'good', 'best']
</code></pre>
<p>Question : Determine the number of tweets in the dataset that can be classified as happy.</p>
<p>MY CODE :</p>
<pre><code>number_of_happy_tweets = 0
 
for i in tweets:
  for x in i:
    if x in happy_words:
      number_of_happy_tweets += len(x)
</code></pre>
<p>Why this code is not working???????</p>
",19223653.0,17507911.0,2022-05-29 04:41:52,2022-05-29 05:17:19,How to compare two lists?,<python><list><loops><data-science>,4,1,N/A,CC BY-SA 4.0
72423061,1,-1.0,2022-05-29 11:02:39,0,50,"<p>I have been trying to split this string but it only gives me the last character of the username I want. for example</p>
<p><img src=""https://i.stack.imgur.com/OFGqR.png"" alt="""" /></p>
<p>in this dataset I want to separate the username from the actual message but after doing this code-</p>
<pre><code>#how can we separate users from messages 
users = []
messages = []
for message in df['user_message']:
    entry = re.split('([a-zA-Z]|[0-9])+#[0-9]+\\n', message)
    if entry[1:]:
        users.append(entry[1])
        messages.append(entry[2])
    else:
        users.append('notif')
        messages.append(entry[0])
        
df['user'] = users
df['message'] = messages
df.drop(columns=['user_message'], inplace = True)

df.head(30)
</code></pre>
<p>I only get</p>
<p><img src=""https://i.stack.imgur.com/fI4h3.png"" alt="""" /></p>
<p>Could someone please tell me why it only gives me the last character of the string i want to split and how I can fix it? thanks a lot. This means a lot</p>
",19225061.0,3494774.0,2022-05-29 11:10:26,2022-05-29 11:35:34,"Problem with re.split() , data extraction from a string (splitting a string)",<python><python-3.x><nlp><data-science>,2,1,N/A,CC BY-SA 4.0
72432133,1,-1.0,2022-05-30 09:34:58,0,22,"<p>If we have a dataframe with 3 columns a b c</p>
<p>and i want to add columns to it in a way that we get:
a b c a/b a/c/ b/a b/c c/a c/b
,</p>
<p>the easiest way I can think of is</p>
<pre class=""lang-py prettyprint-override""><code>
column_list = list(df.columns)
for i in range(len(column_list)):
    p1 = column_list[i]
    for j in range(len(columns_list)):
        p2 = column_list[j]
        df[p1+'/'+p2] = df[p1]/df[p2]
</code></pre>
<p>But is there a faster and more effiecient way I can do this?</p>
<p>I have a task where I have to do this for 1000 parameters,
doing by the above way takes a lot of time.</p>
<p>Any Help will be greatly Appreciated</p>
",18428068.0,-1.0,N/A,2022-05-30 09:34:58,Efficient and faster way to get dataframe column permutations(?) as columns,<python><pandas><dataframe><optimization><data-science>,0,3,2022-05-30 09:40:34,CC BY-SA 4.0
72442231,1,-1.0,2022-05-31 04:15:00,2,703,"<p>I'm trying to run a dedupe with pandas_dedupe but when running it it gives me this error &quot;AttributeError: 'float' object has no attribute 'keys'&quot;</p>
<pre><code>import pandas as pd
from pandas_dedupe import dedupe_dataframe
df = pd.read_csv('/home/biminsal/Documentos/sampleDataDos.csv', sep=&quot;;&quot;)
dfinal = dedupe_dataframe(df, ['id', 'nombre_uno','apellido_uno'], canonicalize=True, sample_size=1)
</code></pre>
<p>Data sample:</p>
<pre><code>    id  nombre_uno  apellido_uno
0   a001    Karlo   Perez
1   a002    Carlos  Perez
2   a003    Juan    Gomez
3   b001    Carlos  Perez
4   b002    Juan    Gomez
</code></pre>
<p>Error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
...
File ~/.local/lib/python3.9/site-packages/dedupe/labeler.py:203, in BlockLearner._sample_indices(self, sample_size)
    201     sample_ids = ((keys[i][0], keys[i][1]) for i in sample_indices)
    202 else:
--&gt; 203     sample_ids = weight.keys()
    205 return sample_ids

AttributeError: 'float' object has no attribute 'keys'
</code></pre>
<p>I have tried to find information and have applied several possible fixes, but the same problem remains. Thanks so much for any help.</p>
",19236938.0,-1.0,N/A,2022-06-02 11:21:54,"pandas_dedupe: ""AttributeError: 'float' object has no attribute 'keys'""",<python><pandas><data-science>,0,2,N/A,CC BY-SA 4.0
72444905,1,-1.0,2022-05-31 08:58:36,0,540,"<p>I want to create a new table using pandas or python which will have the same columns as the picture but I want to add all the dates between two dates of the actual table in the new table.<br>
For example in the picture in 1st row, Effect Date is 13 februray,2022 and Price is 220 and in 2nd row Effect Date is 23 September, 2021. <br>
I want in the new table there will be all the dates between 13 februray,2022 and 23 September, 2021. all other column value will same except the MRP/Unit.<br>
Between 2/13/2022 and 8/23/2021 all values in MRP/Unit will be 220. Between 9/23/2021 and 9/9/2019 all values in MRP/Unit will be 210.</p>
<p><a href=""https://i.stack.imgur.com/POo08.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/POo08.png"" alt=""enter image description here"" /></a></p>
",17126715.0,-1.0,N/A,2022-05-31 09:58:46,"How to populate a dataframe using Date, Month and Year and manipulate that dataframe?",<python><python-3.x><pandas><data-science><google-colaboratory>,2,1,N/A,CC BY-SA 4.0
72456849,1,-1.0,2022-06-01 05:34:30,0,33,"<p>I have a dataset that contains hourly data where data is coming from a sensor-</p>
<pre><code>    date                 Fea-1     Fea-2
0   01-01-1990 00:00    0.0000     0.000
1   01-01-1990 01:00    0.0000     0.000
2   01-01-1990 02:00    0.0000     0.000
3   01-01-1990 03:00    0.0000     0.000
4   01-01-1990 04:00    0.0000     0.000
5   01-01-1990 05:00    0.0000     0.000
6   01-01-1990 06:00    0.4986     0.000
7   01-01-1990 07:00    69.3000    68.725
8   01-01-1990 08:00    171.6000   168.870
9   01-01-1990 09:00    368.3000   372.240
10  01-01-1990 10:00    358.1000   353.210
11  01-01-1990 11:00    393.3000   387.430
12  01-01-1990 12:00    385.9000   379.120
13  01-01-1990 13:00    312.6000   304.240
14  01-01-1990 14:00    199.3100   190.820
15  01-01-1990 15:00    58.9960    54.509
16  01-01-1990 16:00    21.8040    19.572
17  01-01-1990 17:00    7.1959     0.000
18  01-01-1990 18:00    0.0000     0.000
19  01-01-1990 19:00    0.0000     0.000
</code></pre>
<p>Now I want to convert the every hour data to minute data like-</p>
<pre><code>01-01-1990 00:01
01-01-1990 00:02
01-01-1990 00:03
01-01-1990 00:04
upto ..
01-01-1990 00:60
</code></pre>
",13510557.0,-1.0,N/A,2022-06-01 05:34:30,Converting hourly data in to minute python,<python><time-series><data-science>,0,7,2022-06-01 06:30:34,CC BY-SA 4.0
72467117,1,72470747.0,2022-06-01 19:24:55,-3,708,"<p>I am working on a machine learning project and wants to know by using the sklearn how we can find the best feature responsible for predicted label in python.</p>
<p>Let suppose we fit the model and the wants to predict
<code>model.predict([1,2,3])-&gt; let suppose it says you passed the test.</code> but what the weightage of the features for predicting only for this prediction
<code>model.predict([1,2,3]) </code></p>
<p>Suppose a dataset with 5 columns. Let's call them: id, X_1, X_2, X_3, result. X_1,X_2,X_3 have the numerical values 1-5.</p>
<p>I need to show that this result was caused by X_1,X_2 with weightage of 0.8900% and 0.3900% or any graph which through i can use fully understand. How can I show that X_1 and X_2 has more influence on result than X_3? only for this prediction <code>model.predict([1,2,3])</code></p>
<p>I need a simple answer or any code which can helps me with this problem.</p>
",13214782.0,208273.0,2022-06-16 00:55:26,2022-06-16 00:55:26,How to find which features are responsible for predicted label?,<python><pandas><dataframe><machine-learning><data-science>,1,0,2022-06-02 11:48:17,CC BY-SA 4.0
72474074,1,72474334.0,2022-06-02 09:41:35,0,28,"<p>I need a list of the results of an API call using gql for a dictionary of IDs. Let's say I have a dictionary of locations, ex: <code>{&quot;tokyo&quot;: &quot;111&quot;, &quot;rio&quot;: &quot;112&quot;, &quot;LA&quot;: &quot;113&quot;}</code>, and I need to query on an API the info for each location, ex:</p>
<pre><code>    transport = AIOHTTPTransport(url=API_URL, headers=HEADERS)
    client = Client(transport=transport)
    params = {
        &quot;id&quot;: &quot;111&quot;
        &quot;time&quot;: {&quot;from&quot;: &quot;2022-05-10&quot;, &quot;to&quot;: &quot;2022-05-19&quot;},
        }
    q = gql(query)
    r = client.execute(q, variable_values=params)
    pprint.pprint(r) 
</code></pre>
<p>I am trying to design a function that extracts the results of the queries for each ID at once and outputs it as a list. However I am new to python and I am unsure of how to go about it. I started with a function and a for loop, something like this:</p>
<pre><code>total = []

def get_info(dictionary_location):
  for key, value in dictionary_location.items():  
    global params
       params = {
       &quot;lineId&quot;: value,
       &quot;time&quot;: {&quot;from&quot;: &quot;2022-05-10&quot;, &quot;to&quot;: &quot;2022-05-19&quot;}
       }
        q = gql(query)
        r = client.execute(q, variable_values = params)
return total.append(r)
</code></pre>
<p>But it's not working. Does anyone have an input on the logic/syntax?</p>
",14299085.0,-1.0,N/A,2022-06-02 10:00:26,Function to extract API queries results and output them as list or dictionary,<python><function><for-loop><data-science><gqlquery>,1,0,N/A,CC BY-SA 4.0
72367187,1,-1.0,2022-05-24 17:36:42,0,430,"<p>I have a xlsx file where a lot of URLs are stored along with their serial ids. Each of these URLs redirects to a webpage where there is article written. My question is how do I scan all the URLs using python and store the title and the texts of the article in a new text file with the URL serial id as its file name?</p>
",17198326.0,17198326.0,2022-05-24 17:38:08,2022-05-24 17:51:06,How do I extract data from the URLs?,<python><web-scraping><data-science>,1,2,N/A,CC BY-SA 4.0
72369489,1,-1.0,2022-05-24 21:08:06,0,425,"<p>When I try to remove outliers from my dataset, I get this warning:</p>
<p><strong>Code</strong></p>
<pre class=""lang-py prettyprint-override""><code>def remout(df):
    Q1 = df.quantile(0.02)
    Q3 = df.quantile(0.98)
    IQR = Q3 - Q1
    df = df[~((df &lt; (Q1 - 1.5 * IQR)) |(df &gt; (Q3 + 1.5 * IQR))).any(axis=1)]
    return df

df=remout(df)
df
</code></pre>
<p><strong>Warning Message</strong></p>
<pre><code>FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version. Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`
  df = df[~((df &lt; (Q1 - 1.5 * IQR)) |(df &gt; (Q3 + 1.5 * IQR))).any(axis=1)]
</code></pre>
",13214782.0,471376.0,2022-07-05 21:03:07,2023-06-28 10:01:16,"Warning ""Automatic reindexing"" when applying IQR on the Pandas Dataset",<python><pandas><scikit-learn><data-science><sklearn-pandas>,1,1,N/A,CC BY-SA 4.0
72369658,1,-1.0,2022-05-24 21:26:53,0,86,"<p>Question: &quot;What proportion of the subjects have a lower SBP on the second reading compared to the first?&quot; This is from the nhanes_univariate_practice in the Understanding and Visualizing Data with Python Coursera course.</p>
",19192862.0,19192862.0,2022-05-24 22:39:14,2022-05-24 22:39:14,How to get a proportion from two variables in a dataset,<python><pandas><statistics><data-science><jupyter>,0,3,N/A,CC BY-SA 4.0
72376953,1,72381281.0,2022-05-25 11:39:25,-1,88,"<p>I am trying to combine the heating types categories in my dataset, so that the ones with less than 2000 appears are combined into other. However when I try executing the code, I keep getting this error: &quot;Cannot do inplace boolean setting on mixed-types with a non np.nan value&quot;</p>
<p>I tried the code this way:</p>
<pre><code>heats = tidy_housing_cleaned['heatingType'].value_counts()
heating_mask = tidy_housing_cleaned.isin(heats[heats &lt; 2000].index)
tidy_housing_cleaned[heating_mask] = 'Other'
</code></pre>
<p>Data:<br />
<a href=""https://i.stack.imgur.com/4oQNU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4oQNU.png"" alt=""heats"" /></a></p>
<p>Error:<br />
<a href=""https://i.stack.imgur.com/HdYvv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HdYvv.png"" alt=""error"" /></a></p>
<p>Has someone seen this before?</p>
",15480278.0,14909621.0,2022-05-25 16:21:30,2022-05-25 16:27:09,combining categories failure,<python><pandas><data-science><categories>,1,1,2022-05-25 16:18:06,CC BY-SA 4.0
72388656,1,-1.0,2022-05-26 08:03:19,2,247,"<p>I have (11145, 14) shape dataset. In one of the column, I have a really complicated XML values. I am trying to expand this XML column and add them as new columns.
Here is one example of this XML: ( i changed the values for privacy reason but this is the structure)</p>
<pre><code>'
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;modulo
    xmlns=&quot;http://www.sadasdasdasdasd.it/12312312312/Fasdasdasda&quot;
    xmlns:xsi=&quot;http://www.sss1231231233.org/200232321/XMLSchema-instance&quot;&gt;
    &lt;nomeTxt dataFill=&quot;ew.fill()&quot; dataFillMode=&quot;auto&quot; modelCodeMeaning=&quot;Nome&quot; modelCodeScheme=&quot;asdasdasdas&quot; modelCodeSchemeVersion=&quot;01&quot; modelCodeValue=&quot;asdasdasdasdasdasdqw&quot;&gt;
        &lt;![CDATA[*****]]&gt;
    &lt;/nomeTxt&gt;
    &lt;adasdasdasdaq2qwdwasxasxas dataFill=&quot;ew.fill()&quot; dataFillMode=&quot;auto&quot; modelCodeMeaning=&quot;asdasdasdqweqwe&quot; modelCodeScheme=&quot;asdasdasdas&quot; modelCodeSchemeVersion=&quot;01&quot; modelCodeValue=&quot;asdasdasdasdasd2szszxc&quot;&gt;
        &lt;![CDATA[*****]]&gt;
    &lt;/adasdasdasdaq2qwdwasxasxas&gt;
    &lt;qweweqweqweqweqweqwe dataFill=&quot;ew.fill()&quot; dataFillMode=&quot;auto&quot; modelCodeMeaning=&quot;sdsdsds&quot; modelCodeScheme=&quot;asdasdasdas&quot; modelCodeSchemeVersion=&quot;01&quot; modelCodeValue=&quot;asdasdasd&quot;&gt;
        &lt;![CDATA[M]]&gt;
    &lt;/qweweqweqweqweqweqwe&gt;
    &lt;qewtrweqrqwerqwrqweqw dataFill=&quot;ew.fill()&quot; dataFillMode=&quot;auto&quot; modelCodeMeaning=&quot;qewtrweqrqwerqwrqweqw&quot; modelCodeScheme=&quot;asdasdasdas&quot; modelCodeSchemeVersion=&quot;01&quot; modelCodeValue=&quot;asdasdasdas&quot;&gt;
        &lt;![CDATA[213123123123]]&gt;
    &lt;/qewtrweqrqwerqwrqweqw&gt;
    &lt;qewtrweqrqwerqwrqzxczxcasxcasxweqw dataFill=&quot;ew.fill(\'date\')&quot; dataFillMode=&quot;auto&quot; modelCodeMeaning=&quot;Data di nascita&quot; modelCodeScheme=&quot;asdasdasdas&quot; modelCodeSchemeVersion=&quot;01&quot; modelCodeValue=&quot;asdasfafassadasdasdasdas&quot;&gt;
        &lt;![CDATA[1927-21-13]]&gt;
    &lt;/qewtrweqrqwerqwrqzxczxcasxcasxweqw&gt;
    &lt;sadasdasdasdasdsa codeValue=&quot;0&quot; codeScheme=&quot;asdasdasdasdasdasd&quot; codeMeaning=&quot;No&quot; codeSchemeVersion=&quot;01&quot;&gt;
        &lt;![CDATA[No]]&gt;
    &lt;/rbg_allergiefarmacologiche&gt;
    &lt;xczcxzcxzczxczxcz codeValue=&quot;0&quot; codeScheme=&quot;asdasdasdasdasdasd&quot; codeMeaning=&quot;No&quot; codeSchemeVersion=&quot;01&quot;&gt;
        &lt;![CDATA[No]]&gt;
    &lt;/xczcxzcxzczxczxcz&gt;
    &lt;asdasfascasasxasx codeValue=&quot;0&quot; codeScheme=&quot;asdasdasdas&quot; codeMeaning=&quot;No&quot; codeSchemeVersion=&quot;01&quot;&gt;
        &lt;![CDATA[No]]&gt;
    &lt;/asdasfascasasxasx&gt;
    &lt;asdasxcasxasxasxzxxz&gt;
        &lt;![CDATA[false]]&gt;
    &lt;/asdasxcasxasxasxzxxz&gt;
    &lt;asxasxasxsaxasx xsi:nil=&quot;true&quot;&gt;&lt;/asxasxasxsaxasx&gt;
    &lt;saxasx&gt;
        &lt;![CDATA[false]]&gt;
    &lt;/saxasx&gt;
    &lt;asdasxasxasxas xsi:nil=&quot;true&quot;&gt;&lt;/asdasxasxasxas&gt;
    &lt;asasdasdasdas&gt;
        &lt;![CDATA[false]]&gt;
    &lt;/asasdasdasdas&gt;
    &lt;asasdasdasdasasasasd xsi:nil=&quot;true&quot;&gt;&lt;/asasdasdasdasasasasd&gt;
    &lt;asasdasdasasd&gt;
        &lt;![CDATA[false]]&gt;
    &lt;/asasdasdasasd&gt;
    &lt;zcxzcxzc xsi:nil=&quot;true&quot;&gt;&lt;/zcxzcxzc&gt;
&lt;/modulo&gt;'
</code></pre>
<p>I tried to search each column with for loop and and then tried to convert it as dictionary and then save it as columns. The problem with this solution, in each row there are different &lt;xml columns and number of them are different. So my solution is not working.</p>
<pre><code>df[&quot;XML_column&quot;]
0        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
1        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
2        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
3        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
4        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
                               ...                        
11140    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
11141    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
11142    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
11143    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
11144    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;modulo ...
</code></pre>
",19204667.0,-1.0,N/A,2022-05-27 08:34:03,Expanding XML data column in Pandas dataframe and save it as new columns,<python><pandas><xml><data-science>,1,0,N/A,CC BY-SA 4.0
72391728,1,-1.0,2022-05-26 12:20:28,0,134,"<p>The datasets I am working with correspond to individual time series signals. Each signal is unique, with differing total number of data points. here I want to simulate dataset A using dataset B.</p>
<p>Spliting Dataset Code:</p>
<pre><code>x = SmartInsole[:,0:178]
y = Avg[:,0]
y = y.reshape(-1,1)

scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()
scaler_x.fit(x)
xscale = scaler_x.transform(x)
scaler_y.fit(y)
yscale = scaler_y.transform(y)

X_train, X_test, y_train, y_test = train_test_split(xscale, yscale, test_size=0.25, random_state=2)
</code></pre>
<p>The dataset after splitting and normalized:</p>
<pre><code>[0.83974359 0.81818182 0.60264901 0.10457516 0.         0.
 0.         0.         0.         0.66878981 0.7654321  0.77439024
 0.05031447 0.18674699 0.         0.         0.         0.
 0.83892617 0.85620915 0.8590604  0.77852349 0.57236842 0.35333333
 0.         0.         0.         0.05217391 0.6835443  0.85064935
 0.72955975 0.08275862 0.         0.         0.         0.
 0.         0.73758865 0.84868421 0.76923077 0.69230769 0.53472222
 0.53571429 0.65714286 0.49450549 0.47747748 0.72592593 0.77707006
 0.86928105 0.80519481 0.31333333 0.         0.0516129  0.
 0.         0.         0.         0.39316239 0.35036496 0.07086614
 0.38392857 0.57843137 0.58181818 0.68376068 0.74100719 0.84868421
 0.81879195 0.80519481 0.14       0.         0.         0.
 0.         0.         0.83802817 0.89189189 0.88811189 0.48979592
 0.         0.         0.         0.         0.         0.33793103
 0.         0.         0.         0.         0.         0.9929078
 0.97222222 0.81118881 0.45890411 0.         0.         0.
 0.         0.63551402 0.97810219 0.95172414 0.95205479 0.88356164
 0.94630872 0.40384615 0.         0.         0.         0.97222222
 0.9862069  0.96478873 0.76510067 0.52       0.24113475 0.
 0.         0.         0.21568627 0.88970588 0.94594595 0.89864865
 0.08510638 0.37662338 0.0979021  0.         0.         0.
 0.46153846 0.92517007 0.74590164 0.48571429 0.05882353 0.19847328
 0.11428571 0.07857143 0.11510791 0.56375839 0.80794702 0.87012987
 0.81045752 0.21527778 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.07042254 0.21052632 0.62745098 0.75471698 0.80503145
 0.78980892 0.         0.         0.         0.         0.
 0.         0.55357143 0.66878981 0.67272727 0.17682927 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
[0.59662633]
(3000, 178)
(3000, 1)
</code></pre>
<p>I am working with Keras and trying to fit a resnet50 to the data just to evaluate it.
Below is the my renet model structure:</p>
<p>Below is identity blok:</p>
<pre><code>def identity_block(input_tensor,units):
&quot;&quot;&quot;The identity block is the block that has no conv layer at shortcut.
# Arguments
    input_tensor: input tensor
    units:output shape
# Returns
    Output tensor for the block.
&quot;&quot;&quot;
x = layers.Dense(units)(input_tensor)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)

x = layers.Dense(units)(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)

x = layers.Dense(units)(x)
x = layers.BatchNormalization()(x)

x = layers.add([x, input_tensor])
x = layers.Activation('relu')(x)

return x
</code></pre>
<p>Below is dens_block:</p>
<pre><code>def dens_block(input_tensor,units):
&quot;&quot;&quot;A block that has a dense layer at shortcut.
# Arguments
    input_tensor: input tensor
    unit: output tensor shape
# Returns
    Output tensor for the block.
&quot;&quot;&quot;
x = layers.Dense(units)(input_tensor)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)

x = layers.Dense(units)(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)

x = layers.Dense(units)(x)
x = layers.BatchNormalization()(x)

shortcut = layers.Dense(units)(input_tensor)
shortcut = layers.BatchNormalization()(shortcut)

x = layers.add([x, shortcut])
x = layers.Activation('relu')(x)
return x
</code></pre>
<p>Resnet50 model:</p>
<pre><code>def ResNet50Regression():
Res_input = layers.Input(shape=(178,))
width = 16

x = dens_block(Res_input,width)
x = identity_block(x,width)
x = identity_block(x,width)

x = dens_block(x,width)
x = identity_block(x,width)
x = identity_block(x,width)

x = dens_block(x,width)
x = identity_block(x,width)
x = identity_block(x,width)

x = layers.BatchNormalization()(x)
x = layers.Dense(1,activation=&quot;linear&quot;)(x)
model = models.Model(inputs=Res_input, outputs=x)

return model
</code></pre>
<p>Essentially, I am fitting the model to each dataset as follows:</p>
<pre><code>import datetime
from tensorflow.keras import layers,models

model = ResNet50Regression()

model.compile(loss='mse', optimizer=Adam(learning_rate=0.0001), metrics=['mse'])
model.summary()

starttime = datetime.datetime.now()

history = model.fit(X_train, y_train, epochs=200, batch_size=64,  verbose=2, validation_data=(X_test, y_test))
endtime = datetime.datetime.now()
</code></pre>
<p>How can I get optimal prediction results from the above model
below is my results prediction now:</p>
<p><a href=""https://i.stack.imgur.com/L8oHW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L8oHW.png"" alt=""Model Loss"" /></a>
<a href=""https://i.stack.imgur.com/1r3Yb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1r3Yb.png"" alt=""graph prediction results"" /></a></p>
<p>based on the predictions of the model above, the predictions generated are not able to predict properly. how to make prediction results correspondent  the real value</p>
",17870957.0,17870957.0,2022-05-26 13:31:30,2022-05-26 13:31:30,How to train regression model with multiple dataset,<python><data-science><resnet>,0,2,N/A,CC BY-SA 4.0
72422832,1,72634598.0,2022-05-29 10:23:39,0,322,"<p>#Implementing Forward propagation, Backpropagation and Gradient checking
#code to compute forward propogation based on computational graph</p>
<pre><code>    def forward_propagation(x, y, w):
                '''In this function, we will compute the forward propagation '''
                # X: input data point, note that in this assignment you are having 5-d data points
                # y: output varible
                # W: weight array, its of length 9, W[0] corresponds to w1 in graph, W[1] corresponds to w2 in graph,..., W[8] corresponds to w9 in graph.  
                # you have to return the following variables
                # exp= part1 (compute the forward propagation until exp and then store the values in exp)
                # tanh =part2(compute the forward propagation until tanh and then store the values in tanh)
                # sig = part3(compute the forward propagation until sigmoid and then store the values in sig)
                # we are computing one of the values for better understanding
                
                val_1= (w[0]*x[0]+w[1]*x[1]) * (w[0]*x[0]+w[1]*x[1]) + w[5]
                exp = np.exp(val_1)
                tanh = np.tanh(exp + w[6])
                val_2= ((np.sin(w[2]*x[2]))*((w[3]*x[3])+(w[4]*x[4])))+w[7]
                sig = sigmoid(val_2)
                y_pred = tanh+sig*w[8]


            
            
            # after computing part1,part2 and part3 compute the value of y' from the main Computational graph using required equations
            # write code to compute the value of L=(y-y')^2 and store it in variable loss
            loss =pow(y-y_pred,2)
            # compute derivative of L  w.r.to y' and store it in dy_pred 
            dy_pred = -2 * (y-y_pred)
            # Create a dictionary to store all the intermediate values i.e. dy_pred ,loss,exp,tanh,sigmoid
            # we will be using the dictionary to find values in backpropagation, you can add other keys in dictionary as well
            
            forward_dict={}
            forward_dict['exp']= exp
            forward_dict['sigmoid'] = sig
            forward_dict['tanh'] =tanh
            forward_dict['loss'] = loss
            forward_dict['dy_pred'] = dy_pred
            
            return forward_dict 
</code></pre>
<p>#code to compute backward propogation based on computational graph<br />
backward propogation</p>
<pre><code>    def backward_propagation(x,y,w,forward_dict):
        '''In this function, we will compute the backward propagation '''
        # forward_dict: the outputs of the forward_propagation() function
        # write code to compute the gradients of each weight [w1,w2,w3,...,w9]
        # Hint: you can use dict type to store the required variables 
        # dw1 = # in dw1 compute derivative of L w.r.to w1
        dw1 = forward_dict['dy_pred']*(1-(math.pow(forward_dict['tanh'],2)))*forward_dict[&quot;exp&quot;]*2*((w[0]*x[0])+(w[1]*x[1]))*x[0]
        # dw2 = # in dw2 compute derivative of L w.r.to w2
        dw2=forward_dict['dy_pred']*(1-(math.pow(forward_dict['tanh'],2)))*forward_dict[&quot;exp&quot;]*2*((w[0]*x[0])+(w[1]*x[1]))*x[1]
        # dw3 = # in dw3 compute derivative of L w.r.to w3
        dw3 =forward_dict['dy_pred']*(forward_dict['sigmoid']*(1-forward_dict['sigmoid']))*w[8]*((w[3]*x[3])+(w[4]*x[4]))*math.cos(x[2]*w[2])*x[2]
        # dw4 = # in dw4 compute derivative of L w.r.to w4
        dw4 =forward_dict['dy_pred']*(forward_dict['sigmoid']*(1-forward_dict['sigmoid']))*w[8]*math.sin(x[2]*w[2])*x[3]
        # dw5 = # in dw5 compute derivative of L w.r.to w5
        dw5 =forward_dict['dy_pred']*(forward_dict['sigmoid']*(1-forward_dict['sigmoid']))*w[8]*math.sin(x[2]*w[2])*x[4]
        # dw6 = # in dw6 compute derivative of L w.r.to w6
        dw6 = forward_dict['dy_pred']*(1-(math.pow(forward_dict['tanh'],2)))*forward_dict[&quot;exp&quot;]
        # dw7 = # in dw7 compute derivative of L w.r.to w7
        dw7 =forward_dict['dy_pred']*(1-(math.pow(forward_dict['tanh'],2)))
        # dw8 = # in dw8 compute derivative of L w.r.to w8
        dw8 =forward_dict['dy_pred']*(forward_dict['sigmoid']*(1-forward_dict['sigmoid']))*w[8]
        # dw9 = # in dw9 compute derivative of L w.r.to w9
        dw9 =forward_dict['dy_pred']*forward_dict['sigmoid']
        
        
        backward_dict={}
        #store the variables dw1,dw2 etc. in a dict as backward_dict['dw1']= dw1,backward_dict['dw2']= dw2...
        backward_dict['dw1']= dw1
        backward_dict['dw2']= dw2
        backward_dict['dw3']= dw3
        backward_dict['dw4']= dw4
        backward_dict['dw5']= dw5
        backward_dict['dw6']= dw6
        backward_dict['dw7']= dw7
        backward_dict['dw8']= dw8
        backward_dict['dw9']= dw9
    
        
        
        return backward_dict  

def gradient_checking(x,y,w,eps):
    # compute the dict value using forward_propagation()
    # compute the actual gradients of W using backword_propagation()
    forward_dict=forward_propagation(x,y,w)
    backward_dict=backward_propagation(x,y,w,forward_dict)
    
    #we are storing the original gradients for the given datapoints in a list
    
    original_gradients_list=list(backward_dict.values())
    # make sure that the order is correct i.e. first element in the list corresponds to  dw1 ,second element is dw2 etc.
    # you can use reverse function if the values are in reverse order
    
    approx_gradients_list=[]
    eps=0.0001
    w = np.ones(9)*0.1
    #now we have to write code for approx gradients, here you have to make sure that you update only one weight at a time
    #write your code here and append the approximate gradient value for each weight in  approx_gradients_list
    for i in range(len(w)):
      w_plus =w.copy()
      w_plus[i]=w_plus[i]+eps
      Loss1=forward_propagation(x,y,w_plus)['loss']
      w_sub = w.copy()
      w_sub[i]=w_sub[i]-eps
      Loss2=forward_propagation(x,y,w_sub)['loss']
      approx =(Loss1-Loss2)/(2*eps)
      approx_gradients_list.append(approx)
    gradient_check_value =[]
    for i in range(len(w)):
      num = np.linalg.norm(original_gradients_list[i] - approx_gradients_list[i])
      den = np.linalg.norm(original_gradients_list[i]) + np.linalg.norm(approx_gradients_list[i])
      diff = num / den
      
      gradient_check_value.append(diff)
      
    
    return gradient_check_value
</code></pre>
<p>I am trying to implement backpropogation for below graph from scratch. When i try to run grader function forward and back ward propogation returns true. However, While running grader function on gradient_checking function, I am getting the below error.</p>
<pre><code>def grader_grad_check(value):
    print(value)
    assert(np.all(value &lt;= 10**-3))
    return True 

w=[ 0.00271756,  0.01260512,  0.00167639, -0.00207756,  0.00720768,
   0.00114524,  0.00684168,  0.02242521,  0.01296444]



  eps=10**-7
    value= gradient_checking(X[0],y[0],w,eps)
    grader_grad_check(value)
([0.9033700837499321, 0.9033700856470759, 1.0, 0.9950783883268165, 0.9950783883310051, 0.1755656033519971, 0.23240434925625725, 0.7442376971131373, 0.03845869617360365], 0.03845869617360365)

---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-59-a4c2d0b40187&gt; in &lt;module&gt;()
      9 eps=10**-7
     10 value= gradient_checking(X[0],y[0],w,eps)
---&gt; 11 grader_grad_check(value)

&lt;ipython-input-59-a4c2d0b40187&gt; in grader_grad_check(value)
      1 def grader_grad_check(value):
      2     print(value)
----&gt; 3     assert(np.all(value &lt;= 10**-3))
      4     return True
      5 

TypeError: '&lt;=' not supported between instances of 'tuple' and 'float'
</code></pre>
<p><a href=""https://i.stack.imgur.com/YXgnY.png"" rel=""nofollow noreferrer"">graph</a></p>
",18948317.0,18948317.0,2022-05-29 10:27:12,2022-06-15 16:10:36,implement Backpropagation from scratch with gradient checking|| error during graident checking,<python><machine-learning><deep-learning><data-science><backpropagation>,1,0,N/A,CC BY-SA 4.0
72474673,1,72474838.0,2022-06-02 10:26:16,2,5527,"<p>I know polars does not support index by design, so <code>df.filter(expr).index</code> isn't an option, another way I can think of is by adding a new column before applying any filters, not sure if this is an optimal way for doing so in polars</p>
<pre class=""lang-py prettyprint-override""><code>df.with_column(pl.Series('index', range(len(df))).filter(expr).index
</code></pre>
",5257450.0,-1.0,N/A,2022-06-02 10:37:23,What is the recommended way for retrieving row numbers (index) for polars?,<python><dataframe><data-science><python-polars>,1,0,N/A,CC BY-SA 4.0
72455760,1,-1.0,2022-06-01 02:07:31,0,161,"<p>Here is the following code that I am using</p>
<pre><code>import numpy as np
from datetime import timedelta
from datetime import datetime


a = datetime.strftime(datetime.now(), &quot;%Y-%m-%d&quot;)
arr = []
for i in range(0,100):
    b = datetime.strftime(datetime.strptime(a, &quot;%Y-%m-%d&quot;)+timedelta(days=i), &quot;%Y-%m-%d&quot;)
    arr.append(b)

nparr = np.zeros([100],dtype=np.float32)
alist = [0]*100
for x,i in enumerate(nparr):
    val = int(arr[x].replace(&quot;-&quot;,&quot;&quot;))
    nparr[x] = val
    alist[x] = val
    
print(nparr)
print(alist)
</code></pre>
<p>To explain the code above, I am basically making an array called arr which has dates in the format string from todays date till 100 days from today, the array looks something like [&quot;2022-05-31&quot;, &quot;2022-06-01&quot;....]. Then I initialize a numpy array called nparr, and then I loop through the nparr and then basically convert the arr string value into a float by first removing the dashes and then using the python float function. I am also using a list called alist and basically adding to this list same way I am adding to the nparr list.</p>
<p>The output that I am getting for alist is this below. This is the expected result, and I was hoping to get the same result for nparr but this is not what I am getting for nparr:</p>
<pre><code>[20220601, 20220602, 20220603, 20220604, 20220605, 20220606, 20220607, 20220608, 20220609, 20220610, 20220611, 20220612, 20220613, 20220614, 20220615, 20220616, 20220617, 20220618, 20220619, 20220620, 20220621, 20220622, 20220623, 20220624, 20220625, 20220626, 20220627, 20220628, 20220629, 20220630, 20220701, 20220702, 20220703, 20220704, 20220705, 20220706, 20220707, 20220708, 20220709, 20220710, 20220711, 20220712, 20220713, 20220714, 20220715, 20220716, 20220717, 20220718, 20220719, 20220720, 20220721, 20220722, 20220723, 20220724, 20220725, 20220726, 20220727, 20220728, 20220729, 20220730, 20220731, 20220801, 20220802, 20220803, 20220804, 20220805, 20220806, 20220807, 20220808, 20220809, 20220810, 20220811, 20220812, 20220813, 20220814, 20220815, 20220816, 20220817, 20220818, 20220819, 20220820, 20220821, 20220822, 20220823, 20220824, 20220825, 20220826, 20220827, 20220828, 20220829, 20220830, 20220831, 20220901, 20220902, 20220903, 20220904, 20220905, 20220906, 20220907, 20220908]
</code></pre>
<p>Instead I for the nparr value I am getting:</p>
<pre><code>[20220600. 20220602. 20220604. 20220604. 20220604. 20220606. 20220608.
 20220608. 20220608. 20220610. 20220612. 20220612. 20220612. 20220614.
 20220616. 20220616. 20220616. 20220618. 20220620. 20220620. 20220620.
 20220622. 20220624. 20220624. 20220624. 20220626. 20220628. 20220628.
 20220628. 20220630. 20220700. 20220702. 20220704. 20220704. 20220704.
 20220706. 20220708. 20220708. 20220708. 20220710. 20220712. 20220712.
 20220712. 20220714. 20220716. 20220716. 20220716. 20220718. 20220720.
 20220720. 20220720. 20220722. 20220724. 20220724. 20220724. 20220726.
 20220728. 20220728. 20220728. 20220730. 20220732. 20220800. 20220802.
 20220804. 20220804. 20220804. 20220806. 20220808. 20220808. 20220808.
 20220810. 20220812. 20220812. 20220812. 20220814. 20220816. 20220816.
 20220816. 20220818. 20220820. 20220820. 20220820. 20220822. 20220824.
 20220824. 20220824. 20220826. 20220828. 20220828. 20220828. 20220830.
 20220832. 20220900. 20220902. 20220904. 20220904. 20220904. 20220906.
 20220908. 20220908.]
</code></pre>
<p>Why is this? This seems very buggy and I am not quite sure how to fix this. Please help. Thanks!</p>
",9964275.0,-1.0,N/A,2022-06-01 02:07:31,Numpy array of np.float32 type setting incorrect data,<python><numpy><data-science>,0,3,N/A,CC BY-SA 4.0
72475759,1,72475900.0,2022-06-02 11:46:46,1,74,"<p>hope you're doing well .
i tried counting green color row after another green colored row in the table below
In [1]: df = pd.DataFrame([[green], [red], [red]], columns=['A'])</p>
<p>the code i tried to count greengreen:</p>
<pre><code> for index,row in data.iterrows():
   if finalData['Color'].loc[i]=='green' &amp; finalData['Color'].loc[i+1]=='green':
    greengreen+=1
    i+=1
</code></pre>
<p>but it didn't work,hope you can help. note: i'm new to data science</p>
",19229827.0,19229827.0,2022-06-02 12:12:41,2022-06-02 12:12:41,how to count data in a certain column in python(pandas)?,<python><pandas><dataframe><data-science><data-analysis>,3,1,N/A,CC BY-SA 4.0
72438968,1,-1.0,2022-05-30 18:59:59,1,162,"<p>I used to be a Web Backend Developper (using Symfony and Spring-boot).
Now, I'm interested in Data Engineering with Python ... and I see that we use Procedural way more that OOP paradigm.</p>
<p>That is not a big deal but for example, when we do ETL with pandas and all his functionnality, isn't there a structure of programming like MVC but especially for ETL ?
I think that it would be better if everyone followed the same pattern of programming especially that a pipeline could easily turn to be a spaghetti code.</p>
",19213349.0,19213349.0,2022-05-30 19:02:24,2022-05-30 19:02:24,Design pattern for DataEngineer-ETL-pandas,<design-patterns><architecture><data-science><etl><procedural-programming>,0,0,N/A,CC BY-SA 4.0
72443730,1,-1.0,2022-05-31 07:25:11,0,38,"<p>here i have tried to perform pca on my dataset but i dont have any idea how to get the important features and eleminate the feature which is not selected.
here i have given a condition that if data contains more than 10 features then perform PCA else dont perform PCA.</p>
<pre><code>from sklearn.decomposition import PCA
from sklearn import preprocessing
columns = x.columns
def Perform_PCA(x): 
    no_of_col = len(x.columns)
    percent = 90
    my_num = int((percent/100)*no_of_col)
    if no_of_col &gt;= 10:
        pca = PCA(n_components = my_num)
        x_new = pca.fit_transform(x)
        print(&quot;More than 10 columns found Performing PCA&quot;)
        return selected_var
    else:
        print(&quot;Less than 10 columns found no PCA performed&quot;)
        return x
        
        
x = Perform_PCA(x)
x
</code></pre>
",19225087.0,19225087.0,2022-05-31 07:27:21,2022-05-31 08:44:55,how will i get the important features and eleminate the feature which is not selected after performing pca?,<python-3.x><machine-learning><data-science>,2,1,N/A,CC BY-SA 4.0
72446219,1,72446984.0,2022-05-31 10:33:30,2,151,"<p>I have a pandas dataframe given below:</p>
<pre><code>ID       Year       R1  R1_f
KAR1    20201001    1   5
KAR1    20201101    2   6
KAR1    20201201    3   7
KAR1    20210101    4   8
KAR1    20210201    5   9
KAR1    20210301    6   10
KAR1    20210401    7   11
KAR1    20210501    8   12
KAR1    20210601    9   13
KAR1    20210701    10  14
KAR1    20210801    11  15
KAR1    20210901    12  16
KAR2    20201001    4   9
KAR2    20201101    3   8
KAR2    20201201    2   7
KAR2    20210101    1   6
KAR2    20210201    9   5
KAR2    20210301    2   4
KAR2    20210401    6   3
KAR2    20210501    5   2
KAR2    20210601    3   1
KAR2    20210701    30  2
KAR2    20210801    34  3
KAR2    20210901    20  4
</code></pre>
<p>I need to transform above dataframe as given below:</p>
<pre><code>    ID Year      R1_sum 3m_R1 6m_R1 9m_R1 12m_R1 R1_f 3m_R1_f 6m_R1_f 9m_R1_f 12m_R1_f 
   KAR1 20210901   12      33    57    72    78    16    45     81      108      126 
   KAR2 20210901   20      84    98    110   119    4     9      15      30        54
</code></pre>
<p>In above output dataframe:</p>
<p>R1_sum is having value equal to value in year 20210901 for both Id's.</p>
<p>3m_R1 is the summation of values of 3 months 20210901 to 20210701 for column R1</p>
<p>6m_R1 is the summation of values of 6 months from 20210901 to 20210401 for column R1</p>
<p>9m_R1 is the summation of values of 9 months from 20210901 to 20210101 for column R1</p>
<p>12m_R1 is the summation of values of 12 months from 20210901 to 20201001 for column R1</p>
<p>R1_f is having value equal to value in year 20210901 for both Id's.</p>
<p>3m_R1_f is the summation of values of 3 months 20210901 to 20210701 for column R1_f</p>
<p>6m_R1_f is the summation of values of 6 months from 20210901 to 20210401 for column R1_f</p>
<p>9m_R1_f is the summation of values of 9 months from 20210901 to 20210101 for column R1_f</p>
<p>12m_R1_f is the summation of values of 12 months from 20210901 to 20201001 for column R1_f</p>
<p>Please help</p>
",18951121.0,5327068.0,2023-03-30 15:27:58,2023-03-30 15:27:58,Calculating summation over months of pandas dataframe,<python><pandas><time-series><data-science>,1,0,N/A,CC BY-SA 4.0
72481189,1,-1.0,2022-06-02 19:07:29,0,540,"<p>I have the following dataset:</p>
<p><img src=""https://i.stack.imgur.com/lhgmB.png"" alt=""Dataset"" /></p>
<p>I try to plot this dataset with plotly in a streamlit application.
Here is my code:
&quot;PLZ&quot; means the zip code and &quot;Preis&quot; means the price of something.</p>
<pre><code>df = geo_data[['PLZ', 'Preis']].copy()
st.write(df)

fig = px.choropleth(df, geojson=counties, locations='PLZ', color='Preis',
                           color_continuous_scale=&quot;Viridis&quot;,
                           range_color=(0, 12),
                           scope=&quot;europe&quot;,
                           labels={'Preis':'Preis'}
                          )
fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0})
st.plotly_chart(fig)
</code></pre>
<p>I used this as written in the documentation from plotly and I don't know why I am getting this empty map result:</p>
<p><img src=""https://i.stack.imgur.com/rWiOC.png"" alt=""Map result"" /></p>
<p>Does anyone know what's wrong with my code?</p>
",9104834.0,5446749.0,2022-06-03 16:28:16,2022-06-03 19:23:40,Plotly Map Choropleth not plotting anything - Streamlit,<python><web><plotly><data-science><streamlit>,1,1,N/A,CC BY-SA 4.0
72480653,1,72480725.0,2022-06-02 18:13:50,0,41,"<p>Let's assume I have a dataframe like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">name</th>
<th>color</th>
<th>shape</th>
<th style=""text-align: right;"">taste</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Apple</td>
<td>Red</td>
<td>Heart</td>
<td style=""text-align: right;"">Sweet</td>
</tr>
<tr>
<td style=""text-align: left;"">Banana</td>
<td>Yellow</td>
<td>Long</td>
<td style=""text-align: right;"">Sweet</td>
</tr>
<tr>
<td style=""text-align: left;"">Cherry</td>
<td>Pink</td>
<td>Circular</td>
<td style=""text-align: right;"">Sour</td>
</tr>
<tr>
<td style=""text-align: left;"">Damson</td>
<td>Magenta</td>
<td>Circular</td>
<td style=""text-align: right;"">Sour</td>
</tr>
<tr>
<td style=""text-align: left;"">Eggplant</td>
<td>Violet</td>
<td>Long</td>
<td style=""text-align: right;"">Bitter</td>
</tr>
</tbody>
</table>
</div>
<p>And for the input I have a dictionary of one element which be like <code>new_fruit = {'name' : 'Tangerine' , 'color' : 'Orange' , 'shape' : 'Circular', 'taste' : 'Sour'} </code></p>
<p>What I want is to iterate over this dictionary to get the most identical row(value) from the dataframe. In this case, <strong>Circular</strong> and <strong>Sour</strong> which are the <strong>Damson</strong> and <strong>Cherry</strong> rows.</p>
<p>I have tried with code</p>
<pre><code>   for key, value in new_fruit.items():
    if key == 'name':
        continue
    if dataframe[key] == value: # Tried with isin too
        print(dataset[dataset[key] == value])
        break
</code></pre>
<p>I know something is wrong, but couldn't figure it out, can you please help me solve this!</p>
",19258681.0,-1.0,2022-06-02 18:14:12,2022-06-02 18:19:48,How to get the most related row from a pandas dataframe by iterating over a dictionary,<python><pandas><dictionary><data-science>,1,2,N/A,CC BY-SA 4.0
72483747,1,-1.0,2022-06-03 01:06:39,1,947,"<p>I have a daily dataset that has a categorical and numerical column. So, I want to change the daily dataset to the monthly dataset. How can I do that using python? For example, if I have a dataset similar to the picture below how can I bring it in per month having a categorical value and sum for a numerical column. It was easy to sum for the numerical columns but having categorical makes it difficult. The categorical value is most likely to be the same throughout the daily data.
<a href=""https://i.stack.imgur.com/wekns.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wekns.png"" alt=""enter image description here"" /></a></p>
",16875907.0,16875907.0,2022-06-03 03:45:16,2022-06-03 11:12:16,How to convert daily data into weekly or monthly in python with categorical and numerical column?,<python><pandas><data-science><feature-engineering><data-science-experience>,1,2,N/A,CC BY-SA 4.0
72485095,1,72485366.0,2022-06-03 05:31:37,0,1860,"<p>I have a SQL table, and one column of the table has type text[]. I want to create write a query that will create a new table, which consists of all arrays flattened and concatenated. Ex: If there are 3 items in the table, and the array entry for each of those items is [1, 2, 3], NULL, [1, 4, 5], I want the result set to be [1, 2, 3, 1, 4, 5].</p>
<p><a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays"" rel=""nofollow noreferrer"">UNNEST</a> seems like it could be useful here. But, I can't find a way to apply the function to every array in the table. Essentially, I want to &quot;map&quot; this function over every row in the table. Can anyone point me in a good direction?</p>
<pre><code>CREATE TABLE arrs (
  col1 int,
  col2 text[]
);


INSERT INTO arrs (col1, col2) VALUES (1, '{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}');
INSERT INTO arrs (col1, col2) VALUES (2, '{&quot;d&quot;, &quot;e&quot;}');
</code></pre>
<p>I want the query to return a table with 5 rows with text values &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot; for the above table.</p>
<p>Useful REPL for testing: <a href=""https://replit.com/languages/sqlite"" rel=""nofollow noreferrer"">https://replit.com/languages/sqlite</a></p>
<p>Thanks!</p>
",18787146.0,18787146.0,2022-06-03 06:05:11,2022-06-03 06:11:04,SQL - Expand column of arrays into column of elements,<sql><database><postgresql><data-science>,1,9,N/A,CC BY-SA 4.0
72474782,1,-1.0,2022-06-02 10:34:08,-1,58,"<p>I have a dataframe called 'merged'.
On running merged.info(), I get</p>
<p><a href=""https://i.stack.imgur.com/FQHwL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FQHwL.png"" alt=""merged.info()"" /></a></p>
<p>This is when I print the dataframe.</p>
<p><a href=""https://i.stack.imgur.com/s6IRV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s6IRV.png"" alt=""merged"" /></a></p>
<p>All the columns and values in the dataframe are defined and non null.</p>
<p>But the following code, <code>print(merged.get(['PM2.5_CF1_ug/m3', 'Uptime Minutes', 'RSSI_dbm', 'Temperature_F', 'Humidity_%']))</code> prints None instead of printing the values.
This is something which is happening suddenly without changing anything in the code. I even tried loading the merged dataframe completely. I am confident that it is a pandas issue but I couldnt find the solution anywhere to this. Please help me resolve this.</p>
<p>Ran the code on google colab
Pandas version: 1.3.5</p>
<p><a href=""https://i.stack.imgur.com/yBNPw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yBNPw.png"" alt=""enter image description here"" /></a></p>
",12705907.0,12705907.0,2022-06-02 10:39:17,2022-06-02 10:52:10,"dataframe.get(['param1', 'param2']) returning Nonetype even when the columns actually have the data",<python><pandas><dataframe><numpy><data-science>,1,1,N/A,CC BY-SA 4.0
72486049,1,-1.0,2022-06-03 07:21:17,0,15,"<p>I have a csv file with more than a million rows. I made that into a dataframe 'csv1'. I have to update every elements of a specific column depending upon some condition. For example if the column contains a string like : &quot;New Account&quot; , then I have to replace it with &quot;NEW&quot;, if the column contains 'Current Account&quot; I have to replace with '0'. Similarly I have to replace 'Closed Account':'CLSD' etc.
I have used a dict 'mmap' to map the strings and change it accordingly.</p>
<p>But the problem is updating the column values is taking too long time. Is there a better and fast way to do this?</p>
<p>The code I used:</p>
<pre><code>    mmap={'Current Account':'0','New Account':'NEW','Closed Account':'CLSD'}

    csv1=pd.read_csv(&quot;csv_file.csv&quot;)
    else_str='+01'

    for index,row in csv1.iterrows():
         status=csv1.loc[index,&quot;ColumnName&quot;]
         if status in mmap:
            csv1.loc[index,&quot;ColumnName&quot;]=mmap[status]
         else:
            csv1.loc[index,&quot;ColumnName&quot;]=else_str
</code></pre>
",4593203.0,4420967.0,2022-06-03 13:50:57,2022-06-03 13:50:57,Faster way for updating all elements of a specific column in a dataframe using python panda,<python><pandas><dataframe><csv><data-science>,0,2,2022-06-03 07:22:22,CC BY-SA 4.0
72507560,1,-1.0,2022-06-05 13:05:15,3,168,"<p>I'm working on a XGBoost model and I also tried the GridSearchCV from Scikit learn. After I did a search for most optimal parameter settings, I got this result:</p>
<pre><code>Fitting 4 folds for each of 2304 candidates, totalling 9216 fits
Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 11, 'n_estimators': 200, 'subsample': 0.9}
</code></pre>
<p>Now, after training a model with these settings and doing a prediction on unseen testdata (train/test set used), I got a result. As a test I was changing some settings and then I get a better result than with the most optimal parameters from the grid search.</p>
<p>Is this because the test set is different from the training data? If I have another testset, are those settings also different for the best score? I think both answers can be answered with yes, but how are other people working with this effect?</p>
<p>Because you get the results from the grid search, but do you always use these settings or are you doing the same as I do? <strong>What will be your final setting for the model you want to deploy</strong>?</p>
<p>Hope to receive some inspirational thoughts:)</p>
<p>My final code for train/test after manual fine tuning:</p>
<pre><code>xgb_skl_tuned_2 = xgb.XGBRegressor(
    colsample_bytree = 0.7,
    subsample = 0.9, 
    learning_rate = 0.3,
    max_depth = 5, 
    min_child_weight = 13,
    gamma = 10, 
    n_estimators = 50
)

xgb_skl_tuned_2.fit(X_train_2,y_train_2)

preds_2 = xgb_skl_tuned_2.predict(X_test_2)

mse = mean_squared_error(y_test_2, preds_2, squared=False)
print('Model RMSE: {}'.format(mse))
</code></pre>
<p>Also checked this thread: <a href=""https://stackoverflow.com/questions/59607441/parameters-tuning-with-gridsearchcv-not-giving-best-result"">parameters tuning with GridsearchCV not giving best result</a></p>
",18676995.0,7117003.0,2022-06-05 14:00:24,2022-06-05 14:00:24,GridSearchCV not giving the most optimal settings?,<python><machine-learning><scikit-learn><data-science><xgboost>,0,5,N/A,CC BY-SA 4.0
72502563,1,-1.0,2022-06-04 19:27:17,0,198,"<p>I am performing a logistic regression and performing probabilistic modeling. When I go through the definition of this ** Precision, Precision@K, ROC curve, and precision-recall AUC curve** performance metrics I am not able to differentiate differences between them. Please correct me if my understanding is wrong and any suggestion would be much appreciated.</p>
<p>**What is the difference between precision and precision@K? The interpretation of precision score 0.8 and precision@K score 0.8 is same whereas k might give some extra information **</p>
<pre><code>Precision: it gives the ratio of correctly classified positive outcomes out of all predicted positive outcomes

Precision@K: it gives the ratio of correctly classified positive outcomes over the k-value. Out of K-value how many of them are relevant to us. I understand this on the problem of recommendation but how can we use this as a performance metric in prediction classification? For example, employees leaving a company and so on. 
</code></pre>
<p><strong>What is the difference between the ROC curve and Precision-Recall curve AUC? How 0.8 ROC curve and 0.8 precision-recall curve are interpretated</strong></p>
<pre><code>ROC curve is ratio between **True Positive** and **False Positive**

Precision-Recall curve AUC is the ration between **Precision** and **Recall**
</code></pre>
<p>I am having a problem understanding the concept. Can somebody please help me to understand the concept?</p>
<p>Thank You</p>
",16875907.0,-1.0,N/A,2022-06-04 19:27:17,"how can we interpret Precision, Precision@K, ROC curve and precision-recall AUC curve",<precision><logistic-regression><roc><precision-recall><data-science-experience>,0,2,N/A,CC BY-SA 4.0
72505348,1,-1.0,2022-06-05 06:52:39,0,247,"<pre><code>from google.colab import files
file = files.upload()

df = pd.read_csv('Book 1.xlsx')
df.head(10)

print(df['id'])
</code></pre>
<p>When I run this I get a key error for id, which is the first header name in my dataset.
I created this file in numbers by copy pasting text in csv format to a blank numbers spreadsheet and exporting as a csv file. I have &quot;use header names as labels&quot; on in preferences. I have recently gotten my MacBook so there may be some small detail I am overlooking. I have also tried removing footer rows but it isn't working. Please help.</p>
",17848354.0,17848354.0,2022-06-05 06:55:17,2022-06-05 13:12:19,CSV file from Numbers on macOS Not Working on Google Colab,<macos><data-science><google-colaboratory>,1,2,N/A,CC BY-SA 4.0
72505653,1,-1.0,2022-06-05 07:51:14,-1,520,"<p>I am working on a Data Science project which is a model to predict whether the imports are Fake or not. I have a training database on which one of my models is achieving up to 92-93% accuracy but on 51% of the test database, it is achieving only around 83-85% accuracy.</p>
<p>On the other hand, after a few changes, I got around 90% accuracy on the training database but got around 91% on the test database.</p>
<p>Earlier, I was worried that I might be getting a higher accuracy for the train set as I might be overfitting, but now I am worried because the accuracy of my test set is higher than that of my train set(which should not be the case). Can someone help me out by explaining what steps I should take in this case and what might be happening here? As I can only submit one final model.</p>
<p>P.S. the classifier I have used in both cases are the same (KNN and GridSearchCV) but the way I have prepared the data before applying the classifier is different.</p>
",17069872.0,-1.0,N/A,2022-06-05 14:01:14,What is an acceptable enough difference between the accuracy of the Train_set and Test_set?,<database><scikit-learn><data-science><sklearn-pandas>,1,0,2022-06-05 13:30:39,CC BY-SA 4.0
72508521,1,-1.0,2022-06-05 15:08:52,-2,91,"<p>Data:</p>
<pre><code>[
'1-Willa-Northbridge-wnorthbridge0@fema.gov-Female-1215', 
'2-Casie-Arundel-carundel1@amazon.co.jp-Polygender-7059', 
'3-Urbanus-Madrell-umadrell2@purevolume.com-Male-4150', 
'4-Der-Bockett-dbockett3@elpais.com-Male-0', 
'5-Gilbertine-Bligh-gbligh4@princeton.edu-Genderqueer-5853', 
'6-Paul-Tinman-ptinman5@businessinsider.com-Male-7142', 
'7-Daffy-Fazan-dfazan6@dagondesign.com-Female-71', 
'8-Bryan-Dumigan-bdumigan7@disqus.com-Male-3444', 
'9-Wilhelm-Brattell-wbrattell8@w3.org-Male-2062', 
'10-Garey-Gadson-ggadson9@symantec.com-Male-1036', 
'11-Mason-Beartup-mbeartupa@usda.gov-Male-6794', 
'12-Cristina-Sayes-csayesb@phoca.cz-Female-6756', 
'13-Deirdre-Masham-dmashamc@foxnews.com-Female-6690', 
'14-Rosabelle-Antognetti-rantognettid@discovery.com-Female-3763', 
'15-Edy-Lochhead-elochheade@sciencedaily.com-Female-909', 
'16-Jenifer-Seely-jseelyf@foxnews.com-Female-7275', 
'17-Elenore-Filipyev-efilipyevg@tmall.com-Female-0', 
'18-Gavra-Enbury-genburyh@vistaprint.com-Female-3969', 
'19-Fabiano-Bison-fbisoni@nytimes.com-Male-2831', 
'20-Kinnie-Dimbleby-kdimblebyj@netvibes.com-Male-8034'
]

</code></pre>
<p>Data format: <code>[id-firstname-lastname-email-gender-salary]</code></p>
<ul>
<li>Create sorted list according to first name</li>
<li>Separate email is and email provider for each member</li>
<li>Calculate tax paid by each member which is 10% of their salary, if salary = 0 place that member in separate list with tax value default to NaN</li>
<li>Transform gender to F or M instad of full word</li>
</ul>
<p>The final output should be 2 lists, one with transformed data and one with members with <code>salary = 0</code>
Output:</p>
<pre><code>List1 = [[1, “Willa”, “Northbridge”,  “wnorthbridge0”,  “fema.gov”,  “F”, 1215, 121.5], ………………]
List2 = [[4, “Der”, “Bockett”,  “dbockett3”, “elpais.com”, “M”, 0, “NaN”], ………………]
</code></pre>
",14136379.0,14136379.0,2022-06-05 16:35:59,2022-06-06 07:19:47,"With the following data, create functions to perform the following step by step,",<python><function><for-loop><machine-learning><data-science>,3,3,N/A,CC BY-SA 4.0
72511575,1,-1.0,2022-06-05 22:40:20,-2,285,"<p>The website is <a href=""https://en.wikipedia.org/wiki/List_of_national_capitals_by_population"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/List_of_national_capitals_by_population</a>
I'm trying to scrape the table with the national capitals</p>
<pre><code>base_site = &quot;https://en.wikipedia.org/wiki/List_of_national_capitals_by_population&quot;

tables = pd.read_html(base_site)

URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)&gt;
</code></pre>
<p>Why is this error message popping up? Also, there is more to the error message.</p>
",18708212.0,-1.0,N/A,2022-07-13 10:57:37,"How do I fix this error while scraping? It says ""certificate has expired""",<python><pandas><dataframe><web-scraping><data-science>,1,2,N/A,CC BY-SA 4.0
72511719,1,72515845.0,2022-06-05 23:14:25,0,46,"<p>I have 4 Types of nodes :<br />
A it the start point<br />
B<br />
C<br />
D final destination points  <a href=""https://i.stack.imgur.com/ONv2B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ONv2B.png"" alt=""enter image description here"" /></a></p>
<p>I want to display some node D. with these steps :<br />
1 - When we go with node A to node B I want to explore node B when r.val &lt; 5 for example
So in this example I want to explore B2<br />
2- Then I want to explore all Nodes B attached to B when its attached to the same Node C (for example when C nom:Travail), in my example its B1 and B4<br />
3- In the last I want to suggest Nodes D ORDER BY r.s (score between B nodes) (in my example the relationship between B2 - B1 and B2 - B4)<br />
So I want the result are id of D6 and D7 ( ana and moa)</p>
<p>I take this code :</p>
<pre><code>MATCH (a:A1{name: &quot;bale&quot;})-[r1]-&gt;(b:B) 
MATCH (b)-[r2]-(b2:B)
WHERE r1.val &lt; 3 
WITH DISTINCT b.name AS skill_k_1 , a AS a, r1 AS r1, b AS b, r2 AS r2,b2 AS b2
MATCH (b:B{name:skill_k_1})-[r3]-(cat:C)
WITH DISTINCT cat.nom AS cat_k_1, a AS a, r1 AS r1, b AS b, r2 AS r2 ,b2 AS b2, r3 AS r3, cat AS cat
MATCH (b2)-[r]-(b3:B)-[r4]-(cat2:C{nom:cat_k_1})
MATCH (b3)-[r5]-(d:D)
RETURN d.id ORDER BY r.s
</code></pre>
<p>My code display doesn't select just the node who attached to C where C.nom == Travail but he select all nodes and it juggest ALL nodes D not just D6 and D7</p>
",12501275.0,12501275.0,2022-06-06 10:32:53,2022-06-06 10:32:53,Make complex query neo4J (Cypher) for filtring result,<graph><neo4j><cypher><graph-data-science>,1,0,N/A,CC BY-SA 4.0
72513907,1,-1.0,2022-06-06 06:36:51,1,39,"<p>Hello guys hope you're doing well.</p>
<blockquote>
<p>I was trying to count data from a table in python, u can check the table below:
<a href=""https://i.stack.imgur.com/PPLfJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PPLfJ.png"" alt=""the table"" /></a></p>
</blockquote>
<blockquote>
<p>I am trying to count number of data in a column of a table in the picture below: the column (table)</p>
</blockquote>
<p>my main point is for example if: green on row 1 and row 2 then: green green ++ and if red is on row 1 and green on row 2 then : red green++ and if green is on row 1 and red on row 2 then : green red++ but in condition 2 and 3 I am getting the same results but it differs if green or red comes first in the table –
check the code down below:</p>
<pre><code>finalData['Color']
greengreen = 0
greenred = 0
redgreen = 0
redred=0

for i in range(len(finalData)):
  if i &lt; (len(finalData)-1): 
    if finalData['Color'][i] == 'green' and finalData['Color'][i+1] == 'green':
      greengreen += 1
    elif finalData['Color'][i] == 'green' and finalData['Color'][i+1] == 'red':
      greenred += 1
    elif finalData['Color'][i] == 'red' and finalData['Color'][i+1] == 'green':
      redgreen += 1
    elif finalData['Color'][i] == 'red' and finalData['Color'][i+1] == 'red':
      redred += 1
    else:
      print('?')

print(&quot;Value of greengreen bricks: &quot;,greengreen)
print(&quot;Value of greenred bricks: &quot;,greenred)
print(&quot;Value of redred bricks: &quot;,redred)
print(&quot;Value of redgreen bricks: &quot;,redgreen)
</code></pre>
<blockquote>
<p>The error I am facing is that the code is counting green after red(redgreen) and red after green(greenred) the same , meaning that I am get the same results for them and it is not supposed to happen,
for example:
the output
Value of greengreen bricks:  1
Value of greenred bricks:  2
Value of redred bricks:  6
Value of redgreen bricks:  2
(green red and red green are always the same)</p>
</blockquote>
",19229827.0,-1.0,N/A,2022-06-06 06:36:51,I am getting false testing result in counting table data,<python><pandas><numpy><data-science><data-analysis>,0,4,N/A,CC BY-SA 4.0
72490941,1,-1.0,2022-06-03 14:15:21,1,55,"<p>i got this bunch of code :</p>
<pre><code>import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from matplotlib import animation



%matplotlib inline

plt.ion()
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.view_init(azim=270, elev=90)

ax.set_xlabel('X axis')
ax.set_ylabel('Y axis')
ax.set_zlabel('Z axis')

X = -df['x']
Y = -df['y']
Z = -df['z']


ax.scatter(X,Y,Z, color = 'red')



plt.draw()
</code></pre>
<p>this code gives this plot:</p>
<p><a href=""https://i.stack.imgur.com/DCpBv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DCpBv.png"" alt=""Image"" /></a></p>
<p>I would like to set objects over the point clouds like this:</p>
<p><a href=""https://i.stack.imgur.com/bxH7k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bxH7k.png"" alt=""enter image description here"" /></a></p>
<p>is this possible? Does anyone got a proper solution for that?</p>
<p>I would be grateful for any help!</p>
",16321574.0,-1.0,N/A,2022-06-03 14:41:39,draw objects on the center of scatter plots,<python><matplotlib><data-science-experience>,1,0,N/A,CC BY-SA 4.0
72506748,1,72506787.0,2022-06-05 10:57:38,0,40,"<p>Suppose we have a categorical variable</p>
<p><code>Age['0-17','18-25','35-40','55+']</code></p>
<p>What should we prefer; <strong>OneHotEncoding</strong>, <strong>LabelEncoding</strong> or <strong>Mapping</strong> (like assigning data values such as <code>'0-17':1</code>, <code>'18-25':2</code>) and Why?</p>
",17486565.0,1740577.0,2022-06-05 11:15:24,2022-06-05 11:15:24,Encoding method for a categorical variable,<python><machine-learning><data-science><data-analysis>,1,2,N/A,CC BY-SA 4.0
72507450,1,72507856.0,2022-06-05 12:49:00,0,79,"<pre><code>firstpart = D2.loc[(D2['Age'] == &quot;15&quot;) 
                  | (D2['Age'] == &quot;16&quot;)
                  | (D2['City'] == &quot;Paris&quot;)
                  | (D2['City'] == &quot;London&quot;)
                  | (D2['City'] == &quot;Istanbul&quot;)
                  | (D2['Health'] == &quot;Ok&quot;)
                  ]
</code></pre>
<p>This is how I got what I wanted from the dataset but I would like to take <strong>the rest of the dataset</strong> and save it as a new dataset. Does pandas have some functions to do this easily?</p>
",18964569.0,-1.0,N/A,2022-06-05 13:47:38,How to divide datasets in Pandas?,<python><pandas><data-science>,2,0,N/A,CC BY-SA 4.0
72509635,1,72510476.0,2022-06-05 17:31:20,0,107,"<p>Logs generated from a software are dumped in the below manner -</p>
<pre><code> 2:41:04 Start-Date: Thu May 26 2022 02:41:04 MDT
 2:42:50 check in 
 2:42:50 check in
 2:42:50 check in
 8:53:42 check out
 8:53:42 check in
 8:53:43 check in
 8:53:43 check out
 23:59:54 check in
 23:59:55 check out
 23:59:55 check out
 00:11:23 check in
 00:11:25 check in
 00:13:34 check out
</code></pre>
<p>I want to extract data in such a way that the script gets the date from the log, pulls the data for a single day i.e. will time &lt; 24:00:00, and then dumps this data in a file. Then extracts the data for the next day when the time resets and dumps the data in another file.</p>
<p>For eg. It should created a text file 260522.txt containing below data -</p>
<pre><code>2:42:50 check in
2:42:50 check in
8:53:42 check out
8:53:42 check in
8:53:43 check in
8:53:43 check out
23:59:54 check in
23:59:55 check out
23:59:55 check out 
</code></pre>
<p>And create another file 27May2022.txt containing -</p>
<pre><code>00:11:25 check in
00:13:34 check out
</code></pre>
",12041890.0,12041890.0,2022-06-05 18:07:18,2022-06-05 19:23:36,Python: How to dump data from a log file to different files for different dates?,<python><python-3.x><data-science>,1,4,N/A,CC BY-SA 4.0
72528925,1,-1.0,2022-06-07 09:32:59,0,32,"<p>Case 1: let's say I have a defined model (ex: DecisionTreeRegressor ) and I use fit() method on whole train data.</p>
<pre><code>model=DecisionTreeRegressor()
model.fit(X_train,y_train)
</code></pre>
<p>Case 2: let's say I split my train data to 3 part and start to fit them continueusly.</p>
<pre><code>X_train1,X_train2,X_train3=np.array_split(X_train,3)
y_train1,y_train2,y_train3= np.array_split(y_train,3)

model=DecisionTreeRegressor()
model.fit(X_train1,y_train1)
model.fit(X_train2,y_train2)
model.fit(X_train3,y_train3)
</code></pre>
<p>In this two scenario I get same model or different?</p>
",11692168.0,-1.0,N/A,2022-06-07 09:32:59,Fit model on the whole dataset is the same as fit model on a splitted dataset continueusly,<machine-learning><data-science>,0,2,N/A,CC BY-SA 4.0
72533096,1,-1.0,2022-06-07 14:35:00,0,66,"<p>I am new to machine learning, but I have decent experience in python. I am faced with a problem: I need to find a machine learning model that would work well to predict the speed of a boat given current environmental and physical conditions. I have looked into Scikit-Learn, Pytorch, and Tensorflow, but I am having trouble finding information on what type of model I should use. I am almost certain that linear regression models would be useless for this task. I have been told that non-parametric regression models would be ideal for this, but I am unable to find many in the Scikit Library. Should I be trying to use regression models at all, or should I be looking more into Neural Networks? I'm open to any suggestions, thanks in advance.</p>
",19291602.0,-1.0,N/A,2022-06-07 15:00:13,Suggestions for nonparametric machine learning models,<python><machine-learning><data-science><artificial-intelligence>,1,0,N/A,CC BY-SA 4.0
72535088,1,-1.0,2022-06-07 17:05:15,1,97,"<p>I am working on a project where I need to <strong>create segments using geography data</strong> like Geo key, Latitude, and Longitude. The segments should predict the average income per geo key.
The <strong>data I have has geo keys, lat, long</strong>, and average income, and another scenario is where I need to get the geo keys falling into the range of age (people age 18-40).
I tried to use lat and long as features to predict or classify data but the results are not promising. I also tried multiple models like Logistic regression, Random forest, and SGD classifier for multi-class classification.
I want to know what's the best way to create these segments.</p>
<p>Data looks like this (consists of lat and long as a geopoint):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Geopoint</th>
<th>Avg. income</th>
</tr>
</thead>
<tbody>
<tr>
<td>POINT(-122.425123 37.723603)</td>
<td>38199.5</td>
</tr>
<tr>
<td>POINT(-81.27178 28.814807)</td>
<td>681428.3571</td>
</tr>
</tbody>
</table>
</div>",15454165.0,-1.0,N/A,2022-06-07 17:05:15,How to create segment to predict income or age using Machine Learning models,<python><google-cloud-platform><data-science><geography><gcp-ai-platform-notebook>,0,1,N/A,CC BY-SA 4.0
72538931,1,-1.0,2022-06-08 00:54:04,1,265,"<p>I'm working on a gradient boosting model and wanted to apply <code>GridSearchCV</code> to it but I am getting this error: <code>ValueError: Unknown label type: 'continuous'</code>. My data is all continuous.  All of the suggestions I have seen have been to transform my <code>y_train</code> using <code>LabelEncoder()</code> but this is a regression problem so I don't see the need in doing that.  I am using the Boston Housing dataset from Kaggle if that helps at all.</p>
<p>Also, I have used <code>GridSearchCV</code> on a random forest model with the same data and it works.  Not sure why it works on one but not the other.</p>
<pre><code># doesn't work for this one
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV

gbc = GradientBoostingClassifier()

parameters = {
    &quot;n_estimators&quot; : [5 , 50, 250, 500],
    &quot;max_depth&quot; : [1, 3, 5, 7, 9],
    &quot;learning_rate&quot; : [0.01, 0.1, 1, 10, 100]
}

cv = GridSearchCV(gbc, parameters, cv=5)
cv.fit(X_train, y_train.values.ravel())
</code></pre>
<pre><code># Works for this one
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV

tree = DecisionTreeRegressor()

model = GridSearchCV(tree, param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, cv=5)
model.fit(X_train, y_train)

# get metrics
print(model.best_params_)
print(model.best_score_)
</code></pre>
",19294711.0,404970.0,2022-06-08 11:21:02,2022-06-08 12:08:31,Can you use GridSearchCV on continuous data?,<python><data-science><gridsearchcv><gbm>,1,1,N/A,CC BY-SA 4.0
72542980,1,72549659.0,2022-06-08 09:07:50,1,79,"<p>I want to create a DF of the possible combinations from a full factorial design of experiments.
I'm using doepy but it seems to be taking a slowing down as the number of combinations in my DOE grows. I switched to taking the Cartesian product with <code>product_dict()</code> which seems to faster but gives the combinations in a different order.</p>
<p>I need the dataframe of DOE combinations to be in the same order given by doepy, I think thats possible but I'm unsure how.</p>
<p>My question is how to compute the Cartesian product of <code>gas_dict</code> and give the results in the same order as doepy?</p>
<pre><code>import pandas as pd
from doepy import build
from tqdm.contrib.itertools import product

gas_dict = {
 'Velocity (m/s)': [0.00000000E+00, 0.10000000E+00, 0.20000000E+00, 0.30000000E+00, 
         0.40000000E+00, 0.60000000E+00, 0.10000000E+01], 
 
 'Pressure (Pa)': [0.10000000E+06, 0.50000000E+06, 0.10000000E+07, 0.20000000E+07, 
                   0.40000000E+07], 
 
 'Temperature': [0.30000000E+03, 0.40000000E+03, 0.50000000E+03, 0.60000000E+03,],
 'Equivalence Ratio': [0.10000000E+00, 0.50000000E+00, 0.60000000E+00, 0.70000000E+00, 
                      0.80000000E+00, 0.90000000E+00, 0.10000000E+01, 0.11000000E+01, 
                      0.12000000E+01, 0.13000000E+01]    }


def product_dict(**kwargs):
    keys = kwargs.keys()
    vals = kwargs.values()
    for instance in product(*vals):
        yield dict(zip(keys, instance))
        
gas = build.full_fact(gas_dict)      #Correct form 
gas_product = list(product_dict(**gas_dict))  #Incorrect
gas_product = pd.DataFrame(gas_product)

gas_.equals(gas_product)
compare = gas == gas_product
</code></pre>
",10018352.0,-1.0,N/A,2022-06-08 17:06:38,How do I get the cartesian product of a dictionary in a specific order?,<python><pandas><numpy><data-science><cartesian-product>,1,0,N/A,CC BY-SA 4.0
72533524,1,72534504.0,2022-06-07 15:02:28,2,1242,"<p>I have a data table below.</p>
<p>how do I format all columns with variables with comma values?</p>
<p>I know of the scales package but if I use the scales package I won't be able to use the table for some calculating operations any longer</p>
<p>i want something that will still retain the table format type as numeric.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Customers</th>
<th style=""text-align: left;"">telex</th>
<th style=""text-align: left;"">manay</th>
<th style=""text-align: left;"">players</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">babs</td>
<td style=""text-align: left;"">3434323424</td>
<td style=""text-align: left;"">937387573</td>
<td style=""text-align: left;"">96222221</td>
</tr>
<tr>
<td style=""text-align: left;"">bobs</td>
<td style=""text-align: left;"">53545322</td>
<td style=""text-align: left;"">758464938</td>
<td style=""text-align: left;"">122134</td>
</tr>
<tr>
<td style=""text-align: left;"">pint</td>
<td style=""text-align: left;"">43</td>
<td style=""text-align: left;"">7453537384</td>
<td style=""text-align: left;"">223444</td>
</tr>
<tr>
<td style=""text-align: left;"">red</td>
<td style=""text-align: left;"">35435</td>
<td style=""text-align: left;"">624353</td>
<td style=""text-align: left;"">345654</td>
</tr>
<tr>
<td style=""text-align: left;"">yello</td>
<td style=""text-align: left;"">4567</td>
<td style=""text-align: left;"">44</td>
<td style=""text-align: left;"">334</td>
</tr>
</tbody>
</table>
</div>
<p>I want the output to look like this table below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Customers</th>
<th style=""text-align: left;"">telex</th>
<th style=""text-align: left;"">manay</th>
<th style=""text-align: left;"">players</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">babs</td>
<td style=""text-align: left;"">3,434,323,424</td>
<td style=""text-align: left;"">937,387,573</td>
<td style=""text-align: left;"">96,222,221</td>
</tr>
<tr>
<td style=""text-align: left;"">bobs</td>
<td style=""text-align: left;"">53,545,322</td>
<td style=""text-align: left;"">758,464,938</td>
<td style=""text-align: left;"">122,134</td>
</tr>
<tr>
<td style=""text-align: left;"">pint</td>
<td style=""text-align: left;"">43</td>
<td style=""text-align: left;"">7,453,537,384</td>
<td style=""text-align: left;"">223,444</td>
</tr>
<tr>
<td style=""text-align: left;"">red</td>
<td style=""text-align: left;"">35,435</td>
<td style=""text-align: left;"">624,353</td>
<td style=""text-align: left;"">345,654</td>
</tr>
<tr>
<td style=""text-align: left;"">yello</td>
<td style=""text-align: left;"">4,567</td>
<td style=""text-align: left;"">44</td>
<td style=""text-align: left;"">334</td>
</tr>
</tbody>
</table>
</div>",16087142.0,-1.0,N/A,2022-06-08 03:19:05,format all numeric columns with comma to seprate the v,<r><dplyr><tidyverse><data-science>,3,2,N/A,CC BY-SA 4.0
72552953,1,-1.0,2022-06-08 22:41:20,0,72,"<p>Originally, my input dataset had blank spaces. But I have cleaned it, and checked with:
df.isnull().sum()
And everthing is 0.
Now, after fitting my dataset into the LinearRegression model and about to make predictions, it's bringing the above error.</p>
",19302397.0,-1.0,N/A,2022-06-08 23:49:09,"Input contains NaN, infinity or a value too large for dtype('float64') LinearRegression: but there are no empty values",<python><pandas><numpy><data-science><linear-regression>,1,1,N/A,CC BY-SA 4.0
72564197,1,72564252.0,2022-06-09 17:10:59,0,50,"<p>I had a panda DataFrame looks like</p>
<pre><code>   start end
0  10    20
1  30    35
2  20    25
3  35    40
4  40    45
5  60    70
6  70    80
</code></pre>
<p>Expecting Output like</p>
<pre><code>[{&quot;start&quot;:10,&quot;end&quot;:25},{&quot;start&quot;:30,&quot;end&quot;:45},{&quot;start&quot;:60,&quot;end&quot;:80}]
</code></pre>
<p>The output is calculated based on start from min of start and check the end value have any other start and take it and go continue check</p>
",9768643.0,9768643.0,2022-06-09 17:15:22,2022-06-09 17:24:54,group the data as continuous block pandas,<python><python-3.x><pandas><dataframe><data-science>,1,5,N/A,CC BY-SA 4.0
72540377,1,72540407.0,2022-06-08 05:15:18,1,358,"<p>I'm working on pandas in python. I want to create a function to convert categorical variables to numerical based on the number each factor of a categorical variable corresponding with Y dependent variable=1 (where possible Y values are 0, 1 and -1) appears divided by the total count of the factor of that particular categorical variable.</p>
<p><strong>Steps:</strong> For each factor or category of a categorical variable, count how many times the dependent variable Y=1, and divide this count by the total count of that factor. Convert all the categorical variables this way and create a new dataframe of the converted categorical variables.</p>
<p>Below is the code to generate sample data:</p>
<pre><code>df = pd.DataFrame([['iphone5', 'teams', 'shoe', 1],
['iphone6', 'teams', 'shirt', 0], ['iphone5', 'word', 'shoe', 0], 
['iphone7', 'ppt', 'pants', 0], ['iphone8', 'excel', 'umbrella', 1],
['iphone6', 'teams', 'shoe', 1], ['iphone9', 'publisher', 'food', 0]])

df.columns = ['Monday', 'Tuesday', 'Wednesday', 'Y']
df
</code></pre>
<p><a href=""https://i.stack.imgur.com/sHV9L.png"" rel=""nofollow noreferrer"">sample data, df dispayed</a></p>
<p>Checking how many times each factor of the categorical variable had Y=1</p>
<pre><code>monday_check = pd.DataFrame(df.groupby(['Monday', 'Y'])['Y'].count())
monday_check
</code></pre>
<p><a href=""https://i.stack.imgur.com/yiGzq.png"" rel=""nofollow noreferrer"">monday_check displayed</a></p>
<p>Below shows the code to manually convert a categorical variable to numerical as described above.</p>
<pre><code>cond = [df['Monday']=='iphone5',
    df['Monday']=='iphone6',
    df['Monday']=='iphone7',
    df['Monday']=='iphone8',
    df['Monday']=='iphone9']

vals =[monday_check.loc[('iphone5',1)].values.sum()/monday_check.loc['iphone5'].values.sum(),  monday_check.loc[('iphone6',1)].values.sum()/monday_check.loc['iphone6'].values.sum(),
0/monday_check.loc['iphone7'].values.sum(),
monday_check.loc[('iphone8',1)].values.sum()/monday_check.loc['iphone8'].values.sum(),
0/monday_check.loc['iphone9'].values.sum()]

import numpy as np
df['Monday_convert'] = np.select(cond, vals)
df
</code></pre>
<p><a href=""https://i.stack.imgur.com/ST8gH.png"" rel=""nofollow noreferrer"">converted categorical variable to numerical</a></p>
",19295610.0,19295610.0,2022-06-08 05:19:29,2022-06-09 05:35:28,Function in Python to make probability conversion of categorical variable to numerical,<python><pandas><character-encoding><data-science>,2,0,N/A,CC BY-SA 4.0
72586859,1,-1.0,2022-06-11 18:17:47,-2,85,"<p><a href=""https://i.stack.imgur.com/W860z.png"" rel=""nofollow noreferrer"">while I am using simple imputer or KNN imputer, I am getting this error, any idea, why I am getting this and how can i resolve it?</a></p>
",9160565.0,-1.0,N/A,2022-06-12 09:15:50,"ValueError: Shape of passed values is (20453, 769), indices imply (20453, 795)",<pandas><numpy><machine-learning><scikit-learn><data-science>,1,2,N/A,CC BY-SA 4.0
72588238,1,-1.0,2022-06-11 22:24:35,0,53,"<p>My dataset has missing values in GarageYrBlt, which I would like to fill based on YearBuilt. How can I do it?</p>
<p>Below is pivot table for garageYrBlt based on yearblt:</p>
<p>pivot1</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>YearBuilt</th>
<th>GarageYrBlt</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1976</td>
</tr>
<tr>
<td>1</td>
<td>1970</td>
</tr>
<tr>
<td>2</td>
<td>1996</td>
</tr>
<tr>
<td>3</td>
<td>1977</td>
</tr>
<tr>
<td>4</td>
<td>1977</td>
</tr>
<tr>
<td>5</td>
<td>2006</td>
</tr>
<tr>
<td>6</td>
<td>1957</td>
</tr>
<tr>
<td>7</td>
<td>1957</td>
</tr>
<tr>
<td>8</td>
<td>1965</td>
</tr>
<tr>
<td>9</td>
<td>1947</td>
</tr>
</tbody>
</table>
</div>
<pre><code>##### missing = df['GarageYrBlt'].isnull()
##### df_test.loc[missing,'GarageYrBlt'] = df_test.loc[missing,'YearBuilt'].apply(lambda x:pivot1[x])
</code></pre>
<p>but it says key error, how do I overcome this?</p>
",17403284.0,12035742.0,2022-06-11 22:28:07,2022-06-11 22:28:07,Filling Null values based on pivot table,<python><data-science><pivot-table>,0,2,N/A,CC BY-SA 4.0
72516868,1,-1.0,2022-06-06 11:17:13,1,109,"<p>How do I multiply my features inside a Neural Network (not before)?
I am trying to do so with the code below. When running the code I get the following error:</p>
<pre><code>ValueError: Input 0 of layer dense_38 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: (1,)
</code></pre>
<p>It is expecting a two-dimensional input, I do not understand why.</p>
<pre><code>def multiply(x):
    return tf.math.multiply(x=x[0, :], y= x[1, :])

inputs = Input(shape=(2, )) 
r= Dense(units=1, activation=multiply)(inputs) 
fr = Dense(units=1, activation='relu', )(r) ...
</code></pre>
",7763290.0,7763290.0,2022-06-07 12:06:29,2022-07-04 12:52:06,How to multiply inputs in Tensorflow inside the Neural Network?,<python><tensorflow><neural-network><data-science>,1,2,N/A,CC BY-SA 4.0
72536386,1,72536708.0,2022-06-07 19:05:56,1,111,"<p>I have two datasets with different lengths (see below) with lat/lon for different dates. I want to use each location of flooding and calculate distance from that to all storm surge locations for each date and select the location of smallest distance. For example: in first dataset I have one entry for <code>1979-01-24</code> and in second dataset I have five entries for same date. I want to use the location <code>coor (flood)</code> from the first dataset and calculate distances to each entries in second dataset and then select the location (with all data) which is closest to the <code>coor (flood)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>flood_df.head()

   lon_flood  lat_flood  Percentiles_moisture_flood  Percentiles_precip_flood  Percentiles_discharge_flood  Sum_perc_flood           coor (flood) Hazard_start
0  -80.78125   25.15625                    0.777194                  0.849623                     0.960600        2.587417  (-80.78125, 25.15625)   1979-01-24
1  -80.65625   25.28125                    0.694346                  0.960126                     0.980657        2.635129  (-80.65625, 25.28125)   1979-04-29
2  -80.40625   25.28125                    0.974628                  0.974852                     0.933265        2.882745  (-80.40625, 25.28125)   1979-09-03
3  -80.40625   25.28125                    0.994069                  0.995098                     0.995548        2.984716  (-80.40625, 25.28125)   1979-09-12
4  -80.40625   25.28125                    0.895422                  0.998689                     0.997343        2.891454  (-80.40625, 25.28125)   1981-08-16
</code></pre>
<pre class=""lang-py prettyprint-override""><code>surge_df.head()

   latitude  longitude  surge        Date  Percentiles       coor (surge)
0    25.737    -80.142   96.0  1979-01-24     0.981429  (-80.142, 25.737)
1    25.591    -80.142   91.0  1979-01-24     0.978333  (-80.142, 25.591)
2    25.942    -80.112  103.0  1979-01-24     0.985455  (-80.112, 25.942)
3    25.415    -80.171   88.0  1979-01-24     0.977000  (-80.171, 25.415)
4    25.327    -80.229   95.0  1979-01-24     0.982143  (-80.229, 25.327)
</code></pre>
<p>I am using following code to calculate the distances and it's giving me the distances but I also want to have their associated percentiles of surge and other rows.</p>
<pre class=""lang-py prettyprint-override""><code># libraries
import numpy as np
import pandas as pd

# progress bar
from tqdm.notebook import tqdm, trange

# dealing with date and time
from datetime import datetime as dt
from datetime import datetime, timedelta

# for calculating distance
import haversine as hs

# calculate distance
distance_coord_flood_surge = []

for i, r in zip(locations_flood_surge['coor (flood)'], locations_flood_surge['coor (surge)'] ):
    # print(i, r)
    distance = hs.haversine(i, r)
    distance_coord_flood_surge.append(distance)
</code></pre>
<p>I also tried as below but still I am not getting all the information.</p>
<pre class=""lang-py prettyprint-override""><code># find nearest storm surge location to flood location
# dataframe 
surge_flood_dist_df = pd.DataFrame()
        
# get the beginning and ending date of an event
for i in tqdm(zip(range(len(events_dates_2018)))):
    start_date = events_dates_2018['Hazard_start'].iloc[i]
    end_date = events_dates_2018['Hazard_end'].iloc[i]
    
    #lat and lon
    # coord_flood = flood_vars_output_df['coor (flood)']
 
    # get the first and last date of the window
    first_window_date = start_date - timedelta(days=1)
    last_window_date = end_date + timedelta(days=1)
    # get the date range of the window
    window_dates = pd.date_range(first_window_date, last_window_date)
    
    # get the values of the window
    window_values_surge = surge_df.loc[surge_df.index.intersection(window_dates)]

    window_values_flood = flood_df.loc[flood_vars_output_df.index.intersection(window_dates)]

    # get the closest storm surge location to the flood location
    for j, k in zip(window_values_flood['coor (flood)'], window_values_surge['coor (surge)']):
        distance = hs.haversine(j, k)

        # select the closest storm surge location to the flood location and append to the dataframe 
        surge_flood_dist_df.append(distance)
</code></pre>
<p>Could you please help me on how to do it? I will appreciate any help. Thank you.</p>
",13033241.0,15239951.0,2022-06-07 20:00:26,2022-06-07 21:11:08,Calculate distances and find closest location to the location of flooding,<python><python-3.x><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
72558119,1,72558229.0,2022-06-09 09:44:55,0,34,"<p>How  to add  <code>true_prediction</code> variable to the test data</p>
<pre><code>test['prediction'] = true_prediction
</code></pre>
<p>but it's not added as a new column, as shown in <a href=""https://i.stack.imgur.com/Ujjmn.png."" rel=""nofollow noreferrer"">myresult_pic</a></p>
<p>How to make the test data accept it as a new column!!</p>
",15310713.0,15310713.0,2022-09-29 11:44:27,2022-09-29 11:44:27,How to add new column in my test_set datafram?,<python><deep-learning><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
72567006,1,72568961.0,2022-06-09 21:48:47,1,142,"<p>Currently my data is in the following
<a href=""https://i.stack.imgur.com/a0vTf.png"" rel=""nofollow noreferrer"">Data Frame</a></p>
<p>I am trying to create a hierarchal forecast model and one of the first things I need to do is create nodes for it to roll up appropriately</p>
<p>I have converted it to a time series doing the following below</p>
<p><code>myts &lt;- ts(df,start = c(2014,1),end = c(2022,5),frequency = 12)</code>.</p>
<p>I want my two columns &quot;exi&quot; and &quot;new&quot; to be on the same level (level 1) and roll up to total. Though I am having issues understanding the structure on how to do that when using characters.</p>
<pre><code>y &lt;- hts(myts, characters = c(3,0) )
</code></pre>
<p>In this case, how would I specify the level and the number of nodes on the level?</p>
",8799285.0,-1.0,N/A,2022-06-10 03:59:48,Hierarchical Time Series in R - Creating nodes (HTS),<r><machine-learning><time-series><data-science><forecasting>,1,2,N/A,CC BY-SA 4.0
72590584,1,-1.0,2022-06-12 08:28:34,0,58,"<p><a href=""https://ibb.co/b7VHCc3"" rel=""nofollow noreferrer"">https://ibb.co/b7VHCc3</a> [DataFrames]</p>
<p>Whenever I try to run this I keep on getting KeyError</p>
<p><code>masterdata = cabdata.merge(transaction, on= 'Transaction ID').merge(customer, on ='Customer ID').merge(city, on = 'City').merge(holidaydata, on='Holidays')</code></p>
<p>The error :</p>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-33-93bf991ced3b&gt; in &lt;module&gt;
----&gt; 1 masterdata = cabdata.merge(transaction, on= 'Transaction ID').merge(customer, on ='Customer ID').merge(city, on = 'City').merge(holidaydata, on='Holidays')

~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in merge(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
   7295             copy=copy,
   7296             indicator=indicator,
-&gt; 7297             validate=validate,
   7298         )
   7299 

~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
     84         copy=copy,
     85         indicator=indicator,
---&gt; 86         validate=validate,
     87     )
     88     return op.get_result()

~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py in __init__(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)
    625             self.right_join_keys,
    626             self.join_names,
--&gt; 627         ) = self._get_merge_keys()
    628 
    629         # validate the merge keys dtypes. We may need to coerce

~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py in _get_merge_keys(self)
    994                         right_keys.append(rk)
    995                     if lk is not None:
--&gt; 996                         left_keys.append(left._get_label_or_level_values(lk))
    997                         join_names.append(lk)
    998                     else:

~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_label_or_level_values(self, key, axis)
   1690             values = self.axes[axis].get_level_values(key)._values
   1691         else:
-&gt; 1692             raise KeyError(key)
   1693 
   1694         # Check for duplicates

KeyError: 'Holidays'```


  [1]: https://i.stack.imgur.com/amV9C.png
  [2]: https://i.stack.imgur.com/FGBF3.png
</code></pre>
",13668228.0,13668228.0,2022-06-12 09:28:19,2022-06-12 09:28:19,"How do I merge the ""Holidays"" column into my masterdata, I seem to be getting a KeyError",<python><pandas><dataframe><data-science><eda>,1,1,N/A,CC BY-SA 4.0
72559616,1,72559745.0,2022-06-09 11:38:45,2,392,"<p>I want to fit a logistic regression model that predicts Y using X1 and X2.
What I know is that we use the following method:</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size)
</code></pre>
<p>and then</p>
<pre><code>model = LogisticRegression()
model.fit(x_train,y_train)
</code></pre>
<p>To predict Y using X, I don't know how to train the data using more than one predictor. Any help, please?</p>
",18634778.0,12370687.0,2022-08-23 20:49:43,2022-08-23 20:49:43,logistic regression using more than one predictor,<python><machine-learning><scikit-learn><data-science><logistic-regression>,2,0,N/A,CC BY-SA 4.0
72569379,1,-1.0,2022-06-10 05:16:01,0,124,"<p>I have a dataset of which I have attached an image.<a href=""https://i.stack.imgur.com/iuXF6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iuXF6.png"" alt=""Dataset"" /></a></p>
<p>The set of unique values in Origin and Dest are same. Upon doing label encoding of those columns, I thought that value ATL will get same encoding in 'Origin' and 'Dest' but it turns out that the given code:</p>
<pre><code>label_encoder = LabelEncoder()
flight_f['UniqueCarrier'] = label_encoder.fit_transform(flight_f['UniqueCarrier'])
flight_f['Origin'] = label_encoder.fit_transform(flight_f['Origin'])
flight_f['Dest'] = label_encoder.fit_transform(flight_f['Dest'])
</code></pre>
<p>Gives different encoding to a particular value in the two columns. And this is just the training set. I think in test set, I might get different values too which will hamper the predicitive analysis.</p>
<p>Can anyone suggest a solution, please?</p>
",15498962.0,-1.0,N/A,2022-06-18 20:46:00,How to apply label encoding uniformly in all columns?,<python><dataframe><machine-learning><data-science><label-encoding>,2,0,N/A,CC BY-SA 4.0
72586497,1,-1.0,2022-06-11 17:27:26,-1,277,"<p>I have a conceptual question about K fold Cross validation.
In general, we train a model to learn based on test data and validate it with test data, and we assume the system is blind to this data, and this is why we can evaluate if the system really learnt or not.
Now with k fold, the final model actually have seen (indirectly, though) all data, so why it is still valid??? It already has seen all data and we do not know how it predicts unseen data.
This is my question that based on this fact, why we know this method valid?
Thanks.</p>
",17676579.0,4685471.0,2022-06-12 23:39:18,2022-06-12 23:39:18,K fold cross validation makes no part of data blind to the model,<machine-learning><data-science><cross-validation>,2,1,N/A,CC BY-SA 4.0
72593460,1,72593569.0,2022-06-12 15:14:17,1,83,"<p>I want to check a python panda performance</p>
<pre><code>col = [
        &quot;age&quot;,
        &quot;workclass&quot;,
        &quot;fnlwgt&quot;,
        &quot;education&quot;, 
        &quot;educationNum&quot;, 
        &quot;maritalStatus&quot;,
        &quot;occupation&quot;,
        &quot;relationship&quot;,
        &quot;race&quot;,
        &quot;sex&quot;,
        &quot;capitalGain&quot;,
        &quot;capitalLoss&quot;,
        &quot;hoursPerWeek&quot;,
        &quot;nativeCountry&quot;, 
        &quot;Above50K&quot; 
]
import pandas as pd
df = pd.read_csv(&quot;adult.data&quot;, header=None)
df.columns = col

Query 1:
df[(df.education == &quot; HS-grad&quot;) &amp; (df.sex == &quot; Female&quot;) &amp; (df.age &lt;25)]

Query 2 :
df.loc[(df.education == &quot; HS-grad&quot;) &amp; (df.sex == &quot; Female&quot;) &amp; (df.age &lt;25), :]
</code></pre>
<p>Query 1 vs Query 2 ; which is better performance wise</p>
",16529217.0,1740577.0,2022-06-12 16:03:21,2022-06-12 16:03:21,Panda performance check list with .loc() or without,<python><pandas><dataframe><performance><data-science>,1,2,N/A,CC BY-SA 4.0
72600209,1,72603679.0,2022-06-13 08:59:59,0,364,"<p>I have three columns of time series data. I would like to plot the three time series one upon another beautifully in one plot. A star needs to be placed in the respective 5th, 8th, and 10th data points of each time series. My goal is to implement it in Python. I would appreciate if Experts could offer a more efficient method of doing it.</p>
<p>My code:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
data=np.loadtxt(&quot;data_3_timeseries&quot;)
data.plot()
plt.show()
</code></pre>
<p><code>data_3_timeseries</code> is attached here</p>
<pre><code>-0.00831 -0.0213 -0.0182
0.0105 -0.00767 -0.012
0.00326 0.0148 -0.00471
-0.0263 -0.00318 0.011
0.012 0.0398 0.0117
-0.0156 -0.0133 0.02
-0.0482 -0.00783 -0.0162
0.0103 -0.00639 0.0103
0.0158 0.000788 -0.00484
-0.000704 -0.0236 0.00579
0.00151 -0.0135 -0.0195
-0.0163 -0.00185 0.00722
0.0207 0.00998 -0.0387
-0.0246 -0.0274 -0.0108
0.0123 -0.0155 0.0137
-0.00963 0.0023 0.0305
-0.0147 0.0255 -0.00806
0.000488 -0.0187 5.29e-05
-0.0167 0.0105 -0.0204
0.00653 0.0176 -0.00643
0.0154 -0.0136 0.00415
-0.0147 -0.00339 0.0175
-0.0238 -0.00284 0.0204
-0.00629 0.0205 -0.017
0.00449 -0.0135 -0.0127
0.00843 -0.0167 0.00903
-0.00331 7.2e-05 -0.00281
-0.0043 0.0047 0.00681
-0.0356 0.0214 0.0158
-0.0104 -0.0165 0.0092
0.00599 -0.0128 -0.0202
0.015 -0.0272 0.0117
0.012 0.0258 -0.0154
-0.00509 -0.0194 0.00219
-0.00154 -0.00778 -0.00483
-0.00152 -0.0451 0.0187
0.0271 0.0186 -0.0133
-0.0146 -0.0416 0.0154
-0.024 0.00295 0.006
-0.00889 -0.00501 -0.028
-0.00555 0.0124 -0.00406
-0.0185 -0.0114 0.0224
0.0143 0.0204 -0.0193
-0.0168 -0.00608 0.00178
-0.0159 0.0189 0.0109
-0.0213 -0.007 -0.0323
0.0031 0.0207 -0.00333
-0.0202 -0.0157 -0.0105
0.0159 0.00216 -0.0262
0.0105 -0.00292 0.00447
0.0126 0.0163 -0.0141
0.01 0.00679 0.025
0.0237 -0.0142 -0.0149
0.00394 -0.0379 0.00905
-0.00803 0.0186 -0.0176
-0.013 0.0162 0.0208
-0.00197 0.0313 -0.00804
0.0218 -0.0249 0.000412
-0.0164 0.00681 -0.0109
-0.0162 -0.00795 -0.00279
-0.01 -0.00977 -0.0194
-0.00723 -0.0464 0.00453
-0.000533 0.02 -0.0193
0.00706 0.0391 0.0194
</code></pre>
",-1.0,-1.0,2022-06-13 13:38:19,2022-06-13 13:57:49,plotting a beautiful timeseries plot,<pandas><dataframe><numpy><matplotlib><data-science>,2,3,N/A,CC BY-SA 4.0
72574827,1,-1.0,2022-06-10 13:09:06,0,120,"<p>I am building a classification model for a data set containing independent variables as categorical values. As <code>fit()</code> is not supported for non-numeric values, I need to apply either <code>LabelEncoder</code> or <code>OneHotEncoder</code>.</p>
<p>My data set looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>index</th>
<th>outlook</th>
<th>temperature</th>
<th>humidity</th>
<th>windy</th>
<th>play</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>sunny</td>
<td>hot</td>
<td>high</td>
<td>false</td>
<td>no</td>
</tr>
<tr>
<td>1</td>
<td>sunny</td>
<td>hot</td>
<td>high</td>
<td>true</td>
<td>no</td>
</tr>
<tr>
<td>2</td>
<td>overcast</td>
<td>hot</td>
<td>high</td>
<td>false</td>
<td>yes</td>
</tr>
<tr>
<td>3</td>
<td>rainy</td>
<td>mild</td>
<td>high</td>
<td>false</td>
<td>yes</td>
</tr>
<tr>
<td>4</td>
<td>rainy</td>
<td>cool</td>
<td>normal</td>
<td>false</td>
<td>yes</td>
</tr>
<tr>
<td>5</td>
<td>rainy</td>
<td>cool</td>
<td>normal</td>
<td>true</td>
<td>no</td>
</tr>
</tbody>
</table>
</div>
<p>My code is as follows:</p>
<pre><code>w = pd.read_csv(&quot;/content/drive/MyDrive/weather.csv&quot;)

from sklearn import preprocessing
lencoder = preprocessing.LabelEncoder()
    
w['humidity'] = lencoder.fit_transform(w['humidity'])
w['outlook'] = lencoder.fit_transform(w['outlook'])
w['temperature'] = lencoder.fit_transform(w['temperature'])
w['windy'] = lencoder.fit_transform(w['windy'])

x = w.iloc[:, :4].values
y = w.iloc[:, -1].values
X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x, y, test_size=0.1)

model = LinearRegression()
model.fit(X_train, Y_train)
</code></pre>
<p>How can I now predict an individual test sample such as <code>[sunny, hot, high, false]</code>?</p>
",16478047.0,11989081.0,2022-06-10 13:55:40,2022-06-12 14:50:09,How to pass test data for classification model if independent variables are categorical in python?,<python><machine-learning><scikit-learn><data-science>,2,1,N/A,CC BY-SA 4.0
72599857,1,-1.0,2022-06-13 08:32:37,-1,102,"<p>I want to add <code>&quot;'=IF(O{}&lt;=30,&quot;0-30&quot;,IF(O{}&lt;=60,&quot;31-60&quot;,IF(O{}&lt;=90,&quot;61-90&quot;,IF(O{}&lt;=120,&quot;91-120&quot;,IF(O{}&gt;120,&quot;120+&quot;)))))'&quot;</code> this formula to my column. I have done this code</p>
<pre><code>for row_num in range(2, maxRow):
    ws['P{}'.format(row_num)] = '=IF(O{}&lt;=30,&quot;0-30&quot;,IF(O{}&lt;=60,&quot;31-60&quot;,IF(O{}&lt;=90,&quot;61-90&quot;,IF(O{}&lt;=120,&quot;91-120&quot;,IF(O{}&gt;120,&quot;120+&quot;)))))'.format(row_num)
</code></pre>
<p>but it is giving this error :
Traceback (most recent call last):
File &quot;C:/Users/AMacharla/Documents/openpyxltest.py&quot;, line 17, in 
ws['P{}'.format(row_num)] = '=IF(O{}&lt;=30,&quot;0-30&quot;,IF(O{}&lt;=60,&quot;31-60&quot;,IF(O{}&lt;=90,&quot;61-90&quot;,IF(O{}&lt;=120,&quot;91-120&quot;,IF(O{}&gt;120,&quot;120+&quot;)))))'.format(row_num)
IndexError: Replacement index 1 out of range for positional args tuple</p>
<p>plse someone can help?</p>
",19095010.0,2385133.0,2022-06-13 08:54:51,2022-06-13 10:29:17,"how can i add some complex formula to a column using openpyxl,",<python><data-science><openpyxl>,1,2,N/A,CC BY-SA 4.0
72610247,1,72610423.0,2022-06-14 00:08:37,1,1371,"<p>I've already checked out <a href=""https://stackoverflow.com/questions/19324453/add-missing-dates-to-pandas-dataframe"">Add missing dates to pandas dataframe</a>, but I don't want to fill in the new dates with a generic value.</p>
<p>My dataframe looks more or less like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>date (dd/mm/yyyy)</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>01/01/2000</td>
<td>a</td>
</tr>
<tr>
<td>02/01/2000</td>
<td>b</td>
</tr>
<tr>
<td>03/01/2000</td>
<td>c</td>
</tr>
<tr>
<td>06/01/2000</td>
<td>d</td>
</tr>
</tbody>
</table>
</div>
<p>So in this example, days 04/01/2000 and 05/01/2000 are missing. What I want to do is to insert them before the 6th, with a value of <em>c</em>, the last value before the missing days. So the &quot;correct&quot; df should look like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>date (dd/mm/yyyy)</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>01/01/2000</td>
<td>a</td>
</tr>
<tr>
<td>02/01/2000</td>
<td>b</td>
</tr>
<tr>
<td>03/01/2000</td>
<td>c</td>
</tr>
<tr>
<td>04/01/2000</td>
<td>c</td>
</tr>
<tr>
<td>05/01/2000</td>
<td>c</td>
</tr>
<tr>
<td>06/01/2000</td>
<td>d</td>
</tr>
</tbody>
</table>
</div>
<p>There are multiple instances of missing dates, and it's a large df (~9000 rows).</p>
<p>Thanks for your time! :)</p>
",13183369.0,-1.0,N/A,2022-06-14 02:08:31,Add missing dates do datetime column in Pandas using last value,<python><pandas><dataframe><data-structures><data-science>,2,0,N/A,CC BY-SA 4.0
72602157,1,72605320.0,2022-06-13 11:35:56,0,666,"<p>Input dataset:</p>
<p><a href=""https://i.stack.imgur.com/3CjLJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3CjLJ.png"" alt=""enter image description here"" /></a></p>
<p>Output dataset:</p>
<p><a href=""https://i.stack.imgur.com/ge8hq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ge8hq.png"" alt=""enter image description here"" /></a></p>
<p>Basically i want to add one more column &quot;new_month&quot; where no will be in between &quot;dvpt_month&quot; and &quot;lead_month&quot; and all other column's values  will be same for the new_month generated in between these months.
I want to do it with pyspark.</p>
",8654363.0,-1.0,N/A,2022-06-13 15:24:55,To generate the number in between range within Pyspark data frame,<pyspark><data-science>,1,0,N/A,CC BY-SA 4.0
72608978,1,-1.0,2022-06-13 20:59:52,0,126,"<p>I have recently started learning Data Science and I am unable to get grasp of design matrices.</p>
<p>How would I create a design matrix (X) if I have <em><strong>x1 =1,2,3,4,....n</strong></em> , <em><strong>x2 =5,6,7,8....n</strong></em> and target <em><strong>T =9,8,7,6,....n</strong></em> and equation <strong><em>y= 𝑤1 + 𝑤2𝑥1 + 𝑤3𝑥2 + 𝑤4𝑥12 + 𝑤5x2</em>x2</strong>* . I randomly selected x1,x2 and T values . Now how do I create a design matrix (X) from the equation and observations given above ?</p>
<p>I know X is created from x1 and x2 following the equation y but I could not figure out how to do that .</p>
<p>Any help is appreciated . Thank you .</p>
",9988380.0,-1.0,N/A,2022-06-23 13:17:35,Designing a Design matrix,<data-science><linear-regression>,1,0,N/A,CC BY-SA 4.0
72614571,1,72616327.0,2022-06-14 09:26:20,3,41,"<p>Apologies if this is a beginner question. I’m building a text-to-speech model. I was wondering if my training dataset should be “realistically” distributed (i.e. same distribution as the data it will be used on), or should it be uniformly distributed to make sure it performs well on all kinds of sentences. Thanks.</p>
",19336156.0,19336156.0,2022-06-14 09:27:23,2022-06-14 11:57:00,How should a training dataset be distributed?,<dataset><data-science>,1,0,N/A,CC BY-SA 4.0
72616131,1,-1.0,2022-06-14 11:23:15,1,281,"<p>There are multiple ways to calculate the importance of every feature in a dataset. For instance, LogisticRegression from sklearn, or LGBMClassifier from lightgbm. The question is: is there a way to calculate feature importance for a dataset by class?</p>
<p>Assume I have a dataset that describes data of two classes. I need to find out which features are more significant for each of the classes.</p>
<p>Thank you for your time</p>
",12795056.0,-1.0,N/A,2022-06-14 11:23:15,Python classification: feature importance by class,<python><data-science><classification>,0,2,N/A,CC BY-SA 4.0
72630419,1,72631762.0,2022-06-15 11:20:43,0,83,"<p>So I am working on testing financial performance of some stocks. I have a pandas column &quot;signal&quot; that by certain logic has &quot;buy&quot; or &quot;sell&quot; signals.</p>
<p>Now these signals occur at random intervals, but whenever they do, almost always they are repeated in a number of consecutive rows.</p>
<p><a href=""https://i.stack.imgur.com/iOsGD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iOsGD.png"" alt=""enter image description here"" /></a></p>
<p>I would like to only keep the 1st instance every time this happens, and remove the consecutive instances of the &quot;buy&quot; / &quot;sell&quot; word. Something like shown below -</p>
<p><a href=""https://i.stack.imgur.com/Ttllm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ttllm.png"" alt=""enter image description here"" /></a></p>
",8190201.0,-1.0,N/A,2022-06-15 12:52:08,Removing the Duplicate occurrences from a Pandas series without removing entire rows,<python><pandas><data-science><finance><algorithmic-trading>,3,0,N/A,CC BY-SA 4.0
72633821,1,72634088.0,2022-06-15 15:12:07,0,112,"<p>I've the below dataset:</p>
<p><a href=""https://i.stack.imgur.com/aXHzz.png"" rel=""nofollow noreferrer"">Sample Dataset</a></p>
<p>My objective is to create a 2D array of the column 'Products' for this dataset.</p>
<p>Now, if I do the following code:</p>
<pre><code>prodarr = Order_Details[['Product']].to_numpy()
</code></pre>
<p>It returns me the result as follows:</p>
<pre><code> [['PRODUCT_75'],
       ['PRODUCT_75'],
       ['PRODUCT_63'],
       ['PRODUCT_63'],
       ['PRODUCT_34,PRODUCT_86,PRODUCT_57,PRODUCT_89'],
       ['PRODUCT_34,PRODUCT_66,PRODUCT_58,PRODUCT_83'],
       ['PRODUCT_75'],
       ['PRODUCT_63,PRODUCT_90,PRODUCT_27,PRODUCT_5'],
       ['PRODUCT_26'],
       ['PRODUCT_63'],
       ['PRODUCT_63'],
       ['PRODUCT_5,PRODUCT_34'],
       ['PRODUCT_84,PRODUCT_27'],
       ['PRODUCT_27'], ...]
</code></pre>
<p>Now, this is an undesirable situation for me, as I wanted all the distinct products as different elements in a given row. What I mean is that the output should instead be like this:</p>
<pre><code>[['PRODUCT_75'],['PRODUCT_63'],['PRODUCT_63'],['PRODUCT_34','PRODUCT_86','PRODUCT_57','PRODUCT_89'], ['PRODUCT_34','PRODUCT_66','PRODUCT_58','PRODUCT_83'], ['PRODUCT_75'], ['PRODUCT_63','PRODUCT_90','PRODUCT_27','PRODUCT_5'], ...]
</code></pre>
<p>This means that it is that on each row there are multiple columns of strings and not just one.</p>
<p>How should I approach this conundrum? Will have to segregate the strings based on commas and make use of <strong>df.iterrows</strong>?</p>
",19345639.0,-1.0,N/A,2022-06-15 15:31:10,How To Convert A Dataframe Column Into 2D Array?,<python><pandas><dataframe><numpy><data-science>,1,3,N/A,CC BY-SA 4.0
72634871,1,72636288.0,2022-06-15 16:33:00,0,70,"<p>I'm new to Make, and am trying to use it to automate a ml pipeline. I have defined two rules in two files: the first <code>Makefile</code> has file locations and a target to clean a dataset. The second one <code>Makefile.label.surv1</code> has a target to extract labels using a python script. Below is the code for both:</p>
<p>Makefile</p>
<pre><code>#--------------------------------------------------------------------------
# Survival Analysis - Churn prediction top makefile
#--------------------------------------------------------------------------

# date of the snapshot to consider
SNAP_TRN := 2019-06-18
SNAP_TST := 2019-07-24

# directories
DIR_DATA := data
DIR_BUILD := build
DIR_FEATURE := $(DIR_BUILD)/feature
DIR_METRIC := $(DIR_BUILD)/metric
DIR_MODEL := $(DIR_BUILD)/model
DIR_CONFIG := configs

# data files for training and predict
DATA_TRN := $(DIR_DATA)/processed/
DATA_TST := $(DIR_DATA)/processed/

# NOS feature selection
FEATS := $(DIR_CONFIG)/featimp_churnvol.csv
# Config files
CONFIG_PANEL := $(DIR_CONFIG)/config_panel.yaml
CONFIG_INPUT := $(DIR_CONFIG)/config_inpute.yaml

# Generates a clean dataset (inputted and one hot encoded) and labels for train and test
buildDataset: $(DATA_TRN) $(DATA_TST) $(CONFIG_PANEL) $(CONFIG_INPUT) $(FEATS)
    python src/buildDataset.py --train-file $&lt; \
                               --test-file $(word 1, $^) \
                               --config-panel $(word 2, $^) \
                               --config-input $(word 3, $^) \
                               --feats $(lastword $^)   
</code></pre>
<p>Makefile.label.surv1</p>
<pre><code>#--------------------------------------------------------------------------
# surv1: survival labels
#--------------------------------------------------------------------------
include Makefile


FEATURE_NAME := surv1

GRAN := weekly
STUDY_DUR := 1
       
Y_SURV_TRN := $(DIR_DATA)/survival/$(FEATURE_NAME)_train_$(SNAP_TRN)_$(GRAN)_$(STUDY_DUR).pkl
Y_SURV_TST := $(DIR_DATA)/survival/$(FEATURE_NAME)_test_$(SNAP_TST)_$(GRAN)_$(STUDY_DUR).pkl

$(Y_SURV_TRN) $(Y_SURV_TST): $(DATA_TRN) $(DATA_TST) $(CONFIG_PANEL) $(STUDY_DUR) $(GRAN)
    python ./src/generate_surv_labels.py --train-file $&lt; \
                                         --test-file $(word 1, $^) \
                                         --train-label-file $(Y_SURV_TRN) \
                                         --test-label-file $(Y_SURV_TST)\
                                         --config-panel $(word 2, $^) \
                                         --study-dur $(word 3, $^) \
                                         --granularity $(lastword $^)
</code></pre>
<p>So when I run <code>make -f Makefile.label.surv1</code>, it also re-runs the target <code>buildDataset</code>, which I don't want in this case. In this case I haven't made any changes to buildDataset so I don't understand why make re-runs this target... Is there anyway to prevent a target from re-running others?</p>
",14009982.0,14009982.0,2022-06-15 16:49:09,2022-06-15 18:33:57,Avoiding running all makefiles after running make on a target,<makefile><data-science><gnu-make>,1,4,N/A,CC BY-SA 4.0
72636432,1,-1.0,2022-06-15 18:47:03,0,50,"<p>Even during the build of the sklearn, at the end it showing a message like Partial import of sklearn during the build process, though it's successfully installed the scikit-learn</p>
<p>When i tried to run python file which import numpy, it's throwing an circular import(while i run the python file in utils folder of sklearn), while outside it's running fine.</p>
<p><em>build</em><br />
<a href=""https://i.stack.imgur.com/hNQ1Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hNQ1Q.png"" alt=""build"" /></a></p>
<p><em>Numpy circular import error</em><br />
<a href=""https://i.stack.imgur.com/OC036.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OC036.png"" alt=""Numpy circular import error"" /></a></p>
<p>please ignore drive (i.e f and d in the screenshot), while taking screenshot i took it differently. i have the something in the f and d drive.</p>
",11217503.0,14909621.0,2022-06-16 06:15:31,2022-06-16 06:15:31,Circular import of numpy error in the scikit learn,<python><numpy><scikit-learn><data-science>,0,2,N/A,CC BY-SA 4.0
72669191,1,-1.0,2022-06-18 12:19:29,-1,86,"<p>I want to write a function that can show me &quot;Account Name&quot; based on the country I write. The final output should look like this</p>
<p>write_country(&quot;United States&quot;)
So it will provide all &quot;Account Name&quot; which are from United States.</p>
<p>The data is given below</p>
<pre><code>Rank    Account username    Account Name    Followers (millions)    Occupation  Country
0   1   @BarackObama    Barack Obama    132.0   44th President of the United States of America  United States
1   2   @justinbieber   Justin Bieber   114.2   Musician    Canada
2   3   @katyperry  Katy Perry  108.8   Musician    United States

my code is 
entity=df.groupby([ &quot;Country&quot;])
b=df[&quot;Account Name&quot;]
for items in zip(entity,b):
    print(items)

</code></pre>
<p>When I add this code in function, it give me error that boolean operators are not iterable.</p>
",19364323.0,19364323.0,2022-06-19 02:37:33,2022-06-19 06:59:29,How to write a custom function to extract information from a specific row/Series of dataframe if categorical information is provided?,<python><pandas><numpy><data-science><series>,1,2,N/A,CC BY-SA 4.0
72669344,1,-1.0,2022-06-18 12:44:34,0,109,"<p>I started to learn Neo4j a few days ago.</p>
<p>I'm using it to find best path and make some analyzes.</p>
<p>The logic is a Person (id, name) can go to a Restaurant (id, name) via some Street (id, name).
The connection between them have a cost. PS: All streets have a connection between them. For example:</p>
<pre><code>(Person {id: 1})-[CONNECTION {cost:10}]-&gt;(Street {id: 1})
(Person {id: 1})-[CONNECTION {cost:11}]-&gt;(Street {id: 2})
(Street {id: 1})-[CONNECTION {cost:4}]-&gt;(Street {id: 2})
(Street {id: 2})-[CONNECTION {cost:11}]-&gt;(Restaurant {id: 1})
(Street {id: 2})-[CONNECTION {cost:7}]-&gt;(Restaurant {id: 2})
</code></pre>
<p>...</p>
<p>I am using Dijkstra to find all best path to all Restaurant for a specific Person. But the problem is that I can't set the maximum depth, and I would like to limit a maximum of 3 streets. How could I do that?</p>
<pre><code>CALL gds.graph.project(
  'Person-Street-Restaurant',    
  ['Person', 'Street' 'Restaurant'],   
  'CONNECTION',
  {
    relationshipProperties: 'cost'
  }
)

MATCH (source:Person{id:1})
CALL gds.allShortestPaths.dijkstra.stream('Person-Street-Restaurant', {
    sourceNode: source,
    relationshipWeightProperty: 'cost'
})
YIELD sourceNode, targetNode, totalCost, nodeIds
WHERE 'Restaurant' IN LABELS(gds.util.asNode(targetNode))
RETURN
    gds.util.asNode(sourceNode).name AS sourceNodeName,
    gds.util.asNode(targetNode).name AS targetNodeName,
    totalCost,
    [nodeId IN nodeIds | gds.util.asNode(nodeId).name] AS nodeNames,
    SIZE(nodeIds) AS hops
ORDER BY totalCost
</code></pre>
<p><a href=""https://i.stack.imgur.com/jcF6U.png"" rel=""nofollow noreferrer"">Image example</a></p>
<p>The best path is: P-&gt;S#3-&gt;S#4-&gt;S#5-&gt;R, total cost 7. But if I limit to 2 streets, it should be P-&gt;S#1-&gt;S#2-&gt;R, total cost 10.
PS: In this example is easy because we have fewer connections, but in real case we have a lot and all streets have connection between them.</p>
",13248940.0,-1.0,N/A,2022-07-08 03:25:03,How to find shortest path limiting the distance from source?,<graph><neo4j><graph-databases><neo4j-apoc><graph-data-science>,2,0,N/A,CC BY-SA 4.0
72671000,1,-1.0,2022-06-18 16:39:36,0,283,"<p>I'm working on a Machine Learning course and a I need transform my categorical values. Here is the code:</p>
<pre><code>from sklearn.preprocessing import LabelEncoder, OneHotEncoder
LabelEncoder_X = LabelEncoder()
X[:,3] = LabelEncoder_X.fit_transform(X[:,3])

#One Hot encoding o variables dummy. Trata de convertir datos que no son 
#numéricos a datos numéricos de 0 y 1.
onehotencoder = OneHotEncoder(categorical_features=[3])
X = OneHotEncoder.fit_transform(X).toarray() 
</code></pre>
<p>The mistake is with the part of categorical_features, because maybe Python get update to a new version. Thanks</p>
",17258571.0,-1.0,N/A,2022-06-18 16:55:31,TypeError: __init__() got an unexpected keyword argument 'categorical_features' How I can solve it?,<python><data-science>,1,1,N/A,CC BY-SA 4.0
72626523,1,-1.0,2022-06-15 06:00:45,4,4487,"<p>For a particular prediction problem, I observed that a certain variable ranks high in the XGBoost feature importance that gets generated (on the basis of Gain) while it ranks quite low in the SHAP output.</p>
<p>How to interpret this? As in, is the variable highly important or not that important for our prediction problem?</p>
",19016061.0,-1.0,N/A,2022-06-20 09:07:37,Interpreting XGB feature importance and SHAP values,<machine-learning><data-science><classification><xgboost><shap>,1,0,N/A,CC BY-SA 4.0
72644394,1,-1.0,2022-06-16 10:28:59,0,115,"<p>I have a case where I want to predict columns H1 and H2 which are continuous data with all categorical features in the hope of getting a combination of features that give optimal results for H1 and H2, but the distribution of the categories is uneven, there are some categories which only amount to 1,</p>
<p>Heres my data :</p>
<p><a href=""https://i.stack.imgur.com/bJz7h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bJz7h.png"" alt=""enter image description here"" /></a></p>
<p>and my information of categories frequency in each column:</p>
<p><a href=""https://i.stack.imgur.com/XK95T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XK95T.png"" alt=""enter image description here"" /></a></p>
<p>what I want to ask:</p>
<ol>
<li>Does the imbalance of the features of the categories greatly affect the predictions? what is the right solution to deal with the problem?</li>
<li>How do you know the optimal combination? do you have to run a  data test simulation predicting every combination of features with the created model?</li>
<li>What analytical technique is appropriate to determine the relationship between features on H1 and H2? So far I'm converting category data using one hot encoding and then calculating the correlation map</li>
<li>What ML model can be applied to my case? until now I have tried the RF, KNN, and SVR models but the RMSE score still high</li>
<li>What keywords that have similar cases and can help me to search for articles on google, this is my first time working on an ML/DS case for a paper.
thank you very much</li>
</ol>
",16328164.0,-1.0,N/A,2022-06-17 10:43:10,Continous data prediction with all categorical features analysis,<python><regression><data-science><data-analysis><categorical-data>,1,0,N/A,CC BY-SA 4.0
72658009,1,-1.0,2022-06-17 10:34:51,1,208,"<p>Let's say I have a set of data that looks like the this. To my eye, there is a definite vertical &quot;step size&quot; to the signal, at least for the parts that ascend from 0 to max, and that descend from 0 to min. How best to determine that step size?</p>
<p>Of particular note is the fact that occasionally, the sampled value is between one well-defined step and the next, as happens in that first step up (values 474,000 between the step from 0 to 948,000).</p>
<p><a href=""https://i.stack.imgur.com/1KwYv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1KwYv.png"" alt=""enter image description here"" /></a></p>
<p>Data in question:</p>
<pre><code>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 474000, 474000, 948000, 948000, 948000, 948000, 948000, 948000, 948000, 948000, 948000, 948000, 1896000, 1896000, 1896000, 1896000, 1896000, 1896000, 1896000, 1896000, 1896000, 2844000, 2844000, 2844000, 2844000, 2844000, 2844000, 2844000, 2844000, 2844000, 2844000, 3796000, 3796000, 3796000, 3796000, 3796000, 3796000, 3796000, 3796000, 3796000, 4744000, 4744000, 4744000, 4744000, 4744000, 4744000, 4744000, 4744000, 4744000, 4744000, 4744000, 4744000, 5692000, 5692000, 5692000, 5692000, 5692000, 5692000, 5692000, 5692000, 5692000, 5692000, 6640000, 6640000, 6640000, 6640000, 6640000, 6640000, 6640000, 6640000, 6640000, 7592000, 7592000, 7592000, 7592000, 7592000, 7592000, 7592000, 7592000, 7592000, 7592000, 8540000, 8540000, 8540000, 8540000, 8540000, 8540000, 8540000, 8540000, 8540000, 9488000, 9488000, 9488000, 9488000, 9488000, 9488000, 9488000, 9488000, 9488000, 9488000, 9488000, 9488000, 8544000, 8544000, 7592000, 7592000, 6644000, 6644000, 5696000, 5696000, 4748000, 4748000, 4748000, 4748000, 3800000, 2848000, 2848000, 1900000, 1900000, 952000, 952000, 0, 0, 0, 0, -948000, -948000, -948000, -948000, -948000, -948000, -948000, -948000, -948000, -948000, -1896000, -1896000, -1896000, -1896000, -1896000, -1896000, -1896000, -1896000, -1896000, -2844000, -2844000, -2844000, -2844000, -2844000, -2844000, -2844000, -2844000, -2844000, -2844000, -3796000, -3796000, -3796000, -3796000, -3796000, -3796000, -3796000, -3796000, -3796000, -3796000, -4744000, -4744000, -4744000, -4744000, -4744000, -4744000, -4744000, -4744000, -4744000, -5692000, -5692000, -5692000, -5692000, -5692000, -5692000, -5692000, -5692000, -5692000, -5692000, -6640000, -6640000, -6640000, -6640000, -6640000, -6640000, -6640000, -6640000, -6640000, -6640000, -7592000, -7592000, -7592000, -7592000, -7592000, -7592000, -7592000, -7592000, -7592000, -8540000, -8540000, -8540000, -8540000, -8540000, -8540000, -8540000, -8540000, -8540000, -8540000, -9488000, -9488000, -9488000, -9488000, -9488000, -9488000, -9488000, -9488000, -9488000, -9488000, -9488000, -9488000, -8544000, -7592000, -7592000, -6644000, -6644000, -5696000, -5696000, -4748000, -4748000, -4748000, -4748000, -3800000, -3800000, -2848000, -2848000, -1900000, -1900000, -952000, -952000, 0, 0]
</code></pre>
<p><strong>UPDATE</strong> I have some more problematic data now! As you can see, there are significant substeps for many of the steps now. I'm not sure this data lends itself well to analysis.</p>
<p><a href=""https://i.stack.imgur.com/hdttU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hdttU.png"" alt=""enter image description here"" /></a></p>
<pre><code>[0, 0, 0, 0, 0, 0, 0, 0, 3300000, 13860000, 13860000, 13860000, 13860000, 13860000, 13860000, 13860000, 13860000, 18150000, 27720000, 27720000, 27720000, 27720000, 27720000, 27720000, 35970000, 41580000, 41580000, 41580000, 41580000, 41580000, 41580000, 49830000, 49830000, 55440000, 55440000, 55440000, 55440000, 55440000, 65010000, 65010000, 69300000, 69300000, 69300000, 69300000, 69300000, 69300000, 69300000, 82170000, 83160000, 83160000, 83160000, 83160000, 83160000, 83160000, 97020000, 97020000, 97020000, 97020000, 97020000, 97020000, 98010000, 110880000, 110880000, 110880000, 110880000, 110880000, 110880000, 115500000, 124740000, 124740000, 124740000, 124740000, 124740000, 124740000, 124740000, 124740000, 132330000, 138600000, 138600000, 138600000, 138600000, 138600000, 138600000, 129030000, 118140000, 118140000, 105600000, 96030000, 96030000, 83160000, 69300000, 69300000, 58080000, 44880000, 32010000, 32010000, 22440000, 8580000, 8580000, -1980000, -13860000, -13860000, -13860000, -13860000, -13860000, -13860000, -13860000, -16830000, -27720000, -27720000, -27720000, -27720000, -27720000, -27720000, -34320000, -34320000, -41580000, -41580000, -41580000, -41580000, -41580000, -49170000, -49170000, -55440000, -55440000, -55440000, -55440000, -55440000, -55440000, -66660000, -66660000, -69300000, -69300000, -69300000, -69300000, -69300000, -69300000, -80190000, -80190000, -83160000, -83160000, -83160000, -83160000, -83160000, -97020000, -97020000, -97020000, -97020000, -97020000, -97020000, -98670000, -98670000, -110880000, -110880000, -110880000, -110880000, -110880000, -115500000, -115500000, -115500000, -124740000, -124740000, -124740000, -124740000, -124740000, -124740000, -130350000, -138600000, -138600000, -138600000, -138600000, -138600000, -138600000, -131010000, -131010000, -119460000, -108900000, -95370000, -95370000, -83160000, -69300000, -69300000, -57420000, -45870000, -45870000, -34320000, -21450000, -21450000, -21450000, -10890000, 0]
</code></pre>
",4402572.0,4402572.0,2022-06-17 17:40:16,2022-06-17 17:40:16,(Python) Verifying step size in time-series data,<python><data-science><signals>,1,0,N/A,CC BY-SA 4.0
72659580,1,-1.0,2022-06-17 12:42:51,0,682,"<p>My code below returns name error even though it works in a previous cell to fetch a different data set</p>
<pre><code>wages = bls.get_series('CIU2020000000000A') 

wages.to_csv('bls_wages_data_csv')

df = pd.read_csv('bls_wages_data_csv') 

df.columns = 'Date', 'Wages'
df.head()
</code></pre>
<p>​</p>
<p>NameError: name 'bls' is not defined</p>
",19266894.0,-1.0,N/A,2022-06-17 13:26:46,NameError: name 'bls' is not defined,<python><pandas><data-science><data-analysis>,1,3,2022-06-18 06:21:51,CC BY-SA 4.0
72669547,1,-1.0,2022-06-18 13:14:17,-1,39,"<p>data is given like this in a text file:</p>
<p>A B 1 <br />
B C -1 <br />
A C 1 <br />
B D 1  <br />
D A -1 <br />
C D 1</p>
<p>and the matrix has to be like: <br />
\   A  B  C  D <br />
A    0  1   1 -1 <br />
B    1  0  -1  1<br />
C    1 -1   0  1 <br />
D   -1  1   1  0</p>
",19364515.0,-1.0,N/A,2022-06-18 13:30:35,"How to make an N by N matrix using python in which value for each a(i,j) is known?",<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
72672763,1,-1.0,2022-06-18 21:05:39,0,596,"<p>I am trying to bar plot for y_train and y_test data, But i am getting value mismatch error. Can someone help me fix it.</p>
<pre><code>#Read the dataset - Amazon fine food reviews
reviews = pd.read_csv(r&quot;Reviews.csv&quot;)
#get only 2 columns - Text, Score
#drop the NAN values
reviews = reviews[['Text', 'Score']]
reviews.head()
reviews = reviews.dropna()
reviews.drop(reviews[reviews['Score'] == 3].index, inplace = True)
reviews.loc[reviews['Score'] &lt;= 2, 'Score'] = 0
reviews.loc[reviews['Score'] &gt; 3, 'Score'] = 1
reviews
#remove HTML from the Text column and save in the Text column only
from bs4 import BeautifulSoup
#reviews['Text'] = reviews['Text'].str.replace(r'&lt;[^&lt;&gt;]*&gt;', '', regex=True)
reviews['Text'] = [BeautifulSoup(Text,&quot;lxml&quot;).get_text() for Text in reviews['Text'] ]
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(reviews['Text'],reviews['Score'] , test_size=0.20,stratify=reviews['Score'],random_state=33)
</code></pre>
<p>checking the shape of split of data</p>
<pre><code>print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

(80000,) (80000,)
(20000,) (20000,)
</code></pre>
<p>#plot bar graphs of y_train and y_test</p>
<pre><code>import matplotlib.pyplot as plt
plt.figure(figsize = (10, 5))
plt.bar(y_train), y_test), color ='maroon')
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-22-6ec38d3b391c&gt; in &lt;module&gt;()
      2 import matplotlib.pyplot as plt
      3 plt.figure(figsize = (10, 5))
----&gt; 4 plt.bar(np.array(y_train), np.array(y_test), color ='maroon')
      5 

4 frames
&lt;__array_function__ internals&gt; in broadcast_arrays(*args, **kwargs)

/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py in _broadcast_shape(*args)
    418     # use the old-iterator because np.nditer does not handle size 0 arrays
    419     # consistently
--&gt; 420     b = np.broadcast(*args[:32])
    421     # unfortunately, it cannot handle 32 or more arguments directly
    422     for pos in range(32, len(args), 31):

ValueError: shape mismatch: objects cannot be broadcast to a single shape
</code></pre>
",18948317.0,-1.0,N/A,2022-06-18 21:05:39,shape mismatch: objects cannot be broadcast to a single shape while plotting bar plot,<numpy><data-science><bar-chart><data-preprocessing>,0,4,N/A,CC BY-SA 4.0
72640476,1,72640556.0,2022-06-16 04:29:46,2,764,"<pre><code>from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
</code></pre>
<p>What I know is <code>fit()</code> method calculates mean and standard deviation of the feature and then <code>transform()</code> method uses them to transform the feature into a new scaled feature. <code>fit_transform()</code> is nothing but calling <code>fit()</code> &amp; <code>transform()</code> method in a single line.</p>
<p>But here why are we only calling <code>fit()</code> for training data and not for testing data??</p>
<p>Does that means we are using mean &amp; standard deviation of training data to transform our testing data ??</p>
",17965568.0,12370687.0,2022-08-23 21:34:49,2022-08-23 21:34:49,Using fit_transform() and transform(),<machine-learning><scikit-learn><statistics><data-science><data-analysis>,1,3,N/A,CC BY-SA 4.0
72652568,1,72652618.0,2022-06-16 22:36:15,0,265,"<p>I am looking for a way to distribute a positive number proportionally to a list of numbers based on their value (higher gets more). Think of it as distributing a fixed bonus amount based on everyone's performance (pnl).</p>
<p>This is simple if the numbers are all positive. The prorata weight of [1,2,3] would be [1/6, 2/6, 3/6]. But I am not sure how to handle the case for something like [-1, 2, 3] in code?</p>
",19309388.0,19309388.0,2022-06-16 23:00:34,2022-06-17 03:04:54,Calculate the prorata weight of a list of numbers (including negative) in Python,<python><pandas><numpy><statistics><data-science>,3,3,N/A,CC BY-SA 4.0
72658780,1,-1.0,2022-06-17 11:33:45,0,37,"<p>I'm following this Git-hub repository (Link: <a href=""https://github.com/JJN123/Fall-Detection"" rel=""nofollow noreferrer"">https://github.com/JJN123/Fall-Detection</a>)</p>
<p>as per repository i run <code>dstcae_c3d_main_train.py</code> python file but got an error:</p>
<pre><code>ValueError: Error when checking target: expected conv3d_7 to have shape (4, 64, 64, 1) but got array with shape (2, 64, 64, 1)
</code></pre>
<p>please helping me out, how to give an input to train model for 3D fall detection.</p>
",19358401.0,2602877.0,2022-06-17 14:45:56,2022-06-17 14:45:56,How to solve Shape error in Human 3D fall detection using Computer Vision,<python><tensorflow><keras><computer-vision><data-science>,0,2,N/A,CC BY-SA 4.0
72664099,1,-1.0,2022-06-17 19:43:42,0,857,"<p>I am trying to build a reusable docker image for NLP data projects.  I have built a Dockerfile in the following way:</p>
<pre><code>FROM python:3.8

COPY requirements.txt .
RUN pip install -r requirements.txt

ENV PYTHON_PACKAGES=&quot;\
    numpy \
    matplotlib \
    scipy \
    scikit-learn \
    pandas \
    nltk \
    wordcloud \
    spacy \ 
&quot; 

RUN pip install -r requirements.txt
RUN pip install jupyter

CMD [&quot;jupyter&quot;, &quot;notebook&quot;, &quot;--allow-root&quot;]
</code></pre>
<p>Note that the docker image composes correctly with all of my dependencies in the requirements file.  However, when I attempt to connect on the local host, my attempt is rejected.  I ran the container using the following:</p>
<pre><code>docker run -dp 9999:9999 tdnlptools
</code></pre>
<p>I validated that the container is running:</p>
<pre><code>CONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS                    NAMES
c26b647a1403   tdnlptools   &quot;jupyter notebook --…&quot;   7 seconds ago   Up 6 seconds   0.0.0.0:9999-&gt;9999/tcp   modest_mcnulty
</code></pre>
<p>Yet, when I attempt to use the following connection, it won't work:</p>
<pre><code>https://localhost:9999/
</code></pre>
<p>The error is:</p>
<pre><code>This site can’t be reached
localhost unexpectedly closed the connection.

Try:

Checking the connection
Checking the proxy and the firewall
ERR_CONNECTION_CLOSED
</code></pre>
<p>Any idea why my connection is being refused?</p>
",16961408.0,147356.0,2022-06-17 20:10:17,2022-06-17 20:17:42,Cannot connect to docker image with localhost port mapping,<python><docker><data-science>,1,0,N/A,CC BY-SA 4.0
72675983,1,72679720.0,2022-06-19 10:12:01,2,3868,"<p>I have a DataFrame containing intraday, 5 min data, for 1 month, 23 days trading days.
Around 1725 rows, so 75 rows per day.
There is a signal column, that gives buy/sell signal based on ST for now, but I might use another indicator later on.</p>
<p><strong>PHASE 2 (in the Code added below) is where the code has to be corrected/implemented. I started working on it, but been stuck due to my lack of pandas knowledge.</strong></p>
<ol>
<li><p>So I wanna create a new column, that only captures &quot;High&quot; column values, if the &quot;High&quot; value at any point of that same day is greater than the &quot;High&quot; value corresponding to the &quot;buy&quot; signal row.</p>
</li>
<li><p>And also, create a new column, that only captures &quot;Low&quot; column values, if the &quot;Low&quot; value at any point of that same day is Lower than the &quot;Low&quot; value corresponding to &quot;buy&quot; signal row. <strong>Condition 2 is only checked if condition 1 is satisfied.</strong></p>
</li>
</ol>
<p><strong>These 2 columns may help in analyzing the data, or for calculation. Column creation is not important but getting the necessary values for performing the calculations is necessary.</strong></p>
<ol start=""3"">
<li><p>Need a column that calculates returns.</p>
<p>3a. If Condition 1 didn't happen, return 0.</p>
<p>3b. If Condition 1 happened, but condition 2 didn't happen, return (last &quot;Close&quot; of day - &quot;High&quot; of &quot;buy&quot; signal row)</p>
<p>3c. If Condition 1 happened, AND condition 2 happened, return (&quot;Low&quot; of &quot;buy&quot; row - &quot;High&quot; of &quot;buy&quot; row)</p>
</li>
</ol>
<p>Adding some example scenarios below -</p>
<p>Here the Higher High and Lower Low both happened in same row.</p>
<p>return = Condition 3c, i.e. [&quot;Low&quot; of &quot;buy&quot; row - &quot;High&quot; of &quot;buy&quot; row]
<a href=""https://i.stack.imgur.com/c2MAT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c2MAT.png"" alt=""enter image description here"" /></a></p>
<p>Another example below -</p>
<p>Here the Higher High happened first, then after a few rows, Lower Low happened.</p>
<p>return = Condition 3c, i.e. [&quot;Low&quot; of &quot;buy&quot; row - &quot;High&quot; of &quot;buy&quot; row]
<a href=""https://i.stack.imgur.com/Pr8Tu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pr8Tu.png"" alt=""enter image description here"" /></a></p>
<p>In cases where Lower Low doesn't happen, we take the last &quot;Close&quot; of that day.</p>
<p>Return = Condition 3b, i.e. [&quot;Close&quot; at 3 pm of that day - &quot;High&quot; of &quot;buy&quot; signal row]
<a href=""https://i.stack.imgur.com/ZbPXG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZbPXG.png"" alt=""enter image description here"" /></a></p>
<p>All of this above is for &quot;buy&quot; signal condition. I'll have to do it for &quot;sell&quot; as well. But if I can learn how to do it for one, I'll be able to do the other.</p>
<p><strong>Code &amp; Sample Data -</strong></p>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Mon Jun 15 


&quot;&quot;&quot;

import datetime as dt
import yfinance as yf
import pandas as pd
import numpy as np
import pandas_ta as ta
import copy


tickers = [&quot;LT.NS&quot;,&quot;BRITANNIA.NS&quot;,&quot;CIPLA.NS&quot;,&quot;HINDALCO.NS&quot;]

ohlc_intraday = {}

today = dt.datetime.today()
end = today - dt.timedelta(1)
start = today - dt.timedelta(20)

# looping over tickers and creating a dataframe with ohlcv, intraday tf
for ticker in tickers:
    temp = yf.download(ticker, period=&quot;1mo&quot;,interval = &quot;5m&quot;)
    temp.dropna(how=&quot;any&quot;,inplace=True)
    # temp = temp[temp[&quot;Volume&quot;] &gt; 0]
    ohlc_intraday[ticker] = temp
    # ohlc_intraday[ticker].to_csv(&quot;supertrend_data.csv&quot;)

    


tickers = ohlc_intraday.keys()
# dont run above code again &amp; again
# run below code    


# Phase 1 - Data Collection &amp; Signal Generation
ohlc_dict = copy.deepcopy(ohlc_intraday)
tickers_signal = {}
tickers_ret = {}


for ticker in tickers:
    
    ohlc_dict[ticker][&quot;datetime&quot;] = pd.to_datetime(ohlc_dict[ticker].index)
    ohlc_dict[ticker][&quot;hour&quot;] = ohlc_dict[ticker][&quot;datetime&quot;].dt.hour
    ohlc_dict[ticker][&quot;date&quot;] = ohlc_dict[ticker][&quot;datetime&quot;].dt.date
    unique_days = len(ohlc_dict[ticker][&quot;date&quot;].unique())
    num_5min_candles = 75 # number of candles in intraday
    ohlc_dict[ticker][&quot;candle&quot;] = np.array(range(len(ohlc_dict[ticker])))%(num_5min_candles) + 1
    
    
    # Supertrend indicator's value column
    ohlc_dict[ticker][&quot;ST&quot;] = ta.supertrend(high=ohlc_dict[ticker][&quot;High&quot;],low=ohlc_dict[ticker][&quot;Low&quot;],
                                  close=ohlc_dict[ticker][&quot;Close&quot;],length=7,
                                  multiplier=3)[&quot;SUPERT_7_3.0&quot;]
    

    def st_levels(close,stlevel):
        if close &gt; stlevel:
            return &quot;buy&quot;
        elif close &lt; stlevel:
            return &quot;sell&quot;
        
    
    # capture Supertrend buy/sell signals    
    ohlc_dict[ticker][&quot;signal&quot;] = np.vectorize(st_levels)(ohlc_dict[ticker][&quot;Close&quot;],ohlc_dict[ticker][&quot;ST&quot;])
    
    # keep only first occurrence of signal, remove consecutive duplicated signals
    ohlc_dict[ticker].loc[ohlc_dict[ticker][&quot;signal&quot;].eq(ohlc_dict[ticker][&quot;signal&quot;].shift()), &quot;signal&quot;] = &quot;no&quot;
    
    # supertrend length; first 7 rows will contain &quot;no&quot; since ST period length is 7
    ohlc_dict[ticker][&quot;signal&quot;][:7] = &quot;no&quot;
    
    
    # capturing returns
    tickers_signal[ticker] = &quot;&quot;
    tickers_ret[ticker] = []
    # ohlc_dict[ticker].to_csv(&quot;supertrend_data.csv&quot;)


# Phase 2 - Calculating returns for intraday -
for ticker in tickers:
    print(&quot;Calculating returns for &quot;, ticker)
    # ticker = &quot;CIPLA.NS&quot;
    for i in range(len(ohlc_dict[ticker])):
        if tickers_signal[ticker] == &quot;&quot;:
            tickers_ret[ticker].append(0)
            # check if signal candle High/Low is broken for Buy/Sell Scenario respectively
            if ohlc_dict[ticker][&quot;High&quot;][i] &gt;= ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;High&quot;].values[0]:
                tickers_signal[ticker] = &quot;BUY&quot;
            elif ohlc_dict[ticker][&quot;Low&quot;][i] &lt;= ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;sell&quot;][&quot;Low&quot;].values[0]:    
                tickers_signal[ticker] = &quot;SELL&quot;
        
        elif tickers_signal[ticker] == &quot;BUY&quot;:
            
            # calculating returns for StopLoss            
            if ohlc_dict[ticker][&quot;Low&quot;][i] &lt; ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;Low&quot;].values[0]: # and add that both candles be on same day
                tickers_signal[ticker] = &quot;&quot;
                tickers_ret[ticker].append(((ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;Low&quot;].values[0] - ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;High&quot;].values[0])/(ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;High&quot;].values[0])) - 1)
            # calculating returns for 70th candle close in intraday
            else:
                tickers_ret[ticker].append(((ohlc_dict[ticker][ohlc_dict[ticker]['candle']==70][&quot;Close&quot;].values[0] - ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;High&quot;].values[0])/(ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;High&quot;].values[0])) - 1)
                                
        elif tickers_signal[ticker] == &quot;SELL&quot;:
            # calculating returns for StopLoss
            if ohlc_dict[ticker][&quot;High&quot;][i] &gt; ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;High&quot;].values[0]:
                tickers_signal[ticker] = &quot;&quot;
                tickers_ret[ticker].append(((ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;Low&quot;].values[0] - ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;High&quot;].values[0])/(ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;Low&quot;].values[0])) - 1)
            
            # calculating returns for 70th candle close in intraday
            else:
                tickers_ret[ticker].append(((ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;Low&quot;].values[0] - ohlc_dict[ticker][ohlc_dict[ticker]['candle']==70][&quot;Close&quot;].values[0])/(ohlc_dict[ticker][ohlc_dict[ticker]['signal']==&quot;buy&quot;][&quot;High&quot;].values[0])) - 1)


    ohlc_dict[ticker][&quot;ret&quot;] = np.array(tickers_ret[ticker])
                
                
</code></pre>
<p>Below data is for HINDALCO.NS stock -</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Datetime</th>
<th>Open</th>
<th>High</th>
<th>Low</th>
<th>Close</th>
<th>Adj Close</th>
<th>Volume</th>
<th>datetime</th>
<th>hour</th>
<th>date</th>
<th>candle</th>
<th>ST</th>
<th>signal</th>
<th>remarks</th>
<th>Return Calculation</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-05-18 13:35:00+05:30</td>
<td>431.5499878</td>
<td>433.5</td>
<td>431.0499878</td>
<td>433.1000061</td>
<td>433.1000061</td>
<td>263811</td>
<td>2022-05-18 13:35:00+05:30</td>
<td>13</td>
<td>18-05-2022</td>
<td>53</td>
<td>427.2457627</td>
<td>buy</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 13:40:00+05:30</td>
<td>433.2000122</td>
<td>436.7999878</td>
<td>433.1000061</td>
<td>435.5499878</td>
<td>435.5499878</td>
<td>667616</td>
<td>2022-05-18 13:40:00+05:30</td>
<td>13</td>
<td>18-05-2022</td>
<td>54</td>
<td>429.0532754</td>
<td>no</td>
<td>Higher High made</td>
<td></td>
</tr>
<tr>
<td>2022-05-18 13:45:00+05:30</td>
<td>435.6000061</td>
<td>436.5</td>
<td>435.3500061</td>
<td>435.7000122</td>
<td>435.7000122</td>
<td>188908</td>
<td>2022-05-18 13:45:00+05:30</td>
<td>13</td>
<td>18-05-2022</td>
<td>55</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 13:50:00+05:30</td>
<td>435.7000122</td>
<td>435.7999878</td>
<td>434.2999878</td>
<td>434.3500061</td>
<td>434.3500061</td>
<td>296853</td>
<td>2022-05-18 13:50:00+05:30</td>
<td>13</td>
<td>18-05-2022</td>
<td>56</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 13:55:00+05:30</td>
<td>434.6000061</td>
<td>436.1499939</td>
<td>434.4500122</td>
<td>435.4500122</td>
<td>435.4500122</td>
<td>169486</td>
<td>2022-05-18 13:55:00+05:30</td>
<td>13</td>
<td>18-05-2022</td>
<td>57</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:00:00+05:30</td>
<td>435.4500122</td>
<td>436.1000061</td>
<td>434.75</td>
<td>436.0499878</td>
<td>436.0499878</td>
<td>343782</td>
<td>2022-05-18 14:00:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>58</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:05:00+05:30</td>
<td>436.0499878</td>
<td>437.1000061</td>
<td>433.6499939</td>
<td>433.7999878</td>
<td>433.7999878</td>
<td>511022</td>
<td>2022-05-18 14:05:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>59</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:10:00+05:30</td>
<td>433.75</td>
<td>434</td>
<td>432.6499939</td>
<td>433.3999939</td>
<td>433.3999939</td>
<td>249525</td>
<td>2022-05-18 14:10:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>60</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:15:00+05:30</td>
<td>433.6000061</td>
<td>434.5</td>
<td>432.7999878</td>
<td>433.75</td>
<td>433.75</td>
<td>217272</td>
<td>2022-05-18 14:15:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>61</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:20:00+05:30</td>
<td>433.7999878</td>
<td>435.8500061</td>
<td>433.6000061</td>
<td>434.6499939</td>
<td>434.6499939</td>
<td>228628</td>
<td>2022-05-18 14:20:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>62</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:25:00+05:30</td>
<td>434.6499939</td>
<td>435.2000122</td>
<td>434.2000122</td>
<td>434.5499878</td>
<td>434.5499878</td>
<td>149256</td>
<td>2022-05-18 14:25:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>63</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:30:00+05:30</td>
<td>434.4500122</td>
<td>434.75</td>
<td>433.75</td>
<td>433.8999939</td>
<td>433.8999939</td>
<td>155492</td>
<td>2022-05-18 14:30:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>64</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:35:00+05:30</td>
<td>433.8999939</td>
<td>434.9500122</td>
<td>433.6499939</td>
<td>434.5</td>
<td>434.5</td>
<td>128515</td>
<td>2022-05-18 14:35:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>65</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:40:00+05:30</td>
<td>434.6000061</td>
<td>435</td>
<td>434.25</td>
<td>434.5</td>
<td>434.5</td>
<td>117541</td>
<td>2022-05-18 14:40:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>66</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:45:00+05:30</td>
<td>434.4500122</td>
<td>435.1000061</td>
<td>433.1000061</td>
<td>433.2999878</td>
<td>433.2999878</td>
<td>276280</td>
<td>2022-05-18 14:45:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>67</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:50:00+05:30</td>
<td>433.25</td>
<td>434.8999939</td>
<td>433.2000122</td>
<td>434.7000122</td>
<td>434.7000122</td>
<td>163797</td>
<td>2022-05-18 14:50:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>68</td>
<td>430.3779006</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 14:55:00+05:30</td>
<td>434.7000122</td>
<td>435.6499939</td>
<td>434.5499878</td>
<td>435.2000122</td>
<td>435.2000122</td>
<td>201351</td>
<td>2022-05-18 14:55:00+05:30</td>
<td>14</td>
<td>18-05-2022</td>
<td>69</td>
<td>430.5468637</td>
<td>no</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-05-18 15:00:00+05:30</td>
<td>435.0499878</td>
<td>435.75</td>
<td>433.75</td>
<td>435.5</td>
<td>435.5</td>
<td>384571</td>
<td>2022-05-18 15:00:00+05:30</td>
<td>15</td>
<td>18-05-2022</td>
<td>70</td>
<td>430.5468637</td>
<td>no</td>
<td>Lower Low not made, Exited at Market Close</td>
<td>Return = Close (15:00 pm) - High of 'buy' signal row (highlighted in yellow above)</td>
</tr>
</tbody>
</table>
</div>
<p><strong>UPDATE</strong> - In case of &quot;buy&quot; signal happens, and Trade is live, waiting to exit in loss or profit, and if opposite signal &quot;sell&quot; appears, exit trade at that &quot;sell&quot; signal &quot;Close&quot; value.</p>
<p>If this is too complicated, just ignore the signal change happening on same day.</p>
<p><strong>PHASE 2 is where the code has to be corrected/implemented. Checking if signal was generated, if Higher Highs / Lower Lows happened, or just exiting at Market Close (3 pm) and calculate returns accordingly.</strong></p>
<p>Currently as per my code, I'm trying to take &quot;Close&quot; of 70th candle, which is around 15:05 pm. But please feel free to modify/improve it so the condition takes 15:00 pm Close value, either using candle count, or using datetime value.</p>
<p>I know this is a huge favor I'm asking, but I have been trying to get this done for over 2 weeks and couldn't find any help. Lot of tutorials and solutions are on basic, standardized methods. This is a bit unconventional, and so unable to solve it on my own.</p>
",8190201.0,8190201.0,2022-06-19 11:45:55,2022-06-19 19:27:45,Calculating Higher Highs & Lower Lows in Stock Market Dataframe,<python><pandas><data-science><finance><algorithmic-trading>,1,0,N/A,CC BY-SA 4.0
72656719,1,-1.0,2022-06-17 08:52:40,0,156,"<p>Everything is in the title, so basically I have a list of several questions as strings and the idea is to get another list of frequently asked questions within that first list of questions.</p>
<p>I don't know if it'll make sense but I'll try to explain the approach I tried.</p>
<p>The approach consist of calculating the cosine similarity of each element of the list with the rest of the elements not including the element being processed to prevent performing calculations with the same element.</p>
<p>That said, a dictionary will be created containing the keys as the index of each element being processed, while the values will be a list of indexes of each element which has a cosine similarity above the threshold with the index of the key.</p>
<p>Once the dictionary has been created, the keys' indexes with the highest length of list on their values will be considered as being frequent questions, after that you can pick up the top 10 or any number you'd like.</p>
<p>Firstly, a downside is that it takes a lot of time to execute knowing that I've +60k questions (14 days).
Secondly, I don't know if it's the best way to solve this problem, what do you think?
Finally, if you have a more clearer and better idea to solve the problem, I'm all ears, it can also help other people with the same problem.</p>
<pre><code>import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')

list_of_questions = ['How does umap know which high dimensional datapoint belongs to which cluster?',...]

score = dict()
threshold = 0.7
#tokenization

#sw contains the list of stopwords
sw = stopwords.words('english')

for index, main_question in enumerate(list_of_questions):
    similarities = []
    temp_list = list_of_questions.copy()
    X_list = word_tokenize(main_question)
    temp_list.pop(index)
    for question_ in temp_list:
        l1 =[];l2 =[]
        Y_list = word_tokenize(question_)
        
        if len(X_list) == 0 or len(Y_list) == 0:
            continue
        #remove stop words from the string
        X_set = {w for w in X_list if not w in sw} 
        Y_set = {w for w in Y_list if not w in sw}

        #form a set containing keywords of both strings 
        rvector = X_set.union(Y_set) 
        for w in rvector:
            if w in X_set: l1.append(1) # create a vector
            else: l1.append(0)
            if w in Y_set: l2.append(1)
            else: l2.append(0)
        c = 0

        #cosine formula 
        try:
            for i in range(len(rvector)):
                    c+= l1[i]*l2[i]
            cosine = c / float((sum(l1)*sum(l2))**0.5)
            if cosine &gt; threshold:
                similarities.append(list_of_questions.index(question_))
                print(&quot;Cosine similarity: &quot;, cosine)
        except:
            continue
    score[index] = similarities
</code></pre>
",10966818.0,-1.0,N/A,2022-06-19 17:59:14,NLP - How to get the a list of frequently asked questions on a list of questions,<python><machine-learning><deep-learning><nlp><data-science>,1,2,N/A,CC BY-SA 4.0
72680592,1,-1.0,2022-06-19 21:58:36,0,130,"<p>I'm not even sure this is possible but it most certainly is worth asking. This would be a rather simple task in Excel however I'm finding it extremely difficult in Pandas.</p>
<p>I have DF1:</p>
<p>| Date  | Location ID |</p>
<p>| --------   | -------------- |</p>
<p>| DD-MM-YYY   | 1            |</p>
<p>| DD-MM-YYY   | 2            |
(120k Rows Total)</p>
<p>I have DF2:</p>
<p>|Date | Location ID | Location |</p>
<p>|:---- |:------:| -----:|</p>
<p>| DD-MM-YYY  | 1     | India |
(4 Rows Total) - 4 different locations</p>
<p>I want to merge the DFs together on ['Location ID'] and then auto-fill DF1 Location row with all the correct worded locations. So add the column Location to all the 120k rows based upon the Location ID.</p>
<p>Basically
New DF1:
|Date | Location ID | Location |</p>
<p>|:---- |:------:| -----:|</p>
<p>| DD-MM-YYY  | 1     | India |
(120K times)</p>
<p>Thanks in advance. If this is possible that would be great.</p>
",4714014.0,-1.0,N/A,2022-06-19 23:34:30,"Python, Pandas Merge x and y and fill with z from y",<python><pandas><merge><data-science>,1,1,N/A,CC BY-SA 4.0
72675131,1,72676631.0,2022-06-19 07:46:46,0,7071,"<p>I am trying to plot y_train using bar, I am getting the below error. Kindly help me fix it
I am unable to plot this due some error since yesterday.</p>
<pre><code>from sklearn.model_selection import train_test_split
import numpy as np
X = reviews['Text']
y= reviews['Score'].values
X_train, X_test, y_train, y_test =train_test_split(X,y ,test_size=0.20,stratify=y,random_state=33)
</code></pre>
<p>checking the shape of split of data</p>
<pre><code>print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)
(80000,) (80000,)
(20000,) (20000,)
</code></pre>
<p>#plot bar graphs of y_train and y_test</p>
<pre><code>import matplotlib.pyplot as plt
plt.bar([1,0],y_train.value_counts().values,color ='green')

 
plt.xlabel(&quot;Count&quot;)
plt.ylabel(&quot;y_train values&quot;)
plt.title(&quot;Distribution of y_train&quot;)
plt.show()
</code></pre>
<p>error:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-18-62460aedca56&gt; in &lt;module&gt;()
      1 #plot bar graphs of y_train and y_test
      2 import matplotlib.pyplot as plt
----&gt; 3 plt.bar(y_train.value_counts().values,color ='green')
      4 
      5 plt.xlabel(&quot;Count&quot;)

AttributeError: 'numpy.ndarray' object has no attribute 'value_counts'
</code></pre>
",18948317.0,-1.0,N/A,2022-06-19 12:35:23,AttributeError: 'numpy.ndarray' object has no attribute 'value_counts'while plotting bar plot,<numpy><machine-learning><data-science><bar-chart>,1,0,N/A,CC BY-SA 4.0
72675260,1,72675261.0,2022-06-19 08:10:21,0,1199,"<p>The ROUGE metrics were introduced to &quot;automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans&quot; [<a href=""https://aclanthology.org/W04-1013.pdf"" rel=""nofollow noreferrer"">1</a>].</p>
<p>When calculating any ROUGE metric you get an <code>AggregateScore</code> object with 3 parameters: <code>low</code>, <code>mid</code>, <code>high</code>.
How are these aggregate values calculated?</p>
<p>For example, from the huggingface implementation [<a href=""https://huggingface.co/spaces/evaluate-metric/rouge"" rel=""nofollow noreferrer"">2</a>]:</p>
<pre><code>&gt;&gt;&gt; rouge = evaluate.load('rouge')
&gt;&gt;&gt; predictions = [&quot;hello there&quot;, &quot;general kenobi&quot;]
&gt;&gt;&gt; references = [&quot;hello there&quot;, &quot;general kenobi&quot;]
&gt;&gt;&gt; results = rouge.compute(predictions=predictions,
...                         references=references)
&gt;&gt;&gt; print(list(results.keys()))
['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
&gt;&gt;&gt; print(results[&quot;rouge1&quot;])
AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))
&gt;&gt;&gt; print(results[&quot;rouge1&quot;].mid.fmeasure)
1.0
</code></pre>
<hr />
<p>Edit: On July 7th, the huggingface  implementation was simplified to return a cleaner and easier to understand dict: <a href=""https://github.com/huggingface/evaluate/issues/148"" rel=""nofollow noreferrer"">https://github.com/huggingface/evaluate/issues/148</a></p>
",2581506.0,2581506.0,2022-11-25 09:32:55,2023-06-04 11:48:05,"In the ROUGE metrics, what do the low, mid and high values mean?",<machine-learning><nlp><data-science><evaluation><summarization>,1,0,N/A,CC BY-SA 4.0
72681628,1,-1.0,2022-06-20 02:19:00,0,45,"<p>I'm trying to import the database and place the files one under the other</p>
<pre><code>months = {'jan': 1, 'fev':2, 'mar':3, 'abr':4, 'mai':5, 'jun':6, 'jul':7, 'ago':8, 'set':9, 'out':10, 'nov':11, 'dez':12}

base_path = pathlib.Path('dataset')

base_airbnb = pd.DataFrame()

for file in base_path.iterdir():
    month_name = file.name[:3]
    month = months[month_name]
    
    year = file.name[-8:]
    year = int(year.replace('.csv', ''))
    
    df = pd.read_csv(base_path / file.name)
    df['year'] = year
    df['month'] = month
    base_airbnb = base_airbnb.append(df)

display(base_airbnb)
</code></pre>
<blockquote>
<p>is not aligned. A future version of pandas will change to not sort by
default.</p>
<p>To accept the future behavior, pass 'sort=False'.</p>
<p>To retain the current behavior and silence the warning, pass
'sort=True'.</p>
</blockquote>
<p>It appears that I have <code>MemoryError:</code>. But something goes wrong and I don't understand why... I'm sure I'm doing everything correctly in Jupyter Notebook</p>
<blockquote>
<p>To accept the future behavior, pass 'sort=False'.</p>
<p>To retain the current behavior and silence the warning, pass
'sort=True'.</p>
<p>sort=sort,</p>
</blockquote>
<pre><code>C:\Users\CASA\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py:3058: DtypeWarning: Columns (87) have mixed types. Specify dtype option on import or set low_memory=False.   interactivity=interactivity, compiler=compiler, result=result)
--------------------------------------------------------------------------- MemoryError                               Traceback (most recent call last) &lt;ipython-input-7-e9e45fb3206d&gt; in &lt;module&gt;
     15     df['year'] = year
     16     df['month'] = month
---&gt; 17     base_airbnb = base_airbnb.append(df)
     18 
     19 big_data=pd.concat(months, axis=0)

~\Anaconda3\lib\site-packages\pandas\core\frame.py in append(self, other, ignore_index, verify_integrity, sort)    7121             ignore_index=ignore_index,    7122             verify_integrity=verify_integrity,
-&gt; 7123             sort=sort,    7124         )    7125 

~\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    253         verify_integrity=verify_integrity,
    254         copy=copy,
--&gt; 255         sort=sort,
    256     )
    257 

~\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py in
__init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)
    333 
    334             # consolidate
--&gt; 335             obj._consolidate(inplace=True)
    336             ndims.add(obj.ndim)
    337 

~\Anaconda3\lib\site-packages\pandas\core\generic.py in
_consolidate(self, inplace)    5268         inplace = validate_bool_kwarg(inplace, &quot;inplace&quot;)    5269         if inplace:
-&gt; 5270             self._consolidate_inplace()    5271         else:    5272             f = lambda: self._data.consolidate()

~\Anaconda3\lib\site-packages\pandas\core\generic.py in
_consolidate_inplace(self)    5250             self._data = self._data.consolidate()    5251 
-&gt; 5252         self._protect_consolidate(f)    5253     5254     def _consolidate(self, inplace=False):

~\Anaconda3\lib\site-packages\pandas\core\generic.py in
_protect_consolidate(self, f)    5239         &quot;&quot;&quot;    5240         blocks_before = len(self._data.blocks)
-&gt; 5241         result = f()    5242         if len(self._data.blocks) != blocks_before:    5243             self._clear_item_cache()

~\Anaconda3\lib\site-packages\pandas\core\generic.py in f()    5248    5249         def f():
-&gt; 5250             self._data = self._data.consolidate()    5251     5252         self._protect_consolidate(f)

~\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in consolidate(self)
    930         bm = self.__class__(self.blocks, self.axes)
    931         bm._is_consolidated = False
--&gt; 932         bm._consolidate_inplace()
    933         return bm
    934 

~\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in
_consolidate_inplace(self)
    935     def _consolidate_inplace(self):
    936         if not self.is_consolidated():
--&gt; 937             self.blocks = tuple(_consolidate(self.blocks))
    938             self._is_consolidated = True
    939             self._known_consolidated = True

~\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in
_consolidate(blocks)    1911     for (_can_consolidate, dtype), group_blocks in grouper:    1912         merged_blocks =
_merge_blocks(
-&gt; 1913             list(group_blocks), dtype=dtype, _can_consolidate=_can_consolidate    1914         )    1915         new_blocks = _extend_blocks(merged_blocks, new_blocks)

~\Anaconda3\lib\site-packages\pandas\core\internals\blocks.py in
_merge_blocks(blocks, dtype, _can_consolidate)    3318         # combination of those slices is a slice, too.    3319         new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])
-&gt; 3320         new_values = np.vstack([b.values for b in blocks])    3321     3322         argsort = np.argsort(new_mgr_locs)

~\Anaconda3\lib\site-packages\numpy\core\shape_base.py in vstack(tup)
    281     &quot;&quot;&quot;
    282     _warn_for_nonsequence(tup)
--&gt; 283     return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)
    284 
    285 

MemoryError:
</code></pre>
",14144084.0,5014455.0,2022-06-20 19:30:12,2022-06-20 19:30:12,MemoryError on database import in Jupyter Notebook,<python><database><data-science>,0,4,N/A,CC BY-SA 4.0
72701526,1,72841783.0,2022-06-21 13:19:08,0,45,"<p>in my graph we have users that have 2 relation called &quot;favorite&quot; and &quot;seen&quot; with products and product have relation called has with some specification as colors(red blue...) , types(jeans....) and sizes (30...)</p>
<p>so i make some query that when i wanna create favorite or seen relation , it makes a relation with that specific user and the specification of that product calling &quot;weight&quot; and set property for that called &quot;score&quot; and i wanna increase this score every time user that set product to favorite or just seeing that product for example when a user see the product score change to +10 and for favorite change to +20 and then we recommend products with specifications that have most score</p>
<p>my query is</p>
<pre><code>match (user:Users{m_id:&quot;&quot;}),(m:Products{m_id:&quot;&quot;})-[:HAS]-&gt;(a:Specifications)
MERGE (user) -[:FAVORITE]-&gt; (m)
merge (user)-[:WEIGHT{score:0}]-&gt;(a)
</code></pre>
<p>and one more problem with this query is i dont wanna make new relation if i already have it i just wanna increase the score<a href=""https://i.stack.imgur.com/IDS4R.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",19383126.0,-1.0,N/A,2022-07-02 19:07:28,building recommendation system using weight as relation in neo4j,<neo4j><graph-data-science>,1,0,N/A,CC BY-SA 4.0
72690449,1,-1.0,2022-06-20 16:52:13,-3,88,"<p>I joined two data excel sheets together by appending them using python.</p>
<p>I would like to view the list I have appended as rows instead of a list as the actual data I have is quite large.</p>
<p>I would preferably like to move the list to a different excel tab (using python). Where I can view the data in the list as rows instead of a list. is there a python code for this?</p>
<p><strong>Quarter 1</strong><br />
Team    Country     Account PRI 10-k    Date    Name
Family  USA 1002    205 Adjusted    Accumulated 3/31/2022   Jess
Chess   Brazil  1001    109 Translation Comprehensive   4/1/2022    Mike
House   Kenya   1003    303 Cumulation  Income  4/2/2022    Phil
Pool    South Africa    1004    404 Translation Other   4/3/2022    John
Club    Turkey  1005    605 Adjusted    Accumulated 4/4/2022    Nick
Easter  France  1006    808 Translation Other   4/5/2022    Joe
Chair   Greece  1007    707 Key attribute   Comprehensive   4/6/2022    Rick
Road    Italy   1008    303 Cumulation  Income  4/7/2022    Tom
Cross   Ethiopia    1010    405 translation Accumulated 4/8/2022    Jill
Soccer  Cuba    1011    808 adjusted    Other   4/9/2022    Matt
Baseball    Singapore   1012    304 adjusted    Comprehensive   4/10/2022   Tim
Basketball  Qatar   1013    102 Translation Income  4/11/2022   Jane</p>
<p><strong>Quarter 2</strong><br />
Team    Country     Account PRI 10-k    Date    Name
Family  USA 10002   205 Adjusted    Accumulated 3/31/2022   Jess
Chess   Brazil  1001    109 Translation Comprehensive   4/1/2022    Mike
House   Kenya   1003    303 Cumulation  Income  4/2/2022    Phil
Pool    South Africa    1004    404 Translation Other   4/3/2022    John
Club    Turkey  1005    605 Adjusted    Accumulated 4/4/2022    Nick
Easter  France  1006    808 Translation Other   4/5/2022    Joe
Chair   Greece  1007    707 Key attribute   Comprehensive   4/6/2022    Rick
Road    Italy   1008    303 Cumulation  Income  4/7/2022    Tom
Eagle Eye   Netherlands 1015    208 adjustment  Comprehensive   4/12/2022   Chris</p>
<p>This is the code I have so far and the data I was joining.</p>
<p><a href=""https://i.stack.imgur.com/o46FL.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/QsOxE.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/K3lJA.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/nX0kn.png"" rel=""nofollow noreferrer"">enter image description here</a>
<a href=""https://i.stack.imgur.com/yAgyr.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<pre><code>import csv
import os
import sys

path_quarter_1 = os.path.join(sys.path[0], &quot;Quarter_1.csv&quot;)  # path to the csv file
path_quarter_2 = os.path.join(sys.path[0], &quot;Quarter_2.csv&quot;)  # path to the csv file

# coulmns are:- Team, Country, Account, PRI, 10-k, Date, Name

with open(path_quarter_1, &quot;r&quot;) as f:  # opening the csv file
    reader = csv.reader(f)  # creating a reader object
    data_q1 = list(reader)  # converting the reader object to a list for Quarter 1

with open(path_quarter_2, &quot;r&quot;) as f:  # opening the csv file
    reader = csv.reader(f)  # creating a reader object
    data_q2 = list(reader)  # converting the reader object to a list for Quarter 2

# Account that are in both quarters
account_in_both = []  # creating a list to store the accounts that are in both quarters
for i in range(1, len(data_q1)):  # iterating through the list for Quarter 1
    for j in range(1, len(data_q2)):  # iterating through the list for Quarter 2
        if data_q1[i][2] == data_q2[j][2]:  # checking if the accounts are same
            account_in_both.append(data_q1[i][2])  # appending the account to the list
print(&quot;Account in both&quot;, account_in_both)  # printing the list
print(&quot;\n\n&quot;)  # printing a new line

# In Q1 but not in Q2
account_in_q1_not_q2 = (
    []
)  # creating a list to store the accounts that are in Q1 but not in Q2
for i in range(1, len(data_q1)):  # iterating through the list for Quarter 1
    if (
        data_q1[i][2] not in account_in_both
    ):  # checking if the account is not in both quarters
        account_in_q1_not_q2.append(data_q1[i][2])  # appending the account to the list

print(&quot;Account in Q1 but not in Q2&quot;, account_in_q1_not_q2)  # printing the list
print(&quot;\n\n&quot;)  # printing a new line

# In Q2 but not in Q1
account_in_q2_not_q1 = (
    []
)  # creating a list to store the accounts that are in Q2 but not in Q1
for i in range(1, len(data_q2)):  # iterating through the list for Quarter 2
    if (
        data_q2[i][2] not in account_in_both
    ):  # checking if the account is not in both quarters
        account_in_q2_not_q1.append(data_q2[i][2])  # appending the account to the list

print(&quot;Account in Q2 but not in Q1&quot;, account_in_q2_not_q1)  # printing the list
print(&quot;\n\n&quot;)  # printing a new line

# In which both list are same
all_rows_are_same_in_both = (
    []
)  # creating a list to store the rows that are in both quarters
for i in range(1, len(data_q1)):  # iterating through the list for Quarter 1
    for j in range(1, len(data_q2)):  # iterating through the list for Quarter 2
        if data_q1[i] == data_q2[j]:  # checking if the rows are same
            all_rows_are_same_in_both.append(
                data_q1[i]
            )  # appending the row to the list

print(&quot;All rows are same in both&quot;, all_rows_are_same_in_both)  # printing the list
print(&quot;\n\n&quot;)  # printing a new line

# In which rows are in q1 but not in q2
rows_in_q1_not_q2 = []  # creating a list to store the rows that are in Q1 but not in Q2
for i in range(1, len(data_q1)):  # iterating through the list for Quarter 1
    if (
        data_q1[i] not in all_rows_are_same_in_both
    ):  # checking if the row is not in both quarters
        rows_in_q1_not_q2.append(data_q1[i])  # appending the row to the list

print(&quot;Rows in Q1 but not in Q2&quot;, rows_in_q1_not_q2)  # printing the list
print(&quot;\n\n&quot;)  # printing a new line

# In which rows are in q2 but not in q1
rows_in_q2_not_q1 = []  # creating a list to store the rows that are in Q2 but not in Q1
for i in range(1, len(data_q2)):  # iterating through the list for Quarter 2
    if (
        data_q2[i] not in all_rows_are_same_in_both
    ):  # checking if the row is not in both quarters
        rows_in_q2_not_q1.append(data_q2[i])  # appending the row to the list

print(&quot;Rows in Q2 but not in Q1&quot;, rows_in_q2_not_q1)  # printing the list
print(&quot;\n\n&quot;)  # printing a new line
</code></pre>
",19169071.0,19169071.0,2022-06-20 20:39:40,2022-06-22 14:13:04,Changing list to rows in python,<python><python-3.x><data-science><computer-science>,1,4,N/A,CC BY-SA 4.0
72705022,1,-1.0,2022-06-21 17:40:17,0,48,"<pre><code>print(Katalog_final.apply(lambda row: row[Katalog_final['PROGRAM (robna grupa)'].isin(['Alkoholna pića','Bezalkoholna pića','Prerada voća povrća i med'])]))
</code></pre>
<p>This is Form which i want to save in excel delete others form</p>
<pre><code>78       Banjalucka pivara doo Banjaluka        743840             PIVO NEKTAR 0,5l LIM. 10+2 GRATIS  3.877002e+12        Alkoholna pića
179  Molson Coors BH d.o.o.                      305747           Pivo 0,5 l limenka 4/1 Staropramen   8.600105e+12        Alkoholna pića
180       Banjalucka pivara doo Banjaluka        208470                    Pivo 0,33l Six Pack Nektar  3.871137e+12        Alkoholna pića
..                                    ...           ...                                           ...           ...                   ...
322  Venera doo                                  309771           Vino Spricer Bijeli 4,5% 0,5l Rubin  8.600074e+12        Alkoholna pića
323  Venera doo                                  309770       Vino Spricer Rose 4,5% alc. 0,5 l Rubin  8.600074e+12        Alkoholna pića
324                     Mladost trade doo        105193         Vino Vranac Pro Corde 0,75 l Plantaže  3.899003e+12        Alkoholna pića
325                     Mladost trade doo        105138                    Vino Vranac 0,75l Plantaže  8.600143e+12        Alkoholna pića
326                     Mladost trade doo        105142                Vino Chardonay 0,75 l Platnaže  8.600143e+12        Alkoholna pića
</code></pre>
",15944736.0,18189622.0,2022-06-22 07:30:19,2022-06-22 07:30:19,When i filter dataset in Python how to save filtered data in excel,<python><pandas><dataframe><data-science>,0,2,N/A,CC BY-SA 4.0
72706415,1,-1.0,2022-06-21 19:52:54,0,265,"<p>I have pre build machine learning model (saved as pickle file) to predict classification.</p>
<p>My question is when I use new dataset to predict using Pickle file do I need do all preprocessing steps (like transformation and encoding) to the new testing dataset or can I use raw data set.</p>
",17065784.0,-1.0,N/A,2022-06-23 04:01:08,Predict a data using Pickle file,<python><machine-learning><data-science><pickle><data-science-experience>,1,1,N/A,CC BY-SA 4.0
72706589,1,-1.0,2022-06-21 20:11:36,1,184,"<p>I am trying to perform mutual information regression on the Kaggle Houses dataset. However, I get an error when I run mutual_info_regression. I am getting the error:
<code> ValueError: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.</code></p>
<pre><code>X = home_data
X['LotFrontage'] = X['LotFrontage'].fillna(0)
X['MasVnrArea'] = X['MasVnrArea'].fillna(0)
X['GarageYrBlt'] = X['GarageYrBlt'].fillna(X['GarageYrBlt'].median())

X['SalePrice'].dropna(inplace=True)

y = X.pop('SalePrice')

for colname in X.select_dtypes(&quot;object&quot;):
    X[colname], _ = X[colname].factorize()
    
discrete_features = X.dtypes == int

from sklearn.feature_selection import mutual_info_regression

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name=&quot;MI Scores&quot;, index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

mi_scores = make_mi_scores(X, y, discrete_features)
</code></pre>
<p>I've checked the shape of X, y, and discrete_features and they are of shape (1460, 80), (80,), and (1460,) respectively. Any help would be greatly appreciated.</p>
",19385896.0,-1.0,N/A,2022-06-21 20:11:36,"ValueError: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required. While performing mutual information regression",<python><pandas><data-science><mutual-information>,0,0,N/A,CC BY-SA 4.0
72706598,1,-1.0,2022-06-21 20:12:15,0,43,"<p>I'm watching some data science education videos and I am studing list dict comprehensions. As you can see at the first pic, the teacher just executed that <code>df.columns</code> part and the output has shown at the console but when I tried the same thing console didn't show to me directly as I did the same things as the video. Why I can't see the result of <code>df.columns</code> as in video?</p>
<p>First pic from video.education
<img src=""https://i.stack.imgur.com/OR57b.png"" alt=""enter image description here"" /></p>
<p>The second pic from my work <img src=""https://i.stack.imgur.com/s31hI.png"" alt=""enter image description here"" /></p>
",19385911.0,8893686.0,2022-06-23 21:31:52,2022-06-23 21:37:13,Seaburn dataset shows a different output as expected,<python><machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
72707109,1,72710711.0,2022-06-21 21:04:27,2,624,"<p>I'm trying to plot a decision tree but I get this error:</p>
<pre><code>'Pipeline' object has no attribute 'tree_' 
</code></pre>
<p>At first I build my model from a preprocessor (data types <code>int</code> and <code>object</code>):</p>
<pre><code>preprocessor = ColumnTransformer([
    ('one-hot-encoder', categorical_preprocessor, categorical_columns),
    ('standard_scaler', numerical_preprocessor, numerical_columns)])

model3 = make_pipeline(preprocessor, DecisionTreeClassifier())
</code></pre>
<p><a href=""https://i.stack.imgur.com/Lzq6A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lzq6A.png"" alt=""My pipeline"" /></a></p>
<p>Then I fit the model and generate the predictions:</p>
<pre><code>model3 = model3.fit(data_train, target_train)
y_pred3 = model3.predict(data_test)
</code></pre>
<p>After that I try to plot the tree:</p>
<pre><code>tree.plot_tree(model3)
</code></pre>
<p>but I get the error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_22012/3111274197.py in &lt;module&gt;
----&gt; 1 tree.plot_tree(model3)

~\anaconda3\lib\site-packages\sklearn\tree\_export.py in plot_tree(decision_tree, max_depth, feature_names, class_names, label, filled, impurity, node_ids, proportion, rounded, precision, ax, fontsize)
    193         fontsize=fontsize,
    194     )
--&gt; 195     return exporter.export(decision_tree, ax=ax)
    196 
    197 

~\anaconda3\lib\site-packages\sklearn\tree\_export.py in export(self, decision_tree, ax)
    654         ax.clear()
    655         ax.set_axis_off()
--&gt; 656         my_tree = self._make_tree(0, decision_tree.tree_, decision_tree.criterion)
    657         draw_tree = buchheim(my_tree)
    658 

AttributeError: 'Pipeline' object has no attribute 'tree_'
</code></pre>
<p>How can I plot my tree? Or is this impossible because I use a pipeline?</p>
",17789308.0,11989081.0,2022-06-22 04:42:47,2022-06-22 06:35:22,Error when plotting decision tree from pipeline,<python><machine-learning><scikit-learn><data-science>,1,2,N/A,CC BY-SA 4.0
72722355,1,-1.0,2022-06-22 21:46:54,0,118,"<p>How can I load a list of tensorflow datasets without using eager execution?</p>
<p>I have a list of filepaths that point to saved tensorflow datasets. I also have the element_specs for each path. I want to load each dataset and interleave the result, but I am unable to loop through the element specs.</p>
<p>Here is what I have tried so far:</p>
<pre><code>import tensorflow as tf

# Create data1 and save
data1 = tf.data.Dataset.from_tensor_slices(([3, 4], [0, 1]))
print(list(data1.as_numpy_iterator()))
data1.save(path='1')

# Create data2 and save
data2 = tf.data.Dataset.from_tensor_slices(([5, 6], [2, 3]))
print(list(data2.as_numpy_iterator()))
data2.save(path='2')

# Create dataset
dataset = tf.data.Dataset.from_tensor_slices(['1', '2'])

# Create dict of element specs
elem_specs = {0: data1.element_spec, 1: data2.element_spec}

# Enumerate the dataset
dataset = dataset.enumerate()

# I can loop through and load using eager execution
for i, ds in dataset:
    result = tf.data.Dataset.load(ds, element_spec=elem_specs[i.numpy()])

# But if I try with map or interleave, it errors
result = dataset.interleave(lambda i, item: tf.data.Dataset.load(item, element_spec=elem_specs[i]))

# TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.

</code></pre>
<p>Thank you for your help.</p>
",9909857.0,-1.0,N/A,2022-06-22 21:46:54,How can I load tensorflow datasets without using eager execution?,<python><tensorflow><data-science>,0,3,N/A,CC BY-SA 4.0
72740480,1,72740579.0,2022-06-24 07:33:05,-3,40,"<p>Given <code>df1</code></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">id1</th>
<th style=""text-align: right;"">id2</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: right;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: right;"">1</td>
</tr>
</tbody>
</table>
</div>
<p>and <code>df2</code></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">id3</th>
<th style=""text-align: right;"">id4</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: right;"">a</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: right;"">b</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: right;"">c</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: right;"">d</td>
</tr>
</tbody>
</table>
</div>
<p>I want the results of <code>df1</code> to be as follows:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">id1</th>
<th style=""text-align: right;"">id2</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">a</td>
<td style=""text-align: right;"">c</td>
</tr>
<tr>
<td style=""text-align: left;"">d</td>
<td style=""text-align: right;"">b</td>
</tr>
<tr>
<td style=""text-align: left;"">b</td>
<td style=""text-align: right;"">a</td>
</tr>
</tbody>
</table>
</div>
<p>How can I accomplish this?</p>
",13579420.0,17769815.0,2022-06-25 03:29:46,2022-06-25 03:29:46,How to map multiple values from 1 dataset to another using Python?,<python><data-science>,1,1,N/A,CC BY-SA 4.0
72732021,1,-1.0,2022-06-23 14:36:19,0,59,"<p>I have a set of data like these:
<a href=""https://i.stack.imgur.com/Ufoe7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ufoe7.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Or0Yu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Or0Yu.png"" alt=""enter image description here"" /></a></p>
<p>And I want to recognize this 2 kind of corner shape, is there any way? I wrote s snippet but it sucks.
In this way I am trying to find desc corner (from top to down) and to find asc corner I am &quot;rotating&quot; the dataset and I apply the same algorithm for desc corners.</p>
<pre><code>...    
n_rows = len(matrix)
edge_closing_left_total = 0
j=0
for i in range(n_rows):
    edge_closing_left = True
    current_value = matrix[i][j]
    if current_value &gt;= 230:
        current_row = i
        start = j+1
        end = j+1+3
        for k in range(start, end):
            if current_row+2 &lt; n_rows:
                if matrix[current_row][k]&lt;230 and matrix[current_row+1][k]&lt;230 or matrix[current_row+1][k]&lt;230 and matrix[current_row+2][k]&lt;=230:
                    current_row+=1
                else:
                    edge_closing_left = False
                    break
        if edge_closing_left:
            edge_closing_left_total+=1
return edge_closing_left_total
</code></pre>
<p><a href=""https://file.io/coyqt6hvLDdY"" rel=""nofollow noreferrer"">Here</a> the csv dataset file.</p>
",16966945.0,16966945.0,2022-06-23 14:46:13,2022-06-24 11:15:19,Find corner pattern in data set,<python><pandas><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
72758004,1,-1.0,2022-06-25 23:40:25,1,516,"<p>I'm trying to extract information from certain rows in this big dataframe.</p>
<p>When I do use slicing to subset the table (e.g. <code>blast_output_scored.iloc[10:11,:]</code>), The output looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>qseqid</th>
<th>sseqid</th>
<th>%_identity</th>
<th>alignment_length</th>
<th>mismatch</th>
<th>gapopen</th>
<th>qstart</th>
<th>qend</th>
<th>sstart</th>
<th>send</th>
<th>evalue</th>
<th>bitscore</th>
<th>subject_strand</th>
<th>line_in_og_BLAST</th>
<th>Needle_score</th>
</tr>
</thead>
<tbody>
<tr>
<td>IDgene.1</td>
<td>1</td>
<td>100.0</td>
<td>1073</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1073</td>
<td>7704</td>
<td>6632</td>
<td>0.0</td>
<td>1982.0</td>
<td>minus</td>
<td>10</td>
<td>5360.0</td>
</tr>
</tbody>
</table>
</div>
<p>When I go to check the number of rows in this table, I get the correct number with slicing</p>
<pre><code>len(blast_output_scored.iloc[10:11,:].index)

#Output
1
</code></pre>
<p>However when I use indexing (<code>blast_output_scored.iloc[10,:]</code>), the output looks completely different, even if the index is the same range as the slice.</p>
<pre><code>sseqid                   1
%_identity           100.0
alignment_length      1073
mismatch                 0
gapopen                  0
qstart                   1
qend                  1073
sstart                7704
send                  6632
evalue                 0.0
bitscore            1982.0
subject_strand       minus
line_in_og_BLAST        10
Needle_score        5360.0
Name: IDgene.1, dtype: object
</code></pre>
<p>The number of rows in the table now doesn't seems to change to the number of columns - the first column (the first column is also set for indexing rows by names)</p>
<pre><code>len(blast_output_scored.iloc[10,:].index)

#Output
14
</code></pre>
<p>My biggest problem is that I'm using the column names to index and I have to check which names subset tables to a length of 1, so I can't just use the splicing method to bypass this.</p>
<p>e.g. <code>blast_output_scored.loc[&quot;IDgene.1&quot;]</code> outputs</p>
<pre><code>sseqid                   1
%_identity           100.0
alignment_length      1073
mismatch                 0
gapopen                  0
qstart                   1
qend                  1073
sstart                7704
send                  6632
evalue                 0.0
bitscore            1982.0
subject_strand       minus
line_in_og_BLAST        10
Needle_score        5360.0
Name: IDgene.1, dtype: object
</code></pre>
<p>and will say I have 14 rows when I should only have 1.</p>
<p>Is there any way to ensure the output looks like the slicing output in pandas?</p>
",15964560.0,15964560.0,2022-06-26 00:20:59,2022-06-26 00:20:59,Why does indexing and slicing seem to change the way the dataframe looks in pandas,<python><pandas><dataframe><data-structures><data-science>,2,0,N/A,CC BY-SA 4.0
72747814,1,-1.0,2022-06-24 18:02:16,2,129,"<p>I hope you can help me here. I am working on creating a small environment at home for Data Science. I am having trouble understanding how to create the orchestration layer properly (I am also not convinced that the other components of the architecture I have selected are the most appropriated). If anyone has some experience with any of this components and can give me some recommendations I would appreciate greatly.</p>
<p>I am using old computers and laptops to create the environment (cheaper than using the cloud), some of them with NVIDIA GPUs. So here is the architecture I have in mind.</p>
<ul>
<li>For the underlaying infrastructure, I am using <strong>Docker</strong> with Docker Swarm.</li>
<li>I have 3 layers of storage. SSD for hot data (on 1 of the servers), several normal Drives of each different PC joined through <strong>GlusterFS</strong> for the database data, and an NFS Volume from my NAS for Archival.</li>
<li>I have a Container already with a GPU Version of <strong>JupyterLab</strong> (potentially for using tensorflow or pytorch) for development purposes.</li>
<li>Another container with <strong>GitLab</strong> for Version Control/CI</li>
<li>Another container with <strong>Apache NIFI</strong> for Real Time Data Ingestion. I am thinking of using also <strong>Kafka</strong> for better managing the stream data asynchronously (data comes from a websocket)</li>
<li><strong>Apache Druid</strong> as the database for the data</li>
</ul>
<p>So, here it comes my question: Assuming I develop an algorithm that requires training, and I need to orchestrate a re-training from time to time of the model. How do I perform the retraining automatically? I know I can use nifi (I could use alternatively apache airflow), but the re-training needs to be executed on a GPU-docker container. Can I just simply prepare a docker container with gpu and python and somehow tell Nifi (or airflow) that it needs to execute the operations on that container (I don't even know if is possible to do that).</p>
<p>Another question is, for performing operations on real-time as the data lands. Will using kafka and druid suffice, or should I think of using Spark Streaming? I am looking into executing transformations of data, passing the data through the models, etc. Also potentially sending POST commands to an API depending on the data results.</p>
<p>I am used to work only on development environment (Jupyter), so when it comes to putting things on production, I have lots of gaps on how things work. Hence the purpose of this is to practice how different components work together and practice different technologies (Nifi, Kafka, Druid, etc).</p>
<p>I hope you can help me.</p>
<p>Thanks in advance.</p>
",2879368.0,-1.0,N/A,2022-12-27 09:50:15,Building a Production-grade Data Science environment at home - Questions around orchestration,<docker><data-science><apache-nifi><production-environment><data-oriented-design>,1,2,N/A,CC BY-SA 4.0
72762282,1,-1.0,2022-06-26 14:08:25,0,235,"<p>I've got the following set of data :
<a href=""https://i.stack.imgur.com/HOA6z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HOA6z.png"" alt=""enter image description here"" /></a></p>
<p>As you can see, I've got two plateau at 1987 and 3757. The problem is I can't find a good algorithm in order to extract them.
I've already tried this <a href=""https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data"">algo</a>, without any success.</p>
<p>Any help will be welcomed.</p>
<p>Regards</p>
",16271617.0,16582.0,2022-06-26 15:36:03,2022-06-26 15:36:03,Find a algorithm to identify a plateau,<algorithm><time-series><data-science><measure><rtl-sdr>,0,3,N/A,CC BY-SA 4.0
72764454,1,72764579.0,2022-06-26 19:11:39,0,45,"<p>I have data set of over 100,000 rows and 10 columns. these 10 columns should have numeric values but 1% contents in these 10 columns are alpha and alphanumeric.</p>
<p>how do I use FOR loop or any faster method/function to change the values of all alpha and alphanumeric cells to mean of each column or to any numeric values?</p>
<p>e.g.  column a b c &amp; d</p>
<p>a   b   c   d</p>
<p>1   2   5   f5</p>
<p>5   e5  9   6</p>
<p>tg  56  8   r5</p>
<p>q2  4   75   g</p>
<p>above dataset is just an example.</p>
<p>I am looking for any solution you may have.</p>
",19421508.0,7329832.0,2022-06-26 19:33:31,2022-06-26 19:33:31,How do I change values of several cells in a data frame based on cell data type using for loop or any faster method./function?,<python><jupyter-notebook><data-science>,1,1,N/A,CC BY-SA 4.0
72675020,1,-1.0,2022-06-19 07:29:19,1,303,"<p>I was going to extract data from google analytics with Python and google analytics report api v4, in the middle I need the Google analytics Dimensions and matrix there for a specific purpose(so that I can give user to select for which dimensions and matrix he need data).
Also the Google analytics Dimensions and matrix has some valid combinations too. So is there any available REST API or library for that to collect all the dimensions and matrix from google analytics?
(Acknowledge: There exists Python library for Service account but there is no library for web with python).
I am done with authentication part with auth2. Now I need those available dimensions and matrix for the next progress.
References: 1.https://ga-dev-tools.web.app/dimensions-metrics-explorer/
2.https://developers.google.com/analytics/devguides/reporting/core/v4</p>
",7081101.0,-1.0,N/A,2022-06-20 14:35:16,How to extract google analytics dimensions and metrics with python?,<python><google-analytics><data-science><data-extraction><data-pipeline>,1,4,N/A,CC BY-SA 4.0
72707601,1,-1.0,2022-06-21 22:06:27,1,36,"<h1>My set up and minimal example</h1>
<p>I have this set up:</p>
<ul>
<li>SomeProject/
<ul>
<li>bin/
<ul>
<li><code>someMinimalScript.R</code>
<ul>
<li>utils/
<ul>
<li>R/
<ul>
<li><code>someMinimalModule.R</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Contents of <code>someMinimalScript.R</code>:</p>
<pre class=""lang-r prettyprint-override""><code>#!/usr/bin/env Rscript

box::use(./utils/R/someMinimalModule[ someClass ])
</code></pre>
<p>This is importing a module who's contents are <code>someMinimalModule.R</code>:</p>
<pre class=""lang-r prettyprint-override""><code>#' @export
someClass &lt;- setClass(
    &quot;someClass&quot;,
    slots = list(
        example_slot = &quot;character&quot;
    ),
    prototype = list(
        example_slot = character()
    )
)

#' @export
setMethod(&quot;show&quot;, &quot;someClass&quot;, \(object) {
    cat(&quot;\n&quot;)
    cat(&quot;Example slot:&quot;, object@example_slot, &quot;\n&quot;)
    cat(&quot;\n&quot;)
})
</code></pre>
<p>This is a set of tools which is designed to be used from the system command line (CLI tools). This is achieved by exporting the path to the tools in a <code>.bashrc</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>export PATH=&quot;/Users/someUser/someProject/bin:$PATH&quot;
</code></pre>
<p>We need to also do this in the project: <code>chmod 755 ./bin/*</code>.</p>
<h1>My errors</h1>
<p>Once all of this is set up, I am getting this error when calling this tool from the command line:</p>
<pre class=""lang-bash prettyprint-override""><code>someMinimalScript.R
</code></pre>
<pre><code>Error in box::use(./utils/R/someMinimalModule[someClass]) : 
  could not find function &quot;setClass&quot;
(inside “setClass(&quot;someClass&quot;, slots = list(example_slot = &quot;character&quot;), ”
“    prototype = list(example_slot = character()))”)
Calls: &lt;Anonymous&gt; ... tryCatchList -&gt; tryCatchOne -&gt; &lt;Anonymous&gt; -&gt; rethrow -&gt; throw
Execution halted
</code></pre>
<p>First question, why am I getting this above error? It makes no sense, the <code>methods</code> package is part of <code>R</code> or it's included in <code>base</code>. I solved this by appending <code>methods::</code> to my function calls:</p>
<pre class=""lang-r prettyprint-override""><code>#' @export
someClass &lt;- methods::setClass(
    &quot;someClass&quot;,
    slots = list(
        example_slot = &quot;character&quot;
    ),
    prototype = list(
        example_slot = character()
    )
)

#' @export
methods::setMethod(&quot;show&quot;, &quot;someClass&quot;, \(object) {
    cat(&quot;\n&quot;)
    cat(&quot;Example slot:&quot;, object@example_slot, &quot;\n&quot;)
    cat(&quot;\n&quot;)
})
</code></pre>
<p>But now I get this error; I would appreciate any insight anyone could extend to get this resolved:</p>
<pre class=""lang-bash prettyprint-override""><code>someMinimalScript.R
</code></pre>
<pre><code>Error in box::use(./utils/R/someMinimalModule[someClass]) : 
  name '.cacheOnAssign' not found in 'env'
(inside “env$.cacheOnAssign”)
Calls: &lt;Anonymous&gt; ... tryCatchList -&gt; tryCatchOne -&gt; &lt;Anonymous&gt; -&gt; rethrow -&gt; throw
Execution halted
</code></pre>
<h3>R version</h3>
<pre><code>sessionInfo()
R version 4.2.0 (2022-04-22)
Platform: aarch64-apple-darwin21.3.0 (64-bit)
Running under: macOS Monterey 12.2.1

Matrix products: default
BLAS:   /opt/homebrew/Cellar/openblas/0.3.20/lib/libopenblasp-r0.3.20.dylib
LAPACK: /opt/homebrew/Cellar/r/4.2.0/lib/R/lib/libRlapack.dylib

locale:
[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] compiler_4.2.0    data.table_1.14.2
</code></pre>
<h3>‘box’ version</h3>
<p>‘1.1.2’</p>
<hr />
<p>Note this error also occurs when trying to run code interactively. It seems that <code>box</code> does not play nice with <code>S4</code>.</p>
<p>Module:</p>
<pre class=""lang-r prettyprint-override""><code>#' @export
someClass &lt;- methods::setClass(
    &quot;someClass&quot;,
    slots = list(
        example_slot = &quot;character&quot;
    ),
    prototype = list(
        example_slot = character()
    )
)

#' @export
methods::setMethod(&quot;show&quot;, &quot;someClass&quot;, \(object) {
    cat(&quot;\n&quot;)
    cat(&quot;Example slot:&quot;, object@example_slot, &quot;\n&quot;)
    cat(&quot;\n&quot;)
})
</code></pre>
<pre class=""lang-r prettyprint-override""><code>box::use(./utils/R/someModule)
</code></pre>
<pre><code>Error in box::use(./utils/R/someModule) : 
  name '.cacheOnAssign' not found in 'env'
(inside “env$.cacheOnAssign”)
</code></pre>
",14410815.0,14410815.0,2022-06-22 03:07:44,2022-06-22 03:07:44,Cannot use S4 and box in executable R script: name '.cacheOnAssign' not found in 'env',<r><data-science>,0,0,N/A,CC BY-SA 4.0
72715504,1,-1.0,2022-06-22 12:27:53,0,324,"<p>I am new to R and having trouble trying to execute my plan.</p>
<p>I am trying to add another column to my data frame with the values coming from the rows below. However, each row must come from a different number of rows below.</p>
<p>I have 6-7 age cohorts and temperature values for multiple years. Cohort 1 will have that year's temperature value, cohort 2 will have the year prior, cohort 3 will have the temperature value 2 years prior, etc.</p>
<p>This is the data in a wide format. <a href=""https://i.stack.imgur.com/ty2D1.png"" rel=""nofollow noreferrer"">Wide format data</a>. I will need the final output in a long format, such as <a href=""https://i.stack.imgur.com/T2hMN.png"" rel=""nofollow noreferrer"">long format</a></p>
<p>This is my current workings</p>
<pre><code>    long_density %&gt;% 
  mutate( Summer_prior_2 = Summer_max_prior) %&gt;% 
  mutate(Summer_prior_2 = c(Summer_max_prior[-1]))
  select(Year, Cohort, Density, Summer_max_prior, Summer_prior_2, Winter_min_post, Summer_max_post) %&gt;%
  View
  
long_density %&gt;% 
  mutate( Summer_prior_2 = Summer_max_prior) %&gt;% 
  mutate(Summer_prior_2 = ifelse(&quot;Cohort&quot; == &quot;Cohort_1&quot;,  Summer_prior_2,
                                   if_else(&quot;Cohort&quot; == &quot;Cohort_2&quot;,Summer_prior_2[-7], NA))) %&gt;% 
  View
</code></pre>
<p>I was thinking a &quot;ifelse&quot; code where cohort_1 = Cohort_1, Cohort_2= Cohort_2 - 7, Cohort_3= Cohort_3 - 14, Cohort_4= Cohort_4 - 21, etc.. As there are 7 cohorts in in each year the value I want is increasing by 7 for each cohort.</p>
<p><strong>EDIT</strong></p>
<p>Sorry, my original question wasn't too clear.</p>
<p>I know how to pivot_longer. It's mutating the temperature data depending on the cohort that I am having difficulties with.</p>
<p>Here is an example data frame:</p>
<pre><code> df &lt;- data.frame (Year&lt;- as.numeric (c(&quot;2021&quot;,&quot;2020&quot;,&quot;2019&quot;,&quot;2018&quot;,&quot;2017&quot;)),
              Cohort_1 &lt;- as.numeric (c(&quot;12&quot;, &quot;13&quot;, &quot;12&quot;, &quot;14&quot;, &quot;20&quot;)),
              Cohort_2 &lt;- as.numeric (c(&quot;23&quot;, &quot;22&quot;, &quot;23&quot;, &quot;26&quot;, &quot;29&quot;)),
              Cohort_3 &lt;- as.numeric (c(&quot;32&quot;, &quot;32&quot;, &quot;40&quot;, &quot;35&quot;, &quot;34&quot;)),
              Cohort_4 &lt;- as.numeric (c(&quot;44&quot;, &quot;43&quot;, &quot;40&quot;, &quot;49&quot;, &quot;46&quot;)),
              Cohort_5 &lt;- as.numeric (c(&quot;56&quot;, &quot;49&quot;, &quot;41&quot;, &quot;50&quot;, &quot;55&quot;)),
              Cohort_6 &lt;- as.numeric (c(&quot;66&quot;, &quot;61&quot;, &quot;62&quot;, &quot;69&quot;, &quot;68&quot;)),
              Cohort_7 &lt;- as.numeric (c(&quot;77&quot;, &quot;90&quot;, &quot;82&quot;, &quot;84&quot;, &quot;79&quot;)),
              Summer_max_prior &lt;- as.numeric (c(&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;6&quot;,&quot;4&quot;)),
              Winter_min_post &lt;- as.numeric (c(&quot;1&quot;,&quot;2&quot;,&quot;0&quot;,&quot;3&quot;,&quot;1&quot;)))
&gt; df
  Year Cohort_1 Cohort_2 Cohort_3 Cohort_4 Cohort_5 Cohort_6 Cohort_7 Summer_max Winter_min
1 2021       12       23       32       44       56       66       77          2          1
2 2020       13       22       32       43       49       61       90          3          2
3 2019       12       23       40       40       41       62       82          4          0
4 2018       14       26       35       49       50       69       84          6          3
5 2017       20       29       34       46       55       68       79          4          1
</code></pre>
<p>This is the pivot_longer code.</p>
<pre><code>long_density &lt;- pivot_longer(df,cols=c(&quot;Cohort_1&quot;,&quot;Cohort_2&quot;,&quot;Cohort_3&quot;,&quot;Cohort_4&quot;,&quot;Cohort_5&quot;,&quot;Cohort_6&quot;,&quot;Cohort_7&quot;), names_to= &quot;Cohort&quot;,
                 values_to= &quot;Density&quot;)

&gt; long_density %&gt;% select(Year, Cohort, Density, Summer_max, Winter_min)

    Year Cohort   Density Summer_max Winter_min
   
 1  2021 Cohort_1      12          2          1
 2  2021 Cohort_2      23          2          1
 3  2021 Cohort_3      32          2          1
 4  2021 Cohort_4      44          2          1
 5  2021 Cohort_5      56          2          1
 6  2021 Cohort_6      66          2          1
 7  2021 Cohort_7      77          2          1
 8  2020 Cohort_1      13          3          2
 9  2020 Cohort_2      22          3          2
10  2020 Cohort_3      32          3          2
</code></pre>
<p>I need both Summer_max and Winter_min data for cohort 2 (2-year-olds) to be from the year before, cohort 3 (3-year-olds) to be 3 years before, and so on. I need the corresponding temperatures for the year that the cohort of fish spawned.</p>
<p>At the moment all values for each cohort within each year are the same. This is the code I have written so far but r does not like my ifelse code.</p>
<pre><code>long_density %&gt;% 
      mutate(Summer_prior_2 = Summer_max_prior) %&gt;% 
      mutate(Summer_prior_2 = ifelse(long_density$Cohort == &quot;Cohort_1&quot;,  Summer_max_prior,
                                       if_else(long_density$Cohort == &quot;Cohort_2&quot;,Summer_max_prior[-7], 
                                                if_else(long_density$Cohort == &quot;Cohort_3&quot;,Summer_max_prior[-14], 
                                                         if_else(long_density$Cohort == &quot;Cohort_4&quot;,Summer_max_prior[-21], 
                                                                  if_else(long_density$Cohort == &quot;Cohort_5&quot;,Summer_max_prior[-28], 
                                                                           if_else(long_density$Cohort == &quot;Cohort_6&quot;,Summer_max_prior[-35], 
                                                                                    if_else(long_density$Cohort == &quot;Cohort_7&quot;,Summer_max_prior[-42], NA)))))))) 
</code></pre>
<p>Any help would be greatly appreciated!</p>
",19390955.0,19390955.0,2022-06-24 03:46:42,2022-06-25 08:43:52,"Adding a new column from values of rows below, with the row required alternating depending on a variable",<r><dataframe><data-science><data-manipulation>,1,2,N/A,CC BY-SA 4.0
72742435,1,72742532.0,2022-06-24 10:10:55,1,622,"<p>Let's say that I have a df1 like (there are more columns but only this one is relatable):</p>
<pre><code>A
a1
a2
a3
</code></pre>
<p>and a df2 like:</p>
<pre><code>A
a1
a3
a4
a7
</code></pre>
<p>The case is that df2 contains in column A (column names are the same both in df1 and df2) some of the values in df1, but not all of them. Now, I'd like to add a column &quot;Found in df2?&quot; to a df1, representing if the value was found or not. Example:</p>
<pre><code>df1
A  Found in df2?
a1       Y
a2       N
a3       Y
</code></pre>
<p>I've tried np.where and some merging magic but couldn't wrap my head around this.</p>
",10873155.0,-1.0,N/A,2022-06-25 09:47:46,Pandas map between two dataframes into column,<python><pandas><data-science><data-manipulation>,3,0,N/A,CC BY-SA 4.0
72765056,1,72765092.0,2022-06-26 20:52:47,1,43,"<p>I enter this:</p>
<pre><code>test2 = [1, 2, 3, 4]
test3 = pd.Series(test2)

print(test3)
print(test3[3])
print(test3[test3[2]])
</code></pre>
<p>And get this:</p>
<pre><code>0  1
1  2
2  3
3  4
type: int64
4
4
</code></pre>
<p>Basically I'm trying to understand what's happening to the index numbering. Why does a row effectively get cut, or is the index-search-result moved to the next lower row, when you list the series name twice? (as evidenced by selecting different index numbers on what appears to be the same series of unique values, but getting the same answer)</p>
",19176762.0,-1.0,N/A,2022-06-26 21:04:28,Why does the index numbering of a series change when you write the name of the series twice (bracketing it the second time)?,<python><pandas><syntax><data-science>,2,0,N/A,CC BY-SA 4.0
72772472,1,72848546.0,2022-06-27 12:55:42,1,61,"<p>I have four dfs</p>
<pre><code>dfB = pd.DataFrame([[cheapest_brandB[0],wertBereichB]], columns=['brand', 'price'], index= 
       ['cheap'])
dfC = pd.DataFrame([[cheapest_brandC[0],wertBereichC]], columns=['brand', 'price'], index= 
      ['cheap'])
dfG = pd.DataFrame([[cheapest_brandG[0],wertBereichG]], columns=['brand', 'price'], index= 
      ['cheap'])
dfO = pd.DataFrame([[cheapest_brandO[0],wertBereichO]], columns=['brand', 'price'], index= 
      ['cheap'])

</code></pre>
<p>the result  :</p>
<pre><code>
        brand                             price
cheap  ASUS                {'gte': 821.84, 'lte': 1200.91}

        brand                            price
cheap    HP                {'gte': 187.82, 'lte': 993.73}

        brand                            price
cheap  Google              {'gte': 1047.3, 'lte': 2093.59}

        brand                            price
cheap   MSI                {'gte': 1047.3, 'lte': 2093.59}
</code></pre>
<p>and I want to make 3d df so that each of them belongs to a specific index
something like that</p>
<pre><code>                 Gaming                                         Casual                      ....

        brand             price                     brand           price
cheap   ASUS    {'gte': 821.84, 'lte': 1200.91}     HP      {'gte': 187.82, 'lte': 993.73}    ....
light   ..                 ..                       ..                 ..

  
</code></pre>
",17901780.0,-1.0,N/A,2022-07-03 17:23:24,dataframe mergen and make 3d dataframe,<python><pandas><dataframe><3d><data-science>,1,1,N/A,CC BY-SA 4.0
72775333,1,-1.0,2022-06-27 16:20:38,0,129,"<p>I have been trying to impute missing values in my dataset using datawig library. However when I use datawig library to impute the missing values in my dataset. It imputes each and every other column while leaving behind two columns. Both of the columns are of dtype: object. However, it imputes other object columns. Here is the visualization of my dataset:</p>
<pre><code>df.tail(155)
</code></pre>
<p><a href=""https://i.stack.imgur.com/NjKil.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NjKil.png"" alt=""enter image description here"" /></a></p>
<p>The code to impute the missing values is as follows:</p>
<pre><code>import datawig
df = datawig.SimpleImputer.complete(df)
</code></pre>
<p>These are the missing values left behind:</p>
<pre><code>df.isnull().sum()
PassengerId       0
HomePlanet        0
CryoSleep         0
Cabin           199
Destination       0
Age               0
VIP               0
RoomService       0
FoodCourt         0
ShoppingMall      0
Spa               0
VRDeck            0
Name            200
Transported       0
dtype: int64
</code></pre>
<p>The missing values for the column named Cabin and Name were left and were not imputed for I do not know what reason. Also before applying datawig imputation the number of missing values in Name and Cabin column were the same.</p>
",16879380.0,4685471.0,2022-06-28 10:52:58,2022-11-18 15:59:47,Datawig library imputes some of the columns while leaving some unimputed,<python><machine-learning><scikit-learn><data-science><imputation>,0,2,N/A,CC BY-SA 4.0
72776073,1,-1.0,2022-06-27 17:20:22,0,183,"<p>I am trying to concatenate two dataframes. I've tried using merge(), join(), concat() in pandas, but none gave me my desired output.</p>
<blockquote>
<p>df1:</p>
</blockquote>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Index</th>
<th style=""text-align: center;"">value</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: center;"">a</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">b</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">c</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: center;"">d</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: center;"">e</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>df2:</p>
</blockquote>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Index</th>
<th style=""text-align: center;"">value</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">f</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">g</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: center;"">h</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: center;"">i</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td style=""text-align: center;"">j</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>desired output:</p>
</blockquote>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Index</th>
<th style=""text-align: center;"">col1</th>
<th style=""text-align: right;"">col2</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: center;"">a</td>
<td style=""text-align: right;"">f</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">b</td>
<td style=""text-align: right;"">g</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">c</td>
<td style=""text-align: right;"">h</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: center;"">d</td>
<td style=""text-align: right;"">i</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: center;"">e</td>
<td style=""text-align: right;"">j</td>
</tr>
</tbody>
</table>
</div>
<p>Thanks in advance!</p>
",17203565.0,17203565.0,2022-06-27 17:20:50,2022-06-27 18:45:47,Concat two dataframes with different indices,<python><pandas><data-science>,2,2,N/A,CC BY-SA 4.0
72676258,1,-1.0,2022-06-19 10:56:35,5,596,"<p>libraries im using</p>
<pre><code>import pixellib
from pixellib.instance import instance_segmentation
import cv2
import matplotlib.pyplot as plt
</code></pre>
<p>the script:</p>
<pre><code>segment_image = instance_segmentation()
segment_image.load_model('mask_rcnn_coco.h5')
segmask, output = segment_image.segmentImage(&quot;images\example2.jpeg&quot;, show_bboxes = True)
cv2.imwrite(&quot;exampleoutput.jpeg&quot;, output)
print(output.shape)
</code></pre>
<p>I don't understand why it can't highlight different parts of the image.</p>
<p>Here is my output:</p>
<p><a href=""https://i.stack.imgur.com/xmzqW.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xmzqW.jpg"" alt=""Here is my output"" /></a></p>
<p>I looked into how other people used pixellib and it works perfectly with theirs.</p>
<p>output i'm expecting:</p>
<p><a href=""https://i.stack.imgur.com/nTUIp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nTUIp.png"" alt=""output i'm expecting"" /></a></p>
",12031244.0,2602877.0,2022-06-19 20:56:11,2023-06-20 12:46:01,PixelLib not detecting objects properly,<image-processing><data-science><image-segmentation><feature-extraction><pixellib>,3,1,N/A,CC BY-SA 4.0
72732599,1,-1.0,2022-06-23 15:15:53,0,2505,"<pre><code>import pandas as pd
import numpy as np
from sklearn import linear_model

df = pd.read_csv('https://raw.githubusercontent.com/codebasics/py/master/ML/1_linear_reg/Exercise/canada_per_capita_income.csv')


reg = linear_model.LinearRegression()
reg.fit( df[['year']], df['per capita income (US$)'] )

reg.predict(2020)
</code></pre>
<p>I am using scikit 1.0.2 version. I am new learner and exactly copied this from a video it works in video but do not work on me, What should I do?</p>
<pre><code>X does not have valid feature names, but LinearRegression was fitted with feature names
  warnings.warn(
</code></pre>
",13938389.0,13938389.0,2022-06-23 16:26:07,2022-06-23 17:05:28,"predict() error : X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn(",<python><pandas><machine-learning><scikit-learn><data-science>,1,1,N/A,CC BY-SA 4.0
72733587,1,-1.0,2022-06-23 16:27:31,2,127,"<p>I'm relatively new to Neo4j and graph databases, so bear with me.</p>
<p>I want to traverse through our Neo4j database in the most efficient way possible. While traversing, I need to do some math along the way on edges and their values. <strong>At scale, our database will have possibly millions of nodes and edges, so I'm very concerned with efficiency.</strong></p>
<p>My database has nodes with people where some people are &quot;marked&quot;. And there are transactions between these people. See the images below. In my algorithm, I basically take any person and see how much money they've received from every marked person.</p>
<p>Up until this point, I've been using the <a href=""https://neo4j.com/docs/api/python-driver/current/"" rel=""nofollow noreferrer"">Neo4j python driver</a> and <a href=""https://neomodel.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">neomodel</a> to perform these traversals.</p>
<p>To do this, I've created an algorithm which is basically a modified version of recursive depth-first traversal. I recursively traverse deeper and deeper through a node's senders until I can't anymore. When I encounter a &quot;marked person&quot; (e.g. a criminal whose money I want to track), I add a record for them.</p>
<p>As the recursion goes back towards the source, I repeatedly multiply the money sources by the fraction of how much the given node received from its sender. For example, when the recursion returns back to John, I first multiply all of Sally's sources by what fraction of Sally's money was sent to John, which in this case is (3/17) since Sally received 17 dollars and sent 3 dollars to John. I'll then do the same for Frank's sources. I multiply each of his sources by (2/11) since Frank received 11 dollars and John received 2 dollars from Frank.</p>
<p>Here is the python code I wrote to perform this algorithm:</p>
<pre class=""lang-py prettyprint-override""><code>def get_sources(node):
    source_record = {}
    for sender in node.senders:
        # retrieve the edge between these two nodes
        transaction = node.senders.relationship(sender)
        amount_from_sender = transaction.value
        sender_total_received = sender.total_received()
        if isinstance(sender, MarkedPerson):  # Base Case
            source_record[sender.name] = amount_from_sender
        if len(sender.senders) &gt; 0:  # Recursive Case
            sender_sources = get_sources(sender)
            for source_name, source_value in sender_sources.items():
                # find what fraction of the sender's money they sent to 'node', then
                # multiply this by how much money the sender has of this source to find
                # how much money 'node' has from the source
                amount_from_source = (amount_from_sender / sender_total_received) * source_value
                if source_name in source_record:
                    source_record[source_name] += amount_from_source
                else:
                    source_record[source_name] = amount_from_source

    return source_record
</code></pre>
<p>Here are some examples of what results it gives:</p>
<p>Result when Querying John: <code>{'Bill': 2.310160427807487, 'Rob': 2.6898395721925135}</code>
<a href=""https://i.stack.imgur.com/mF3ta.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mF3ta.png"" alt=""Transaction graph 1"" /></a></p>
<p>Result for Querying John: <code>{'Bill': 2.310160427807487, 'Rob': 2.6898395721925135, 'Sal': 2.6898395721925135}</code>
<a href=""https://i.stack.imgur.com/rvFQW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rvFQW.png"" alt=""Transaction graph 2"" /></a></p>
<p>So I have the following questions:</p>
<ol>
<li>Is a traversal of this type possible using cypher queries? From my initial investigations, it seems not to be.</li>
<li>I've seen people using gremlin to perform similarly complex graph queries. Would this be something worth looking into?</li>
<li>Are there any other tools for dealing with computations with similarly complex data models which would be better suited for our needs?</li>
<li>Is there another well-known graph algorithm I could use or adapt to perform this same task?</li>
</ol>
<p>Any thoughts or comments would be greatly appreciated. Thanks!</p>
",6946463.0,6946463.0,2023-02-02 01:29:31,2023-02-02 01:29:31,Complex Cypher Traversal with Math,<neo4j><cypher><gremlin><graph-databases><graph-data-science>,1,0,N/A,CC BY-SA 4.0
72739825,1,72741156.0,2022-06-24 06:28:20,0,90,"<p>I'm converting excel data source to json by taking all data in a file and saving it. Now I'm looking for number of records in the output file, number of input, source path of excel file, etc. look at it if somebody can guide me thanks.</p>
<p><a href=""https://i.stack.imgur.com/Ybhsp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ybhsp.png"" alt=""thanks in Advance"" /></a></p>
",11649635.0,17493431.0,2022-06-27 03:46:40,2022-06-27 03:46:40,"File destination, source, number of rows , columns and other information while processing excel data source using Talend open studio",<data-science><talend>,1,0,N/A,CC BY-SA 4.0
70549667,1,-1.0,2022-01-01 15:00:00,0,56,"<p>I am new to this field and have done a decent amount of research on this, but every time, I stumble upon handling the imbalanced label by using f1 score, recall, precision as metrics, and using methods like random oversampling, etc, confusion matrix, etc. What I want to know is how would we go with the imbalanced features? Here is a picture of the CRIM category by using the &quot;binning&quot; method from <a href=""https://www.kaggle.com/ktakuma/boston-house-prices-dnn-regression/data"" rel=""nofollow noreferrer"">advanced Boston housing dataset</a></p>
<p><a href=""https://i.stack.imgur.com/4xb6U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4xb6U.png"" alt=""enter image description here"" /></a></p>
<p>Here is the code:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv5(&quot;boston.csv5&quot;)
binnedCRIM = pd.cut(df.CRIM.values,  bins=[0,20,70,100], labels=[0 ,1 ,2])
print(binnedCRIM[binnedCRIM == 0].shape, binnedCRIM[binnedCRIM == 1].shape, binnedCRIM[binnedCRIM == 2].shape)
# 0: low CRIM
# 1: mid CRIM
# 2: high CRIM
df2.CRIM = binnedCRIM
df2.head()
df2.CRIM.hist()
</code></pre>
<p>As you can see, the low class has almost all of the rows, while the mid and high classes are almost negligible as compared to the low class. How should we handle this? The number of total samples is 506, while the number of samples made up of low and high class combined is 18. How should we tackle this kind of scenario? While dropping them is the most obvious and easiest solution, I am more curious about other methods which can be applied if there are any.</p>
<p>Thanks in advance to all those who helped!</p>
",9795654.0,9795654.0,2022-01-01 15:42:28,2022-01-09 10:20:10,Handling imbalanced Feature (X) not lavbel (Y) in machine learning,<pandas><machine-learning><statistics><data-science>,1,0,N/A,CC BY-SA 4.0
72768712,1,-1.0,2022-06-27 07:56:05,0,76,"<p>I have around 30000 sheets of excel with standardized report.
The goal is to provide online dashboard to view the data.
My first thought as programmer is to create a database and find a way to import the data and then build front end on the top to create a customized dashboard.
Any easier way to do this?</p>
",3918180.0,-1.0,N/A,2022-06-27 08:29:47,From local Excel sheets into dashboard online,<data-structures><data-science>,1,0,N/A,CC BY-SA 4.0
72770507,1,-1.0,2022-06-27 10:22:33,0,105,"<p>I have a specific algorithm I execute in Python.
This algorithm uses 3 variables: <code>x</code>, <code>y</code> and <code>z</code>.
These variables affect the time needed to execute the algorithm.
I used Python to write the data to an excel worksheet (using openpyxl)
This is an example of how it looks like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">x</th>
<th style=""text-align: center;"">y</th>
<th style=""text-align: center;"">z</th>
<th style=""text-align: center;"">time in seconds</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1200</td>
<td style=""text-align: center;"">15000</td>
<td style=""text-align: center;"">1500</td>
<td style=""text-align: center;"">800</td>
</tr>
<tr>
<td style=""text-align: center;"">900</td>
<td style=""text-align: center;"">20000</td>
<td style=""text-align: center;"">1800</td>
<td style=""text-align: center;"">900</td>
</tr>
<tr>
<td style=""text-align: center;"">1300</td>
<td style=""text-align: center;"">20000</td>
<td style=""text-align: center;"">1800</td>
<td style=""text-align: center;"">1100</td>
</tr>
</tbody>
</table>
</div>
<p>I would like to print, in Python, the estimated time needed to execute the algorithm before execution, so the user knows how long he has to wait.</p>
<p>Edit for clarification: I would like to predict the needed time from a given x, y and z.</p>
<p>How can I do this?
Can I do it in excel, and read the value in Python?
Or is there a way to do it in Python?</p>
<p>I am looking for a simple and compact solution that delivers decent results, rather than a complicated solution that delivers very accurate results.</p>
<p>Edit: I am <strong>not</strong> looking for a written and ready to implement solution. A name of a method or just a place to start searching would be nice as I have never done this before.</p>
",8650088.0,8650088.0,2022-06-27 10:34:35,2022-06-27 10:56:16,How to predict a variable in python using previous data contained in excel?,<python><excel><database><data-science><prediction>,1,3,N/A,CC BY-SA 4.0
72776921,1,72851371.0,2022-06-27 18:38:45,0,650,"<p>I'm trying to use sentence_transformers to get bert embeddings, but it can't process for example 300 documents, i keep getting error <strong>IndexError: list index out of range</strong>. How to fix that?</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-mean-tokens')
embeddings = model.encode(tokenized_docs_smaller, show_progress_bar=True)
</code></pre>
",13247822.0,-1.0,N/A,2022-07-04 02:58:37,bert sentence_transformers list index out of range,<python><nlp><data-science><bert-language-model><sentence-transformers>,1,2,N/A,CC BY-SA 4.0
72800373,1,-1.0,2022-06-29 11:08:40,0,72,"<p>I want to use a linear regression model on my data. However, some columns have NaN values, and I dont know how to go about it.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>F_1</th>
<th>F_2</th>
<th>F_3</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.5</td>
<td>1.5 -</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0.8</td>
<td>2.3</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>NaN</td>
<td>NaN</td>
<td>3</td>
</tr>
<tr>
<td>3</td>
<td>1.2</td>
<td>3.0</td>
<td>NaN</td>
</tr>
<tr>
<td>4</td>
<td>NaN</td>
<td>1.9</td>
<td>1.4</td>
</tr>
<tr>
<td>5</td>
<td>0.7</td>
<td>NaN</td>
<td>1.6</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>2.6</td>
<td>2.2</td>
</tr>
</tbody>
</table>
</div>
<p>To fit the data, I could delete the columns with NaN values:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>F_1</th>
<th>F_2</th>
<th>F_3</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.5</td>
<td>1.5 -</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0.8</td>
<td>2.3</td>
<td>2</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>2.6</td>
<td>2.2</td>
</tr>
</tbody>
</table>
</div>
<p>But I want to keep all my data and still be able to process it. How do I handle the columns with NaN values?</p>
",15940016.0,19354807.0,2022-06-30 13:28:52,2022-06-30 13:28:52,Python Data Science Imputation. Index with two or more NaN values,<python><data-science><nan><imputation>,1,5,N/A,CC BY-SA 4.0
72781248,1,72782570.0,2022-06-28 05:42:21,-1,402,"<p>I'm using the request module to do some web scraping in python, but everytime i send the requests with headers and proxies, i get a connection aborted error, even though i've been told that it would solve the problem if i did put the header.</p>
<p>This is my full code:</p>
<pre><code>import requests 
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
import pickle 


session = requests.Session()
unique = {}
retry = Retry(connect=3,backoff_factor=0.5)
adapter = HTTPAdapter(max_retries=retry)
session.mount('http://',adapter)
headers= {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:101.0) Gecko/20100101 Firefox/101.0'}
proxies1 = {&quot;http&quot;:&quot;http://23.238.33.186:80&quot;}


lstofwords = pickle.load(f)
dic={}
first = int(len(lstofwords)/3)
for x in range(0,first):
    r = session.get(f'https://www.merriam-webster.com/dictionary/{lstofwords[x]}/',headers=headers,proxies=proxies1)
    soup = BeautifulSoup(r.content,'html.parser')
    descriptions = soup.find_all('span', class_='unText') 
    descs = [i.text for i in descriptions]
    dic[lstofwords[x]]= descs

for x in range(first,second):
    r = session.get(f'https://www.merriam-webster.com/dictionary/{lstofwords[x]}/',headers=headers,proxies=proxies2)
    soup = BeautifulSoup(r.content,'html.parser')
    descriptions = soup.find_all('span', class_='unText') 
    descs = [i.text for i in descriptions]
    dic[lstofwords[x]]= descs

for x in range(second,third):
    r = session.get(f'https://www.merriam-webster.com/dictionary/{lstofwords[x]}/',headers=headers,proxies=proxies3)
    soup = BeautifulSoup(r.content,'html.parser')
    descriptions = soup.find_all('span', class_='unText') 
    descs = [i.text for i in descriptions]
    dic[lstofwords[x]]= descs

with open('dictionary.pkl','wb') as f:
    pickle.dump(dic, f)
print(&quot;Success&quot;)

</code></pre>
<p>Below is the error message I'm getting:</p>
<pre><code>
Traceback (most recent call last):
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 703, in urlopen
    httplib_response = self._make_request(
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 449, in _make_request
    six.raise_from(e, None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 444, in _make_request
    httplib_response = conn.getresponse()
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1374, in getresponse
    response.begin()
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 318, in begin
    version, status, reason = self._read_status()
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 287, in _read_status
    raise RemoteDisconnected(&quot;Remote end closed connection without&quot;
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\adapters.py&quot;, line 440, in send
    resp = conn.urlopen(
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 785, in urlopen
    retries = retries.increment(
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\util\retry.py&quot;, line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\packages\six.py&quot;, line 769, in reraise
    raise value.with_traceback(tb)
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 703, in urlopen
    httplib_response = self._make_request(
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 449, in _make_request
    six.raise_from(e, None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\urllib3\connectionpool.py&quot;, line 444, in _make_request
    httplib_response = conn.getresponse()
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 1374, in getresponse
    response.begin()
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 318, in begin
    version, status, reason = self._read_status()
  File &quot;C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\lib\http\client.py&quot;, line 287, in _read_status
    raise RemoteDisconnected(&quot;Remote end closed connection without&quot;
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\ehabb\Downloads\research.py&quot;, line 53, in &lt;module&gt;
    r = session.get(f'https://www.merriam-webster.com/dictionary/{lstofwords[x]}/',headers=headers,proxies=proxies1)
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\sessions.py&quot;, line 542, in get
    return self.request('GET', url, **kwargs)
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\sessions.py&quot;, line 529, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\sessions.py&quot;, line 667, in send
    history = [resp for resp in gen]
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\sessions.py&quot;, line 667, in &lt;listcomp&gt;
    history = [resp for resp in gen]
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\sessions.py&quot;, line 237, in resolve_redirects
    resp = self.send(
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\sessions.py&quot;, line 645, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\ehabb\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\requests\adapters.py&quot;, line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
</code></pre>
",10357240.0,-1.0,N/A,2022-06-28 07:48:50,Connection Aborted Error when scraping with requests module,<python><web-scraping><beautifulsoup><python-requests><data-science>,1,1,N/A,CC BY-SA 4.0
70552653,1,-1.0,2022-01-01 23:35:24,0,1233,"<p>I'm writing an r program which lists sales prices for various items. I have a column called InvoiceDate, which lists date and time as follows: '12/1/2009 7:45'.  I'm trying to isolate the date only in a separate field called date, and then arrange the dates sequentially.  The code I'm using is as follows:</p>
<pre><code>library(dplyr)
library(ggplot2)

setwd(&quot;C:/Users/cshor/OneDrive/Environment/Restoration_Ecology/Udemy/Stat_Thinking_&amp;_Data_Sci_with_R/Assignments/Sect_5&quot;)
retail_clean &lt;- read.csv(&quot;C:/Users/cshor/OneDrive/Environment/Restoration_Ecology/Udemy/Stat_Thinking_&amp;_Data_Sci_with_R/Data/retail_clean.csv&quot;)

retail_clean$date &lt;- as.Date(retail_clean$InvoiceDate)#, format = &quot;%d/%m/%Y&quot;)

   total_sales = sum(retail_clean$Quantity, na.rm=TRUE) %&gt;% 
arrange(retail_clean$date) %&gt;% ggplot(aes(x=date, y=total_sales)) + geom_line()
</code></pre>
<p>Initially, everything works fine, and the date field is created.  However, I get the following error for the arrange() function:</p>
<pre><code>Error in UseMethod(&quot;arrange&quot;) :no applicable method for 'arrange' applied to an object of class &quot;c('integer', 'numeric')&quot;
</code></pre>
<p>I've searched for over a week for a solution to this problem, but have found nothing that specifically addresses this issue.  I've also used '.asPosixct' instead of .asDate, with similar results.  Any help as to why the program interprets Date data as numeric, and how I can correct the problem, would be greatly appreciated.</p>
",17811892.0,13321647.0,2022-01-01 23:46:52,2022-01-04 17:39:16,Why am I getting an 'Error in UseMethod(arrange) in r?,<r><function><dplyr><data-science>,1,2,N/A,CC BY-SA 4.0
70556372,1,-1.0,2022-01-02 13:30:38,0,89,"<p>How to combine/re-writes 'minute' in pandas.to_datetime?
My data frame looks like below:</p>
<pre><code>df_2 = pd.DataFrame({
               'years' : df.year,
               'months': df.Month,
               'days'  : df.DayofMonth,
               'hours' : df.hour,
               'mins'  : df.min

                })
pd.to_datetime(df_2).head()
</code></pre>
<p>Result:</p>
<blockquote>
<p>ValueError: extra keys have been passed to the datetime assemblage: [mins]</p>
</blockquote>
",17815293.0,4420967.0,2022-01-02 14:11:39,2022-01-03 00:27:38,How to combine/re-writes 'minute' in pandas.to_datetime?,<python><pandas><datetime><time-series><data-science>,2,2,N/A,CC BY-SA 4.0
72804674,1,-1.0,2022-06-29 16:12:16,0,72,"<p>Can someone, basically tell me the differences between these 3 encoders below, in which cases we use them, and how to write code to use them in a simple way ( with coding in Python sklearn if possible )</p>
<p>I am having quite difficult time to get to know the logic behind them in sklearn therefore I decided to ask in here.</p>
<ul>
<li>One-Hot Encoding</li>
<li>Ordinal Encoding</li>
<li>Label Encoding</li>
</ul>
<p>Thanks for your responses and helps</p>
<p>Happy coding</p>
",19445334.0,-1.0,N/A,2022-07-17 18:24:47,Encoding categorical features in Python with Encoders,<python><machine-learning><scikit-learn><encoding><data-science>,1,1,N/A,CC BY-SA 4.0
72807824,1,-1.0,2022-06-29 21:01:24,0,60,"<p>I have a list in python which is showing like this just below. I would like to turn it into a data frame. I tried it: <strong>pd.DataFrame(myList)</strong>, however the <strong>'origins' column</strong> stores a <strong>list</strong>, however I would like to store the <strong>origin and quantityLeads keys</strong> in that same dataframe</p>
<pre><code>myList = [
   {
      &quot;id&quot;:3105052,
      &quot;title&quot;:&quot;Ebook Relat�rios Gerenciais&quot;,
      &quot;offering&quot;:&quot;Institucional&quot;,
      &quot;created_date&quot;:&quot;2022-06-28&quot;
      &quot;inserted_date&quot;:&quot;2022-06-28&quot;,
      &quot;channel&quot;:&quot;Social&quot;,
      &quot;start_date&quot;:&quot;2022-06-28&quot;,
      &quot;end_date&quot;:&quot;2022-06-28&quot;,
      &quot;origins&quot;:[
         {
            &quot;origin&quot;:&quot;LinkedIn&quot;,
            &quot;quantityLeads&quot;:&quot;1&quot;
         },
         {
            &quot;origin&quot;:&quot;Facebook&quot;,
            &quot;quantityLeads&quot;:&quot;1&quot;
         }
      ]
   },
   {
      &quot;id&quot;:3105052,
      &quot;title&quot;:&quot;Ebook Relat�rios Gerenciais&quot;,
      &quot;offering&quot;:&quot;Institucional&quot;,
      &quot;inserted_date&quot;:&quot;2022-06-28&quot;,
      &quot;created_date&quot;:&quot;2022-06-28&quot;,
      &quot;channel&quot;:&quot;Direct&quot;,
      &quot;start_date&quot;:&quot;2022-06-28&quot;,
      &quot;end_date&quot;:&quot;2022-06-28&quot;,
      &quot;origins&quot;:[
         {
            &quot;origin&quot;:&quot;Desconhecida&quot;,
            &quot;quantityLeads&quot;:&quot;2&quot;
         }
      ]
   },
   {
      &quot;id&quot;:2918513,
      &quot;title&quot;:&quot;Ebook Direct To Consumer&quot;,
      &quot;offering&quot;:&quot;Supply Chain&quot;,
      &quot;created_date&quot;:&quot;2022-06-28&quot;,
      &quot;inserted_date&quot;:&quot;2022-06-28&quot;,
      &quot;channel&quot;:&quot;Social&quot;,
      &quot;start_date&quot;:&quot;2022-06-28&quot;,
      &quot;end_date&quot;:&quot;2022-06-28&quot;,
      &quot;origins&quot;:[
         {
            &quot;origin&quot;:&quot;LinkedIn&quot;,
            &quot;quantityLeads&quot;:&quot;1&quot;
         }
      ]
   }
]
</code></pre>
",12054877.0,12054877.0,2022-06-29 21:24:52,2022-07-07 16:25:37,How to turn this list into a data frame? I would like to know how to do this in python?,<python><pandas><dataframe><data-science><helper>,3,4,N/A,CC BY-SA 4.0
72801555,1,72966418.0,2022-06-29 12:37:03,1,1481,"<p>I got this error when implementing my model. I think the erros come from the bert model which i have imported.</p>
<pre class=""lang-py prettyprint-override""><code>def create_text_encoder(
    num_projection_layers, projection_dims, dropout_rate, trainable=False
):
    # Load the BERT preprocessing module.
    preprocess = hub.KerasLayer(
        &quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&quot;,
        name=&quot;text_preprocessing&quot;,
    )
    # Load the pre-trained BERT model to be used as the base encoder.
    bert = hub.KerasLayer(
        &quot;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&quot;,
        &quot;bert&quot;,
    )
    # Set the trainability of the base encoder.
    bert.trainable = trainable
    # Receive the text as inputs.
    inputs = layers.Input(shape=(), dtype=tf.string, name=&quot;text_input&quot;)
    # Preprocess the text.
    bert_inputs = preprocess(inputs)
    # Generate embeddings for the preprocessed text using the BERT model.
    embeddings = bert(bert_inputs)[&quot;pooled_output&quot;]
    # Project the embeddings produced by the model.
    outputs = project_embeddings(
        embeddings, num_projection_layers, projection_dims, dropout_rate
    )
    # Create the text encoder model.
    return keras.Model(inputs, outputs, name=&quot;text_encoder&quot;)
</code></pre>
<p>The error is showing in below code but I think problem is in above part.</p>
<pre class=""lang-py prettyprint-override""><code>num_epochs = 5  # In practice, train for at least 30 epochs
batch_size = 256

vision_encoder = create_vision_encoder(
    num_projection_layers=1, projection_dims=256, dropout_rate=0.1
)
text_encoder = create_text_encoder(
    num_projection_layers=1, projection_dims=256, dropout_rate=0.1
)
dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)
dual_encoder.compile(
    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)
)
</code></pre>
<p>Thanks.</p>
",19443650.0,14909621.0,2022-06-29 16:08:44,2022-07-13 12:36:15,"TypeError: Expected `trainable` argument to be a boolean, but got: bert",<machine-learning><deep-learning><nlp><data-science><bert-language-model>,1,0,N/A,CC BY-SA 4.0
70562768,1,-1.0,2022-01-03 07:32:17,-1,218,"<p>How could I handle the datetime data for high frequency time series prediction (predict the value of next 2 seconds)?</p>
<p>I am working on time series data for predicting the value of next 2 seconds. The data time column is originally timestamp type and I have converted to datetime type with</p>
<pre><code>pd.to_datetime(data.time,unit='us')
</code></pre>
<p>How could I change the followed date format to seconds for prediction? Any advice on model selection for high frequency time series prediction?</p>
<pre><code>2020-01-12 15:12:20.354390   low
2020-01-12 15:12:20.354390    low
2020-01-12 15:12:20.500599    medium
2020-01-12 15:12:21.501825    high
2020-01-12 15:12:22.501052    .... 
2020-01-12 15:12:23.500284      
2020-01-12 15:12:24.501484    ...
</code></pre>
",3871587.0,4685471.0,2022-01-03 11:23:19,2022-01-03 11:23:57,How could I handle the datetime for high frequency time series prediction (based on seconds),<python><pandas><machine-learning><time-series><data-science>,3,2,N/A,CC BY-SA 4.0
70567204,1,-1.0,2022-01-03 14:24:48,0,27,"<p>I apologize if the question doesn't make much sense.
I'm trying to convert data into inputs for a classifier. I have multiple layers of many to one data which I need to &quot;attach&quot; to the top layer. I will try to explain with an example:</p>
<p>Let's say you have a household, each household contains at least one person (represented with categorical data male/female/other), each person has some amount of pets (represented with categorical data dog/cat/rat/etc..). Is it possible to represent this data into one row (for the household) without losing information?</p>
<p>One way I could think of doing it was the count of the amount of data for each category, so a household would have 2 males, 1 female, 2 dogs and 1 cat. Except this loses the information about how the data itself is structured, like if the female has all of the pets, that data doesn't tell you that.</p>
<p>The other way would be structuring each household into a database, so each row is a person containing m/f/o and the amount of each pet, then performing some dimensionality reduction technique to put it all on one row for the household but I'm not sure if this is feasible.</p>
<p>So yeah, any advice would be appreciated.</p>
",12667951.0,-1.0,N/A,2022-01-04 11:15:34,How to convert layers of many-to-one relationships into input data without losing information?,<data-structures><data-science>,1,2,N/A,CC BY-SA 4.0
72819212,1,-1.0,2022-06-30 16:34:31,0,61,"<p>There is a dataset of vehicles by type (sedan, SUV, truck, etc), odometer, cylinders, price, etc.
I am addressing the missing values in the column 'cylinders', which contains the number of cylinders in the engine of the vehicle. My approach to fill in the missing values is to use the median number of cylinders per type of vehicle. Using a pivot table it looks like this:
<a href=""https://i.stack.imgur.com/yBRTP.png"" rel=""nofollow noreferrer"">Screenshot of the pivot table</a></p>
<p>Now I want to create a for loop that goes through every row and when it finds a NaN value in column 'cylinders' replaces it with the median value seen in the pivot table according to the type.</p>
<p>Thanks</p>
",-1.0,-1.0,N/A,2022-07-01 19:44:38,How to replace NaN value in one column based on the value of another column in the same row using Pandas?,<python><pandas><data-science>,1,2,N/A,CC BY-SA 4.0
73914765,1,-1.0,2022-09-30 23:20:41,0,361,"<p>So I have a dataset that looks similar to the following:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Procedure</th>
<th>Country</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr>
<td>Colectomy</td>
<td>China</td>
<td>2019</td>
</tr>
<tr>
<td>Colectomy</td>
<td>India</td>
<td>2020</td>
</tr>
<tr>
<td>Appendectomy</td>
<td>China</td>
<td>2020</td>
</tr>
<tr>
<td>Appendectomy</td>
<td>USA</td>
<td>2010</td>
</tr>
</tbody>
</table>
</div>
<p>I currently am plotting a histogram of procedure frequencies with the following code:</p>
<p><code>plotly.express.histogram(df, x = 'Procedure', color = 'Country')</code></p>
<p>It looks exactly how I want it to. However, I am looking to make some sort of slider or dropdown that allows me to filter the data presented based on the 'Year' column. Ideally, I could either only visualize procedures from a certain year, or visualize all procedures that occur after a particularly year. I am looking to do this in real time with Plotly. I've looked at the documentation and they seem to only have examples that let you filter based on the x or y argument. Does anybody know how to do this?
Note: I am using regular Plotly, not Dash.</p>
",16368933.0,16368933.0,2022-10-01 00:00:00,2022-10-01 00:00:00,How to filter Plotly histogram based on an additional column,<python><plotly><data-science>,1,0,N/A,CC BY-SA 4.0
70578172,1,-1.0,2022-01-04 11:35:23,2,511,"<p>I am using Sagemaker platform for model development and deployment. Data is read from RDS tables and then spitted to train and test df.
To create the training job in Sagemaker, I found that it takes data source only as s3 and EFS. For that I need to keep train and test data back to s3, which is repeating the data storing process in RDS and s3.
I would want to directly pass the df from RDS as a parameter in tarining job code. Is there any way we can pass df in fit method</p>
<pre><code>    image=&quot;581132636225.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-ols-model:latest&quot;
    model_output_folder = &quot;model-output&quot;
    print(image)
    tree = sagemaker.estimator.Estimator(
        image,
        role,
        1,
        &quot;ml.c4.2xlarge&quot;,
        output_path=&quot;s3://{}/{}&quot;.format(sess.default_bucket(), model_output_folder),
        sagemaker_session=sess,
    )

**tree.fit({'train': &quot;s3_path_having_test_data&quot;}, wait=True)**
</code></pre>
",10838658.0,404970.0,2022-01-04 17:22:12,2023-07-28 08:32:33,"Can we use data directly from RDS or df as a data source for training job in Sagemaker, rather than pulling it from from s3 or EFS?",<python><amazon-web-services><data-science><amazon-sagemaker>,2,2,N/A,CC BY-SA 4.0
70579743,1,-1.0,2022-01-04 13:44:09,0,40,"<p>It is possible to reference a dataframe column if you know its name (for example: <code>dataframe1.time</code>). I want to iterate each variable from a dataframe and call it like: 'for each x, dataframe1.x'</p>
<pre><code>numeric_vars = ['CO_Mean','CO_Min','CO_Max','CO_Std','NO2_Mean', 'NO2_Min', 'NO2_Max','NO2_Std', 'O3_Mean','O3_Min'
                              ,'O3_Max', 'O3_Std', 'PM2.5_Mean', 'PM2.5_Min', 'PM2.5_Max', 'PM2.5_Std', 'PM10_Mean',
                              'PM10_Min', 'PM10_Max', 'PM10_Std', 'SO2_Mean', 'SO2_Min', 'SO2_Max', 'SO2_Std']
for num_var in new_dataset_2[numeric_vars]:
    mean, std = np.mean(new_dataset_2[var]), np.std(new_dataset_2[var])
    cut_off = std * 3
    lower, upper = mean - cut_off, mean + cut_off
    #outliers = [new_dataset_2.index[new_dataset_2[var] == x].tolist() for x in new_dataset_2[var] if x &lt; lower or x &gt; upper]
    new_dataset_2 = new_dataset_2[(new_dataset_2.num_var &gt; lower) &amp; (new_dataset_2.num_var &lt; upper)]
    #for outlier in outliers:
     #   new_dataset_2.drop(outlier)
</code></pre>
<p>The line new_dataset2.num_var does not work. Is there a way to iterate each column and reference it dynamically?</p>
",17351530.0,509840.0,2022-01-04 14:09:27,2022-01-04 14:17:09,How to iterate each variable from a dataframe a reference to it dynamically?,<python><python-3.x><dataframe><data-science><outliers>,1,0,N/A,CC BY-SA 4.0
73931848,1,73933627.0,2022-10-03 06:33:22,-1,136,"<p>How to get the title and description on every detail page regarding the topic of market from this link: <a href=""https://www.cnbcindonesia.com/market/indeks/5"" rel=""nofollow noreferrer"">https://www.cnbcindonesia.com/market/indeks/5</a> ?.</p>
<p><a href=""https://i.stack.imgur.com/3XoRN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3XoRN.png"" alt=""enter image description here"" /></a></p>
<p>I want the results of the data scraping in the form of a dataframe contains title, description per detail page.</p>
<p>This is my current code, and now i confused to scraped per detail page for get the description.</p>
<pre><code>from ast import parse
import scrapy


class CnbcSpider(scrapy.Spider):
    name = 'cnbc'
    allowed_domains = ['cnbcindonesia.com']
    start_urls = ['https://www.cnbcindonesia.com/market/indeks/5']

    def parse(self, response):
        for berita in response.xpath(&quot;//article&quot;):
            the_title= berita.xpath(&quot;./a/div[@class='box_text']/h2/text()&quot;).get()
            the_category= berita.xpath(&quot;./a/div[@class='box_text']/span/span/text()&quot;).get()
            link = berita.xpath(&quot;./a/@href&quot;).get()
            # yield{
            #     'Title_page': the_title,
            #     'category_page': the_category,
            #     'link': link 
            # }
            yield scrapy.Request(url=link, callback=self.parse_detail_page)

    def parse_detail_page(self, response):
        title = response.xpath(&quot;//h1/text()']&quot;).get()
        yield{
                'Title': title,
            }
</code></pre>
",17154482.0,17154482.0,2022-10-03 08:33:16,2022-10-03 09:44:44,How to get title and description on every detail page when scraping data using scrapy python,<python><web-scraping><scrapy><data-science><data-analysis>,1,6,N/A,CC BY-SA 4.0
70587853,1,-1.0,2022-01-05 04:37:09,1,3563,"<p>Sagemaker Studio worked flawlessly for me for the first 6 months. Then I started observing this issue. <a href=""https://i.stack.imgur.com/Ry5Se.png"" rel=""nofollow noreferrer"">Screenshot of the error message</a></p>
<p>The screen holds up at this stage forever. Here's what I have tried:</p>
<ol>
<li>Clearing my cache, even using a different machine. So I don't think the issue lies with the browser or my machine.</li>
<li>Pressing 'Clear workspace' in the screenshot above.</li>
<li>Shutting down all the apps in my sagemaker domain (excluding the 'default' app). This used to work initially but now this has stopped working all-together.</li>
<li>Created a new sagemaker domain with fraction of the files in the previous domain. Still, I see the same error message in the new domain as well.</li>
</ol>
<p>This is severely affecting my work and I can't find a solution for this anywhere on the internet.</p>
",17837275.0,-1.0,N/A,2022-03-01 19:42:45,Sagemaker studio does not load up,<amazon-web-services><machine-learning><artificial-intelligence><data-science><amazon-sagemaker>,1,2,N/A,CC BY-SA 4.0
73949023,1,-1.0,2022-10-04 13:55:09,0,101,"<p>I want to create two charts. One is boxplot and second is line plot.
I've tried to give them individual labels but line plot's top label is not shown.</p>
<p><strong>Code that I tried:</strong></p>
<pre><code>fig = plt.figure()
ax0 = fig.add_subplot(1,2,1)
ax1 = fig.add_subplot(1,2,2)

new_df.iloc[:,0:3].plot(
    kind='box',
    figsize=(20,7),
    color='red',
    ax=ax0
)

ax0.set_title('Boxdiagramm für Ländern')
ax0.set_ylabel('Nummern')
ax0.set_xlabel('Jahre')

new_df['Total'].plot(
    kind='line',
    figsize=(20,7),
    ax=ax1
)

ax1.set_label('Linediagramm für Ländern')
ax1.set_ylabel('Nummern')
ax1.set_xlabel('Ländern')
ax1.legend(labels=None)

plt.show()
</code></pre>
<p><strong>Output of the code:</strong></p>
<p><a href=""https://i.stack.imgur.com/hevIr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hevIr.png"" alt=""Output image link"" /></a></p>
<p>How can I add the second plot's label?</p>
",19573776.0,13138364.0,2022-10-04 14:05:28,2022-10-05 06:20:11,Second plot's label doesn't shown using artist layer of matplotlib,<python><matplotlib><data-science><visualization>,1,3,N/A,CC BY-SA 4.0
73932504,1,-1.0,2022-10-03 07:52:17,0,56,"<p><img src=""https://i.stack.imgur.com/Up7od.jpg"" alt=""dataframe"" /></p>
<p>This is my dataframe and i want to create a seperate columns (dummies) for each genre(like comedy....) which is stored in lists</p>
",20147798.0,3494754.0,2022-10-03 16:40:09,2022-10-03 16:40:09,how to get dummies from lists of elements from a particular column,<dataframe><machine-learning><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
70588013,1,-1.0,2022-01-05 05:04:33,2,62,"<p>I have a list of 500K different type of Addresses and Have also List of specific Point names in BD. You want to find out these Point names according to the addresses. But there is a problem, Many Point names are not spelled correctly in the addresses;</p>
<p>like -
Wrong Spelled Point Names in a different addresses: Narayangonj, Norayanganj, Nuraiyagonj
Right Spelled Point Name in my list: Narayanganj</p>
<p>How should I code it? - If the words of the name of the Point names name match closely or similarly, then it will pick up the estimated or appropriate Point names according to the addresses.</p>
<p><a href=""https://i.stack.imgur.com/GUtbo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GUtbo.png"" alt=""enter image description here"" /></a></p>
",16620749.0,12370687.0,2022-08-28 04:33:33,2022-08-28 04:33:33,How Can I Find Out Similar or Estimated Point Names From a list of Addresses By NLP Or Any Better Solution?,<python><machine-learning><deep-learning><nlp><data-science>,1,1,N/A,CC BY-SA 4.0
73955395,1,-1.0,2022-10-05 02:22:44,-1,53,"<p>I have the data of dictionary:</p>
<pre><code>mydict = {
    'K93': [((1200, 1, 'K93', 'AMK2FJVGB', 0.0, 600.0, 300), [300, 15]), 
            ((1200, 1, 'K93', 'AMK2FJSRM', 0.0, 600.0, 300), [300, 15]), 
            ((1200, 2, 'K93', 'AMK2FJVGB', 0.0, 600.0, 300), [300, 15]), 
            ((1200, 3, 'K93', 'AMK2FJABK', 0.125, 800.0, 400), [300, 15]), 
            ((1200, 2, 'K93', 'AMK2FJJBM', 0.6666666666666666, 900.0, 600), [300, 15]), 
            ((1200, 3, 'K93', 'AMK2FJJBM', 0.6666666666666666, 900.0, 600), [300, 15]), 
            ((1200, 1, 'K93', 'AMK2FJNBK', 0.0, 800.0, 400), [300.0, 15])], 
    'K0JA': [((1200, 1, 'K0JA', 'AMK0JA  ', 0.6, 700.0, 400), [600, 30]), 
            ((1200, 3, 'K0JA', 'AMK0JA  ', 0.0, 700.0, 400), [300, 15])], 

}
</code></pre>
<p>i want to access index 5 every key in the dictionary like this:</p>
<pre><code>600
600
600
...
700
</code></pre>
<p>This is my current code:</p>
<pre><code>for key,val in mydict.items():
    print(mydict[key][0][5])
</code></pre>
",20026319.0,20026319.0,2022-10-05 02:23:24,2022-10-05 02:50:19,How to access index 5 in the dictionary,<python><list><dictionary><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
73957260,1,-1.0,2022-10-05 07:47:23,0,377,"<p>I want to look-up 8760 times for a single lat/lon combo in <strong>less than a second</strong> from 43.82 GB file of wind data containing:</p>
<ul>
<li>8760 times (every hour in a year)</li>
<li>721 latitudes (every 0.25° from -90.0° to 90.0°)</li>
<li>1440 longitude (every 0.25° from -180.0° to 179.75°)</li>
</ul>
<p><a href=""https://i.stack.imgur.com/HhpTc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HhpTc.png"" alt=""enter image description here"" /></a></p>
<p>The best time we achieved for a single-year look-up was 16 seconds for both u100 and v100 wind speed at 100m vectors. We want to have a <strong>sub-second look-up</strong> for the whole year as such file read will need to happen on <strong>every user request</strong> in our API.</p>
<pre><code>if __name__ == '__main__':
    start_time = time.time()

    ds = xr.open_dataset(&quot;2021.zarr&quot;, engine=&quot;zarr&quot;, chunks={&quot;time&quot;: 50})

    print(f&quot;Took {round((time.time() - start_time) * 1000, 2)}ms&quot;)

    location = ds.sel(indexers={&quot;latitude&quot;: 53.494, &quot;longitude&quot;: 9.979}, method='nearest')

    wind_speed = (location.u100.values ** 2 + location.v100.values ** 2) ** 0.5

    print(f&quot;Wind Speed: {wind_speed} m/s&quot;)
    print(f&quot;Took {round((time.time() - start_time) * 1000, 2)}ms&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Took 94.28ms
Wind Speed: [5.8021994 5.504477  5.4270387 ... 9.563195  8.701231  9.133655 ] m/s
Took 16299.59ms
</code></pre>
<p>I would be very thankful for any help!</p>
",10846114.0,-1.0,N/A,2022-10-06 17:10:38,Zarr slow read speed of 43.82 GB file with Xarray,<data-science><netcdf><python-xarray><grib><zarr>,0,7,N/A,CC BY-SA 4.0
70546522,1,70546541.0,2022-01-01 01:12:02,1,858,"<p>I am trying to select a row based on a certain condition from a .csv file.</p>
<p><a href=""https://i.stack.imgur.com/eS56x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eS56x.png"" alt=""enter image description here"" /></a></p>
<p>As you can see in the image. I have a .csv file containing that table. At the last row the &quot;raw_flow_rate&quot; and &quot;avg_flow_rate&quot; are same. And I am selecting that row with this line of code.</p>
<p><code>select_data = New_data[New_data[&quot;raw_flow_rate&quot;] == New_data[&quot;avg_flow_rate&quot;]]</code></p>
<p>And it works perfectly. But my target is to select the previous row of the selected one. Index number 4 in this case. I have tried the ID column to simply select the previous row based on (ID-1) operation.</p>
<pre><code>storeID = select_data[&quot;ID&quot;] - 1
final_data = New_data[New16_data[&quot;ID&quot;] == storeID]
</code></pre>
<p>But this gives the error <code>can only compare identically-labeled series objects</code></p>
<p>Is there any simple way to select the previous row from the one which met the condition? I'm using Pandas to handle the .csv files. The files are quite big, so a simpler method is preferable. Thanks</p>
",3945119.0,-1.0,N/A,2022-01-01 01:16:49,Selecting a row based on conditions on .csv file,<python><pandas><data-science>,1,3,N/A,CC BY-SA 4.0
72784962,1,72786100.0,2022-06-28 10:39:17,-1,51,"<p>In the code I wrote, .add_patch() is giving error. Have been trying to figure this out for past 5 days but couldn't do so.
The same call is working when I use it separately for just an ellipse instead of combining it with another one.
Kindly look into this!!</p>
<pre><code># Importing Necessary Libraries
import pandas as pd
import numpy as np
import random as rd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.style as stl

xys = [[10,125],[100,26],[25,66],[67,1],[74,10]]
xys=np.array(xys)

mean = np.mean(xys,0)
stdDev = np.std(xys,0)

ellipse = patches.Ellipse(mean[0],mean[1],stdDev[0]*2,stdDev[1]*2)

fig, graph=plt.subplots()
graph.scatter(xys[:,0],xys[:,1])
graph.scatter(mean[0],mean[1])
graph.add_patch(ellipse)
</code></pre>
<p><a href=""https://i.stack.imgur.com/ZEj2f.jpg"" rel=""nofollow noreferrer"">This is the result I am getting on the terminal when I am running the file</a></p>
",19434068.0,19434068.0,2022-06-28 11:24:19,2022-06-28 12:04:37,.add_patch() base file issue,<python><matplotlib><data-science>,1,1,N/A,CC BY-SA 4.0
73916972,1,-1.0,2022-10-01 08:44:46,0,61,"<p>I had calculated the percentage of outliers in my dataset using the IQR method and I got the next result:</p>
<p><a href=""https://i.stack.imgur.com/JILAX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JILAX.png"" alt=""enter image description here"" /></a></p>
<p>I want to remove outliers for Wind Speed and Pressure using z1 score is that possible?</p>
<p>I got my data set from the next site: <a href=""https://www.kaggle.com/datasets/budincsevity/szeged-weather"" rel=""nofollow noreferrer"">my data</a> and I used the next function to calculate the outliers percentage:</p>
<pre><code>def outlier_percent(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    minimum = Q1 - (1.5 * IQR)
    maximum = Q3 + (1.5 * IQR)
    num_outliers =  np.sum((data &lt; minimum) |(data &gt; maximum))
    num_total = data.count()
    return (num_outliers/num_total)*100
</code></pre>
",10863083.0,10197418.0,2022-10-01 11:17:45,2022-10-01 11:17:45,Detect outliers using IQR and remove them using the zscore,<python><pandas><data-science><outliers>,0,4,N/A,CC BY-SA 4.0
73969723,1,-1.0,2022-10-06 06:58:46,1,400,"<p>I am aware that OPC HDA is an outdated protocol and shouldn't be used if there is any more modern protocol available. Sadly, in this industrial context, it seems there is no real alternative.</p>
<p>I tried to find Python libraries with OPC HDA support without success. So I'm asking here: Is it somehow possible to access data from OPC HDA with Python? If there is no way to directly read the data with Python, is there any workaround possible, like using a third-party tool as a bridge between OPC HDA and Python?</p>
<p>The closest thing I could find are these <a href=""https://integrationobjects.com/sioth-opc/sioth-opc-data-archiving/opc-hda-logger-for-csv/"" rel=""nofollow noreferrer"">OPC HDA loggers</a>, which connect to an OPC HDA server, and log the data to flat files. That would probably somehow work, but I'm wondering if there is any better solution.</p>
<p>I'm curious for your suggestions, thank you!</p>
",6156346.0,-1.0,N/A,2022-10-21 14:51:40,Read data from OPC HDA server with Python?,<python><data-science><implementation><opc>,1,0,N/A,CC BY-SA 4.0
73955719,1,-1.0,2022-10-05 03:42:16,0,62,"<p><a href=""https://i.stack.imgur.com/JzAIa.png"" rel=""nofollow noreferrer"">enter image description here</a>table of movie data</p>
<p><img src=""https://i.stack.imgur.com/Aixgg.png"" alt="""" /></p>
<p>As you can see the column of release date puts the month in a string.
I want to change all of them into numbers.
For example, Dec 18, 2009 can just be 12. I am not interested in the year.</p>
<p>Update: I think I got it. They still come out as objects when I do .info() but at least I was able to get to the number</p>
",20163618.0,20163618.0,2022-10-05 04:55:13,2022-10-05 04:55:13,Change a string in a column to an integer in pandas,<python><pandas><dataframe><replace><data-science>,1,1,N/A,CC BY-SA 4.0
70601323,1,70730950.0,2022-01-06 00:55:23,5,168,"<pre><code>n_level = range(1, steps + 2)
</code></pre>
<p><code>steps</code> is user input, using multi-index dataframe</p>
<pre><code>    df = {'crest': [754, 755, 762, 785], 'trough': [752, 725, 759, 765], 'L1T': [761, 761, 761, 761], 'L2T': [772, 772, 772, 772], 'L3T': [783, 783, 783, 783], 'L4T': [794, 794, 794, 794], 'L1M': [740, 740, 740, 740], 'L2M': [729, 729, 729, 729], 'L3M': [718, 718, 718, 718], 'L4T': [707, 707, 707, 707]}


    for i in n_level:
        if df['Crest'] &gt;= df[f'L{i}T']:
            df['Marker'] = i
        elif df['Trough'] &lt;= df[f'L{i}M']:
            df['Marker'] = -i
        else:
            df['Marker'] = 0
</code></pre>
<p>I am expecting below df with the col marker</p>
<pre><code>    df = {'crest': [754, 755, 762, 785], 'trough': [752, 725, 759, 765], 'L1T': [761, 761, 761, 761], 'L2T': [772, 772, 772, 772], 'L3T': [783, 783, 783, 783], 'L4T': [794, 794, 794, 794], 'L1M': [740, 740, 740, 740], 'L2M': [729, 729, 729, 729], 'L3M': [718, 718, 718, 718], 'L4T': [707, 707, 707, 707], 'Marker': [0, -2, 1, 3]}, 
</code></pre>
<p>The if statement is returning True or False, using that can we convert it to the ith Value (+/-)</p>
<p>what I need is another col df['Marker'], which will measure if the crest or trough crossed L{i}T or L{i}M, any breach on the upside in case of the crest and breach on the downside in case of the trough</p>
<p><a href=""https://i.stack.imgur.com/0YRDZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0YRDZ.png"" alt=""Sample Dataframe"" /></a></p>
",14550977.0,14550977.0,2022-01-07 06:57:01,2022-01-16 14:28:48,Is there a way to return float or integer from a conditional True/False,<pandas><data-science><python-3.7><physics>,2,3,N/A,CC BY-SA 4.0
73976957,1,-1.0,2022-10-06 16:07:47,0,65,"<p>I am running this animation code example in vs code but animations are not running used in the code. Can I have to install extension in vs code?</p>
<pre><code>from matplotlib import pyplot as plt
from matplotlib.animation import FuncAnimation
import random
import numpy as np

x = []
y = []
colors = []
fig = plt.figure(figsize=(7,5))

def animation_func(i):
    x.append(random.randint(0,100))
    y.append(random.randint(0,100))
    colors.append(np.random.rand(1))
    area = random.randint(0,30) * random.randint(0,30)
    plt.xlim(0,100)
    plt.ylim(0,100)
    plt.scatter(x, y, c = colors, s = area, alpha = 0.5)

animation = FuncAnimation(fig, animation_func,
                        interval = 100)
plt.show()
</code></pre>
",20100383.0,-1.0,N/A,2022-10-06 17:53:29,Animations in Python,<python><data-science>,2,0,N/A,CC BY-SA 4.0
73983629,1,-1.0,2022-10-07 07:12:28,0,32,"<p>The data I work with consists of multiple tables (around 5-10) with single tables containing up to 10 million entries. So, overall I'd describe it as a large data set, but not too large to work with on a 'normal' computer. I'm in the need of a synthetic data set with the same structure and internal dependencies, i.e. a dummy data set. I can't use the data I work with as the data contains sensitive information.</p>
<p>I did research on synthetic data and came across different solutions. The first would be online providers where one uploads the original data and synthetic data is created based on the given input. This sounds like a nice solution, but I'd rather not share the original data with any external sources, so this is currently not an option for me.</p>
<p>The second solution I came across isthe <em>synthpop</em> package in R. I tried that, however, I encountered two problems: the first one being that for larger tables (as the tables in the orginal data sets are) it takes a very long time to execute. The second one being that I only got it working for a single table, however, I need to keep the dependencies between the tables, otherwhise the synthetic data doesn't make any sense.</p>
<p>The third option would be to do the whole data creation by myself. I have good and solid knowledge about the domain and the data, so I would be able to define the internal constraints formally and then write a script to follow these. The problem I see here is that it would obviously be a lot of work and as I'm no expert on synthetic data creation, I might still overlook something important.</p>
<p>So basically I have two questions:</p>
<ol>
<li>Is there a good package/solution you can recommend (preferably in R, but ultimately the programming language doesn't matter so much) for automatically creating synthetic (and private) data based on original input data consisting of multiple tables?</li>
<li>Would you recommend the manual approach or would you recommend to spend more time on the <em>synthpop</em> package for example and try that approach?</li>
</ol>
",19939373.0,-1.0,N/A,2022-10-07 07:12:28,Complex Synthetic Data - Create manually or use a package/tool?,<r><data-science>,0,2,N/A,CC BY-SA 4.0
72818221,1,-1.0,2022-06-30 15:15:56,-1,603,"<p>I used <code>torch.quantization.quantize_dynamic</code> to reduce the model size but it is reducing my prediction Accuracy score.</p>
<p>I'm using that model file inside the Flask and doing some real time predictions, Due to the large size i'm facing issues while predicting. So could anyone please help me on reducing the bert model size using pytorch and guide me on who to do the real time predictions.</p>
",9799698.0,9799698.0,2022-06-30 15:22:31,2022-06-30 19:25:41,How to reduce the size of Bert model(checkpoint/model_state.bin) using pytorch,<machine-learning><pytorch><data-science><bert-language-model>,1,0,N/A,CC BY-SA 4.0
73943975,1,73944213.0,2022-10-04 06:42:23,-2,93,"<p>How to grouping list by value and put in new dynamic variable using Python.
I have some data list like this and i want to store in new dynamic variable list based on value K93, K94, K0JA =&gt; (this will be a dynamic value)</p>
<pre><code>data_list = [
(('K93', 'AMK2FJVGB', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJSRM', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJVGB', 0.0, 600.0, 300), [300, 15]),
(('K94', 'AMK2FJSRM', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJVGB', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJSRM', 0.0, 600.0, 300), [300, 15]),
(('K0JA', 'AMK0JA  ', 0.0, 700.0, 400), [600, 30]),
(('K0JA', 'AMK0JA  ', 0.0, 700.0, 400), [300, 15]),
(('K93', 'AMK2FJNBK', 0.0, 800.0, 400), [300.0, 15]),
(('K94', 'AMK2FJNBK', 0.0, 800.0, 400), [300.0, 15])]
</code></pre>
<p>i want result like this:</p>
<pre><code>list_K93 = [
(('K93', 'AMK2FJVGB', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJSRM', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJVGB', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJVGB', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJSRM', 0.0, 600.0, 300), [300, 15]),
(('K93', 'AMK2FJNBK', 0.0, 800.0, 400), [300.0, 15]),
]

list_K94 = [
(('K94', 'AMK2FJSRM', 0.0, 600.0, 300), [300, 15]),
(('K94', 'AMK2FJNBK', 0.0, 800.0, 400), [300.0, 15])
]

list_K0JA = [
(('K0JA', 'AMK0JA  ', 0.0, 700.0, 400), [600, 30]),
(('K0JA', 'AMK0JA  ', 0.0, 700.0, 400), [300, 15])
]
</code></pre>
",17154482.0,-1.0,N/A,2022-10-08 05:33:13,How to grouping list by value and put in new dynamic variable using Python,<python><list><data-science><data-analysis>,3,1,N/A,CC BY-SA 4.0
73967796,1,-1.0,2022-10-06 01:20:49,0,95,"<p>So I am trying to convert a .mat file into a dataframe in order to run some data analysis After converting it, I have a dataframe structure (see <a href=""https://i.stack.imgur.com/nWBxm.png"" rel=""nofollow noreferrer"">1</a>), but I have no idea how to remove the brackets from the objects in the dataframe. I have tried utilizing:</p>
<p>mdataframe['0'] = mdataframe['0'].str[0]</p>
<p>and</p>
<p>mdataframe['0'] = mdataframe['0'].str.get(0)</p>
<p>as an attempt to fix the 0th column to no avail. Any help and guidance would be appreciated.</p>
<p>Thank you!</p>
",20171403.0,20171403.0,2022-10-06 01:22:47,2022-10-06 02:34:02,Removing nested brackets from a pandas dataframe?,<python><pandas><matlab><data-science>,1,1,N/A,CC BY-SA 4.0
70620467,1,-1.0,2022-01-07 11:20:28,0,70,"<p>I have the xls data ( Master Sheet)  attached in the [![sample xlsx data][1]][1]screenshot here.</p>
<p>As a beginner in Python and Data science , I am trying to read the sheets in xlsx file as below:</p>
<h1>Load pandas</h1>
<pre><code>import pandas as pd

# Read CSV file into DataFrame df
sheet1, sheet2 = None, None
with pd.ExcelFile(&quot;blood_data.xlsx&quot;) as reader:
    sheet1 = pd.read_excel(reader, sheet_name='Master sheet')
    sheet2 = pd.read_excel(reader, sheet_name='Another Sheet')


# Show dataframe
print(sheet1)
</code></pre>
<p>I am getting all the 110 rows and columns in the output, but I want to filter based on the condition which is shown in green in the screenshot. Hb Male 13-17 and Bilirubinn 0.3-1.2 . If the values are not in range I have to print the name of the candidate. Please help me in applying a multi-column-based filter using panda to achieve my result.</p>
<p>Sample input file: <a href=""https://github.com/wittymindstech/medical-data-analysis/blob/main/blood_data.xlsx"" rel=""nofollow noreferrer"">https://github.com/wittymindstech/medical-data-analysis/blob/main/blood_data.xlsx</a></p>
<p>output:  MEHRAWAN, RAKHI etc</p>
",16253282.0,16253282.0,2022-01-07 13:30:24,2022-01-07 13:55:07,How to apply python pandas formula to categorise individuals based on their haemoglobin samples,<python><pandas><data-science><data-analysis><xlsx>,1,3,N/A,CC BY-SA 4.0
70616759,1,71054619.0,2022-01-07 04:54:55,-1,1745,"<p>I have build a model and saved it using pickle library but I wanna use it again by changing the values of my input variables and for that I don't have any output.</p>
<p>Here is the code :</p>
<pre><code># Save Model Using Pickle
import pandas
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
import pickle
url = &quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians- 
diabetes.data.csv&quot;
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values 
X = array[:,0:8]
Y = array[:,8]
test_size = 0.33
seed = 7
X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, 
random_state=seed)
# Fit the model on training set
model = LogisticRegression()
model.fit(X_train, Y_train)
# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))

# some time later...

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test, Y_test)
print(result)
</code></pre>
<p>Now, I have only data for my X_test. I don't have any data point for my Y_test. In such case how I can use it again and pass only X_test and it gives me Y.
Please help!!</p>
",16702275.0,-1.0,N/A,2022-02-09 18:08:28,How to save and reuse model in python,<python><machine-learning><model><data-science><pickle>,2,6,N/A,CC BY-SA 4.0
70616785,1,-1.0,2022-01-07 04:59:04,0,1541,"<p>I tried this :<br>
df['CC'].head(20)=2000 (Got Error)<br>
df.head(20)</p>
<p>This is my data:</p>
<pre>
    Price   Age       KM FuelType  ...      CC  Doors  Weight  Horse Power
0   13500  23.0  46986.0   Diesel  ...  2000.0      3  1165.0        180.0
1   13750  23.0  72937.0   Diesel  ...  2000.0      3  1165.0        180.0
2   13950  24.0  41711.0   Diesel  ...  2000.0      3  1165.0        180.0
3   14950  26.0  48000.0   Diesel  ...  2000.0      3  1165.0        180.0
4   13750  30.0  38500.0   Diesel  ...  2000.0      3  1170.0        180.0
5   12950  32.0  61000.0   Diesel  ...  2000.0      3  1170.0        180.0
6   16900  27.0  94612.0   Diesel  ...  2000.0      3  1245.0        180.0
7   18600  30.0  75889.0   Diesel  ...  2000.0      3  1245.0        180.0
8   21500  27.0  19700.0   Petrol  ...  1800.0      3  1185.0        384.0
9   12950  23.0  71138.0   Diesel  ...  1900.0      3  1105.0        138.0
10  20950  25.0  31461.0   Petrol  ...  1800.0      3  1185.0        384.0
11  19950  22.0  43610.0   Petrol  ...  1800.0      3  1185.0        384.0
12  19600  25.0  32189.0   Petrol  ...  2000.0      3  1185.0        384.0
13  21500  31.0  23000.0   Petrol  ...  1800.0      3  1185.0        384.0
14  22500  32.0  34131.0   Petrol  ...  1800.0      3  1185.0        384.0
15  22000  28.0      NaN   Petrol  ...  1800.0      3  1185.0        384.0
16  22750  30.0  34000.0   Petrol  ...  1800.0      3  1185.0        384.0
17  17950  24.0  21716.0   Petrol  ...  2000.0      3  1105.0        220.0
18  16750  24.0  25563.0   Petrol  ...  1600.0      3  1065.0        220.0
19  16950  30.0  64359.0   Petrol  ...  1600.0      3  1105.0        220.0
</pre>
<p><a href=""https://i.stack.imgur.com/YJwfn.png"" rel=""nofollow noreferrer"">Image of the database table</a><br>
Can anybody tell me how to replace the first n rows of a column with new values?</p>
",15773752.0,15773752.0,2022-01-07 05:02:55,2022-01-07 05:04:29,Replace first n rows with a new value,<python><pandas><dataframe><data-science>,0,3,2022-01-07 05:03:23,CC BY-SA 4.0
70621605,1,70622175.0,2022-01-07 12:56:01,1,257,"<p>I need to extract a name and a quote for a given text such as:</p>
<blockquote>
<p>Homer Simpson said: &quot;Okay, here we go...&quot;</p>
</blockquote>
<p>The returned values:</p>
<blockquote>
<ul>
<li>extracted_person_name - The extracted person name, as appearing in the patterns explained above</li>
<li>extracted_quotation   - The extracted quoted text (withot the surrounding quotation marks).</li>
</ul>
<ul>
<li>Important Note: if the pattern is not found, return None values for both the <br />
extracted person name and the extracted text.</li>
</ul>
</blockquote>
<p>You could expect the input text to look similar to the following pattern:</p>
<blockquote>
<p>Person name said: &quot;quoted text&quot;</p>
</blockquote>
<p>Variations of the above pattern:</p>
<blockquote>
<p>The colon punctuation mark (:) is optional, and and might not appear
in the input sentence. Instead of the word said you could also expect
the words:</p>
<p>answered, responded, replied</p>
</blockquote>
<p><strong>this is what I got so far:</strong></p>
<pre><code>def person_quotation_pattern_extraction(raw_text):
    
    name_pattern = &quot;\w+\s\w+&quot;
    quote_pattern = &quot;[&quot;]\w+[&quot;]&quot;
    
    quote = re.search(quote_pattern, raw_text)
    name = re.search(name_pattern, raw_text)

    
    if re.search(quote_pattern,raw_text):
        extracted_quotation = quote.group(0) 
    else:
        extracted_quotation=None

    if re.search(name_pattern,raw_text):
        extracted_person_name = name.group(0)
    else:
        extracted_person_name=None

    return extracted_person_name, extracted_quotation 
</code></pre>
<p><strong>problem is it returns <code>Null</code>. I'm assuming the patterns are incorrect can you tell me what's wrong with them?</strong></p>
",-1.0,-1.0,N/A,2022-01-07 13:47:27,Text Analysis: extracting person name and quotation: how to create a pattern,<python><pandas><nlp><data-science><python-re>,1,0,N/A,CC BY-SA 4.0
70601090,1,-1.0,2022-01-06 00:16:07,0,156,"<p>---Update: problem is solveed(partly)----------</p>
<p>Can someone explain why this made such a huge difference:</p>
<p>I changed the following:</p>
<pre><code> output = Dense(3, activation='softmax')(batchnorm)

    model = Model(
    inputs=[model_one_input, model_two_input],
    outputs=output
    )
</code></pre>
<p>to</p>
<pre><code>    output = Dense(3)(batchnorm)
    softmax = Activation(&quot;softmax&quot;)(output)

    model = Model(
    inputs=[model_one_input, model_two_input],
    outputs=softmax
    )
</code></pre>
<p>Now i get still weird results but better when testing:</p>
<pre><code># Epoch 2:
 loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.0632 - val_accuracy: 0.6667
# Epoch 10:
loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.0570 - val_accuracy: 1.0000

</code></pre>
<p>I have a shallow convolutional model which is used to predict 3 different images.</p>
<p>I have two datasets and i have balanced the data so that all three class are distributed the same.</p>
<p>My problem is that the model training acc and validation acc cannot get above 80%.
When i run the model predict it only predicts one of the three classes which i find weird.</p>
<p>I have tried many different setup and checked the labels of the images and so on. It all looks fine. So i am starting to think it is a problem with my architecture maybe?</p>
<pre><code>
def get_covn_base(input_layer, img_shape):
    input = Conv2D(64, (3, 3), input_shape=img_shape)(input_layer)
    acti01 = Activation(&quot;relu&quot;)(input)
    pool01 = MaxPooling2D(pool_size=(2, 2))(acti01)
    covn02 = Conv2D(128, (3, 3))(pool01)
    acti02 = Activation(&quot;relu&quot;)(covn02)
    pool02 = MaxPooling2D(pool_size=(2, 2))(acti02)
    covn03 = Conv2D(256, (3, 3))(pool02)
    acti03 = Activation(&quot;relu&quot;)(covn03)
    pool03 = MaxPooling2D(pool_size=(2,2))(acti03)
    covn04 = Conv2D(64, (3, 3))(pool03)
    acti04 = Activation(&quot;relu&quot;)(covn04)
    pool04 = MaxPooling2D(pool_size=(2,2))(acti04)
    flatten = Flatten()(pool04)
    return flatten

def get_shallow_cnn(img_shape):

    model_one_input = Input(shape=img_shape)
    model_one = get_covn_base(model_one_input, img_shape)

    model_two_input = Input(shape=img_shape)
    model_two = get_covn_base(model_two_input, img_shape)

    concat_feature_layer = concatenate([model_one, model_two])
    fully_connected_dense_big = Dense(1024, activation=&quot;relu&quot;)(concat_feature_layer)
    batchnorm = BatchNormalization()(fully_connected_dense_big)
    #acti = Activation('relu')(fully_connected_dense_big)
    fully_connected_dense_medium = Dense(512, activation=&quot;relu&quot;)(batchnorm)
    batchnorm = BatchNormalization()(fully_connected_dense_medium)
    fully_connected_dense_small = Dense(256, activation=&quot;relu&quot;)(batchnorm)
    batchnorm = BatchNormalization()(fully_connected_dense_small)
    output = Dense(3, activation='softmax')(batchnorm)

    model = Model(
    inputs=[model_one_input, model_two_input],
    outputs=output
    )

    
    return model


 model = get_shallow_cnn(get_img_input_shape(True))


    opt = Adam(learning_rate=start_lr)

    model.compile(
        loss=loss_function,
        optimizer=opt,
        metrics=[tensorflow.keras.metrics.CategoricalAccuracy()]
    )


</code></pre>
<p>Here's the test and training of the above:</p>
<p><a href=""https://i.stack.imgur.com/BDZ8G.png"" rel=""nofollow noreferrer"">training</a></p>
<p><a href=""https://i.stack.imgur.com/tzr9p.png"" rel=""nofollow noreferrer"">test</a></p>
<p>The labels of the prediction and true looks fine to me so if someone has experienced this problem or has some advice on this?</p>
<p><a href=""https://i.stack.imgur.com/6TTfy.png"" rel=""nofollow noreferrer"">Arhcitecture</a></p>
<p>-----------UPDATE------------</p>
<p>I have just run a few test with only 3 images one image pr. class. I used the same images for training and validation with no shuffle. The metrics acc gave me the below</p>
<p>The label:</p>
<pre><code>#labels
i = [1. 0. 0.]
i = [0. 0. 1.]
i = [0. 1. 0.]

# Model result 10 epochs:
epoch 2:
 loss: 0.0110 - accuracy: 1.0000 - val_loss: 1.0467 - val_accuracy: 0.3333
epoch 10:
loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.0331 - val_accuracy: 0.6667
</code></pre>
<p>so something is clearly off... How can it only predict 33% when its the same exact images?</p>
",12134748.0,12134748.0,2022-01-06 01:38:08,2022-01-06 01:46:21,Keras multi input CNN only predicts one class,<python><tensorflow><keras><deep-learning><data-science>,1,4,N/A,CC BY-SA 4.0
73970907,1,73973205.0,2022-10-06 08:42:37,1,3422,"<pre><code>import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
logistic_regression = LogisticRegression(random_state = 10)
logistic_regression.fit(X_train, y_train)
y_pred_logistic_regression = logistic_regression.predict(X_test)
print(y_pred_logistic_regression.shape)
</code></pre>
<blockquote>
<p>ValueError: y should be a 1d array, got an array of shape (56000, 10)instead</p>
</blockquote>
",19000217.0,19000217.0,2022-10-06 10:34:06,2022-10-06 11:41:58,"python code error. ValueError: y should be a 1d array, got an array of shape (56000, 10) instead",<python><data-science><logistic-regression>,1,3,N/A,CC BY-SA 4.0
70619212,1,73516536.0,2022-01-07 09:33:43,1,34,"<p>I used random search and got the best hyper parameters for my model, can I pass that model to the AutoML?</p>
<p>Does AutoML do the random search for the best hyper parameters by itself? or is there something I need to pass?</p>
",-1.0,12370687.0,2022-08-28 06:40:22,2022-08-28 06:40:22,Does AutoML accept external models?,<machine-learning><scikit-learn><data-science><automl>,1,1,N/A,CC BY-SA 4.0
70620157,1,70620287.0,2022-01-07 10:55:07,0,966,"<p>I have a dataframe with only two columns, one is text and one is with numpy array values, which can  have more than 170000 elements. The problem is how to save this large numpy arrays as files, so I can load them back later. When trying to save as CVS, it just saves it as a short string, loosing the real values:</p>
<pre><code>[-8.0152120e-07  2.8887976e-05  3.3898741e-05 ... -1.3205040e-01
 -9.4032057e-02  0.0000000e+00]
</code></pre>
<p>I thought maybe to save as text one by one, but is there any elegant way to do this?</p>
",1680859.0,-1.0,N/A,2022-01-07 11:05:28,Saving large numpy arrays using Python,<python><dataframe><numpy><data-science><large-data>,1,3,N/A,CC BY-SA 4.0
73990122,1,-1.0,2022-10-07 16:32:10,0,37,"<p>This is how I've set it up the proportions to define the children who survived to later use in a bargraph. How can I wrangle this data to define the missing value of deaths?</p>
<pre><code>propCS &lt;- matrix(, nrow = 3, ncol = 2)
dataChildren &lt;- data[data$Age &gt; 18,]
dataMale &lt;- dataChildren[dataChildren$Sex == &quot;male&quot;,]
dataFirst &lt;- dataMale[dataMale$Pclass == &quot;First&quot;, ]
dataSecond &lt;- dataMale[dataMale$Pclass == &quot;Second&quot;, ]
dataThird &lt;- dataMale[dataMale$Pclass == &quot;Third&quot;, ]

propCS[1,1] &lt;- sum(dataFirst$Survived) / length(dataFirst$Survived)
propCS[2,1] &lt;- sum(dataSecond$Survived) / length(dataSecond$Survived)
propCS[3,1] &lt;- sum(dataThird$Survived) / length(dataThird$Survived)

dataFemale &lt;- dataChildren[dataChildren$Sex == &quot;female&quot;,]
dataFirst &lt;- dataFemale[dataFemale$Pclass == &quot;First&quot;, ]
dataSecond &lt;- dataFemale[dataFemale$Pclass == &quot;Second&quot;, ]
dataThird &lt;- dataFemale[dataFemale$Pclass == &quot;Third&quot;, ]

propCS[1,2] &lt;- sum(dataFirst$Survived) / length(dataFirst$Survived)
propCS[2,2] &lt;- sum(dataSecond$Survived) / length(dataSecond$Survived)
propCS[3,2] &lt;- sum(dataThird$Survived) / length(dataThird$Survived)
</code></pre>
",18326344.0,6851825.0,2022-10-07 16:36:19,2022-10-07 17:22:00,"Using base R, how can I set parameters to define people who died on the titanic if the csv file only has a column for survived (0,1) and not deaths?",<r><data-science><data-wrangling>,1,1,N/A,CC BY-SA 4.0
70625092,1,-1.0,2022-01-07 17:35:49,0,56,"<p>I have a traffic dataset of camera sightings of license plates. It has approx. 100M points that can be thought of as { camera ID, license plate, timestamp } 3-tuples. There are around 2000 unique cameras and a few million unique license plates. The cameras cover most, but not all of the city and most data points (around 70%) have a license plate (as opposed to a blank). I am trying to reconstruct a flow network of common driving routes, and want to this network to be as follows. There should be an edge between two cameras A and B if and only if</p>
<ol>
<li>There are a lot of direct trips from A to B (ie. a lot of cars that are seen at camera A immediately before being seen at camera B)</li>
<li>Most of these trips took a reasonable amount of time/didn't take too long (ie. we compute the median time of all trips A-&gt;B and then only count a trip as a &quot;direct trip&quot; if it's &lt;150% of the median). This eliminates the possibility that most trips A-&gt;B went through other cameras but those just happened not to be in the dataset because they are blanks.</li>
</ol>
<p>The way I'm doing this is as follows. I first sort by time into car-days (trips) so each group represents one car's movement through cameras on one day, in chronological order. This is done by</p>
<p><code>grouped = full_data.sort_values('time_seconds').groupby(['placa_encoded', 'date'])</code></p>
<p>Then I create a matrix <code>count_mat = np.zeros((num_cameras, num_cameras))</code> where <code>count_mat[i, j]</code> represents the number of times camera <code>i</code> immediately precedes camera <code>j</code> in the data (if the given duration between the sightings isn't 'too long'). This is done as follows.</p>
<pre><code>for name, group in grouped: 
    for x in range(len(group)-1):
        c1, c2 = group.iat[x, 0], group.iat[x+1, 0]
        t1, t2 = group.iat[x, 3], group.iat[x+1, 3]
        # from, to camera codes, eg. 480502114, 480221121 
        i, j = code2index(c1), code2index(c2)
        if (abs(t2-t1) &lt; 1.5 * median_mat[i][j]): 
            # this means A-&gt;B was a direct trip that someone took in a reasonable time
            count_mat[i][j] += 1
</code></pre>
<p>Here, <code>median_mat[i,j]</code> is the pre-calculated median times over all trips to go from camera <code>i</code> to camera <code>j</code>. <code>t1</code> and <code>t2</code> are the timestamps for a data point, and <code>c1</code> and <code>c2</code> are camera codes.</p>
<p>The problem is that the above code is taking hours to run over all 100M points, even though it is theoretically O(n) if I assume that <code>iat</code> is constant time, since it goes through each data point once and all matrix accesses are random access and so constant time. I used to use <code>iloc</code> but that was much slower still. Having looked up similar questions on SO, I people say that vectorizing or using list comprehensions in similar situations goes much faster, but I can't see how either of those would apply/be used here, or how I could do this more efficiently. Currently, it takes a couple of hours to go through all the points on my MacBook Pro, but I'm hoping to find a way to have it done in a couple minutes, if not less.</p>
<p>For completeness, <code>code2index</code> is an O(1) helper that maps camera IDs into the range [0, num_cameras] so that we know where each camera goes in the various matrices. It is as follows.</p>
<pre><code>def code2index(code): 
    # takes camera code like 480502114 and converts to index into count_mat 0 &lt;= i &lt; 1832
    if (code in c2i.keys()): return c2i[code]
    else: 
        n = len(c2i.keys())
        # eg. if there are 2 keys, then we want to put new code into dict[2]
        c2i[code] = n 
        return n 
</code></pre>
<p>Any ideas/optimizations are much appreciated. Any general pointers on how to conceptually think about optimality <em>a priori</em> before sitting down and writing this code would also be great. Thank you!</p>
",11848672.0,-1.0,N/A,2022-01-07 22:02:48,df.iat not fast enough for linear scan over large dataset when grouped,<python-3.x><pandas><numpy><data-science><graph-theory>,2,2,N/A,CC BY-SA 4.0
70625147,1,-1.0,2022-01-07 17:40:57,0,111,"<p>I have a dataframe with the birthdays of the users of specific clients in a dataframe, and i want to<br />
count occurences per month, but also per client, so in the end id have all the occurences of birthdates per month for client #1, all the occurences of birthdates per month for client #2 etc..., similarly to what is described in <a href=""https://stackoverflow.com/questions/38792122/how-to-group-and-count-rows-by-month-and-year-using-pandas"">this</a> question in which they suggest using this:</p>
<pre><code>df.groupby([df['birthdate'].dt.year.rename('year'), df['birthdate'].dt.month.rename('month')]).agg({'count'})
</code></pre>
<p>for a scenario where one wants to count the birthdays per month (not also per client).</p>
<p>How could i go about doing this?</p>
",11982183.0,11982183.0,2022-01-07 18:24:34,2022-01-07 18:44:03,counting birthdays per month per client in a dataframe,<python><pandas><dataframe><numpy><data-science>,1,1,N/A,CC BY-SA 4.0
73976550,1,-1.0,2022-10-06 15:36:49,0,35,"<p>Consider this toy dataset</p>
<pre class=""lang-r prettyprint-override""><code>library(dplyr)
set.seed(1)

data = tibble(
  code = rep(paste0(&quot;CODE&quot;, 1:3), each = 2),
  visit = rep(factor(c(&quot;VS&quot;, &quot;V2&quot;), levels = c(&quot;VS&quot;, &quot;V2&quot;)), 3),
  value = runif(6)
) %&gt;%
  bind_rows(tibble(code = &quot;CODE4&quot;, value = 1))
</code></pre>
<pre class=""lang-r prettyprint-override""><code>r$&gt; data
# A tibble: 7 × 3
  code  visit value
  &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt;
1 CODE1 VS    0.266
2 CODE1 V2    0.372
3 CODE2 VS    0.573
4 CODE2 V2    0.908
5 CODE3 VS    0.202
6 CODE3 V2    0.898
7 CODE4 NA    1    
</code></pre>
<p>(values are random so your output will surely change)</p>
<p>The following code runs as expected:</p>
<pre class=""lang-r prettyprint-override""><code>r$&gt; data %&gt;% group_by(code) %&gt;% mutate(ratio = value / lag(value))
# A tibble: 7 × 4
# Groups:   code [4]
  code  visit value ratio
  &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;
1 CODE1 VS    0.266 NA   
2 CODE1 V2    0.372  1.40
3 CODE2 VS    0.573 NA   
4 CODE2 V2    0.908  1.59
5 CODE3 VS    0.202 NA   
6 CODE3 V2    0.898  4.45
7 CODE4 NA    1     NA  
</code></pre>
<p>Whereas adding the <code>min</code> operator makes the ratio value return NAs everywhere.</p>
<pre class=""lang-r prettyprint-override""><code>r$&gt; data %&gt;% group_by(code) %&gt;% mutate(ratio = min(value / lag(value), 1e4))
# A tibble: 7 × 4
# Groups:   code [4]
  code  visit value ratio
  &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;
1 CODE1 VS    0.266    NA
2 CODE1 V2    0.372    NA
3 CODE2 VS    0.573    NA
4 CODE2 V2    0.908    NA
5 CODE3 VS    0.202    NA
6 CODE3 V2    0.898    NA
7 CODE4 NA    1        NA
</code></pre>
<p>What is the reason for this behaviour ?</p>
<p>EDIT: Now I understand that the <code>min</code> in <code>mutate</code> is called in a vectorized manner, so because there are NAs in some rows, the NA value is returned to all rows, hence the use of <code>pmin</code> solves the problem.</p>
",5118757.0,5118757.0,2022-10-06 15:55:18,2022-10-06 15:55:18,min results in NA after group_by then mutate,<r><dplyr><data-science>,1,6,N/A,CC BY-SA 4.0
74006076,1,-1.0,2022-10-09 15:30:44,0,46,"<p>W​hy len(data) does not give me the info for a specific country. What function should I use instead of it.???</p>
<pre><code>import requests  
import pandas as pd
import json  

api_url=&quot;https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/module%201/Accessing%20Data%20Using%20APIs/jobs.json?utm_medium=Exinfluencer&amp;utm_source=Exinfluencer&amp;utm_content=000026UJ&amp;utm_term=10006555&amp;utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork21426264-2022-01-01&quot;
 

locations=[&quot;Los Angeles&quot;, &quot;New York&quot;, &quot;San Francisco&quot;]

def get_number_of_jobs_Loc_list(locations):
    number_of_jobs_list = []
    for location in locations:
        payload={&quot;Location&quot;:location}
        response=requests.get(api_url, params=payload)
        if response.ok:
            data=response.json()
            number_of_jobs = len(data)
            number_of_jobs_list.append({location: number_of_jobs})
            
    return number_of_jobs_list
get_number_of_jobs_Loc_list(locations) 
</code></pre>
<p>T​he output is:</p>
<pre><code>[{'Los Angeles': 27005}, {'New York': 27005}, {'San Francisco': 27005}]   
</code></pre>
",10588805.0,-1.0,N/A,2022-10-09 16:01:38,Problem with counting the data for a specific key,<python><api><data-science><data-analysis>,1,5,N/A,CC BY-SA 4.0
70632395,1,70639993.0,2022-01-08 12:41:56,1,44306,"<p>I was working with Jupiter notebook but I entered a difficulty. Could you help me?
I have to use <code> from scipy.special import j</code>. Even though I installed scipy lib, It could not run properly. After I searched, I used<code>%pip install scipy --upgrade</code>.
Then I got this message like:
&quot;Requirement already satisfied&quot;. But at the end of the MSG, it said:</p>
<p>&quot;Note: you may need to restart the kernel to use updated packages.&quot;</p>
<p>I reseat kernel from toolbar thousand times, even I tried this code:</p>
<pre><code>HTML(&quot;&lt;script&gt;Jupyter.notebook.kernel.restart()&lt;/script&gt;&quot;)
</code></pre>
<p>Still, it said:
&quot;Note: you may need to restart the kernel to use updated packages.&quot;</p>
<p>Because I already reset the kernel many times and I do not know what else to do, I ran my import sentence again:
<code>from scipy.special import j</code>
but I see:
&quot;ImportError: cannot import name 'j' from 'scipy.special'&quot;</p>
<p>please help me if you can. Now I'm stuck!</p>
",8496414.0,8496414.0,2022-01-08 13:21:00,2022-01-09 09:34:51,Note: you may need to restart the kernel to use updated packages. jupyter,<python><jupyter-notebook><data-science>,1,1,N/A,CC BY-SA 4.0
73999482,1,73999654.0,2022-10-08 18:19:29,1,105,"<p>I have started learning Python Pandas. So basically I am an R user and heavily use tidyverse. So I am trying to use Pandas in the same manner as the Tidyverse. So I am trying to execute this code which throws me an error.</p>
<pre><code>(
    pd.DataFrame(
        {'A':[1,2,3],
         'B':[4,5,6]}
                )
    .assign(A = lambda x: x.A + 1,
            B = lambda x: x.B + x.A,
            A = 1)
    
)
</code></pre>
<p><em>SyntaxError: keyword argument repeated: A</em></p>
<p>So how could I use pandas in a <code>tidyverse</code> manner? More specifically is there any method in pandas that works like the <code>dplyr::mutate</code>?</p>
",14252611.0,14314520.0,2022-10-08 18:24:56,2022-10-08 18:42:41,How to use the same variable again in the pandas assign() method?,<python><pandas><dplyr><data-science><data-analysis>,2,1,N/A,CC BY-SA 4.0
73999494,1,74114115.0,2022-10-08 18:21:17,1,444,"<p>I'm using <a href=""https://plotly.com/python/"" rel=""nofollow noreferrer"">plotly</a> python library in order to make a choropleth. The data of choropleth (<code>choc_df</code>) has been handled by pandas. The choropleth is drawn by this code:</p>
<pre><code>import plotly.graph_objects as go

go.Figure(data=go.Choropleth(
    locations=choc_df['ISO'],
    text=choc_df['company_location'],
    marker_line_color='black',
    marker_line_width=0.5,
    autocolorscale=False,
    colorscale='brbg_r',
    z=choc_df['rating'],
    customdata=choc_df['number'],
    hovertemplate=&quot;&lt;br&gt;&quot;.join([
        &quot;&lt;b&gt;%{text}&lt;/b&gt;&quot;,
        &quot;Number of brands: %{customdata}&quot;,
        &quot;Rating: %{z}&quot;,
    ]) + '&lt;extra&gt;&lt;/extra&gt;',
), layout=dict(title='Rating',
        geo=dict(showcountries=True, countrycolor='#444444', showlakes=False)
))
</code></pre>
<p>I'm happy with the result: <a href=""https://i.stack.imgur.com/lQOzm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQOzm.png"" alt=""resulting_choropleth"" /></a>
The only thing that bothers me is the amount of space that Antartica takes for no reason :D. Is there a way to remove it from map?</p>
",15389768.0,-1.0,N/A,2022-10-18 16:07:08,How to remove Antarctica from plotly world map?,<python><plotly><data-science><plotly-python>,2,0,N/A,CC BY-SA 4.0
74006593,1,-1.0,2022-10-09 16:41:40,0,56,"<p>I have been trying to figure out whether the following problem has a solution. Almost having given up, I would like to ask whether someone can confirm that there is no solution, or maybe give me a hint.</p>
<ul>
<li><p>We have two vectors <code>v</code> and <code>w</code> in 3D space and know that the ratio of their magnitudes is <code>||v|| / ||w|| = 0.8019</code>.</p>
</li>
<li><p>in 3D space an observer would see that they form an angle of <code>27.017</code> degrees.</p>
</li>
<li><p>on the other side, an observer in 2D (only seeing the <code>x</code> and <code>z</code> axis), observes an angle of <code>7.125</code> degrees between the vectors.</p>
</li>
<li><p>From their view, the vector coordinates are <code>v = (x: 2, z: 1)</code> and <code>w = (x: 3, z: 2)</code>.</p>
</li>
</ul>
<p><strong>Is there somehow a way that the 2D observer can calculate the actual angle between these vectors in 3D space?</strong></p>
<p>I would be more than happy for any input. All my tries have failed so far and I just want to know whether there could be a possible solution.</p>
",17703181.0,17703181.0,2022-10-09 18:20:00,2022-10-09 19:05:57,Compute 3D angle given 2D vectors and their magnitude ratio,<vector><graphics><computer-vision><data-science><angle>,2,0,N/A,CC BY-SA 4.0
74008162,1,-1.0,2022-10-09 20:32:08,0,95,"<p>I have a list of 16 dataframes that contain stats for each player in the NBA during the respective season. My end goal is to run unsupervised learning algorithms on the data frames. For example, I want to see if I can determine a player's position by their stats or if I can determine their total points during the season based on their stats.</p>
<p><strong>What I would like to do is modify <em>the list</em>(<code>df_list</code>), unless there's a better solution, of these dataframes instead modifying <em>each</em> dataframe to:</strong></p>
<ol>
<li><p>Change the datatype of the <code>MP</code>(minutes played column from <code>str</code> to <code>int</code>.</p>
</li>
<li><p>Modify the dataframe where there are only players with 1000 or more <code>MP</code> and there are no duplicate players(<code>Rk</code>)</p>
</li>
</ol>
<p><em>(for instance in a season, a player(<code>Rk</code>) can play for three teams in a season and have 200<code>MP</code>, 300<code>MP</code>, and 400<code>MP</code> mins with each team. He'll have a column for each team <strong>and</strong> a column called <code>TOT</code> which will render his <code>MP</code> as 900(200+300+400) <em>for a total of four rows in the dataframe</em>. <strong>I only need the <code>TOT</code> row</strong></em></p>
<ol start=""3"">
<li>Use simple algebra with various and individual columns columns, for example: being able to total the <code>MP</code> column and the <code>PTS</code> column and then diving the sum of the <code>PTS</code> column by the <code>MP</code> column.
Or dividing the total of the <code>PTS</code> column by the <code>len</code> of the <code>PTS</code> column.</li>
</ol>
<p><strong>What I've done so far is this:</strong></p>
<p>Import my libraries and create 16 dataframes using <code>pd.read_html(url)</code>.</p>
<p>The first dataframes created using two lines of code:</p>
<pre><code>url = &quot;https://www.basketball-reference.com/leagues/NBA_1997_totals.html&quot;

ninetysix = pd.read_html(url)[0]
</code></pre>
<p><a href=""https://i.stack.imgur.com/kkm2v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kkm2v.png"" alt=""enter image description here"" /></a></p>
<p><strong>HOWEVER</strong>, the next four data frames had to be created using a few additional line of code(I received an error code that said <em>&quot;html5lib not found, please install it&quot;</em> so I downloaded both <code>html5lib</code> and <code>requests</code>). I say that to say...this distinction in creating the DF may have to considered in a solution.</p>
<p><strong>The code I used:</strong></p>
<pre><code>import requests
import uuid

url = 'https://www.basketball-reference.com/leagues/NBA_1998_totals.html'
cookies = {'euConsentId': str(uuid.uuid4())}

html = requests.get(url, cookies=cookies).content
ninetyseven = pd.read_html(html)[0]
</code></pre>
<p><strong>These four data frames look like this:</strong></p>
<p><a href=""https://i.stack.imgur.com/oBgqA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oBgqA.png"" alt=""enter image description here"" /></a></p>
<p><strong>I tried this but it didn't do anything:</strong></p>
<pre><code>df_list = [
    eightyfour, eightyfive, eightysix, eightyseven, 
           eightyeight, eightynine, ninety, ninetyone, 
           ninetytwo, ninetyfour, ninetyfive,
    ninetysix, ninetyseven, ninetyeight, owe_one, owe_two
          ]
for df in df_list:
    df = df.loc[df['Tm'] == 'TOT']
    df = df.copy()
    df['MP'] = df['MP'].astype(int)
    df['Rk'] = df['Rk'].astype(int)
    df = list(df[df['MP'] &gt;= 1000]['Rk'])
    df = df[df['Rk'].isin(df)]
owe_two
</code></pre>
<p>============================UPDATE===================================</p>
<p>This code will solves a portion of problem # 2</p>
<pre><code>url = 'https://www.basketball-reference.com/leagues/NBA_1997_totals.html'
dd = pd.read_html(url)[0]
dd = dd[dd['Rk'].ne('Rk')]  
dd['MP'] = dd['MP'].astype(int)

players_1000_rk_list = list(dd[dd['MP'] &gt;= 1000]['Rk'])
players_dd = dd[dd['Rk'].isin(players_1000_rk_list)]
</code></pre>
<p>But it doesn't remove the duplicates.</p>
<p>==================== UPDATE 10/11/22 ================================</p>
<p>Let's say I take rows with values &quot;TOT&quot; in the &quot;Tm&quot; and create a <em>new</em> DF with them, and these rows from the <em>original</em> data frame...</p>
<p>could I then compare the <em>new</em> DF with the <em>original</em> data frame and remove the names from the <em>original data</em> <strong>IF</strong> they match the names from the <em>new data</em> frame?</p>
",8146119.0,8146119.0,2022-10-11 07:14:35,2022-10-11 07:14:35,"Working on multiple data frames with data for NBA players during the season, how can I modify all the dataframes at the same time?",<pandas><dataframe><machine-learning><data-science><multiple-columns>,1,5,N/A,CC BY-SA 4.0
70633347,1,70633471.0,2022-01-08 14:38:09,2,573,"<p>I have a simple dataframe like this:</p>
<pre><code>df = pd.DataFrame({'class':['a','b','c','d','e'],
                  'name':['Adi','leon','adi','leo','andy'],
                  'age':['9','8','9','9','8'],
                   'score':['40','90','35','95','85']})
</code></pre>
<p>then the result is like this</p>
<pre><code> class  name   age  score
    a   Adi     9   40
    b   leon    8   90
    a   adi     9   35
    d   leo     9   95
    e   andy    8   85
</code></pre>
<p>how can I combine the row named 'Adi' with 'adi' in the same column while he is only one person and the score 'Adi' is 75, not 40 and 35</p>
",16702442.0,4865723.0,2023-02-16 14:25:43,2023-02-16 14:25:43,How to concatenate same row names in same column on Python DataFrame,<python><dataframe><data-science>,2,1,N/A,CC BY-SA 4.0
70636513,1,70636597.0,2022-01-08 21:18:04,0,98,"<pre><code>import requests
from bs4 import BeautifulSoup

URL = &quot;https://www.hockey-reference.com/leagues/NHL_2021_games.html&quot;
page = requests.get(URL)

soup = BeautifulSoup(page.content, &quot;html.parser&quot;)
results = soup.find(id=&quot;all_games&quot;)

table = soup.find('div', attrs = {'id':'div_games'}) 
print(table.prettify())
</code></pre>
",13842892.0,10987432.0,2022-01-08 21:22:09,2022-01-08 21:38:27,How can scrape the team names and goals from this site into a table? Ive been trying a few different methods but can't quite figure it out,<python><web-scraping><beautifulsoup><scrapy><data-science>,2,0,N/A,CC BY-SA 4.0
70636678,1,-1.0,2022-01-08 21:41:45,0,198,"<p>I want to find a good metric to mesure how big is the span of a set of vectors.</p>
<p>More practically: I'm encoding some words with a 100 dimensions word embedding and I want to measure how much their vectorial representations span in space. This is meant to be a vocabulary &quot;diversity&quot; metric.</p>
<p>For example:</p>
<pre><code>span([&quot;computer&quot;,&quot;code&quot;,&quot;data&quot;]) &lt; span([&quot;burger&quot;,&quot;politics&quot;,&quot;hamster&quot;]) 
</code></pre>
",8681882.0,355230.0,2022-01-08 23:30:56,2022-01-08 23:30:56,Metric to mesure the span of a set of vectors,<python><data-science><metrics>,0,5,N/A,CC BY-SA 4.0
74022832,1,-1.0,2022-10-11 03:59:10,0,154,"<p>I thought it would be pretty easy to write a function which measures entropy in a string (or more likely copy / paste a method someone else made) in order to detect encoded PowerShell scripts. Googled around, found shannon entropy, but the result is not helpful:</p>
<pre><code>from collections import Counter
from math import log

def shannon(string):
    counts = Counter(string)
    frequencies = ((i / len(string)) for i in counts.values())
    print(- sum(f * log(f, 2) for f in frequencies), string)

shannon(&quot;lsass.exe lsass.exe lsass.exe lsass.exe&quot;)
shannon(&quot;33fsd.exe 33fsd.exe 33fsd.exe 33fsd.exe&quot;)
shannon(&quot;This is an encoded string encoded string&quot;)
shannon(&quot;hxxxttps://www.google.com https://www.google.com&quot;)
shannon(&quot;hxxxttps://3lkl3h4kljnl.fruityflies.com&quot;)
shannon(&quot;VGhpcyBpcyBhbiBlbmNvZGVkIHN0cmluZw==&quot;)
shannon('''powershell.exe (New-Object System.Net.WebClient).DownloadFile('htxxtp://broke.ip.for.safety/~yahoo/csrsv.exe',&quot;$env:APPDATA\csrsv.exe&quot;);Start-Process (&quot;$env:APPDATA\csrsv.exe&quot;)''')
shannon('''powershell.exe –EncodedCo-broke-syntax-for-safety-mmand ZQBjAGgAbwAgACIARABvAHIAbwB0AGgAeAZQBjAGgAbwARABvAHIAbwB0AGgAeQAiAAZQBjAGgAbwAgACIARABvAHIAbwB0AGgAeQAiAAZQBjAGgAbgergegerge==''')
</code></pre>
<p>Note that I sort of went in and broke all the IPs and URLs in this post manually to be safe and break those scripts (not that we're actually executing any PowerShell), so ignore the inconsistencies.</p>
<p>The results:</p>
<pre><code>2.624519205764368 lsass.exe lsass.exe lsass.exe lsass.exe
2.9070717700888262 33fsd.exe 33fsd.exe 33fsd.exe 33fsd.exe
3.593942707918268 This is an encoded string encoded string
3.7358630961373667 htxxxtps://www.google.com https://www.google.com
4.270910335425637 htxxxtps://3lkl3h4kljnl.fruityflies.com
4.294653473544341 VGhpcyBpcyBhbiBlbmNvZGVkIHN0cmluZw==
5.214435811329176 powershell.exe (New-Object System.Net.WebClient).DownloadFile('htxxxtp://broke.ip.for.safety/~yahoo/csrsv.exe',&quot;$env:APPDATA\csrsv.exe&quot;);Start-Process (&quot;$env:APPDATA\csrsv.exe&quot;)
4.270483739208834 powershell.exe –EncodedCo-broke-syntax-for-safety-mmand ZQBjAGgAbwAgACIARABvAHIAbwB0AGgAeQAiAAZQBjAGgAbwAgACIARABvAHIAbwB0AGgAeQAiAAZQBjAGgAbwAgACIARABvAHIAbwB0AGgAeQAiAAZQBjAGgAbgergegerge==
</code></pre>
<p>As you can see, when it came to longer examples, the obviously encoded command measures at a lower entropy level.</p>
<p>Is there a different entropy measurement approach which might fare better?</p>
",9400421.0,9400421.0,2022-10-11 04:04:16,2022-10-11 04:04:16,Is there a better method for entropy calculation than shannon entropy for measuring entropy of a PowerShell script?,<python><data-science><entropy>,0,2,N/A,CC BY-SA 4.0
74000163,1,-1.0,2022-10-08 19:59:00,0,60,"<p>I am writing a program to perform 2 steps.</p>
<h1>Step 1. calculate Q1, Q2, Q3</h1>
<h1>Step 2. calculate D1, D2, D3, D4, D4, D5, D6, D7, D8, D9</h1>
<p>The program that I have written is constructed like so:</p>
<pre><code># Author: Evan Gertis
# Date 10/08
# program: quartiles &amp; deciles

wages       = [250.00,259.99,260.00,269.99,270.00,279.99,280.00,289.99,290.00,299.99,300.00,309.99,310.00,319.99]
n_employees = [8,10,16,14,10,5,2]
total       = 65

def quartile(n,w,employees):
    return wages[2*n-1] + total/n - n_employees[2*n-1]/n_employees[2*n]*10

# Step 1. calculate Q1, Q2, Q3
print(quartile(2,wages,n_employees))

# Step 2. calculate D1, D2, D3, D4, D4, D5, D6, D7, D8, D9
</code></pre>
<p>The data set is:</p>
<p><a href=""https://i.stack.imgur.com/tXh8k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tXh8k.png"" alt=""enter image description here"" /></a></p>
<p>As part of this question I am seeking an understanding of the decile calculation shown below:</p>
<p><a href=""https://i.stack.imgur.com/i7IX5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i7IX5.png"" alt=""enter image description here"" /></a></p>
<p>I'd like to use numpy to perform this operation.</p>
",3937811.0,472495.0,2022-11-20 21:55:32,2022-11-20 21:55:32,How to determine the quartiles and deciles from a given data set,<python><data-science>,1,0,N/A,CC BY-SA 4.0
74001352,1,74006851.0,2022-10-09 00:19:42,1,883,"<p>My main objective is to use a CNN later on. And I found a way to divide it into 3 parts: train, validation, and test datasets respectively. However, I don't know if they are balanced. So, I need to count the elements for each class in a specific dataset.</p>
<pre><code>dataset = tf.keras.preprocessing.image_dataset_from_directory(
  DATASET_DIRECTION,
  seed = 37,
  image_size = (128, 128),
  batch_size = 25, 
  labels = 'inferred'
)
</code></pre>
",16008014.0,-1.0,N/A,2022-10-09 17:13:02,How to check the number (counting) of elements by class from a batchdataset from keras?,<python><data-science><tf.keras>,1,0,N/A,CC BY-SA 4.0
74015191,1,74016332.0,2022-10-10 12:45:52,1,495,"<p>So I am trying to fit a Lasso Regression model with Polynomial Features.</p>
<p>To create these Polynomial Features, I am using <code>from sklearn.preprocessing import PolynomialFeatures</code></p>
<p>It has a parameter inclue_bias which can be set to True or False as <code>PolynomialFeatures(degree=5, include_bias=False)</code>. This bias is nothing but the column of 1 that represents the intercept in the final equation. As explained in detail <a href=""https://stackoverflow.com/a/60253780/2126357"">here</a> in another answer.</p>
<p>The issue is arising, when I either set <code>include_bias=True</code> and then fit Lasso regression without an intercept as it has already been taken care of or I choose to not include bias and set <code>fit_intercept=True</code> in from <code>sklearn.linear_model import Lasso</code>. They technically should give result for the intercept coefficient right? But turns out they aren't coming out to be same.</p>
<p>Is there internally some bug in scikit learn or what am I missing here? Let me know if anyone wants to try this on their own with some data. I'll share some.</p>
",2126357.0,-1.0,N/A,2022-10-10 14:17:57,Explain why Lasso regression intercept coming different when we include intercept in data vs model fitting intercept by itself?,<python><machine-learning><scikit-learn><data-science><lasso-regression>,1,0,N/A,CC BY-SA 4.0
74023456,1,-1.0,2022-10-11 05:43:54,1,27,"<p>please help, this is the code function. I'll post the error i'm getting when I run the function and use pickle.dump</p>
<pre><code>    def location(cord):
        latitude=str(cord[0])
        longitude=str(cord[1])
        location=geolocater.reverse(&quot;{}, {}&quot;.format(latitude, longitude) ).raw['address']

        #if the values are missing replace by empty string

        if location.get('Road') is None:
            location['Road'] = None
    
        if location.get('County') is None:
            location['County'] = None
    
        loc_update['County'].append(location['County'])
        [enter image description here][1]loc_update['Road'].append(location['Road'])
</code></pre>
",19971938.0,-1.0,N/A,2022-10-11 05:47:50,"This is a function to be able to reverse a coordinate in the california housing dataset so as to get the specific address. But I have a problem,",<python><jupyter-notebook><data-science><feature-engineering>,1,0,N/A,CC BY-SA 4.0
70626880,1,-1.0,2022-01-07 20:35:13,1,329,"<p>I created a multi-subplot plot in matplotlib using the hist2d function.
Each one of the subplots shows a different range of data, and the fourth subplot shows the sum of all the other subplots.</p>
<p>Everything is perfect except the fact that the color bar isn't united for all the subplots...
I saw that all of the solutions for this problem require converting the data to an image (imshow for example), and that really messes up my plot.</p>
<p>I want the plots to stay the way they are and just change the colors.
Does anyone have an idea how to do it?</p>
<p>Thanks :-)</p>
",13483689.0,-1.0,N/A,2022-08-15 02:51:53,How to unite the color bar for hist2d subplots?,<python><matplotlib><data-science><histogram>,2,3,N/A,CC BY-SA 4.0
74014567,1,-1.0,2022-10-10 11:55:55,-1,35,"<p>A cat shelter has many cats living there. Stray cat that frequently enters the shelter is given a collar containing an RF (Radio Frequency) tag. However, sometimes not all cats enter the cat shelter every day.</p>
<p>At the shelter door there is a sensor that detects a collar containing an RF tag when a cat enters. So, one cat can be detected several times in and out of the shelter per day.
This is sensor data (simplified).</p>
<pre><code>data = {'Date': ['01-09-2022', '01-09-2022', '01-09-2022', '01-09-2022', '02-09-2022', '02-09-2022', '02-09-2022', '02-09-2022', '03-09-2022', '03-09-2022', '03-09-2022', '03-09-2022', '03-09-2022'],
        'Name': ['A', 'A', 'A', 'A', 'B', 'C', 'C', 'B', 'D', 'C', 'C', 'D', 'A']}
df = pd.DataFrame(data)
df
</code></pre>
<p>The data like this</p>
<pre><code>    Date        Name
0   01-09-2022  A
1   01-09-2022  A
2   01-09-2022  A
3   01-09-2022  A
4   02-09-2022  B
5   02-09-2022  C
6   02-09-2022  C
7   02-09-2022  B
8   03-09-2022  D
9   03-09-2022  C
10  03-09-2022  C
11  03-09-2022  D
12  03-09-2022  A
</code></pre>
<p>The question is, how do I know how many days a cat named &quot;A&quot; comes to the shelter per three days? (although the cat named &quot;A&quot; appears several times per day, it still counts as one).</p>
",12942986.0,-1.0,N/A,2022-10-10 12:36:37,How to count duplicated value as one per day?,<python><dataframe><csv><date><data-science>,2,0,N/A,CC BY-SA 4.0
70653689,1,70653747.0,2022-01-10 14:08:27,2,212,"<p>if a dataset contains both nominal and ordinal columns, is the dataset need different encoders like OneHotEncoder for nominal and OrdinalEncoder for ordinal values?</p>
",13704980.0,-1.0,N/A,2022-01-10 14:13:08,Should I use both OneHotEncoder and OrdinalEncoder in one dataset?,<data-science><data-analysis><one-hot-encoding><data-preprocessing>,1,0,N/A,CC BY-SA 4.0
70656302,1,-1.0,2022-01-10 17:10:26,0,296,"<p>I am trying to create a bar plot from a dataset that has descending integers as the y values, and a last name as the x values. All values seem to be plotted properly, but the first few values seem to default to having an error bar that offsets the full bar itself. Instead of going up to the actual value, it stops at the middle of the error bar, and then the top of the error bar goes to the actual value of the x value. What could be going on here and why is the offset automatically happening for those values?</p>
<pre><code>import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

pd.set_option(&quot;display.max_rows&quot;, 50)
pd.set_option(&quot;display.min_rows&quot;, 30)

sns.set(rc = {&quot;figure.figsize&quot;: (15, 8)})

df = pd.read_csv(&quot;skaters.csv&quot;)

df[[&quot;name&quot;, &quot;last_name&quot;]] = df[&quot;name&quot;].str.split(&quot; &quot;, 1, expand = True)

df = df.rename(columns = {&quot;name&quot;: &quot;first_name&quot;})
df[&quot;faceoffsWon&quot;] = df[&quot;faceoffsWon&quot;].astype(int)

smalldf = df.sort_values(by = [&quot;faceoffsWon&quot;], ascending = False).head(30)
plot = sns.barplot(data = smalldf, x = &quot;last_name&quot;, y = &quot;faceoffsWon&quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/26E0q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/26E0q.png"" alt=""enter image description here"" /></a></p>
",17894938.0,8881141.0,2022-01-10 17:15:00,2022-01-10 17:15:00,Seaborn Bar Plot Offsetting Values by Error,<python><matplotlib><seaborn><data-science>,0,2,N/A,CC BY-SA 4.0
74026782,1,-1.0,2022-10-11 10:41:09,0,71,"<p>Please see code below. For some reason when i split using the kfold method, after the second split, the length of the sample changes.</p>
<pre><code>import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np


from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
from sklearn import datasets, ensemble
from scipy.interpolate import make_interp_spline

import random
import math
import statistics
random.seed(1)

from numpy import array
from sklearn.model_selection import KFold
from statistics import mean

df=pd.read_csv('auto-mpg.csv')
df= df.drop(columns=['carname'])
X = np.array(df.iloc[:, 1:7])
y = np.array(df['mpg'])

kf = KFold(n_splits=10)

def prop_reg(X_test,X_train,y_train,y_test,kappa):
    d = np.zeros((len(X_test),len(X_train)))
    for i in range(len(X_test)):
        for j in range(len(X_train)):
            d[i,j] = np.linalg.norm(X_test[i]-X_train[j])
    c=np.zeros(len(y_test))
    for i in range(len(y_test)):
        l=0
        v=0
        for j in range(len(y_train)):
            l+=y_train[j]/((1+d[i,j])**kappa)
            v+=1/((1+d[i,j])**kappa)
        c[i]=l/v
    return(c)
 
mse = 0.0
for kappa in [10,20,30,40,50]:
    mse = 0.0
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        print(len(y[test_index]))
        pr = prop_reg(X_test,X_train,y_train,y_test,kappa)
#        mse += mean_squared_error(y_test,c)/10
#    print(&quot;mse {} kappa {}&quot;.format(mse,kappa))
#    print(&quot;======================================&quot;)
</code></pre>
<p>The results for this are</p>
<p>40
40
39
39
39
39
39
39
39
39
40
40
39
39
39
39
39
39
39
39
40
40
39
39
39
39
39
39
39
39
40
40
39
39
39
39
39
39
39
39
40
40
39
39
39
39
39
39
39
39</p>
",20212616.0,-1.0,N/A,2022-10-11 10:54:54,Trying to use a KFold split to create test and train datasets and the test_index values are different for each split after the second split,<python><data-science><k-fold>,1,0,N/A,CC BY-SA 4.0
74030658,1,-1.0,2022-10-11 15:36:57,0,25,"<p>I am taking the Data Science course on DataCamp.On one of the examples there were some kind of lack of an explanation about the numpy addittion rules. I am sending the picture of the example and the question below. What i did not understood was how a 2 array with diffrent values can be add up and give a solution like that.</p>
<p><a href=""https://i.stack.imgur.com/zvwXl.png"" rel=""nofollow noreferrer"">DataCamp Numpy example </a></p>
<p><a href=""https://i.stack.imgur.com/sHJlH.png"" rel=""nofollow noreferrer"">Code Python</a></p>
<pre><code>
In [1]:
np.array([True, 1, 2]) + np.array([3, 4, False])
Out[1]:
array([4, 5, 2])

</code></pre>
",15599955.0,15599955.0,2022-10-11 15:43:15,2022-10-11 16:05:32,"Adding 2 Diffrent NumPy arrays with diffrent values inside (Boolean , int)",<python><arrays><numpy><data-science><addition>,1,5,N/A,CC BY-SA 4.0
74052198,1,74052417.0,2022-10-13 07:50:42,1,58,"<p>I have two tables, the first one contains 300 rows, each row presents a case with 3 columns in which we find 2 constant values presenting the case, the second table is my data table collected from sensors  contains the same indicators as the first except the case column, the idea is to detect to which case belongs each line of the second table knowing that the data are not the same as the first but in the range.</p>
<p>example:</p>
<p>First table:</p>
<pre><code>  [[1500, 22, 0], [1100, 40, 1], [2400, 19, 2]]
    columns=['analog', 'temperature', 'case'])**
</code></pre>
<p>second table:</p>
<pre><code>[[1420, 20], [1000, 39], [2300, 29]]
 columns=['analog', 'temperature']
</code></pre>
<p>I want to detect my first row (1420 20) belongs to which case?</p>
",20228817.0,214143.0,2022-10-13 15:17:53,2022-10-13 15:17:53,Python get specific value from HDF table,<python><pandas><dataframe><data-science><iot>,2,0,N/A,CC BY-SA 4.0
70645760,1,-1.0,2022-01-09 21:40:59,0,339,"<p>I'm trying to subplot annotated_heatmap instance with go table , when Im subploting both the annotations on the heatmap are gone and I can't reproduce it .</p>
<pre><code>[I want my heatmap will be as this on ][1]

[but this is what I get][2]

  [1]: https://i.stack.imgur.com/V8aEc.png
  [2]: https://i.stack.imgur.com/wP9cp.png

here is my code- 
</code></pre>
<p>#here I create the subplots</p>
<pre><code>fig = make_subplots(
    rows=2, cols=1,
    shared_xaxes=False,
    vertical_spacing=0.05,
    specs=[[{&quot;type&quot;: &quot;table&quot;}],
           [{&quot;type&quot;: &quot;scatter&quot;}]],
    subplot_titles=(&quot;AA table&quot;,&quot;AA heatmap&quot;)
)
#here i add go table 
fig.add_trace(fig1.data[0],
    row=1, col=1)
#here i add the heatmap
fig.add_trace(fig2.data[0],row=2,col=1)            
             


fig.update_yaxes(title_text=&quot;Wild Type amino acid&quot;, row=2, col=1)
fig.update_xaxes(title_text=&quot;New amino acid&quot;, row=2, col=1)



fig.update_layout(
    height=800,
    showlegend=False,
    title_text=&quot;Amino acids changes&quot;,
)




fig.show()
</code></pre>
",17693340.0,-1.0,N/A,2022-01-10 20:21:47,annotations in annotated_heatmap plotly lost when adding a subplot,<data-science><data-visualization><heatmap><interactive><plotly-python>,1,0,N/A,CC BY-SA 4.0
74054226,1,74054247.0,2022-10-13 10:27:24,1,33,"<p>I want to extract random samples from a <code>groupby</code> data frame object. I'd like to dynamically change the n parameter in the <code>sample(n=&quot;dynamic_value&quot;)</code> function by the <code>groupby key value</code>. I didn't come across a question or answer like this.</p>
<pre><code>d = {'name': [&quot;n1&quot;, &quot;n2&quot;, &quot;n3&quot;, &quot;n4&quot;, &quot;n5&quot;, &quot;n6&quot;], 'cc': [&quot;US&quot;, &quot;UK&quot;, &quot;US&quot;, &quot;UK&quot;, &quot;US&quot;, &quot;US&quot;], 'selected_count':[3, 1, 3, 1, 3, 3], 'view':[4, 64, 52, 2, 65, 21]}
pdf_candidate_names = pd.DataFrame(data=d)
</code></pre>
<p>The data frame output looks like this:</p>
<pre><code> name  cc  selected_count  view
0   n1  US               3     4
1   n2  UK               1    64
2   n3  US               3    52
3   n4  UK               1     2
4   n5  US               3    65
5   n6  US               3    21
</code></pre>
<p>According to the above sample data frame, I'd like to get random rows for the given <code>cc</code> using <code>sample()</code> and assign the <code>n</code> parameter according to the number in <code>selected_count</code>.  So, for example; <code>when the groupby key is US n=3, when it's UK n=1</code></p>
<p>I tried below but it didn't work since <code>x[&quot;selection_count&quot;]</code> is not an integer but a column.</p>
<pre><code>pdf_selected_names = pd.concat([
    pdf_candidate_names.groupby(&quot;cc&quot;).apply(lambda x: x.sample(n=x[&quot;selection_count&quot;], weights='views')),
    pdf_candidate_names.groupby(&quot;cc&quot;).apply(lambda x: x.sample(n=x[&quot;selection_count&quot;], weights='views'))
]).sample(frac=1.0).reset_index(drop=True)
</code></pre>
",3542650.0,-1.0,N/A,2022-10-13 10:28:56,Get random sample with dynamic n parameter from a grouped dataframe using group key,<python><pandas><dataframe><group-by><data-science>,1,0,N/A,CC BY-SA 4.0
74062433,1,-1.0,2022-10-13 22:24:22,0,102,"<p>I am trying to use the requests package of Python to make an API call to load data in json format. I intend to use the data for analysis, so I am hoping to take away only embedded data. When I run the python code below, I receive a json object like the one listed:</p>
<pre><code>import requests
req = requests.get(url, headers=headers)
print(req.json())
</code></pre>
<p>This returns:</p>
<pre><code>{
&quot;count&quot;: 15,
&quot;next&quot;: &quot;https:&lt;&lt;url&gt;&gt;/hours/?page=2&quot;,
&quot;previous&quot;: null
}
</code></pre>
<p>Is there a way to get the json object response of &quot;https:&lt;&gt;/hours/?page=2&quot; when I make the initial api call? The actual structure I'm working with contains thousands of these urls.</p>
",19026375.0,19026375.0,2022-10-13 23:33:05,2022-10-13 23:33:05,"When parsing a json response object from an api call in python, is there a way to evaluate links from response to get only embedded data?",<python><json><api><python-requests><data-science>,0,3,N/A,CC BY-SA 4.0
74057850,1,74112503.0,2022-10-13 14:52:59,0,64,"<p>I have a dataframe like this one with different dates, channels, products and ratings.</p>
<pre><code>Date       Channel       Product       Rating
2015-01    Ch1           Pr1           4.0
2015-04    Ch1           Pr1           5.0
2016-02    Ch1           Pr1           4.5
2016-12    Ch1           Pr1           3.4
...
2015-02    Ch1           Pr2           3.0
2015-04    Ch1           Pr2           4.0
2016-07    Ch1           Pr2           3.5
2016-11    Ch1           Pr2           4.3
...
2015-01    Ch2           Pr1           4.0
2015-04    Ch2           Pr1           5.0
2016-02    Ch2           Pr1           4.5
2016-12    Ch2           Pr1           3.4
...
2015-02    Ch2           Pr2           3.0
2015-04    Ch2           Pr2           4.0
2016-07    Ch2           Pr2           3.5
2016-11    Ch2           Pr2           4.3
...
2015-02    Chn           Prm           3.0
2015-04    Chn           Prm           4.0
2016-07    Chn           Prm           3.5
2016-11    Chn           Prm           4.3
</code></pre>
<p>I have to find correlation matrixes on <code>Rating</code> by products and channels by year using <code>.resample('Y').mean()</code> on the <code>Rating</code> column.<br />
So after that I expect output like:</p>
<pre><code>Pr1:
        Ch1    Ch2    ...    Chn
    Ch1 1.0    0.8    ...    -0.3
    Ch2 0.8    1.0    ...    0.6
    ...
    Chn -0.3  0.6     ...    1.0

Pr2:
        Ch1    Ch2    ...    Chn
    Ch1 1.0    0.4    ...    0.5
    Ch2 0.4    1.0    ...    -0.1
    ...
    Chn 0.5    -0.1   ...    1.0

...

Prm:
        Ch1    Ch2    ...    Chn
    Ch1 1.0    0.4    ...    0.5
    Ch2 0.4    1.0    ...    -0.1
    ...
    Chn 0.5    -0.1   ...    1.0
</code></pre>
",12536551.0,12536551.0,2022-10-18 13:56:45,2022-10-18 14:14:32,Compute correlation using pandas on a dataset with multiple groups,<python><pandas><data-science>,1,1,N/A,CC BY-SA 4.0
70668470,1,-1.0,2022-01-11 14:27:05,1,164,"<p>I have a sampling type:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Text</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>TEXT</td>
<td>Yes</td>
</tr>
<tr>
<td>TEXT</td>
<td>No</td>
</tr>
<tr>
<td>TEXT</td>
<td>Yes</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p>each text can only belong to one class. But the sample contains items with only 2 out of 3 possible target values.</p>
<p>I use the GradientBoostingClassifier model to train text classification, and the .predict_proba function to get a probabilistic answer. But the sample contains only 2 of the 3 possible values, so the function returns answers of type [float,float] (e.g. [0.8,0.2]), although I want an answer of type [float, float, float] (e.g. [0.7,0.2,0.1]). So I converted the sample values as follows:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Text</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>TEXT</td>
<td><code>[1,0,0]</code></td>
</tr>
<tr>
<td>TEXT</td>
<td><code>[0,1,0]</code></td>
</tr>
<tr>
<td>TEXT</td>
<td><code>[1,0,0]</code></td>
</tr>
<tr>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p>But the model doesn't want to learn from them. An error is displayed.</p>
<pre><code>~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py in _assert_all_finite(X, allow_nan, msg_dtype)
    108     # for object dtype data, we only check for NaNs (GH-13254)
    109     elif X.dtype == np.dtype('object') and not allow_nan:
--&gt; 110         if _object_dtype_isnan(X).any():
    111             raise ValueError(&quot;Input contains NaN&quot;)
    112 

AttributeError: 'bool' object has no attribute 'any'
</code></pre>
<p>How can I make the model be trained on lists?</p>
<p>My code:</p>
<pre><code>gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
                                max_depth=1, random_state=0)
text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', gbc)
                    ])
text_clf.fit(x,y)
classes = text_clf.classes_
logits = text_clf.predict_proba(X_val)
</code></pre>
<p>P.S. I'm going to use not only GradientBoostingClassifier, but also GaussianProcessClassifier, LinearSVC, LogisticRegression, LogisticRegressionCV.</p>
",17234913.0,12370687.0,2022-08-28 01:41:52,2022-08-28 01:41:52,How do I train a sklearn model on a list of numbers?,<python-3.x><machine-learning><scikit-learn><data-science><multilabel-classification>,2,0,N/A,CC BY-SA 4.0
74077210,1,-1.0,2022-10-15 06:28:59,0,548,"<p>I have lots of pdf in same format (contents are different).
the pdf contains text, tables,etc.
The pdf also has bold text which I want to extract and convert it into column name and the details under the bold text, I want to extract and convert it into rows.
the pdf contains tables also.
I want to do this all-in python.
any idea?</p>
<p>this is what I tried so far.
no idea after that.</p>
<pre><code>import PyPDF2
df=PyPDF2.PdfFileReader(&quot;246427 postop note.pdf&quot;)
print(df.getNumPages())

str1=&quot;&quot;&quot;&quot;&quot;&quot;
for i in range(0,4):
    str1+=df.getPage(i).extractText()
print(str1)
</code></pre>
",-1.0,-1.0,N/A,2022-10-15 06:28:59,"how to extract bold text , unbold text and tables from pdf in python?",<python><dataframe><data-science><data-analysis><exploratory-data-analysis>,0,2,N/A,CC BY-SA 4.0
70675944,1,70678693.0,2022-01-12 03:23:19,1,37,"<p>I am applying PCA in my csv data.
After normalization, seems PCA is working.
I want to plot projection by making 4 components. but I am stuck with this error :</p>
<pre><code> type         x         y  ...             fx             fy   fz
0     0 -0.639547 -1.013450  ... -8.600000e-231 -1.390000e-230  0.0
0     1 -0.497006 -2.311890  ...   0.000000e+00   0.000000e+00  0.0
1     0  0.154376 -0.873189  ...  1.150000e-228 -1.480000e-226  0.0
1     1 -0.342055 -2.179370  ...   0.000000e+00   0.000000e+00  0.0
2     0  0.312719 -0.872756  ... -2.370000e-221  2.420000e-221  0.0

[5 rows x 10 columns]

(1047064, 10)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-28-0b631a51ce61&gt; in &lt;module&gt;()
     33 
     34 
---&gt; 35 finalDf = pd.concat([principalDf, df[['type']]], axis = 1)

4 frames
/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py in _verify_integrity(self)
    327         for block in self.blocks:
    328             if block.shape[1:] != mgr_shape[1:]:
--&gt; 329                 raise construction_error(tot_items, block.shape[1:], self.axes)
    330         if len(self.items) != tot_items:
    331             raise AssertionError(

ValueError: Shape of passed values is (2617660, 5), indices imply (1570596, 5)
</code></pre>
<p>This is my code:</p>
<pre><code>import sys
import pandas as pd
import pylab as pl
import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


df1=pd.read_csv('./data/1.csv')
df2=pd.read_csv('./data/2.csv')
df = pd.concat([df1, df2], axis=0).sort_index()
print(df.head())
print(df.shape)

features = ['x', 'y', 'z', 'vx', 'vy', 'vz', 'fx', 'fy', 'fz']
# Separating out the features
x = df.loc[:, features].values
# Separating out the target
y = df.loc[:,['type']].values
# Standardizing the features
x = StandardScaler().fit_transform(x)

pca = PCA(n_components=4)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
             , columns = ['pcc1','pcc2','pcc3', 'pcc4'])


finalDf = pd.concat([principalDf, df[['type']]], axis = 1)
</code></pre>
<p>I guess I am getting error while concat my components and df['type'].</p>
<p>Can I get idea to get rid of this error?</p>
<p>Thank you.</p>
",16780162.0,-1.0,N/A,2022-01-12 08:57:01,shape error while concating columns after Principal Analysis in csv,<python><python-3.x><pandas><data-science><pca>,1,0,N/A,CC BY-SA 4.0
74081015,1,74082641.0,2022-10-15 16:05:23,0,1423,"<p>I have encountered  this error</p>
<pre><code>&quot;ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.&quot; 
</code></pre>
<p>while running this python code line 5</p>
<pre><code>1    print(data.datasetsNames)
2    for dataset in data.datasetsNames:
3       X, Y, dictActivities = data.getData(dataset)
4
5       for train, test in kfold.split(X, Y):
.
.
.
.
10 def getData(datasetName):
11    X = np.load('./npy/' + datasetName + '-x.npy')
12    Y = np.load('./npy/' + datasetName + '-y.npy')
13    dictActivities = np.load('./npy/' + datasetName + '-labels.npy').item()
14   return X, Y, dictActivities
</code></pre>
<p>Y is output of getdata function and the result is a 1d array which its variables is in range 0 to 6.
Y=[1,2,5,0,0,0,6]</p>
<p>I checked with the bellow code the target type for X and Y:</p>
<p>X was <code>multiclass-multioutput</code>
Y was <code>unknown</code>.</p>
<pre><code>from sklearn.utils.multiclass import type_of_target

print(type_of_target(X))
print(type_of_target(Y))
</code></pre>
<p>I read somewhere that the label_encoder can solve the error but I could not to solve it.</p>
<pre><code>from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(target_labels)
</code></pre>
<p>any help please.....Thanks</p>
<p>The src code is hear : <a href=""https://github.com/danielelic/deep-casas/blob/master/train.py"" rel=""nofollow noreferrer"">https://github.com/danielelic/deep-casas/blob/master/train.py</a></p>
",11289831.0,11289831.0,2022-10-15 16:09:29,2022-10-15 20:06:44,"""ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead."" in dataset kfold split",<python><deep-learning><data-science><valueerror><k-fold>,1,2,N/A,CC BY-SA 4.0
70681490,1,-1.0,2022-01-12 12:22:07,0,483,"<p>I have a data set whereby the data in ever cell is mixed up with the column name in each cell as illustrated below:</p>
<pre><code>Gender
“Gender”:”male”
“Gender”:”female”
“Gender”:”male”
“Gender”:”female”
</code></pre>
<p>I am in the process of cleaning it via anaconda and I have tried all but to no avail. I want it to look as illustrated below:</p>
<pre><code>Gender
Male
Female
Male
Female
</code></pre>
",17914473.0,13302.0,2022-01-12 21:41:09,2022-01-12 21:41:09,How do I separate these data from each other,<python><pandas><anaconda><data-science><data-cleaning>,3,2,N/A,CC BY-SA 4.0
70682509,1,70682867.0,2022-01-12 13:36:01,1,586,"<p>I have a pandas dataframe with some very extreme value - more than 5 std.
I want to replace, per column, each value that is more than 5 std with the max other value.
For example,</p>
<pre><code>df = A B
     1 2
     1 6
     2 8
     1 115
     191 1
</code></pre>
<p>Will become:</p>
<pre><code>df = A B
     1 2
     1 6
     2 8
     1 8
     2 1
</code></pre>
<p>What is the best way to do it without a for loop over the columns?</p>
",6057371.0,6057371.0,2022-01-12 13:45:29,2022-01-12 14:10:47,pandas dataframe how to replace extreme outliers for all columns,<pandas><dataframe><data-science><outliers>,3,7,N/A,CC BY-SA 4.0
74085472,1,-1.0,2022-10-16 08:03:31,0,56,"<p>I want to remove the word <code>DISCONTINUED</code> from a column using a loop and train the model for further use. The code that I have tried:</p>
<pre><code># 1.

for i in  df['DESCRIPTION'][0]:
       if i[0]== 'DISCONTINUED':
           df.i[0].pop(0)

# 2.

for item in df['DESCRIPTION']:
       if str(item)[0]=='DISCONTINUED':
           df.remove(item[0])
</code></pre>
<p>Note: <code>df</code> is the dataset name and <code>DESCRIPTION</code> is the column name. The column has the dtype <code>object</code>. I have tried to convert that into <code>str</code>, but it didn't work.</p>
<p>The values in the column DESCRIPTION are:</p>
<pre><code>data = {'DESCRIPTION': 
        ['ANDREW, 245173, 1/2-1/2 COLD SHRINK KIT, CEQ.24038',
         'COMMSCOPE, 245174, 1/2, 3/8 COLDSRINK WTHRPRFNG KIT, CEQ.24753',
         'DISCONTINUED, COMMSCOPE, 252107, LACE UP I-LINE HOISTING GRIP FOR 1/2 CABLES',
         'COMMSCOPE, 252110, LACE UP HOISTING GRIP FOR 1-1/4 COAX &amp; EW63/64 WAVEGUIDE',
         'ANDREW, 252111, 1-5/8 HOISTING GRIP, LACE UP']}
</code></pre>
<p>I want to remove <code>DISCONTINUED</code> from a column having multiple values in it separated by a comma.</p>
",15811628.0,18470692.0,2022-10-16 09:03:21,2022-10-16 09:08:41,Is there any solution to remove the 0th value from multiple values of a column separated by comma,<python><python-3.x><machine-learning><jupyter-notebook><data-science>,2,1,N/A,CC BY-SA 4.0
74086964,1,-1.0,2022-10-16 12:06:30,0,18,"<p>I've got a total of 9 sensors in the ground, which measure the water content of the soil. 1-3 are in a depth of 1m, 4-6 are in a depth of 2m and sensors 7-9 are in a depth of 3m.</p>
<p>My dataset also contains the precipiation of the location. It is hourly data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Time</th>
<th style=""text-align: center;"">Sensor-ID</th>
<th style=""text-align: center;"">Precipitation</th>
<th style=""text-align: right;"">Soil Water Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">2022-01-01 11:00</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">74</td>
<td style=""text-align: right;"">120</td>
</tr>
<tr>
<td style=""text-align: left;"">2022-01-01 11:00</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">74</td>
<td style=""text-align: right;"">100</td>
</tr>
<tr>
<td style=""text-align: left;"">2022-01-01 11:00</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">74</td>
<td style=""text-align: right;"">110</td>
</tr>
<tr>
<td style=""text-align: left;"">...</td>
<td style=""text-align: center;"">...</td>
<td style=""text-align: center;"">...</td>
<td style=""text-align: right;"">...</td>
</tr>
<tr>
<td style=""text-align: left;"">2022-01-01 11:00</td>
<td style=""text-align: center;"">9</td>
<td style=""text-align: center;"">74</td>
<td style=""text-align: right;"">30</td>
</tr>
</tbody>
</table>
</div>
<p>The goal now is to find out if the different ground / soil depths behave differently regarding the water content after raining (over time).</p>
<p>I thought about a clustering method to find out if the sensors can be clustered based on the data and confirm this. Since I'm not very experienced in data science, would that be the right approach and is it even possible to analyse it with clustering?</p>
",17562098.0,-1.0,N/A,2022-10-17 11:44:35,Suitable Clustering Approach,<data-science><cluster-analysis>,1,0,N/A,CC BY-SA 4.0
70684084,1,70685116.0,2022-01-12 15:21:47,0,304,"<p>I have a df with incidents and warnings and I would like to store the last 10 minutes of data that came before each incident in another df called &quot;df1&quot;. I have added a boolean column to identify incidents in the df but I need a way to loop through the whole df and store the rows that occurred 10 minutes before each incident along with the incident itself.</p>
<p>for ex:</p>
<pre><code>A                B                   Incident
warning      2018-11-01 01:37:12        F
warning      2018-11-01 01:37:41        F
warning      2018-10-31 01:37:48        F
warning      2018-10-30 01:40:31        F
warning      2018-10-29 01:43:41        F
warning      2018-10-28 01:45:51        F
incident     2018-10-27 01:47:41        T
</code></pre>
<p>In this case i would need to store</p>
<pre><code>warning      2018-11-01 01:37:41        F
warning      2018-10-31 01:37:48        F
warning      2018-10-30 01:40:31        F
warning      2018-10-29 01:43:41        F
warning      2018-10-28 01:45:51        F
incident     2018-10-27 01:47:41        T
</code></pre>
<p>If there is another incident contained within the last 10 minutes of the first incident, I would like to use all the data before the second incident occurred. i.e., if no incident in last 10 mins, append df A, else append df B to df1.</p>
",17907122.0,17907122.0,2022-01-12 15:31:49,2022-01-20 23:36:53,How to print out the last n minutes of rows before a specific row in pandas?,<python><pandas><data-science>,2,2,N/A,CC BY-SA 4.0
74088692,1,-1.0,2022-10-16 16:08:58,0,40,"<p>I am  working on a case study where I have to figure out some trends but the data is not in one dataset so I used <strong>bind_row</strong> function in R to bind all the datasets and get a dataset of the whole data but this error popped up.</p>
<p>all_trips_2013_2022&lt;-
bind_rows(jun_to_dec_2013_trip_data,
jan_to_jun_2014_trip_data,
jul_2014_trip_data,
aug_to_sep_2014_trip_data)</p>
<p>Error in <code>bind_rows()</code>:
! cannot allocate vector of size 268.3 Mb.</p>
",20253127.0,20253127.0,2022-10-16 18:01:37,2022-10-16 18:01:37,how to bind datasets in r if the size of the vector is more than 300mb,<r><dataframe><dataset><data-science><data-cleaning>,1,2,N/A,CC BY-SA 4.0
74089984,1,-1.0,2022-10-16 19:06:13,-1,119,"<p>I'm working in a project with more than 2000 class with 230000 rows , The dataset consists of two columns product name and category name. I applied NLP techniques to vectorize the texts and used linear svm to predict the category of the products and I reached a training accuracy of 92 % and 84% on testing.The problem here is that I have imbalanced classes as some categories have 30000 product or more and some have 200 or 1000 or 10000 product so I tried to over sample the minor classes to the majority class but it gives me an error so is oversampling the minor classes is the right technique or should I try something else? <a href=""https://i.stack.imgur.com/RqDYZ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",20257578.0,4685471.0,2022-10-16 21:52:29,2022-10-16 21:52:29,Multi text classification problem with more than 2000 class,<python><machine-learning><data-science><data-preprocessing>,1,4,N/A,CC BY-SA 4.0
70690081,1,-1.0,2022-01-13 00:30:07,1,1008,"<p>I'm trying to merge two Xarray datasets. The resolutions of the datasets are different (one has more points than the other). Ultimately, I need to multiply the values together into one dataset.</p>
<p>I need it to be pretty fast, so nested &quot;for&quot; loops through x and y coordinates won't be optimal (I'm working with big datasets). Is there any clean way to do this that I'm not yet aware of? Thanks so much.</p>
",17770232.0,-1.0,N/A,2023-05-10 23:21:48,Xarray combine datasets with different dimensions?,<pandas><numpy><data-science><python-xarray><grib>,2,1,N/A,CC BY-SA 4.0
74098485,1,-1.0,2022-10-17 13:59:14,0,394,"<p>I have created a ML model with Random forest it has 6000+ data with 27 features out of which about 22 were categorical data i have used label encoder on it.Now when i have to predict the result is their a way that when i enter details into as categorical data it is converted to the same numbers assigned earlier by label encoder to the data.Or what approach should i use ?
<a href=""https://i.stack.imgur.com/7kPbF.png"" rel=""nofollow noreferrer"">Heres the Screenshot of data that has been encoded by label encoder</a></p>
",16728211.0,-1.0,N/A,2022-10-17 14:16:43,Label Encoder to Categories,<python><dataframe><machine-learning><data-science><label-encoding>,1,1,N/A,CC BY-SA 4.0
74024710,1,74025118.0,2022-10-11 07:51:33,0,52,"<p>This is my code-</p>
<pre class=""lang-py prettyprint-override""><code>def read_data():
    filename=(r&quot;\School_Project\Data\votes.csv&quot;)
    with open(filename) as csvfile:
        for data in csvfile: #here data 
            csvfile.seek(0)
            for x in csvfile.readlines():
                return print(x)

read_data()
</code></pre>
<p>Here the <strong>data</strong> is not iterating i.e. for loop isnt working well inside function body and cannot print all the values in the file only 1st line is being printed
<code>Please help me out with this error</code></p>
",16358041.0,-1.0,N/A,2022-10-11 08:24:57,python for loop not working inside a function body,<python><csv><for-loop><data-science>,3,0,N/A,CC BY-SA 4.0
74090460,1,74090675.0,2022-10-16 20:20:31,0,43,"<p>I am working now on plot my dataset by boxplot as in below code</p>
<pre><code>plt.figure(figsize=(8,5))
fig = plt.figure()
num_list=Final_dataset.columns.values.tolist()
for i in range(len(num_list)):
    column=num_list[i]
    sns.boxplot(x=&quot;label&quot;, y=column, data=Final_dataset, palette='Set2')
    plt.savefig('{}.png'. format(i))
    plt.show()
</code></pre>
<p>I need to produce one image that combine all attributes figures as in this <a href=""https://i.stack.imgur.com/EYITq.png"" rel=""nofollow noreferrer"">figure</a> rather than several figures. how Ican fix it? thanks, a lot</p>
",18900893.0,-1.0,N/A,2022-10-16 20:58:47,How to combine boxplot figures into one?,<python><dataframe><matplotlib><data-science><boxplot>,1,0,N/A,CC BY-SA 4.0
70689359,1,70689614.0,2022-01-12 22:47:20,0,238,"<p>Python: How can I make column values as column headers<br />
Problem:</p>
<p><a href=""https://i.stack.imgur.com/d8SGj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d8SGj.png"" alt=""Question"" /></a></p>
<p>Possible solution (Index NOC)</p>
<p><a href=""https://i.stack.imgur.com/inxeW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/inxeW.png"" alt=""enter image description here"" /></a></p>
",14601834.0,-1.0,N/A,2022-01-12 23:18:24,Python: How can I make column values as column headers,<python><group-by><jupyter-notebook><data-science>,1,5,N/A,CC BY-SA 4.0
74100095,1,-1.0,2022-10-17 16:05:38,0,139,"<p>from the book data-science for supply chain forecast by Nicolas Vandeput
first chap, in this code snippet plt.show() dispaly data but I can not find at what point the arguments passed to it.</p>
<pre><code>from chap_01_05_moving_average_function import (
    moving_average,
)

import matplotlib.pyplot as plt

d = [28, 19, 18, 13, 19, 16, 19, 18, 13, 16, 16, 11, 18, 15, 13, 15, 13, 11, 13, 10, 12]
df = moving_average(d)
print(df)

df[[&quot;Demand&quot;, &quot;Forecast&quot;]].plot()

df.index.name = &quot;Periods&quot;
print(&quot;index&quot;, df.index.name)
df[[&quot;Demand&quot;, &quot;Forecast&quot;]].plot(
    figsize=(8, 3), title=&quot;Moving average&quot;, ylim=(0, 30), style=[&quot;-&quot;, &quot;--&quot;]
)

plt.show()
</code></pre>
<p>this is chap_01_05_moving_average_function.py:</p>
<pre><code># page 5 - moving average

import numpy as np
import pandas as pd


def moving_average(d, extra_periods=1, n=3):
    d = np.array(d)
    cols = len(d)
    d = np.append(d, [np.nan] * extra_periods)
    f = np.full(cols + extra_periods, np.nan)

    for t in range(n, cols + 1):
        f[t] = np.mean(d[t - n : t])

    f[cols + 1 :] = f[t]

    df = pd.DataFrame.from_dict({&quot;Demand&quot;: d, &quot;Forecast&quot;: f, &quot;Error&quot;: d - f})

    return df


# numpy - add arrays
ts = np.array([1, 2, 3, 4, 5, 6])
ts2 = np.array([10, 20, 30, 40, 50, 60])
print(ts + ts2)

# numpy - list

alist = [1, 2, 3]
alistmean = np.mean(alist)
print(alistmean)

# slicing

alist = [&quot;cat&quot;, &quot;dog&quot;, &quot;mouse&quot;]
print(alist[1])

anarray = np.array([1, 2, 3])
print(anarray[0])

# slicing - start:end

alist = [&quot;cat&quot;, &quot;dog&quot;, &quot;mouse&quot;]
print(alist[1:])

anarray = np.array([1, 2, 3])
print(anarray[:1])

# slicing - negative

alist = [&quot;cat&quot;, &quot;dog&quot;, &quot;mouse&quot;]
print(alist[-1])

print(alist[:-1])

</code></pre>
<p>I am new to python I dont know if there is an implicit way of passing parameters</p>
",12096637.0,-1.0,N/A,2022-10-17 16:05:38,how plt.show() gets the parameters here?,<python><pandas><numpy><matplotlib><data-science>,0,3,N/A,CC BY-SA 4.0
70660666,1,70660691.0,2022-01-11 01:30:45,0,1165,"<p>Assume there is a list that includes a numpy array such as</p>
<pre><code>import numpy as np 
output = [np.array([[[5.21]],
                    [[2.22]],
                    [[1.10]],
                    [[3.76]]], dtype=np.float32)]
</code></pre>
<p>Is there any quick way to extract values from this output list such as</p>
<pre><code>result = [5.21, 2.22, 1.10, 3.76]
</code></pre>
<p>many thanks</p>
",10934417.0,-1.0,N/A,2022-01-11 01:35:29,Is there a quick way to extract values from a numpy array?,<python><numpy><data-science>,1,2,2022-01-11 01:37:43,CC BY-SA 4.0
74063122,1,-1.0,2022-10-14 00:24:29,3,458,"<p>recently, I fell in love with jupyter notebook in vs-code. but I got into trouble while I was trying to profile my code. I was looking for something similar as in google-colab or even the original browser base jupyter-nb. I'm actually asking about some extensions or options or even cell(s) of code that can approximately give me some information about resources (such as ROM, RAM,...) by my nb?</p>
",20234819.0,-1.0,N/A,2022-10-14 00:24:29,is there any way to monitor resources used by my Jupyter notebook in visual studio code?,<python><jupyter-notebook><data-science><monitoring>,0,0,N/A,CC BY-SA 4.0
74088490,1,-1.0,2022-10-16 15:46:49,0,12,"<p><a href=""https://i.stack.imgur.com/xwbUT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xwbUT.png"" alt=""enter image description here"" /></a></p>
<p>This is the architecture of my folder I want to select all patients' data weather positive or negative and copy them into the new folder name Poisitive_Img_folder and Negative_Img_folder.</p>
<p>Kindly suggest me any technique. i used glob and os walk but failed to do so. In python</p>
",5782604.0,-1.0,N/A,2022-10-16 15:46:49,Want to move subfolder files with specific name to new folder,<python><data-science><glob><subdirectory><os.walk>,0,3,N/A,CC BY-SA 4.0
74106022,1,-1.0,2022-10-18 05:20:37,0,24,"<p>Currently i am working on a project in which the use case is for example:
there are two data frames one from the db and the other from the user uploaded file.The user uploaded file will be having unnamed columns</p>
<pre><code>DB DATA FRAME
|customer_id | name | age | days_as_customer | revenue | payment |
|------------|------|-----|------------------|---------|---------|
| 00001      | x1   | 25  | 40               | 2000    |monthly  |
| 00002      | x2   | 51  | 200              | 10000   | yearly  |


USER UPLOADED FILE
|customer_id | C1   | C2  | C3               | C4      | c5        |
|------------|------|-----|------------------|---------|-----------|
| 00011      | x5   | 45  | 1                | 8000    |quarterly   |
| 00022      | x6   | 33  | 20               | 1000    |half-yearly|

</code></pre>
<p>What i need is that i have to match the columns from the user uploaded file to the db columns ie:</p>
<pre><code>output 
{
age : C2,
days_as_customer : C3,
revenue : C4
}
</code></pre>
<p>Previously i tried using ks-test, qunatiles, skewness, central tendencies but the results are not as expected because most of time the uploaded file has central tendencies which is completely different from the db data frame</p>
<p>Can anyone suggest me any techniques to solve this ?</p>
",17961980.0,17961980.0,2022-10-18 05:49:30,2022-10-18 05:49:30,How to find the similar numeric column from one table with the column in another table,<statistics><data-science>,0,2,N/A,CC BY-SA 4.0
74107696,1,74107714.0,2022-10-18 08:07:23,1,228,"<p>I have this type of dataframe;</p>
<p><code>A =  [&quot;axa&quot;,&quot;axb&quot;,&quot;axc&quot;,&quot;axd&quot;,&quot;bxa&quot;,&quot;bxb&quot;,&quot;bxc&quot;,&quot;bxd&quot;,&quot;cxa&quot;.......]</code></p>
<p>My question is I have this type of data but there are more than 350 columns and for example i need only 'c' including column names in new dataframe. How can i do that?</p>
<p>new dataframe columns should look like this;</p>
<p><code>B =  A[[&quot;axc&quot;,&quot;bxc&quot;,&quot;cxa&quot;,&quot;cxb&quot;,&quot;cxc&quot;,&quot;cxd&quot;,&quot;dxc&quot;,&quot;exc&quot;,&quot;fxc&quot;.......]]</code></p>
",19648467.0,19648467.0,2022-10-18 08:08:29,2022-10-18 08:09:42,pandas filtering column names,<pandas><data-science>,2,0,2022-10-18 08:19:00,CC BY-SA 4.0
74087059,1,-1.0,2022-10-16 12:21:11,-1,25,"<p>I'm working on a AI-driven project, and I'm stuck on a data formatting problem.</p>
<p>I collect phone sensors data for training my AI. The output of a single recording session looks like this:
<a href=""https://i.stack.imgur.com/y2Txc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y2Txc.png"" alt=""enter image description here"" /></a></p>
<p>My goal is to combine CSV files of inertial sensors (i.e. Accelerometer, Gravity, Gyroscope, Orientation etc) into one file. Most of the times such files have equal length and timestamps and work on a same frequency, but sometimes they happen to be slightly different. Example:</p>
<p><a href=""https://i.stack.imgur.com/AC3kX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AC3kX.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/DaYVm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DaYVm.png"" alt=""enter image description here"" /></a></p>
<p>I suppose that this happens due to gaps in one of the files. As far as I know, it's possible to deal with this problem using such libraries as pandas, but I'm not sure how. How to solve this problem?</p>
",20183643.0,4685471.0,2022-10-16 16:13:07,2022-10-16 16:13:07,"Resampling, interpolating and merging several CSV files",<python><csv><data-science>,1,0,N/A,CC BY-SA 4.0
74089585,1,-1.0,2022-10-16 18:09:23,0,65,"<p>I'm new to python, and trying to work through some problems. I have a dataset.csv loaded to jupyter. After running through some functions I'm trying to change some columns from str to float, some from str to int and replace empty fields with None. I don't know how to apply it to the entire column. I think this code is just applying to the header, but it doesn't make sense because the actual values of the header are all strings, ie. age itself is a string, but I need the following rows of data to change the age column from str to float, or int.</p>
<p>I've done this:</p>
<pre><code>[input]
new_type = []
last = str(last)
first = str(first)
sex = str(sex)
age = float(age)
sibsp = int(sibsp) 
pclass = int(pclass)
fare = float(fare)
survived = int(survived) 

#empty sting to none conversion 
res = [str(i or None) for i in new_type]

[output]
[['Braund', ' Mr. Owen Harris', 'male', '22', '1', '3', '7.25', '0'], ['Cumings', ' Mrs. John Bradley (Florence Briggs Thayer)', 'female', '38', '1', '1', '71.2833', '1'], ['Heikkinen', ' Miss. Laina', 'female', '26', '0', '3', '7.925', '1']]
</code></pre>
<p>Any help or suggestions would be greatly appreciated.</p>
",20257279.0,20257279.0,2022-10-16 18:11:23,2022-10-17 12:33:34,How to convert data type of different columns in a dataframe without using pandas or numpy functions?,<python><python-3.x><data-science>,0,4,N/A,CC BY-SA 4.0
74097863,1,74098240.0,2022-10-17 13:10:45,0,363,"<p>I working with the Titanic dataset and made some basic preprocessing (such as normalization, ohe, etc.).</p>
<p>Then, I tried to use H2O algorithm and got following error:</p>
<pre><code>from h2o.estimators.gbm import H2OGradientBoostingEstimator
classifier  =  H2OGradientBoostingEstimator(nfolds =    5,
                                            ntrees =   15,
                                            seed   =    42,
                                            max_depth = 4)

classifier.train(predictors, target, training_frame = train_data)
</code></pre>
<blockquote>
<p>H2OTypeError: Argument <code>x</code> should be a None | integer | string |
ModelBase | list(string | integer) | set(integer | string), got
H2OFrame</p>
</blockquote>
<p>My target is <code>train_data[&quot;Survived&quot;].asfactor()</code></p>
<p>I tried to read to dataframe from file, instead of coverting the preprocessed df into H2OFrame but to no vail.</p>
<p>Any ideas would be appreciated.</p>
",10978122.0,-1.0,N/A,2022-10-17 13:39:11,"H2OTypeError: Argument `x` should be a None | integer | string | ModelBase | list(string | integer) | set(integer | string), got H2OFrame",<python><data-science><h2o>,1,0,N/A,CC BY-SA 4.0
74108504,1,-1.0,2022-10-18 09:16:22,0,57,"<p>I am in my second year of Data-Science at University so I am not that good at python yet, I currently have an Internship at an IT company and I have two CSV files; they contain the same people but are listed in differently named columns.</p>
<p>(Victoria Gonzales is in the ADA.csv file but also in the HRS.csv file, however the columns don't have the same name or there are different things in front of the first name column f.e the salary.)</p>
<p>I want to merge the files without duplicates, how can I do that? Thanks in advance!</p>
",20271049.0,-1.0,N/A,2022-10-18 09:33:50,How to combine 2 CSV files in python using pandas with different column names?,<python><csv><merge><data-science>,1,2,N/A,CC BY-SA 4.0
74098518,1,-1.0,2022-10-17 14:01:56,0,81,"<p>i am new to this so please be tolerant for mistakes i am making. i'll try to describe my problem as good as i can. feel free to give me advises on how to improve my description of the problem.</p>
<p>my goal: i do have this timeseries plot about different kinds of air quality values for 3 different locations. one graph for each pollutant.
in addition i do have a xarray dataarray with different variables from which i only use the one called 'kind'.
i was thinking of a horizontal bar parallelly attached to each graph inside the same plot, just indicating 3 different colors which depend on the value of the array 'kind' ('A', 'B', 'NaN') for every single timestamp.</p>
<p>for the timeseries i use matplotlib.</p>
<p>i am grateful for any help provided!</p>
<p>edit: this is what i'm looking for.. a horizontal bar linked to the same xaxis so it gives me additional info on my time series values. the additional information comes from another file (xarray.dataarray)</p>
<p><a href=""https://i.stack.imgur.com/ly1TP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ly1TP.png"" alt=""enter image description here"" /></a></p>
",20213528.0,3888719.0,2022-10-18 19:08:46,2022-10-18 19:08:46,implementing an independent color bar in timeseries plot,<pandas><matplotlib><data-science><data-analysis><python-xarray>,0,6,N/A,CC BY-SA 4.0
70697979,1,-1.0,2022-01-13 14:09:00,-4,434,"<p>df = pd.DataFrame({'Fruits':['apple,guava','banana','orange'],'counts':[10,20,30]})
df.loc[df['Fruits']=='apple']</p>
<p>how can I get the count value as 10 with only one value i.e. apple ?</p>
",8511553.0,8511553.0,2022-01-13 14:44:03,2022-01-13 16:11:55,How to Select Rows of Pandas Dataframe with one Column Value which has multiple values?,<python><pandas><dataframe><data-science><pandas-loc>,1,2,N/A,CC BY-SA 4.0
70698444,1,-1.0,2022-01-13 14:40:12,0,164,"<p>I'm trying to adapt the code snippets presented on the following page:</p>
<p><a href=""https://www.ibm.com/docs/en/watson-studio-local/1.2.1?topic=data-write-sources"" rel=""nofollow noreferrer"">https://www.ibm.com/docs/en/watson-studio-local/1.2.1?topic=data-write-sources</a></p>
<p>However, I don't understand where to get the <em>dsx_core_utils</em> package at the beginning and how to install it.
If I didn't miss anything, nothing is, unfortunately, documented on the IBM homepage and google only delivers a few results all redirecting to the IBM homepage.</p>
<p>Does anybody know if this package was moved or renamed etc.? Or maybe knows another package that would accomplish the tasks that the original package is used for?</p>
",3856569.0,-1.0,N/A,2022-01-14 09:57:11,Python: how to install dsx_core_utils package,<python><db2><data-science-experience>,1,1,N/A,CC BY-SA 4.0
74117905,1,-1.0,2022-10-18 22:01:28,0,28,"<p>I am trying to merge two dataframes with pandas. When I look at the data there should be two output rows, but I am only getting one row.</p>
<pre><code>bankDF = bankDF[bankDF['transaction detail'].str.contains(&quot;RSA&quot;)]
bankDF['bank index'] = bankDF['bank index'].map(str)
bankDF = bankDF.groupby(['post date']).agg({'amount': 'sum', 'bank index': ','.join}).reset_index()
bankDF['amount type'] = bankDF['amount'].dtypes

glDF = glDF[glDF['ref3'].str.contains(&quot;RSA&quot;, na=False)]

matchedGL = pd.merge(bankDF, glDF, on='amount', suffixes=(&quot;_left&quot;, &quot;_right&quot;))
</code></pre>
<p>bankDF is:</p>
<pre><code> post date     amount       amount type
 2022-05-04   -410372.62     float64
 2022-05-26   -407418.78     float64
</code></pre>
<p>glDF is:</p>
<pre><code>journal   eff date    amount      amount type
81132.0   2022-05-31  -407418.78   float64
81132.0   2022-05-31  -417118.28   float64
81592.0   2022-05-31  -410372.62   float64
</code></pre>
<p>And my output is:</p>
<pre><code>post date   amount      amount type_left  journal   eff date    amount type_right
2022-05-26  -407418.78   float64          81132.0   2022-05-31  float64
</code></pre>
<p>My expectation would be that both the rows from bankDF would match the two rows on glDF. What am I missing that would cause the -410372.62 amount not match?</p>
",12915508.0,-1.0,N/A,2022-10-18 22:01:28,Pandas not merging all rows,<python><pandas><dataframe><data-science>,0,3,N/A,CC BY-SA 4.0
74101479,1,-1.0,2022-10-17 18:15:40,0,49,"<p>Here is the beginning of my code</p>
<pre><code>library(car, quietly = TRUE)

dataTrain &lt;- read.csv(&quot;~/Downloads/AdultsTrainDataset.csv&quot;, na.strings = &quot;?&quot;)
dataTest &lt;- read.csv(&quot;~/Downloads/AdultsTestDataset.csv&quot;,   na.strings = &quot;?&quot;)

dataTrainNAremoval &lt;- sum(is.na(dataTrain))
dataTestNAremoval &lt;- sum(is.na(dataTest))

dataTrain &lt;- na.omit(dataTrain)
dataTest &lt;- na.omit(dataTest)

dataReduced &lt;- dataTrain[dataTrain$race %in% c(&quot;Black&quot;, &quot;White&quot;),]
dataReduced[dataReduced$marital.status %in% c(&quot;Married-civ-spouse&quot;, &quot;Married-AF-spouse&quot;),&quot;marital.status&quot;] &lt;- &quot;married&quot;
dataReduced[dataReduced$marital.status %in% c(&quot;Divorced&quot;, &quot;Separated&quot;,&quot;Widowed&quot;,&quot;Married-spouse-absent&quot;),&quot;marital.status&quot;] &lt;- &quot;pMarried&quot;

dataReduced[dataReduced$education %in% c(&quot;11th&quot;,&quot;12th&quot;,&quot;1st-4th&quot;,&quot;5th-6th&quot;,&quot;7th-8th&quot;, &quot;9th&quot;, &quot;HS-grad&quot;,&quot;10th&quot;, &quot;Preschool&quot;, &quot;Some-college&quot;), &quot;education&quot;] &lt;- &quot;less-than-Uni&quot; 
dataReduced[dataReduced$education %in% c(&quot;Assoc-acdm&quot;,&quot;Assoc-voc&quot;, &quot;Prof-school&quot;, &quot;Bachelors&quot;), &quot;education&quot;] &lt;- &quot;University&quot;
dataReduced[dataReduced$education %in% c(&quot;Masters&quot;,&quot;Doctorate&quot;, &quot;Prof-school&quot;), &quot;education&quot;] &lt;- &quot;advancedDegree&quot;

dataReduced$incomeNum &lt;- dataReduced$income == &quot;&gt;50K&quot; 
</code></pre>
<p>Here is how I've created my classifier</p>
<pre><code>dataToModel &lt;- dataReduced[,c(&quot;sex&quot;,&quot;marital.status&quot;,&quot;race&quot;,&quot;education&quot;,&quot;incomeNum&quot;)]
theModel &lt;- glm(formula = incomeNum ~ ., data = dataToModel, family = &quot;binomial&quot;)
summary(theModel)
</code></pre>
<p>and I would like clarification on how I can use it to return the probability of making 50k a year and then transform it into a categorical variable to use in my dataTestReduced dataframe. I've tried the following but am unsure on what my newdata input should be.</p>
<pre><code>probabilities &lt;- predict(theModel, newdata =  , type = &quot;response&quot;)
dataReduced$probabilities &lt;- probabilities &gt; &quot;50k&quot;
</code></pre>
",18326344.0,18326344.0,2022-10-18 00:47:52,2023-03-03 01:43:37,How do I use the predict function to return the probability of making 50k a year and add it as a categorical variable to my dataTestReduced dataframe?,<r><data-science><computer-science>,1,6,N/A,CC BY-SA 4.0
74134237,1,-1.0,2022-10-20 03:50:41,0,131,"<p>I've used the following code in order to find the sum of a column's values and store it:</p>
<pre><code>FavCountSum=Merge3['Favorite_Count'].sum()
</code></pre>
<p>and then I've used the following code in order to divide each number in the original column by the stored sum in order to create a new column with the respective percentages and I'd like to know how to round the percentages two decimal places.</p>
<pre><code>Merge3.assign(Favorite_Color_Percent=lambda x: x.Favorite_Count/FavCountSum)
</code></pre>
",18326344.0,-1.0,N/A,2022-10-20 04:05:16,How to round values created in dataframe column using lambda?,<python><data-science><computer-science>,1,0,N/A,CC BY-SA 4.0
74136518,1,-1.0,2022-10-20 08:15:47,1,74,"<p>My dashboard is composed of cards that inside have a header with text and a button that I want to use to expand these cards, in the body is a figure for each card.</p>
<p><a href=""https://i.stack.imgur.com/DFzvr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DFzvr.png"" alt=""enter image description here"" /></a></p>
<p>I want to ask if there is a way to press the header button and the card opens or expands and the graphics inside change their size automatically.</p>
<p>Best regards.</p>
",20282164.0,-1.0,N/A,2022-10-20 08:15:47,Expand cards and autosize figures in Dash Python,<python><data-science><plotly-dash>,0,0,N/A,CC BY-SA 4.0
70712114,1,71292077.0,2022-01-14 14:27:10,3,363,"<p>I have trained successfully a multi-output Gaussian Process model using an <code>GPy.models.GPCoregionalizedRegression</code> model of the <code>GPy</code> package. The model has ~25 inputs and 6 outputs.</p>
<p>The underlying kernel is an <code>GPy.util.multioutput.ICM</code> kernel consisting of an RationalQuadratic kernel <code>GPy.kern.RatQuad</code> and the <code>GPy.kern.Coregionalize</code> Kernel.</p>
<p>I am now interested in the feature importance on each individual output. The RatQuad kernel provides an <code>ARD=True</code> (Automatic Relevance Determination) keyword, which allows to get the feature importance of its output for a single output model (which is also exploited by the <code>get_most_significant_input_dimension()</code> method of the GPy model).</p>
<p>However, calling the <code>get_most_significant_input_dimension()</code> method on the <code>GPy.models.GPCoregionalizedRegression</code> model gives me a list of indices I assume to be the most significant inputs somehow for <strong>all</strong> outputs.</p>
<p>How can I calculate/obtain the lengthscale values or most significant features for <strong>each individual</strong> output of the model?</p>
",10309342.0,-1.0,N/A,2022-02-28 08:04:48,Most significant input dimensions for GPy.GPCoregionalizedRegression?,<python><data-science><gpy>,1,0,N/A,CC BY-SA 4.0
70712175,1,-1.0,2022-01-14 14:31:46,0,374,"<p>we are trying to use the evaluate_rc-lerc pretrained model provided by allennlp in page
<a href=""https://docs.allennlp.org/models/main/#pre-trained-models"" rel=""nofollow noreferrer"">https://docs.allennlp.org/models/main/#pre-trained-models</a>
using below colab code</p>
<pre><code>!pip install allennlp==1.0.0 allennlp-models==1.0.0
!pip install --pre allennlp-models
!pip install -U nltk

from allennlp.predictors.predictor import Predictor
import allennlp_models
# The instance we want to get LERC score for in a JSON format
input_json = {
    'context': 'context string',
    'question': 'question string',
    'reference': 'reference string', 
    'candidate': 'candidate string'
}
# evaluate_rc-lerc - A BERT model that scores candidate answers from 0 to 1.
# Loads an AllenNLP Predictor that wraps our trained model
predictor = Predictor.from_path(
    archive_path='https://storage.googleapis.com/allennlp-public-models/lerc-2020-11-18.tar.gz',
    predictor_name='lerc',
    cuda_device=0
)

output_dict = predictor.predict_json(input_json)
print('Predicted LERC Score:', output_dict['pred_score'])
</code></pre>
<p>Error:
ConfigurationError: lerc is not a registered name for Model. You probably need to use the --include-package flag to load your custom code. Alternatively, you can specify your choices using fully-qualified paths, e.g. {&quot;model&quot;: &quot;my_module.models.MyModel&quot;} in which case they will be automatically imported correctly.</p>
<p>even after using latest packages with pip install allennlp-models==2.2.0</p>
<p>getting the same error. Any working code snippets for 'evaluate_rc-lerc' will be greatly appreciated that directly uses the saved archived to avoid training/finetuning</p>
",17933949.0,17933949.0,2022-01-14 14:58:37,2022-01-16 10:08:00,ConfigurationError: lerc not in acceptable choices for dataset_reader.type,<python><data-science><allennlp><text-analytics-api>,2,0,N/A,CC BY-SA 4.0
74140028,1,-1.0,2022-10-20 12:38:53,1,24,"<p>I have a table:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Component</th>
<th>Revenue</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>4</td>
<td>10</td>
</tr>
<tr>
<td>1</td>
<td>5</td>
<td>20</td>
</tr>
<tr>
<td>2</td>
<td>4</td>
<td>15</td>
</tr>
<tr>
<td>3</td>
<td>6</td>
<td>30</td>
</tr>
</tbody>
</table>
</div>
<p>and I'd like to group by <code>ID</code>, creating a column with a dictionary or list as such:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Grouped</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>[[4, 10], [5,20]]</td>
</tr>
<tr>
<td>2</td>
<td>[4, 15]</td>
</tr>
<tr>
<td>3</td>
<td>[6, 30]</td>
</tr>
</tbody>
</table>
</div>
<p>I know using</p>
<pre><code>df.groupby(['ID']).Component.apply(list).reset_index()
</code></pre>
<p>will do so for one column but I'm not sure for many columns.</p>
",10442949.0,16343464.0,2022-10-20 12:42:28,2022-10-20 12:47:04,Pandas group by with list applied to multiple columns,<python><pandas><group-by><data-science>,1,1,N/A,CC BY-SA 4.0
74117995,1,-1.0,2022-10-18 22:13:33,0,31,"<p>I have a data frame for the stocks of coca cola and tesla and have only its daily returns and daily volatility and want to make a new dataframe that is organized in such that it will two columns, &quot;Daily Returns&quot; and &quot;Daily Volatility&quot; and an index &quot;ID&quot; which will either be TSLA or KO depending on what stock the value belongs too however I have had zero luck in doing so. Here is what I have so far:</p>
<pre><code>    
    #Make modifiable variables
    tesla = teslaData # teslaData = dr.get_data_yahoo(teslaStock, start='2021-01-01', end='2021-12-31')

    cc = cocaData # dr.get_data_yahoo(cocacolaStock, start='2021-01-01', end='2021-12-31')
    
    #Get the required data from Coca-Cola
    cc['Daily_Returns_KO'] = cc['Close'].pct_change()
    cc['Daily_Volatility_KO'] = cc['Daily_Returns_KO'].rolling(window_size).std() * np.sqrt(252)
    
    #Get the required data from Tesla
    tesla['Daily_Returns_TSLA'] = tesla['Close'].pct_change()
    tesla['Daily_Volatility_TSLA'] = tesla['Daily_Returns_TSLA'].rolling(window_size).std() * np.sqrt(252)

    #Drop unneeded data, only keep daily returns and volatility
    cc = cc.drop(['High','Low','Open','Close','Volume','Adj Close'], axis = 1)
    tesla = tesla.drop(['High','Low','Open','Close','Volume','Adj Close'], axis = 1)
    
    #Give ID's to cc and tesla
    cc['ID'] = 'KO'
    cc.set_index(&quot;ID&quot;, inplace=True)
    tesla['ID'] = 'TSLA'
    tesla.set_index(&quot;ID&quot;, inplace=True)
    
    
    # #Concat the data frames into table
    # frames = [cc,tesla]
    # table = pd.concat(frames, ignore_index=False,axis=0)
</code></pre>
<p>I tried to concat the data and modify a table from there but that lead nowhere, I tried to use stack however I couldnt use the query() command which I need for later. How would I be able to organize my data as described above?</p>
",13451606.0,-1.0,N/A,2022-10-18 22:13:33,How do I combine data frames in one data frame of very specific format in Pandas?,<python><pandas><dataframe><data-science><stock>,0,4,N/A,CC BY-SA 4.0
70702012,1,-1.0,2022-01-13 19:24:10,-1,868,"<p>Using Python 3.9.7(64-bit).</p>
<p><a href=""https://i.stack.imgur.com/sCfm7.png"" rel=""nofollow noreferrer"">Gets forever to install the packages</a></p>
<p>After installing, it shows which environment to download Tensorflow in. After I choose 'base' it loads for sometime and gives out error.</p>
<p>What can be wrong ??</p>
",13459773.0,-1.0,N/A,2022-01-14 20:15:27,How to install Tensorflow in base environment on Anaconda?,<tensorflow><keras><anaconda><data-science>,1,1,2022-01-16 22:30:09,CC BY-SA 4.0
74130606,1,74130971.0,2022-10-19 19:03:25,0,682,"<p>I would like to create a measure where I filter data between two dates.
I would like to see how many customers I had in the previous years so I would like to see the sum of the column &quot;names&quot; from my dataset.</p>
<p>My attempt:</p>
<pre><code>L2020 = 
COUNT('customers'[name], DATESBETWEEN('customers'[name], DATE(2020,01,01), DATE(2020,12,31)))
</code></pre>
<p>I am getting an error code that says &quot;Too many arguments were passed to the SUM function. The maximum argument count for the function is 1.&quot;</p>
",15188629.0,-1.0,N/A,2022-10-19 19:39:06,DAX Between Dates Function,<powerbi><data-science><data-analysis><powerbi-desktop>,1,1,N/A,CC BY-SA 4.0
74140416,1,-1.0,2022-10-20 13:07:46,0,41,"<p>Using base R, I've created a model and am trying to test it using the predict function to return the probability of making more than $50k in a year, turn it into a usable categorical variable, and then add the predicted outcome to my test dataframe dataToModel2 using the following coding and am unsure if I've done it right. Have I correctly fed my binary model prediction values into the dataframe used to test my models and what would represents the real outcomes here?</p>
<pre><code>probabilities &lt;- predict(theModel, newdata = dataToModel2 , type = &quot;response&quot;)   
dataToModel2$predictions &lt;- probabilities &gt; .5
str(dataToModel2)
</code></pre>
<p>If so, is there a formula to use that calculates the accuracy, false negatives, false positives, and positive predict values? I understand slightly that it has to do with making both the column for real outcome and the column for my model's predictions the same units(making real outcome True/False or 1/0), but am unsure on how to do that or why it is necessary.</p>
",18326344.0,-1.0,N/A,2022-10-20 13:07:46,"Using Base R, how would I accomplish the following tasks?",<r><data-science><linear-regression><computer-science>,0,3,N/A,CC BY-SA 4.0
74113010,1,-1.0,2022-10-18 14:49:06,-1,27,"<p>I made a function that calculates a price change by taking the original price of something and the new price of that something, the function is:</p>
<pre><code>#the formula is ((NewPrice-OrignalPrice)/(OrignalPrice))*100

def DeltaPC(NP,OP):
  DPC=((NP-OP)/(OP))*100
</code></pre>
<p>I want to apply this function on a large data frame I have which has a price column, but I need to the first price, then the new one, and repeat this over and over again until it does the whole column.</p>
<p>the column looks something like:</p>
<ol>
<li>Price:1 2 2.2 3.2 3.3 2.9</li>
</ol>
<hr />
",20273942.0,3494754.0,2022-10-18 14:55:24,2022-10-18 15:03:42,How do i get the function I made to run through a column data frame in a certain way,<python><pandas><data-science>,1,4,N/A,CC BY-SA 4.0
74118068,1,-1.0,2022-10-18 22:23:54,0,145,"<p>I would like to get suggestions about a time series problem. The data is about strain gauge on the wing of flight which is measured using different sensors. Basically, we are creating the anomalies by simulating the physics model. We have a baseline which is working fine and then created some anomalies by changing some of the factors and recorded over time. Our aim is to create a model which can find out the anomaly during the live testing(it can be a crack on the wing), basically a real time anomaly detection using statistical methods or machine learning.</p>
",16111313.0,-1.0,N/A,2022-10-19 23:45:31,Time series anomaly detection,<statistics><time-series><data-science><sensors><multivariate-time-series>,1,0,N/A,CC BY-SA 4.0
74119307,1,-1.0,2022-10-19 02:41:01,0,127,"<p>This is my data frame:</p>
<pre><code> &quot;Column 1&quot;         &quot;Column 2&quot;
    XYZ55 Data Tech    128
    XYZ59 Data Tech    117
    XYZ54 Data Tech     53
    XYZ64 Data Tech      7
    XYZ57 Data Tech     12
    XYZ56 Data Tech     10
    XYZ53 Data Tech      9
</code></pre>
<p>I got this from running the below on my original data frame:</p>
<pre><code>counts = df[&quot;AssignedGroup&quot;].value_counts(sort=False)
</code></pre>
<p>My desired data frame:</p>
<pre><code>XYZ53 Data Tech    9
XYZ56 Data Tech    10
XYZ57 Data Tech    12
XYZ54 Data Tech    53
XYZ64 Data Tech    7
XYZ55 Data Tech    128
XYZ59 Data Tech    117
</code></pre>
<p>Am I able to manually pick the order that is outputted as a result of running value_counts(), an order that is not based on an ascending/descending/alphabetic order?</p>
",20278279.0,-1.0,N/A,2022-10-19 02:56:19,Can I manually reorder rows in a column in Python pandas based on a string value?,<python><pandas><indexing><data-science>,1,4,N/A,CC BY-SA 4.0
74126213,1,-1.0,2022-10-19 13:28:06,0,33,"<p>I have a movie rating/watched<code>enter code here</code> dataset with columns userid, movieId and timestamp.</p>
<p>I want to groupby the dataframe according to the users and each row should contain the movies not more than a certain time (lets say 500 in my case ) but no of items at each entry should not be greater than 100.</p>
<pre><code>input_data={'userId':[1,1,1,2,2,3,3,3,1,1],'movieId':[10,20,30,40,50,60,70,80,90,100],'timestamp':[100,200,300,400,500,600,700,800,900,1000]}

input_df=pd.DataFrame(columns=['userId','movieId','timestamp'],data=input_data)
input_df
</code></pre>
<p><a href=""https://i.stack.imgur.com/1Fi1v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Fi1v.png"" alt=""enter image description here"" /></a></p>
<p>The Output should look like:</p>
<pre><code>output_data={'userId':[1,2,3,1],'movies':[[10,20,30],[40,50],[60,70,80],[90,100]]}
output_df=pd.DataFrame(columns=['userId','movies'],data=output_data)
output_df
</code></pre>
<p><a href=""https://i.stack.imgur.com/w4L0o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w4L0o.png"" alt=""enter image description here"" /></a></p>
",16125703.0,16125703.0,2022-10-20 05:13:19,2022-10-20 05:13:19,group a column of dataframe based on another column of maximum k size?,<python><pandas><data-science><data-handling>,0,6,N/A,CC BY-SA 4.0
74144813,1,-1.0,2022-10-20 18:38:36,0,266,"<p><strong><strong>EDIT This must be done in vanilla JS or PYTHON</strong></strong></p>
<p>I've been having some issues with my code - I am doing some data science applications and decided to build a web-gui for my team and some other engineering partners (I am not a classically trained CS guy). I have the following going on:</p>
<p>1.) I am managing my front and back end via flask</p>
<p>2.) My data scientist algo is in python</p>
<p>3.) My front end is written in JS, CSS, and html</p>
<p>My code has some proprietary things in it, so I cannot copy and paste it verbatim; however, I have done my best effort to replicate it on here (This code plots fine, but timedate data is not in human readable):</p>
<p>Python Code in Question (in my @routes):</p>
<pre><code>@app.route(&quot;/backend&quot;, methods=[&quot;POST&quot;]
def data_sci_alg
    rdy2plot = {}
    #stuf happens and I get a dictionary that looks like this after for looping, other than the time array, everything is 100%

    time = [1666202064.0, 1666202065.0, 1666202066.0, 1666202067.0, 1666202068.0, 1666202069.0, 1666202070.0, 1666202071.0, 1666202072.0, 1666202073.0]
    atn = ['N930NN', 'N930NN', 'N930NN', 'N930NN', 'N930NN', 'N930NN', 'N930NN', 'N930NN', 'N930NN', 'N930NN']
    alt = [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]
    cspeed = [1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200]
    data = {
            &quot;aircraft_tail_number&quot;: atn,
            &quot;time&quot;: time,
            &quot;altitude&quot;: alt,
            &quot;current_speed&quot;: cspeed,
        }    
    rdy2plot.update(data)
    return jsonify(rdy2plot)
</code></pre>
<p>print(type(time[0])) is a 'float'</p>
<p>JS Code in Question (in my static/index):</p>
<pre><code>TESTER = document.getElementById('graph');

let data_update = []
function plot(data){
    var atn = data['atn']
    var time = data['time']
    var alt = data['alt']
    var cspeed = data['cspeed']
    data_update.push({x: time, y: alt, name: `${atn} Altitude`, type:&quot;scatter&quot;});
    data_update.push({x: time, y: cspeed, name: `${atn} Speed`, type:&quot;scatter&quot;});
    Plotly.react('graph', data_update, layout);
</code></pre>
<p>The concept of this, is that it loops through different aircraft parameters. via tail number, and plots them one at a time (This is why &quot;data_update&quot; is outside the function, so the graph updates as intended).</p>
<p>At this point, the data plots fine, but when I highlight the data in plotly, it shows my timestamp data, which is not really human readable.</p>
<p>I am using this (<a href=""https://plotly.com/chart-studio-help/date-format-and-time-series/"" rel=""nofollow noreferrer"">https://plotly.com/chart-studio-help/date-format-and-time-series/</a>) to try and see what plotly wants, and I am now attempting to plot the yyy-mm-dd HH:MM:SS with the following code (does not plot!):</p>
<p>The output of python now looks like (I cannot paste exactly what I am doing here, but the output looks like below, via a print statement:</p>
<pre><code>time = ['2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53']
</code></pre>
<pre><code>print(type(time[0])) is a 'str'
</code></pre>
<p>JS console.log(time):</p>
<pre><code>time = ['2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53', '2022-10-19 18:18:53']
</code></pre>
<p>Nothing is getting lost in translation, I don't understand why plotly is not taking this. Please help!</p>
<p>Sorry in advance for any typos or formatting issues, I had to transcribe this by hand - The main points are:</p>
<p>1.) It plots fine as of now, but the datetime information is in seconds since unix epoch (not human readable)</p>
<p>2.) When I transform my data into the above second list, nothing plots.</p>
<p>3.) My goal, regardless how it's accomplished, is human readable time data on my graph.</p>
<p>4.) Please be kind, I am not a trained CS guru.</p>
",20286580.0,20286580.0,2022-10-20 20:33:33,2022-10-20 20:33:33,Plotly won't plot my time data when converted to a datetime object but will plot my unix epoch time array,<javascript><python><css><plotly><data-science>,1,0,N/A,CC BY-SA 4.0
74109097,1,-1.0,2022-10-18 09:58:59,0,17,"<p>I want to speed up this code:</p>
<pre><code>a=np.linspace(0, 10, 2**20)
b=np.random.rand(a.shape[0])
res=np.zeros_like(a)
func = lambda x, y: np.exp((x-y)**2)
for i in range(a.shape[0]):
    res=res+func(a, b[i])
res=res/a.shape[0]
</code></pre>
<h2>this code will take a lot of time to execute, I also can use np.mean() after i calculated the matrix of func(X, Y) but it requires a lot of memory which i do not have.
Do anyone know how to deal with this problem? or how to have a trade off between the two methods?</h2>
",12042615.0,12042615.0,2022-10-18 13:10:43,2022-10-18 13:10:43,averaging a large vector after elementwise value assignment in a loop,<python><algorithm><optimization><data-science><bigdata>,0,2,N/A,CC BY-SA 4.0
74123381,1,-1.0,2022-10-19 09:56:56,0,110,"<p>I use <a href=""https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/split-relationships/"" rel=""nofollow noreferrer"">this</a> site to run this query:</p>
<pre><code>CALL gds.graph.project(
    'graph',
    'Label',
    { TYPE: { orientation: 'UNDIRECTED' } }
)
</code></pre>
<p>But it arise this error:</p>
<blockquote>
<p><code>gds.graph.project</code> is unavailable because it is <code>sandboxed</code> and has
dependencies outside of the sandbox. Sandboxing is controlled by the
dbms.security.procedures.unrestricted setting. Only unrestrict
procedures you can trust with access to database internals.</p>
</blockquote>
<p>How can use gds in neo4j desktop?</p>
",806160.0,5211833.0,2022-10-20 13:16:41,2022-10-20 13:16:41,How Graph Data Science Library use in neo4j desktop?,<neo4j><graph-data-science>,1,1,N/A,CC BY-SA 4.0
74142259,1,-1.0,2022-10-20 15:11:33,0,76,"<p><a href=""https://i.stack.imgur.com/JAnYl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JAnYl.jpg"" alt=""dataset"" /></a></p>
<p>How do i calculate the time period in days for which the customer was active using pandas?</p>
",19689487.0,10315163.0,2022-10-20 15:20:12,2022-10-20 15:28:56,How do i calculate the time period in days for which the customer was active using pandas?,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
74142304,1,74164939.0,2022-10-20 15:14:38,0,49,"<p>I'm trying to make a script that can read through a table of company names from one website and take the names of each company and put them in an url (the url exists, and it contains more data specific to each company. This data is what I want to analyze).</p>
<p>However, I cannot get the names to be put in the url without python also putting in parts of the table, giving me the error below:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import requests

url1 = &quot;http://openinsider.com/latest-penny-stock-buys&quot;
df1 = pd.read_html(url1)
table = df1[11]
# sorting
n = np.quantile(table[&quot;Qty&quot;], [0.99])
print(&quot;20th percentile: &quot;, n)
q = table.sort_values(&quot;Qty&quot;, ascending=False)
name = q[&quot;Ticker&quot;].str.replace(&quot;\d+&quot;, &quot;&quot;)
page = requests.get(url1)
name = table[&quot;Ticker&quot;]
# Buyers for the company
url = &quot;http://openinsider.com/&quot;
for entry in name:  # &lt;- Question starts here
    name = entry + 1
    table2 = pd.read_html(url + str(name))
    df2 = table2[11]
    print(df2)
</code></pre>
<blockquote>
<p>Error: InvalidURL: URL can't contain control characters. '/0      OPK\n1     VEII\n2      NGM\n3     STRR\n4
IMRA\n      ... \n95     NaN\n96    CDXC\n97     PED\n98     FOA\n99    CAMP\nName:
Ticker, Length: 100, dtype: object' (found at least ' ')```</p>
</blockquote>
<p>Thanks!</p>
",19362012.0,11246056.0,2022-10-22 15:48:12,2022-10-22 15:48:12,Python Pandas-Automating Data Gathering from Website,<python><pandas><database><automation><data-science>,1,0,N/A,CC BY-SA 4.0
74148534,1,-1.0,2022-10-21 03:39:17,1,34,"<p>Here is the code I have written:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = sns.load_dataset('dowjones')

fig, ax = plt.subplots(figsize=(10,10))
plot = sns.lineplot(data=df, x='Date', y='Price')

for ind, label in enumerate(plot.get_xticklabels()):
if ind % 200 == 0:
    label.set_visible(True)
else:
    label.set_visible(False)
</code></pre>
<p>Here is the graph it creates:
<a href=""https://i.stack.imgur.com/JsMQK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JsMQK.png"" alt="""" /></a></p>
<p>The above code only removed only the labels but not the ticks itself.
How do I remove the lines?</p>
",19208266.0,-1.0,N/A,2022-10-21 03:39:17,How do I remove the tick lines from the following graph?,<python><matplotlib><seaborn><data-science><visualization>,0,3,N/A,CC BY-SA 4.0
74146964,1,-1.0,2022-10-20 22:33:29,1,226,"<p>Im using Base R to test this model:</p>
<pre><code>probabilities &lt;- predict(theModel, newdata = dataToModel2 , type = &quot;response&quot;)   
dataToModel2$predictions &lt;- ifelse(probabilities &gt;= .5, &quot;True&quot;, &quot;False&quot;)
</code></pre>
<p>and then when I try to test for accuracy using this code:</p>
<pre><code> accuracy &lt;- sum(dataToModel2$predictions == dataToModel2$incomeNum)/dim(dataToModel2)[1]
</code></pre>
<p>I get a 0 rather than a number depicting how accurate my model is. Why is this and how do you fix such an error?</p>
<p>I hope this can help. Data for the original model:</p>
<pre><code>dataToModel &lt;- structure(
  list(
    sex = c(&quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;),
    marital.status = c(&quot;Never-married&quot;, &quot;married&quot;, &quot;pMarried&quot;,
                       &quot;married&quot;, &quot;married&quot;),
    race = c(&quot;White&quot;, &quot;White&quot;, &quot;White&quot;, &quot;Black&quot;,
             &quot;Black&quot;),
    education = c(
      &quot;University&quot;,
      &quot;University&quot;,
      &quot;less-than-Uni&quot;,
      &quot;less-than-Uni&quot;,
      &quot;University&quot;
    ),
    incomeNum = c(FALSE, FALSE, FALSE,
                  FALSE, FALSE)
  ),
  row.names = c(NA, 5L),
  class = &quot;data.frame&quot;
)
</code></pre>
<p>And data for predictions:</p>
<pre><code>dataToModel2 &lt;- structure(
  list(
    sex = c(&quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;),
    marital.status = c(
      &quot;Never-married&quot;,
      &quot;married&quot;,
      &quot;married&quot;,
      &quot;married&quot;,
      &quot;Never-married&quot;
    ),
    race = c(&quot;Black&quot;, &quot;White&quot;, &quot;White&quot;,
             &quot;Black&quot;, &quot;White&quot;),
    education = c(
      &quot;less-than-Uni&quot;,
      &quot;less-than-Uni&quot;,
      &quot;University&quot;,
      &quot;less-than-Uni&quot;,
      &quot;less-than-Uni&quot;
    ),
    incomeNum = c(FALSE,
                  FALSE, FALSE, FALSE, FALSE),
    predictions = c(&quot;False&quot;, &quot;False&quot;,
                    &quot;True&quot;, &quot;False&quot;, &quot;False&quot;)
  ),
  row.names = c(1L, 2L, 3L, 4L, 6L),
  class = &quot;data.frame&quot;
)
</code></pre>
",18326344.0,16631565.0,2022-10-23 01:40:05,2022-10-23 01:40:05,Why is my code for accuracy returning a 0?,<r><data-science><linear-regression><computer-science>,1,2,N/A,CC BY-SA 4.0
74149132,1,74156301.0,2022-10-21 05:30:05,0,436,"<p>[![dataset<a href=""https://i.stack.imgur.com/Hmi8i.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hmi8i.jpg"" alt=""][1]"" /></a>][2]</p>
<p>how to find the country with highest number of unique customers?</p>
",19689487.0,-1.0,N/A,2022-10-21 16:00:43,"how do i find the city & country with the highest number of unique customers, also the count of unique players?",<pandas><data-science>,1,3,N/A,CC BY-SA 4.0
70719615,1,-1.0,2022-01-15 07:31:47,2,135,"<p>I have to use a for loop to cycle through mean_absolute_errors for the list of leaf nodes and get the number of nodes for minimum value of <code>get_mae()</code>. This is from the Kaggle &quot;Intro to Machine Learning&quot; tutorial.</p>
<pre><code>candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]
# Write loop to find the ideal tree size from candidate_max_leaf_nodes
_
min = get_mae(candidate_max_leaf_nodes[0], train_X, val_X, train_y, val_y)
nodes = 0
for max_leaf_nodes in candidate_max_leaf_nodes:
    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    if my_mae &lt; min:
        nodes = max_leaf_nodes

# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)
best_tree_size = nodes

# Check your answer
step_1.check()
</code></pre>
<p>My answer is coming as 500 and it is not the correct answer.</p>
",12661271.0,12370687.0,2022-08-28 02:01:44,2022-08-28 02:01:44,What is wrong in this code to get number of leaf nodes for minimum mean absolute error,<python><machine-learning><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
74140761,1,76557091.0,2022-10-20 13:33:03,1,53,"<pre><code>getDecade &lt;- function(year) {
  case_when(
    year &lt; 1900 ~ &quot;1890s&quot;,
    year &gt;= 1900 &amp;&amp; year &lt; 1910 ~ &quot;1900s&quot;,
    year &gt;= 1910 &amp;&amp; year &lt; 1920 ~ &quot;1910s&quot;,
    year &gt;= 1920 &amp;&amp; year &lt; 1930 ~ &quot;1920s&quot;,
    year &gt;= 1930 &amp;&amp; year &lt; 1940 ~ &quot;1930s&quot;,
    year &gt;= 1940 &amp;&amp; year &lt; 1950 ~ &quot;1940s&quot;,
    year &gt;= 1950 &amp;&amp; year &lt; 1960 ~ &quot;1950s&quot;,
    year &gt;= 1960 &amp;&amp; year &lt; 1970 ~ &quot;1960s&quot;,
    year &gt;= 1970 &amp;&amp; year &lt; 1980 ~ &quot;1970s&quot;,
    year &gt;= 1980 &amp;&amp; year &lt; 1990 ~ &quot;1980s&quot;,
    year &gt;= 1990 &amp;&amp; year &lt; 2000 ~ &quot;1990s&quot;,
    year &gt;= 2000 &amp;&amp; year &lt; 2010 ~ &quot;2000s&quot;,
    TRUE ~ &quot;other&quot;
  )
}

# (b) Check that your decade function works as intended.
movies &lt;- ggplot2movies::movies %&gt;% 
  mutate (decade = getDecade(year))
print(unique(movies$decade))
</code></pre>
<p>The output is:
[1] &quot;1970s&quot; &quot;1890s&quot;</p>
<p>Why is almost every movie from 1970s??? What is happening???</p>
",20292084.0,-1.0,N/A,2023-06-26 13:20:34,Every movie is from 1970 what did I do wrong?,<r><dataframe><function><data-science><case>,1,3,N/A,CC BY-SA 4.0
74170749,1,-1.0,2022-10-23 11:23:25,-1,53,"<p>I have a dataset of multiple columns where I want to compare one column Named <code>df.BRAND</code> to every entry of <code>df.my_Brand</code> and if the entry of <code>df.BRAND</code> didn't match any entry of <code>df.my_Brand</code>, I want to shift <code>df.BRAND</code> value to another column named <code>df.New_DESCRIPTION</code> and if it matches the value then I want to keep it as it is.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">BRAND</th>
<th style=""text-align: center;"">NEW_DESCRIPTION</th>
<th style=""text-align: center;"">my_Brand</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">USE 615880.ATT GROUND ROD</td>
<td style=""text-align: center;"">HOISTING GRIP FOR PWRT-608 POWER CABLE, CEQ.7...</td>
<td style=""text-align: center;"">123EWIRELESS</td>
</tr>
<tr>
<td style=""text-align: center;"">COMMSCOPE</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">3M Products</td>
</tr>
<tr>
<td style=""text-align: center;"">ELECTRO-WIRE INC.</td>
<td style=""text-align: center;"">TELCOFLEX II 6 AWG L2 GREEN, KS24194-SOUTHWIR.</td>
<td style=""text-align: center;"">COMMSCOPE</td>
</tr>
<tr>
<td style=""text-align: center;"">GALTRONICS</td>
<td style=""text-align: center;"">12 X 12' GRIP SPAN ICE BRIDGE</td>
<td style=""text-align: center;"">o-m6 Technologies</td>
</tr>
<tr>
<td style=""text-align: center;"">...</td>
<td style=""text-align: center;"">...</td>
<td style=""text-align: center;"">...</td>
</tr>
</tbody>
</table>
</div>
<p>I am new to python and don't know how to proceed, Here is the code I was trying:</p>
<pre><code>df_Brand['BRAND'] = [y if x == '' else x for x,y in zip(df_Brand['BRAND'],df_Brand['my_Brand'])]
</code></pre>
<p>Any solution code will be helpful as I am not sure if the code I was trying is correct or not.</p>
",15811628.0,15811628.0,2022-10-24 10:45:48,2022-10-24 10:45:48,Compare string columns in pandas Dataset and extracting unmatched values,<python-3.x><pandas><machine-learning><data-science><artificial-intelligence>,1,0,N/A,CC BY-SA 4.0
74170826,1,-1.0,2022-10-23 11:37:07,1,342,"<p>I want to change collate_fn as below:</p>
<pre><code>def collate_fn(batch,text_transform,SRC_LANGUAGE,TGT_LANGUAGE,PAD_IDX):
    src_batch, tgt_batch = [], []
    for src_sample, tgt_sample in batch:
        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(&quot;\n&quot;)))
        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(&quot;\n&quot;)))
</code></pre>
<p>and the error that I take is:</p>
<blockquote>
<p>TypeError: collate_fn() missing 4 required positional arguments: 'text_transform', 'SRC_LANGUAGE', 'TGT_LANGUAGE', and 'PAD_IDX'</p>
</blockquote>
<p>cause the collate_fn in torch\utils\data_utils\fetch.py is defined with just taking one argument. what should I do? should I override this function?</p>
",10364621.0,-1.0,N/A,2022-10-23 11:37:07,changing collate_fn of DataLoader in pytorch,<python><pytorch><data-science><torch><pytorch-dataloader>,0,0,N/A,CC BY-SA 4.0
74173003,1,-1.0,2022-10-23 16:47:16,0,76,"<p>I am trying to normalize a dataframe of candlestick values. Here is the original graph:</p>
<p><a href=""https://i.stack.imgur.com/M3MFU.png"" rel=""nofollow noreferrer"">not normalized data</a></p>
<p>And here is the normalised graph:</p>
<p><a href=""https://i.stack.imgur.com/WpKtM.png"" rel=""nofollow noreferrer"">normalized graph</a></p>
<p>As you can see the graph has a similar shape however most of the wicks and bodies are completely unproportionate and some wicks even go inside of the body. Heres the code maybe im missing something.</p>
<pre><code>import MetaTrader5 as mt5
import pandas as pd
from creds import user, password
import plotly
import plotly.graph_objs as go

if not mt5.initialize(login=user, password=password, server=&quot;FTMO-Demo&quot;):
    print(&quot;initialize() failed, error code =&quot;, mt5.last_error())
    quit()

symbols = mt5.symbols_get()
convertedCandles = pd.DataFrame(columns=[&quot;candleType&quot;, &quot;wicksUp&quot;, &quot;wicksDown&quot;, &quot;bodySize&quot;, &quot;time&quot;])

rates = mt5.copy_rates_from_pos(&quot;GBPJPY&quot;, mt5.TIMEFRAME_M15, 0, 15000)
ticker = pd.DataFrame(rates)

time = pd.DataFrame()

ticker['time'] = pd.to_datetime(ticker['time'], unit='s')
time['time'] = ticker['time']
ticker['time'] = time['time']
ticker = ticker.set_index('time')
ticker = ticker.tail(100)
df_max = ticker[&quot;high&quot;].max()
df_min = ticker[&quot;low&quot;].min()

# Normalize data
ticker=(ticker-ticker.min())/(ticker.max()-ticker.min())

qf = go.Figure(data=[go.Candlestick(x=ticker.index,
                open=ticker['open'],
                high=ticker['high'],
                low=ticker['low'],
                close=ticker['close'])])

plotly.offline.plot(qf, filename='../gauge-meter-chart.html')
</code></pre>
<p>My aim is to have the same candlestick pattern however I would like the x axis to be from 0 to 1.</p>
<p>As seen here:
<a href=""https://i.stack.imgur.com/E7zOd.png"" rel=""nofollow noreferrer"">desired graph</a></p>
<p>Dataframe not normalized:</p>
<pre><code>                        open      high       low     close
time                                                       
2022-10-21 10:00:00  0.725194  0.659922  0.750700  0.723379
2022-10-21 10:15:00  0.720716  0.751341  0.774720  0.832202
2022-10-21 10:30:00  0.830544  0.747684  0.848647  0.798268
2022-10-21 10:45:00  0.796842  0.707216  0.776586  0.722443
2022-10-21 11:00:00  0.719774  0.628230  0.737174  0.688977
2022-10-21 11:15:00  0.686778  0.635056  0.734608  0.695764
2022-10-21 11:30:00  0.692435  0.615310  0.711054  0.684999
2022-10-21 11:45:00  0.682536  0.599464  0.696595  0.668149
2022-10-21 12:00:00  0.665802  0.640907  0.710354  0.707231
2022-10-21 12:15:00  0.704926  0.610678  0.721315  0.687807
2022-10-21 12:30:00  0.685600  0.595076  0.662313  0.619237
2022-10-21 12:45:00  0.615366  0.564359  0.674674  0.643576
2022-10-21 13:00:00  0.640584  0.584837  0.683769  0.644980
2022-10-21 13:15:00  0.642706  0.544369  0.659981  0.634449
2022-10-21 13:30:00  0.631393  0.569478  0.681903  0.665106
2022-10-21 13:45:00  0.662032  0.682106  0.719916  0.744208
2022-10-21 14:00:00  0.742399  0.715992  0.796175  0.793822
2022-10-21 14:15:00  0.791657  0.760848  0.849347  0.823777
2022-10-21 14:30:00  0.821353  0.729400  0.812034  0.773929
2022-10-21 14:45:00  0.771624  0.682838  0.785448  0.747016
2022-10-21 15:00:00  0.745699  0.685032  0.771688  0.722209
2022-10-21 15:15:00  0.720481  0.635544  0.744869  0.695062
2022-10-21 15:30:00  0.692670  0.635544  0.714319  0.714018
2022-10-21 15:45:00  0.711289  0.737933  0.768190  0.807161
2022-10-21 16:00:00  0.803912  0.822038  0.860774  0.843904
2022-10-21 16:15:00  0.842329  0.819600  0.871269  0.880412
2022-10-21 16:30:00  0.878859  0.846904  0.891791  0.870583
2022-10-21 16:45:00  0.869903  0.879327  0.920476  0.963960
2022-10-21 17:00:00  0.963941  0.954900  0.987407  0.989937
2022-10-21 17:15:00  0.989159  0.914188  0.954757  0.919729
2022-10-21 17:30:00  0.918218  0.845929  0.733675  0.735081
2022-10-21 17:45:00  0.728494  0.674549  0.463853  0.573134
2022-10-21 18:00:00  0.569408  0.470502  0.204058  0.170606
2022-10-21 18:15:00  0.164271  0.219405  0.077659  0.232155
2022-10-21 18:30:00  0.225784  0.189907  0.125466  0.161011
2022-10-21 18:45:00  0.152015  0.048513  0.000000  0.000000
2022-10-21 19:00:00  0.000000  0.117747  0.045243  0.212965
2022-10-21 19:15:00  0.210229  0.140907  0.060168  0.070910
2022-10-21 19:30:00  0.063634  0.000000  0.042444  0.095951
2022-10-21 19:45:00  0.088381  0.055826  0.106810  0.154926
2022-10-21 20:00:00  0.148008  0.102389  0.205924  0.202668
2022-10-21 20:15:00  0.196795  0.203559  0.261894  0.314767
2022-10-21 20:30:00  0.309215  0.318625  0.361007  0.427802
2022-10-21 20:45:00  0.424700  0.368357  0.484608  0.445823
2022-10-21 21:00:00  0.440726  0.337396  0.363806  0.404166
2022-10-21 21:15:00  0.399481  0.328376  0.415345  0.436462
2022-10-21 21:30:00  0.432713  0.342760  0.391325  0.338170
2022-10-21 21:45:00  0.334433  0.276207  0.272155  0.249239
2022-10-21 22:00:00  0.243224  0.145295  0.182836  0.192605
2022-10-21 22:15:00  0.186189  0.148708  0.242537  0.248303
2022-10-21 22:30:00  0.242281  0.220868  0.304804  0.321086
2022-10-21 22:45:00  0.315107  0.294003  0.373368  0.402059
2022-10-21 23:00:00  0.397360  0.329352  0.335821  0.354084
2022-10-21 23:15:00  0.349045  0.295709  0.371968  0.399251
2022-10-21 23:30:00  0.393589  0.305705  0.425606  0.389422
2022-10-21 23:45:00  0.384634  0.312774  0.417211  0.366721
2022-10-24 00:00:00  0.684421  0.609703  0.702659  0.706763
2022-10-24 00:15:00  0.703983  0.637981  0.740205  0.722677
2022-10-24 00:30:00  0.718360  0.688445  0.774953  0.783758
2022-10-24 00:45:00  0.781758  0.738664  0.840019  0.817692
2022-10-24 01:00:00  0.816168  1.000000  0.848647  0.902879
2022-10-24 01:15:00  0.901485  0.908581  0.892957  0.869647
2022-10-24 01:30:00  0.870846  0.842028  0.903685  0.893049
2022-10-24 01:45:00  0.891822  0.890541  0.887593  0.946876
2022-10-24 02:00:00  0.946264  0.865431  0.924207  0.904985
2022-10-24 02:15:00  0.904784  0.848854  0.945896  0.935408
2022-10-24 02:30:00  0.933773  0.899074  0.125000  0.168734
2022-10-24 02:45:00  0.109357  0.559483  0.158116  0.514159
2022-10-24 03:00:00  0.518265  0.639688  0.548741  0.693892
2022-10-24 03:15:00  0.691492  0.654559  0.709188  0.743038
2022-10-24 03:30:00  0.740749  0.671624  0.687500  0.742570
2022-10-24 03:45:00  0.739807  0.761824  0.776819  0.779312
2022-10-24 04:00:00  0.778694  0.750609  0.822062  0.836181
2022-10-24 04:15:00  0.834551  0.753047  0.774021  0.797800
2022-10-24 04:30:00  0.794721  0.771819  0.853545  0.824713
2022-10-24 04:45:00  0.823003  0.755729  0.840252  0.811842
2022-10-24 05:00:00  0.810511  0.730132  0.793377  0.757781
2022-10-24 05:15:00  0.755362  0.723549  0.807369  0.817692
2022-10-24 05:30:00  0.814989  0.731107  0.853778  0.797800
2022-10-24 05:45:00  0.795899  0.719405  0.826959  0.773461
2022-10-24 06:00:00  0.772802  0.687470  0.811800  0.768547
2022-10-24 06:15:00  0.766203  0.680400  0.796175  0.756377
2022-10-24 06:30:00  0.754183  0.675280  0.804104  0.760356
2022-10-24 06:45:00  0.759133  0.665773  0.795476  0.743974
2022-10-24 07:00:00  0.742164  0.683813  0.800373  0.751463
2022-10-24 07:15:00  0.749234  0.729644  0.803638  0.819331
2022-10-24 07:30:00  0.817582  0.788152  0.870336  0.868945
2022-10-24 07:45:00  0.867311  0.817894  0.925373  0.895858
2022-10-24 08:00:00  0.892058  0.804486  0.902752  0.876901
2022-10-24 08:15:00  0.876267  0.802779  0.900886  0.868476
2022-10-24 08:30:00  0.867547  0.807411  0.919076  0.891879
2022-10-24 08:45:00  0.891115  0.874939  0.940532  0.925813
2022-10-24 09:00:00  0.925289  0.935885  0.975047  1.000000
2022-10-24 09:15:00  1.000000  0.917845  1.000000  0.945940
2022-10-24 09:30:00  0.944615  0.872257  0.942397  0.954599
2022-10-24 09:45:00  0.952864  0.905900  0.992537  0.973789
2022-10-24 10:00:00  0.972425  0.922964  0.993937  0.957875
2022-10-24 10:15:00  0.956399  0.903706  0.988106  0.948514
2022-10-24 10:30:00  0.947679  0.879815  0.939832  0.907559
2022-10-24 10:45:00  0.905491  0.849829  0.945196  0.909431

Process finished with exit code 0
</code></pre>
",16314515.0,16314515.0,2022-10-24 07:57:25,2022-10-24 07:57:25,Pandas not normalizing data properly,<python><pandas><dataframe><data-science><candlestick-chart>,0,5,N/A,CC BY-SA 4.0
74179097,1,74179151.0,2022-10-24 09:28:52,0,146,"<p>I have a dataframe:</p>
<pre><code>df = c1 c2 c3 code
     1. 2. 3. 200
     1. 5. 7. 220
     1. 2. 3. 200
     2. 4. 1. 340
     6. 1. 1. 370
     6. 1. 5. 270
     9. 8. 2. 300
     1. 6. 9. 700
     9. 2. 1. 200 
     8. 1. 2  400
     1. 2  1. 200
     2. 5. 3  900
     8. 0. 4. 300
     9. 1. 2. 620
</code></pre>
<p>I want to take only the rows that are between any row with 300 code to its previous 200 code.
So here I will have</p>
<pre><code>df.  c1 c2 c3 code batch_num
     1. 2. 3. 200.   0
     2. 4. 1. 340.   0
     6. 1. 1. 370.   0 
     6. 1. 5. 270.   0
     9. 8. 2. 300.   0
     1. 2  1. 200.   1
     2. 5. 3  900.   1
     8. 0. 4. 300.   1
</code></pre>
<p>So basically what I need is to:
find each 300, and for each - find the nearest previous 200, and take the rows between them.
It is guaranteed that there will always be at least one 200 before each 300.
Than, add a columns that indicate the proper batch.
How can I do it efficiently in pandas?</p>
",6057371.0,6057371.0,2022-10-24 10:03:31,2022-10-25 13:11:12,dataframe get rows between certain value to a certain value in previous row,<pandas><dataframe><data-science><data-munging>,2,6,N/A,CC BY-SA 4.0
74185067,1,-1.0,2022-10-24 18:00:27,1,142,"<p>I'm working on a classification project, where I try out various types of models like logistic regression, decision trees etc, to see which model can most accurately predict if a patient is at risk for heart disease (given an existing data set of over 3600 rows).</p>
<p>I'm currently trying to work on my decision tree, and have plotted ROC curves to find the optimized values for tuning the max_depth and min_samples_split hyperparameters. However when I try to create my new model I get the warning:</p>
<blockquote>
<p>&quot;UndefinedMetricWarning: Precision is ill-defined and being set to 0.0
due to no predicted samples. Use <code>zero_division</code> parameter to control
this behavior.&quot;</p>
</blockquote>
<p>I have already googled the warning, and semi understand why it's happening, but not how to fix it. I don't want to just get rid of the warning or ignore the values that weren't predicted. I want to actually fix the issue. From my understanding, it has something to do with how I processed my data. However, I'm not sure where I went wrong with my data processing.</p>
<p>I started off with doing a train-test split, then used StandardScaler like so:</p>
<pre><code>#Let's split the data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


X = df.drop(&quot;TenYearCHD&quot;, axis = 1)
y = df[&quot;TenYearCHD&quot;]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)

#Let's scale our data
SS = StandardScaler()
X_train = SS.fit_transform(X_train)
X_test = SS.transform(X_test)
</code></pre>
<p>I then created my initial decision tree, and received no warnings:</p>
<pre><code>from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(criterion = &quot;entropy&quot;)

#Fit our model and predict
dtc.fit(X_train, y_train)
dtc_pred = dtc.predict(X_test)
</code></pre>
<p>After looking at my ROC curve and AOC scores, I attempted to create another more optimized decision tree, <strong>which is where I then received my warning</strong>:</p>
<pre><code>dtc3 = DecisionTreeClassifier(criterion = &quot;entropy&quot;, max_depth = 4, min_samples_split= .25)
dtc3.fit(X_train, y_train)
dtc3_pred = dtc3.predict(X_test)
</code></pre>
<p>Essentially i'm at a loss at what to do. Should I use a different method like StratifiedKFolds in addition to train-test split to process my data? Should I do something else entirely? Any help would be greatly appreciated.</p>
",10689045.0,-1.0,N/A,2022-10-24 18:00:27,How to fix warning: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples,<python><data-science><classification><decision-tree>,0,0,N/A,CC BY-SA 4.0
74150666,1,74157476.0,2022-10-21 08:16:16,-2,641,"<p>I want to add to my dashboard a sidebar that allows hiding pulsing a button, is that possible?</p>
<p>Thanks.</p>
",20282164.0,-1.0,N/A,2022-10-21 17:50:16,Is it possible to create a sidebar that can be hidden in dash?,<python><data-science><plotly-dash>,1,0,2023-01-07 21:04:25,CC BY-SA 4.0
74166291,1,74166510.0,2022-10-22 18:50:19,0,453,"<p>What I'm trying to do:</p>
<ol>
<li>Fit a linear regression model on data from PCA transformation</li>
<li>Use that linear regression model to perform cross-validation</li>
</ol>
<pre><code>ks = [1,2,3,4,5,6,8,10,12,15,20]
mean_val_mse = []
# loop through all k values
for k in ks:
    # instantiate pca and set n_components = k
    pca = PCA(n_components=k)
    # fit and transform
    # fit learns the number of components, transform actually reduces the dimensions
    x_train_pca = pca.fit_transform(X_train_scaled)
    
    # create linreg and fit it on pca data
    lr = LinearRegression().fit(x_train_pca, y_train)
    cv = cross_validate(lr, X=x_train_pca, cv=10, return_train_score=True, scoring='neg_mean_squared_error', error_score='raise')
</code></pre>
<p>When I do so, my cross-validation is throwing the error:</p>
<pre><code>TypeError: fit() missing 1 required positional argument: 'y'
from line: cv = cross_validate(lr, X=x_train_pca, cv=10, return_train_score=True, scoring='neg_mean_squared_error', error_score='raise')
</code></pre>
<p>I've checked that my y_train is the correct shape and run through the syntax but I'm not seeing a reason why it would give this error.  Am I instantiating the LinearRegression wrong or is the error somewhere else?</p>
",15356151.0,-1.0,N/A,2022-10-22 19:26:59,"Getting ""TypeError: fit() missing 1 required positional argument: 'y'"" for linear regression in python",<python><data-science><linear-regression><cross-validation>,1,0,N/A,CC BY-SA 4.0
74193648,1,74218619.0,2022-10-25 11:51:16,0,31,"<p>I want to create a base dataframe from the existing one, which does not contain all I want, for example, I have the dataframe collecting the number of candies each people (tracked by &quot;id&quot;) bought each year-month (but in this case each person didn't buy candies every month)</p>
<pre><code>|id|year_month|num_of_candies_bought
  1  2022-01           5
  1  2022-03          10
  1  2022-04           2
</code></pre>
<p>What I want is that to track them by fix the year-month I'm interested like this (for the first 5 months this year)</p>
<pre><code>|id|year_month|num_of_candies_bought
  1  2022-01           5
  1  2022-02           0
  1  2022-03          10
  1  2022-04           2
  1  2022-05           0
</code></pre>
<p>I think one way to do this is to use &quot;crossjoin&quot; but it turns out that this takes long time to process. Is there any way to do this without any join? In my work the first dataframe is very very huge (a million rows for instance) while the second is just fixed (like in this case only 5 rows) and much much smaller. Is it possible (if it is needed to use crossjoin) to improve performance drastically?</p>
<p>P.S. I want this to seperate each person (so I need to use window.partition thing)</p>
",10208975.0,10208975.0,2022-10-25 11:56:58,2022-11-08 13:26:05,Pyspark efficiently create patterns within each window,<python><dataframe><pyspark><data-science><tabula>,1,2,N/A,CC BY-SA 4.0
70743088,1,70898102.0,2022-01-17 14:37:06,1,235,"<p>Would then infinitely deep decision tree on a binary classification task guarantee to achieve 100% accuracy on a training set with N examples such that there is no examples with the same feature values, but a different class label?</p>
",3958031.0,12370687.0,2022-08-27 15:34:26,2022-08-27 15:34:26,Would infinitely deep decision tree guarantee 100% accuracy for binary classification task?,<machine-learning><data-science><decision-tree><theory>,1,0,N/A,CC BY-SA 4.0
70744256,1,-1.0,2022-01-17 16:06:11,0,1423,"<p>I am training an artificial intelligence model, google colab provides me with 13 GB approximately, but when I run the training I only see that they run in 2 GB, the script has been running for 5 hours and it does not give the first result.</p>
<p>Does anyone know how I can use all the ram memory capacity?</p>
<p>This is the code I am running</p>
<pre><code>scores_TEC = []

for model_name, mp in model_params.items():
  clf = GridSearchCV(mp['model'], mp['params'], cv=10, return_train_score=False)
  clf.fit(x_t,y_t)
  scores.append({
      'model': model_name,
      'best_score': clf.best_score_,
      'best_params': clf.best_params_
  })

# Models params

model_params = {
    'neighbors': {
        'model': KNeighborsClassifier(),
        'params': {
            'n_neighbors': np.arange(1,20),
            'weights': ['uniform', 'distance'],
            'p': [1,2,3,5]
        }
    },
    'svm': {
        'model': svm.SVC(),
        'params': {
            'C': np.arange(1,20),
            'kernel': ['lineal', 'poli', 'rbf', 'sigmoide'],
            'degree' : [1,2,3,4,5],
            'gamma': ['scale', 'auto'],
            'coef0': [1,2,3,4,5]
            
        }
    },
    
    'tree' : {
        'model': DecisionTreeClassifier(),
        'params': {
             'max_depth': [1,3,5,7,9,11,13,15,17,19,21],
              'criterion': ['gini', 'entropy'],
              'min_samples_split': [50,100,150],
              'min_samples_leaf': np.arange(50,100)
            
        }
    }

}

</code></pre>
",17587622.0,-1.0,N/A,2022-01-17 17:54:33,Use google colab full ram memory,<python><machine-learning><data-science><google-colaboratory>,1,1,N/A,CC BY-SA 4.0
74142664,1,-1.0,2022-10-20 15:40:53,1,44,"<p>Guys I have created my graph using <code>gds.graph.project</code> command and this command will make my graph in-memory and it's create a named graph for me.
is there any way to add some nodes or relationship into named (in-memory) graph?
thanks for helping!</p>
",20291754.0,-1.0,N/A,2022-10-20 15:40:53,How to add nodes and relationship into named graph with neo4j?,<graph><neo4j><data-science><path-finding>,0,1,N/A,CC BY-SA 4.0
74177881,1,-1.0,2022-10-24 07:37:30,2,104,"<p>I am reading data from sensor over time and I need to check whether it is trending upwards. I know how to do that, but sensor at some point reaches max value and starts over and I need to be able to ignore this rollover. How to correctly deal with this so that I correctly find the data trend, but also correctly deal with the overflow at the same time. Examples of data that I could receive include (overflows at 255):</p>
<pre><code>data = [191, 198, 204, 217, 230, 241, 255, 17, 32, 67, 90, 117]
</code></pre>
<pre><code>data = [113, 182, 201, 9, 74, 91, 148, 182, 231, 41, 72, 100]
</code></pre>
",9085936.0,-1.0,N/A,2022-10-24 08:41:20,Detect if sensor data is trending upwards with value overflow,<python><data-science>,2,2,N/A,CC BY-SA 4.0
74186607,1,-1.0,2022-10-24 20:41:46,0,371,"<p>A couple days back I started getting these weird errors when working on my jupyter notebook.  I realized that the <a href=""https://nbdev.fast.ai/"" rel=""nofollow noreferrer"">nbdev</a> package I had installed an hour ago required WSL and was likely to be the cause of these issues.</p>
<p>Thus, I've been trying to remove this package, at least for now.  But unfortunately, after an entire weekend I am still right where I started.</p>
<p>When using the command &quot;conda remove nbdev&quot;, I get the following response:</p>
<pre><code>(base) C:\Users\Ben&gt;conda remove nbdev
Collecting package metadata (repodata.json): done
Solving environment: |
</code></pre>
<p>&quot;solving environment&quot; seemingly never ends.  I've left this running overnight before, to no avail.  My CPU and SSD usage are often pinned to nearly 100% while this is running, and python is currently using 8GB of RAM, so there is definitely something wrong here.</p>
<p>Anaconda has been slow for a while and I've just dealt with it, but it's now clear that there is something seriously wrong.  Do you have any tips for how to fix this or should I jump ship to something like miniconda (will that even make a difference)? -- edit: I was thinking of mamba, not miniconda.</p>
<p>I've tried a couple of the solutions mentioned before, such as removing and re-adding the conda-forge channel.  It doesn't seem to have helped.</p>
",11680544.0,11680544.0,2022-10-24 20:51:44,2022-10-24 20:51:44,"conda remove package extremely slow, not working at all",<python><deep-learning><anaconda><data-science><conda>,0,2,N/A,CC BY-SA 4.0
74200345,1,74200504.0,2022-10-25 21:23:02,0,319,"<p>I tried searching it on stack overflow, and I got a lot of similar titles but the problem isn't quite the same (it appears very different). Also, read some of the documentation (not all from Pandas) but couldn't find any method to do this.</p>
<p>Suppose I have a dataframe like:
<a href=""https://i.stack.imgur.com/3Vg7R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Vg7R.png"" alt=""enter image description here"" /></a></p>
<p>How do I combine this into one row in Pandas? That is, how can I have one line with the values:</p>
<p><code>0.000181  0.10139  0.009276 ... 0.0043778 ... 0.001094  0.004550  0.002879  ... 0.000435 0.003431 ...</code></p>
<p>Literally that's all I was trying to find. These other things are suggesting I do a group by, or a join, or something.</p>
",19149243.0,-1.0,N/A,2022-10-25 21:43:46,How to combine multiple rows into one row WITHOUT group by in Python Pandas?,<python><pandas><data-science>,1,3,N/A,CC BY-SA 4.0
70747751,1,70881847.0,2022-01-17 21:30:00,0,429,"<p>I'm working on a project where I need to predict if a person should be frontend dev, backend dev, DevOps eng or data scientist, etc on the basis of his skills.</p>
<p><strong>For example:</strong></p>
<p>Sam has skills ['python','sql','machine learning','flask'] so he can be called as <em>Data Scientist</em>.</p>
<p>I've scrapped LinkedIn and got the job titles of people and their skills as well. I need guidance in a few things:</p>
<ol>
<li>How I can normalize data using NLP</li>
<li>How I can predict a new user that he belongs into particular job or domain.</li>
</ol>
<p>What should be the procedure to implement it using python and any reference articles or tutorials are most appreciated thanks in advance</p>
",11518340.0,-1.0,N/A,2022-01-27 16:07:17,predict job title on the basis of skills,<tensorflow><nlp><artificial-intelligence><data-science><svm>,2,1,N/A,CC BY-SA 4.0
74171353,1,74171382.0,2022-10-23 13:02:50,1,53,"<p>My problem is the following:
Create a binary variable in which:</p>
<p>𝑌=1 indicates that the house was sold for over $200,000
𝑌=0 indicates that the house was sold for less than or equal to $200,000</p>
<p>sac.loc[sac[&quot;price&quot;]&gt; 200000]= 1
sac.loc[sac[&quot;price&quot;]&lt;= 200000]= 0</p>
<p>It changes all the values ​​to 0 and I don't know how to make that change.</p>
",20289481.0,-1.0,N/A,2022-10-23 13:07:28,change a numeric column to binary?,<python><pandas><dataframe><binary><data-science>,1,0,N/A,CC BY-SA 4.0
70746551,1,70746693.0,2022-01-17 19:28:14,-1,121,"<p>So I have a pretty large dataset so I need to write something kind of efficient.
My data contains release years of albums of various artist in one list and the average songlength of each album in another list.</p>
<p>As an example here is some made up data. The song length is here given in minutes.</p>
<pre><code>release_year=[2017,2017,2019,2020,2020,2021]
avg_songlength=[3,5,3,4,2,3]
</code></pre>
<p>I want to get a dataset which removes duplicates in the release_year list and for every duplicate it averages the songlength again. So the result I want to get is:</p>
<pre><code>years_without duplicates=[2017,2019,2020,2021]
avg_length_of_year=[3+5/2,3,4+2/2,3]
</code></pre>
<p>I found set() to be efficient for removing duplicates, but I don't know how to combine the entires in the other list then
what's an easy way to do this?</p>
",9921810.0,9921810.0,2022-01-17 19:32:00,2022-01-17 21:23:15,Remove duplicates in one list and average corresponding list entries of another list,<python><arrays><list><data-science>,4,1,N/A,CC BY-SA 4.0
70748516,1,-1.0,2022-01-17 23:03:06,0,155,"<p>I followed <a href=""https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-libraries.html"" rel=""nofollow noreferrer"">this tutorial</a> to run glue scripts locally. I checked in cmd if spark and maven are installed and it confirmed it by showing the installed version.</p>
<p>Now when I try to run a glue script by typing:</p>
<pre><code>./bin/gluesparksubmit &quot;path_to_python_script&quot;
</code></pre>
<p>I get the error that the dot operator could not be found. How could I solve this? I already googled that and could not find a solution for that</p>
",15993614.0,-1.0,N/A,2022-01-17 23:03:06,I get an error when trying to run glue job locally in cmd,<python><amazon-web-services><pyspark><data-science><aws-glue>,0,3,N/A,CC BY-SA 4.0
74207648,1,-1.0,2022-10-26 12:18:23,0,104,"<p>I have a bunch of huge sensor data which is in excel file the data something looks like json format(not exactly)need to parse the data i dont have any idea the data something looks like this
<code>FM4 [Priority=0] [GPS element=[X=776517049] [Y=128887449] [Speed=4] [Angle=102] [Altitude=900] [Satellites=8]] [IO=[239=1] [240=1] [80=1] [21=5] [200=0] [69=1] [1=1] [179=0] [2=1] [180=0] [113=99] [841=0] [842=0] [181=14] [182=10] [66=12087] [24=4] [205=28441] [206=31908] [67=4117] [68=0] [9=12157] [17=-152] [18=0] [19=123] [6=-21015] [241=40486] [800=12087] [840=44496] [14=4619303992] [11=899186004] ] [Timestamp=1666676090000] [EventSource=0] [IMEI = 357544375160179]:::5548###[[IMEI = 357544375160179],[Latitude = 128887449],[Longitude = 776517049],[Speed = 4],[Angle = 102],[Altitude = 900],[Odometer = 0],[IO = [Power = 0] [Digital Input 1 = 1] [Digital Input 2 = 1] [Battery Current = 0] [Analog Input 2 = 44521] [Sleep Mode = 0] [Analog Input 1 = 12157] [ICCID = 899186004] [ICCID2 = 4619303992] [Axis X = -152] [Asis Y = 0] [Asis Z = 123] [Data Mode = 1] [PDOP = 14] [Speed = 4] [External Voltage = 12087] [Battery Voltage = 4117] [GSM Signal = 5] [GPS Validity = 1] [Battery Percentage = 99] [Ignition = 1] [Movement = 1] [Active GSM Operator = 40486] [Digital Output 1 = 0] [Digital Output 2 = 0] [Voltage = 12087] ]]######Device Time : 2022-10-25 05:34:50, Time Zone Of Device Set On System : Plus 00:00###</code> this is only one row data
and expected output is
<a href=""https://i.stack.imgur.com/DmncW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DmncW.png"" alt=""enter image description here"" /></a>
<a href=""https://docs.google.com/spreadsheets/d/1p4CgX1ZLQVgAOOAw8cBUtQ5p3RwFfci4hcsf7oPB-qE/edit?usp=sharing"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/1p4CgX1ZLQVgAOOAw8cBUtQ5p3RwFfci4hcsf7oPB-qE/edit?usp=sharing</a> this is my piece of data first 12 columns are formatted correctly and 13 column data is merged and not able to parse the data how to do it in python.</p>
<p>how to do this by using python</p>
",19419337.0,19419337.0,2022-10-26 16:01:23,2022-10-26 16:01:23,how to convert the unstructured data to structured data using pandas,<json><pandas><numpy><data-science>,0,10,N/A,CC BY-SA 4.0
74199808,1,-1.0,2022-10-25 20:26:32,0,1271,"<p>Hello Im fairly new to all of these stuff trying to learn on my own but I keep getting stuck here and there, till now I figured it out somehow but this time the matplotlib dont want to work for me , can anyone help my with this ?</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
us_babies = pd.read_csv(r&quot;C:\Users\Asuma\Downloads\Ex_Files_Intro_Data_Science\Exercise Files\us_baby_names.csv&quot;)
print(us_babies)

my_name = us_babies.loc[us_babies[&quot;Name&quot;] ==&quot;Mehdi&quot;, :]
print(my_name)

my_chart = my_name.plot.barh(x=&quot;Year&quot;, y=&quot;Count&quot;)
print(my_chart)
</code></pre>
<p>and after this I'm keep getting this message instead of a plot &gt; <a href=""https://i.stack.imgur.com/iFNrC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iFNrC.png"" alt=""enter image description here"" /></a></p>
",20258332.0,-1.0,N/A,2022-10-25 20:26:32,"AxesSubplot(0.125,0.11;0.775x0.77)",<matplotlib><pycharm><data-science>,0,3,N/A,CC BY-SA 4.0
74208216,1,-1.0,2022-10-26 13:00:09,3,1087,"<p>Can someone help me with this error of matplotlib?
I'm using jupyter for some data science project from a famous book (hands-on machine learning...) but I have a problem with an unusual error.</p>
<p>This is the code:</p>
<pre><code>%matplotlib inline  
import matplotlib.pyplot as plt
housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4,
             s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7),
             c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True,
             sharex=False)
plt.legend()
save_fig(&quot;housing_prices_scatterplot&quot;)
</code></pre>
<p>And this is the error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Cell In [85], line 3
      1 get_ipython().run_line_magic('matplotlib', 'inline')
      2 import matplotlib.pyplot as plt
----&gt; 3 housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4,
      4              s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7),
      5              c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True,
      6              sharex=False)
      7 plt.legend()
      8 save_fig(&quot;housing_prices_scatterplot&quot;)

File ~/my_env/lib/python3.9/site-packages/pandas/plotting/_core.py:945, in PlotAccessor.__call__(self, *args, **kwargs)
    943 if kind in self._dataframe_kinds:
    944     if isinstance(data, ABCDataFrame):
--&gt; 945         return plot_backend.plot(data, x=x, y=y, kind=kind, **kwargs)
    946     else:
    947         raise ValueError(f&quot;plot kind {kind} can only be used for data frames&quot;)

File ~/my_env/lib/python3.9/site-packages/pandas/plotting/_matplotlib/__init__.py:71, in plot(data, kind, **kwargs)
     69         kwargs[&quot;ax&quot;] = getattr(ax, &quot;left_ax&quot;, ax)
     70 plot_obj = PLOT_CLASSES[kind](data, **kwargs)
---&gt; 71 plot_obj.generate()
     72 plot_obj.draw()
     73 return plot_obj.result

File ~/my_env/lib/python3.9/site-packages/pandas/plotting/_matplotlib/core.py:452, in MPLPlot.generate(self)
    450 self._compute_plot_data()
    451 self._setup_subplots()
--&gt; 452 self._make_plot()
    453 self._add_table()
    454 self._make_legend()

File ~/my_env/lib/python3.9/site-packages/pandas/plotting/_matplotlib/core.py:1225, in ScatterPlot._make_plot(self)
   1223 if self.colormap is not None:
   1224     if mpl_ge_3_6_0():
-&gt; 1225         cmap = mpl.colormaps[self.colormap]
   1226     else:
   1227         cmap = self.plt.cm.get_cmap(self.colormap)

File ~/my_env/lib/python3.9/site-packages/matplotlib/cm.py:87, in ColormapRegistry.__getitem__(self, item)
     85 def __getitem__(self, item):
     86     try:
---&gt; 87         return self._cmaps[item].copy()
     88     except KeyError:
     89         raise KeyError(f&quot;{item!r} is not a known colormap name&quot;) from None

TypeError: unhashable type: 'LinearSegmentedColormap'
</code></pre>
<p>I just want to use matplotlib for a simple and normal graph but I can't find the problem.</p>
",17233805.0,-1.0,N/A,2022-12-08 19:12:50,"Matplotlib ""LinearSegmentedColormap"" error",<matplotlib><data-science><jupyter>,3,1,N/A,CC BY-SA 4.0
74209882,1,-1.0,2022-10-26 14:55:56,2,824,"<p>I am new to datascience and tried running a tutorial. snippet of code:</p>
<pre><code>import pandas as pd
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

data = pd.read_parquet('files/house_sales.parquet')
X = data.drop(['SalePrice', 'Target']).fillna(-1)
y = data['Target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)

sfs = SFS(KNeighborsClassifier(), k_features=10, verbose=2)
sfs.fit(X_train, y_train)
</code></pre>
<p>Produces tons of warnings (seems to be from iteration):</p>
<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
C:\Users\Asus-PC\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)
C:\Users\Asus-PC\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)
C:\Users\Asus-PC\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

.... # repeated
</code></pre>
<p>C:\Users\Asus-PC\anaconda3\lib\site-packages\sklearn\neighbors_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. <code>skew</code>, <code>kurtosis</code>), the default behavior of <code>mode</code> typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of <code>keepdims</code> will become False, the <code>axis</code> over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set <code>keepdims</code> to True or False to avoid this warning.</p>
<p>I googled the answer and get that it might be from scipy / numpy modules. It is however unclear on how to set keepdims, as it is not included in the SFS function arguments</p>
<p>Expected example:</p>
<pre><code>[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:   11.5s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:   11.5s finished

[2021-02-28 08:34:58] Features: 1/7 -- score: 0.7605911330049261[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    7.6s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    7.6s finished
</code></pre>
<p>Does someone know how to remove the warnings?</p>
",19069543.0,-1.0,N/A,2022-10-26 14:55:56,mlxtend SequentialFeatureSelector give keepdims warning,<data-science><mlxtend><sequentialfeatureselector>,0,0,N/A,CC BY-SA 4.0
74212709,1,74216409.0,2022-10-26 18:39:50,2,44,"<p><strong>Problem:</strong></p>
<p>Suppose I have a PANDAS data frame titled, <code>recorded_values_of_accelerometer</code> which appears as a one column by 22272 row table in the figure below:</p>
<p><a href=""https://i.stack.imgur.com/yoT69.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yoT69.png"" alt=""ImageOne: DataFrame"" /></a></p>
<p>Now, suppose I need to plot this table using <code>recorded_values_of_accelerometer.plot()</code>. However, I need certain rows in the plot to be of a particular color. Say, rows 78 to 295 should appear red instead of blue. Running, <code>recorded_values_of_accelerometer.plot()</code> we have something like:</p>
<p><a href=""https://i.stack.imgur.com/e7HTN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e7HTN.png"" alt=""Image2: Graphed Image"" /></a></p>
<p>How do I do that?</p>
<p><strong>What I have tried:</strong></p>
<p>I have tried to mimic the solution to this thread:
<a href=""https://stackoverflow.com/questions/60253229/set-a-different-color-to-a-row-of-dataframe"">Set a different color to a row of dataframe</a></p>
<p>I did the following:</p>
<pre class=""lang-py prettyprint-override""><code>colorOfPoints = 22272 * ['red']

for values in range(78, 295):
    colorOfPoints[values] = 'blue'
    
recorded_values_of_accelerometer.plot(color = colorOfPoints)
</code></pre>
<p>However, it just makes the entire plot blue (the most recent color) instead of only columns 78 to 295.</p>
",20334834.0,-1.0,N/A,2022-10-27 03:18:42,Changing Colors of the Plot of A DataFrame of a Single Line in Pandas,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
70758784,1,70759085.0,2022-01-18 16:09:02,0,567,"<p>I have a dataframe:</p>
<pre><code>df = 
time id ser1 ser2 ... ser20 N0ch0 N1ch0 N2ch0 N0ch1 N1ch1 N2ch1 N0ch2 N1ch2 N2ch2 N0ch3 N1ch3 N2ch3
  1   2  4    5         3     8     7     8    5     1      4    6     2      7    9    8      6
</code></pre>
<p>And I want to pivot it based on the channel ('ch' substring), such that it will become a column, so new dataframe will be:</p>
<pre><code>time id channel ser1 ser2 ... ser20 N0 N1 N2
  1   2   0      4    5         3   8  7  8
  1   2   1      4    5         3   5  1  4
  1   2   2      4    5         3   6  2  7
  1   2   3      4    5         3   9  8  6
</code></pre>
<p>What is the best way to do so?</p>
",6057371.0,-1.0,N/A,2022-01-18 19:58:35,dataframe how pivot table based on substring of the column,<pandas><dataframe><data-science><pivot-table><pandas-melt>,2,0,N/A,CC BY-SA 4.0
74226055,1,-1.0,2022-10-27 17:29:10,1,60,"<p>I have data of tenders and bidders. I want to find the set of bidders (let's say set of 2 bidders), bidding in different tenders along with bidding count of such sets.
I have the following data:</p>
<pre><code>data={'Tender_id':['T1','T1','T1','T1','T1','T1','T1','T1','T1','T1','T2','T2','T2','T2','T2','T2','T2','T2','T2','T2','T3','T3','T3','T3','T3','T3','T3','T3','T3','T3','T4','T4','T4','T4','T4','T4','T4','T4','T4','T4','T5','T5','T5','T5','T5','T5','T5','T5','T5','T5'], 'Bidder_id':['B1','B2','B3','B4','B5','B6','B7','B8','B9','B10','B2','B3','B4','B5','B6','B7','B8','B9','B10','B11','B1','B2','B3','B14','B15','B16','B7','B92','B9','B1','B91','B2','B3','B4','B5','B6','B17','B18','B19','B10','B1','B2','B93','B14','B15','B16','B17','B18','B19','B10']}

df=pd.DataFrame(data)
</code></pre>
<p>which looks like this:</p>
<pre><code>Index Tender_ID Bidder_ID
0         T1        B1
1         T1        B2
2         T1        B3
3         T1        B4
4         T1        B5
5         T1        B6
6         T1        B7
7         T1        B8
8         T1        B9
9         T1       B10
10        T2        B2
11        T2        B3
12        T2        B4
13        T2        B5
14        T2        B6
15        T2        B7
16        T2        B8
17        T2        B9
18        T2       B10
19        T2       B11
20        T3        B1
21        T3        B2
22        T3        B3
23        T3       B14
24        T3       B15
25        T3       B16
26        T3        B7
27        T3       B92
28        T3        B9
29        T3        B1
30        T4       B91
31        T4        B2
32        T4        B3
33        T4        B4
34        T4        B5
35        T4        B6
36        T4       B17
37        T4       B18
38        T4       B19
39        T4       B10
40        T5        B1
41        T5        B2
42        T5       B93
43        T5       B14
44        T5       B15
45        T5       B16
46        T5       B17
47        T5       B18
48        T5       B19
49        T5       B10
</code></pre>
<p>I tried to make all unique combination of two bidders in each tender by using this code:</p>
<pre><code>df1 = pd.DataFrame([
    [n, x, y]
    for n, g in df.groupby('Tender_ID').Bidder_ID
    for x, y in combinations(g, 2)
], columns=['Tender', 'sgm_1', 'sgm_2'])
df2 = df1.groupby(['sgm_1', 'sgm_2']).size().reset_index(name=&quot;count&quot;)
df3 = df2[(df2['sgm_1']!=df2['sgm_2'])]
new_df = pd.merge(df1, df2,  how='left', left_on=['sgm_1','sgm_2'], right_on = ['sgm_1','sgm_2'])

print (new_df)
</code></pre>
<p>which gave me the following result:</p>
<pre><code>    Tender sgm_1 sgm_2  count
0       T1    B1    B2      3
1       T1    B1    B3      2
2       T1    B1    B4      1
3       T1    B1    B5      1
4       T1    B1    B6      1
..     ...   ...   ...    ...
220     T5   B17   B19      2
221     T5   B17   B10      2
222     T5   B18   B19      2
223     T5   B18   B10      2
224     T5   B19   B10      2
</code></pre>
<p>But it is showing same bidders set across different tenders, such as</p>
<p><a href=""https://i.stack.imgur.com/gIaA2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gIaA2.png"" alt=""enter image description here"" /></a></p>
<p>However, my intended result is like this:
<a href=""https://i.stack.imgur.com/zptqw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zptqw.jpg"" alt=""Intended results"" /></a></p>
<p>May somebody help me to get the desired results.</p>
",12572154.0,12572154.0,2022-10-28 04:37:45,2022-10-28 04:37:45,unique combinations of subgroups across all groups in selected columns in pandas data frame and their count,<python><pandas><dataframe><data-science><analytics>,0,3,N/A,CC BY-SA 4.0
74213091,1,-1.0,2022-10-26 19:12:16,0,35,"<p>I have a dataframe. It shows the page journeys of the users. The columns are; user-id, created_at, action, target. The variable I want to get in target has more than one name and I want to reach the number of them. But these should count only one of the targets that both mean the same thing. Count only one of two different target values ​​for rows containing the same user_id, action, created_at</p>
<p><a href=""https://i.stack.imgur.com/kYqDC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kYqDC.png"" alt=""enter image description here"" /></a></p>
<p>For example, if the target value of rows with the same user-id, created-at, action value contains either of these two values, count that row only once. my output should be 2 in the data in the photo</p>
",20342523.0,-1.0,N/A,2022-10-27 01:55:08,counting only one of the values ​in another column of rows with certain columns being equal to each other,<pandas><dataframe><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
74213176,1,-1.0,2022-10-26 19:19:32,0,58,"<p>I am working on an EEG dataset and I have been trying to use mne python but I haven't been able to load my .mat data or visualise it.
I would be very grateful if anyone could helpe with it.
Thank you!</p>
<p>For loading the .mat data when I use mne.read_evoked_filedtrip()or mne.read_epochs_fieldtrip. I get errors usually TypeError:missing 1 required positional argument:'info'.
I was able to load the data with the help of scipy loadmat but I am not sure how to visualise and process the data.</p>
",20342474.0,-1.0,N/A,2023-09-14 07:48:37,Issues with loading and visualising .mat data with use of mne python,<python><data-science><visualization><loaddata><mne-python>,1,1,N/A,CC BY-SA 4.0
70757202,1,70757300.0,2022-01-18 14:26:37,1,266,"<p>I have trained an XGBoost Regressor model on data that has a different shape to the test data I intend to predict on. Is there a way to go around this or a model that can tolerate feature mismatches?</p>
<p>The input training data and test data got mismatched during One Hot Encoding of categorical features.</p>
<pre><code>best_xgb = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
             gamma=0, gpu_id=-1, importance_type=None,
             interaction_constraints='', learning_rate=0.05, max_delta_step=0,
             max_depth=6, min_child_weight=10,monotone_constraints='()', n_estimators=400, n_jobs=4,
             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,
             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',
             validate_parameters=1, verbosity=None)

best_xgb.fit(X, y)

best_xgb.predict(test_data)

</code></pre>
<p>I get the following error:
<a href=""https://i.stack.imgur.com/SylRw.png"" rel=""nofollow noreferrer"">Shape Mismatch Error</a></p>
",13114057.0,-1.0,N/A,2022-01-18 14:32:41,Shape Mismatch XGBoost Regressor,<python><data-science><xgboost>,1,1,N/A,CC BY-SA 4.0
74221787,1,74221816.0,2022-10-27 12:08:50,1,56,"<p>I just started learning matplotlib. The code I used is here below:</p>
<pre><code>import matplotlib.pyplot as plt

x = [1,2,3]
y = [2,4,6]

plt.plot([x, y])


</code></pre>
<p>The graph doesn't show up. It just gets stuck. Is there a way to fix it?</p>
<p>I am a beginner matplotlibber, so I'm just following a tutorial, but this doesn't work for me. I just simply want the graph to show up.</p>
",18077471.0,-1.0,N/A,2022-10-27 12:10:35,Matplotlib - The graph does not show in Pycharm,<python><matplotlib><plot><graph><data-science>,1,0,N/A,CC BY-SA 4.0
74230740,1,74230828.0,2022-10-28 04:44:18,2,48,"<p>new to pandas, I have a challenge regard checking values and performing multiple actions based on four variables (reception_date,final_date,Status,ID) the problem has the following table:</p>
<pre><code>      id             user_email reception_date   end_date    status
0  42872     judahena@ia.com.co      3/30/2022  3/30/2022  Accepted
1  42872   vanvalen@etst.com.co       3/1/2022   3/4/2022  Returned
2  42872  luaquint@maila.com.co       3/7/2022  3/30/2022  In Study
3  99999                a@a.com       3/6/2022  3/28/2022  In Study
4  42872           test@test.es      3/23/2022  3/25/2022  In Study
5  99999                 a@b.es      3/28/2022   4/5/2022  Accepted
6  78787                 a@b.es      3/15/2022  3/16/2022  In Study
</code></pre>
<p>Firstly, it is required to perform operations for the same ID, (in this example only few are found, however, the database consists of more than 50,000 data), check if in the Status column contains &quot;Accepted&quot;, once verified this, check if the &quot;end_date&quot; of the status &quot;In Study&quot; is equal to the &quot;reception_date&quot; of the status &quot;Accepted&quot;, if this condition is true, change the status from &quot;In Study&quot; to &quot;Accepted&quot;, the expected output would be as follows:</p>
<pre><code>      id             user_email reception_date   end_date    status
0  42872     judahena@ia.com.co      3/30/2022  3/30/2022  Accepted
1  42872   vanvalen@etst.com.co       3/1/2022   3/4/2022  Returned
2  42872  luaquint@maila.com.co       3/7/2022  3/30/2022  Accepted
3  99999                a@a.com       3/6/2022  3/28/2022  Accepted
4  42872           test@test.es      3/23/2022  3/25/2022  In Study
5  99999                 a@b.es      3/28/2022   4/5/2022  Accepted
6  78787                 a@b.es      3/15/2022  3/16/2022  In Study
</code></pre>
<p>Since I'm relative new to pandas I've tried several methods, one of them being my last attemp,using</p>
<pre><code>Test=Test.merge(Test.loc[Test.status== 'Accepted'], how='left', left_on=['id'], right_on=['id'], suffixes=(&quot;&quot;, &quot;_y&quot;))\
.assign(status=lambda x:np.where((x.end_date_y==x.reception_date) &amp; (x.status== 'In Study'), 'Accepted',x.status))
</code></pre>
<p>However the result of this wasn't the expected output, I hope you can help me with this, it's driving me crazy.</p>
",11263455.0,-1.0,N/A,2022-10-28 04:57:41,Pandas - Check if two dates are the same with determined conditions (2 more variables) and perform actions,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
74222506,1,74222666.0,2022-10-27 13:02:48,-3,93,"<p>this is an IBM skill lab code. try running it and keeps getting a error</p>
<pre><code>pearson_coef, p_value = stats.pearsonr(df['city-mpg'], df['price'])
print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value)  
</code></pre>
<p>error message</p>
<pre><code>---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Input In [71], in &lt;cell line: 1&gt;()
----&gt; 1 pearson_coef, p_value = stats.pearsonr(df['city-mpg'], df['price'])
      2 print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value)

NameError: name 'stats' is not defined
​
</code></pre>
",18570670.0,1055118.0,2022-10-27 13:07:35,2022-10-27 13:14:07,Why do i keep gettig an error message o this code,<python><data-science><pearson-correlation>,1,1,N/A,CC BY-SA 4.0
74233831,1,74234399.0,2022-10-28 10:11:17,0,185,"<p>in jupyter notebook when we input this we expect this output as shown in screenshot
<a href=""https://i.stack.imgur.com/xnB7m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xnB7m.png"" alt=""correct answer"" /></a></p>
<p>But in my notebook when i enter same query i am getting different output why<a href=""https://i.stack.imgur.com/FCciO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FCciO.png"" alt=""wrong output"" /></a></p>
",20357176.0,-1.0,N/A,2022-12-29 06:24:19,Jupyter notebook linear regression problem,<python-3.x><jupyter-notebook><data-science><linear-regression><prediction>,1,0,N/A,CC BY-SA 4.0
74233909,1,-1.0,2022-10-28 10:18:22,0,39,"<pre><code>n_df = pd.DataFrame(columns=[&quot;Geoid&quot;, &quot;Occupancy&quot;, &quot;BDG_Weights&quot;, &quot;CTS_Weights&quot;, &quot;BI_Weights&quot;])

for idx, row in dd.iterrows():
    for token in ['SFD_', 'MFD_', 'COM_', 'IND_']:
        n_df = pd.concat([n_df, pd.DataFrame([[idx, token.split('_')[0], row[token+'STR_WT'], row[token+'CNT_WT'], row[token+'BI_WT']]], columns=[&quot;Geoid&quot;, &quot;Occupancy&quot;, &quot;BDG_Weights&quot;, &quot;CTS_Weights&quot;, &quot;BI_Weights&quot;])])
break
</code></pre>
<p>Here 'dd' is a dataframe with 26million rows and this loop is taking hours to concatenate. is there any other way to concatenate soon.</p>
",19043417.0,-1.0,N/A,2022-10-31 16:03:10,How to reduce the time taken by this loop for huge csv data with 26M rows?,<python><pandas><dataframe><csv><data-science>,1,2,N/A,CC BY-SA 4.0
74236261,1,-1.0,2022-10-28 13:38:38,0,68,"<p>I am working on the titanic dataset and I needed to make predictions of survival rate. Everything seems to be working good, the issue is on the last three lines of the code (the third line of the code at the bottom = df_KNN[&quot;PassengerId&quot;] = test2[&quot;PassengerId&quot;]). It's giving me a NameError, and I dont understand why. I have checked over and over but can seem to figure out where the problem lies precisely.</p>
<pre><code>    titanic_data = pd.read_csv('titanic_dataset/train.csv')
    titanic_test = pd.read_csv('titanic_dataset/test.csv')
 
    titanic_data.head()
 
    titanic_test.head()
 
    survived = titanic_data[titanic_data[&quot;Survived&quot;]==1] 
   [&quot;Sex&quot;].value_counts()
    dead = titanic_data[titanic_data[&quot;Survived&quot;]==0] 
   [&quot;Sex&quot;].value_counts()
    df_sex = pd.DataFrame([survived,dead])
    df_sex.index = [&quot;Survived&quot;,&quot;Dead&quot;]
    df_sex.plot(kind=&quot;bar&quot;,stacked = True, figsize = (8,4))
 
    # We can see from the chart above that females have a higher chances 
    of survival than male. 
 
    combined_data = [titanic_data, titanic_test]
 
    for data in combined_data:
        data[&quot;Prefix&quot;] = data[&quot;Name&quot;].str.extract(' ([A-Za-z]+)\.', 
    expand = False)
 
    titanic_data[&quot;Prefix&quot;].value_counts()
 
    # We map our data accordingly. 
    Prefix_mapping = 
 
 
 
 {&quot;Mr&quot;:0,&quot;Miss&quot;:1,&quot;Mrs&quot;:2,&quot;Master&quot;:0,&quot;Dr&quot;:3,&quot;Rev&quot;:3,&quot;Major&quot;:3,&quot;Mlle&quot;:1,&quot;Co 
 
 
 l&quot;:3,&quot;Capt&quot;:3,&quot;Sir&quot;:3,&quot;Ms&quot;:1,&quot;Lady&quot;:3,&quot;Mme&quot;:2,&quot;Countess&quot;:3,&quot;Jonkheer&quot;:3,&quot; 
   Don&quot;:3}
 
 
    for data in combined_data:
         data[&quot;Prefix&quot;] = data[&quot;Prefix&quot;].map(Prefix_mapping)
 
    titanic_data.head()
 
    titanic_test.head()
 
titanic_test[&quot;Prefix&quot;] = titanic_test[&quot;Prefix&quot;].fillna(3)
titanic_test.describe()
 
for data in combined_data:
    data.drop(columns = &quot;Name&quot;,inplace=True)
 
titanic_data.head()
 
titanic_test.head()
 
# Cabin Vs Survival 
 
import seaborn as sns
 
sns.heatmap(titanic_data.isnull(), yticklabels = False, cbar = False)
 
titanic_data[&quot;Cabin&quot;].isnull().value_counts()
 
 
titanic_data[&quot;Cabin&quot;] = titanic_data[&quot;Cabin&quot;].fillna(0)
for i in range(891):
    if(titanic_data.at[i,&quot;Cabin&quot;]!=0):
        titanic_data.at[i,&quot;Cabin&quot;]=1
titanic_data.head()
 
sns.countplot(x = &quot;Survived&quot;, hue = &quot;Cabin&quot;, data= titanic_data)
 
survived = titanic_data[titanic_data[&quot;Survived&quot;]==1][&quot;Cabin&quot;].value_counts()
dead = titanic_data[titanic_data[&quot;Survived&quot;]==0][&quot;Cabin&quot;].value_counts()
df_cabin = pd.DataFrame([survived,dead])
df_cabin.index = [&quot;Survived&quot;,&quot;Dead&quot;]
df_cabin.plot(kind=&quot;bar&quot;,stacked = True, figsize = (8,4))
 
# Too many null values in the cabin. 
# we give cabin numbers to passengers accordingly. 
# No cabin = 0 
# Have cabin = 1
 
titanic_test[&quot;Cabin&quot;] = titanic_test[&quot;Cabin&quot;].fillna(0)
for i in range(417):
    if(titanic_test.at[i,&quot;Cabin&quot;]!=0):
        titanic_test.at[i,&quot;Cabin&quot;]=1
 
titanic_test.head()
 
# Passenger Class and Survival Rate: 
sns.set_style(&quot;whitegrid&quot;)
sns.countplot(x = &quot;Survived&quot;, hue = &quot;Pclass&quot;, data = titanic_data)
 
# From the bar above, passengers from the first class had a higher survival rate 
# Many passengers in Third Class did not survive.
# Passenger Class is an important factor while predicting the survival rate of the passengers.
 
# Age 
# data visulaization to see how age influences the survival rate of the passengers 
# Check if age has any null value 
 
sns.heatmap(titanic_data.isnull(), yticklabels = False, cbar = False)
 
plt.figure(figsize=(10,6))
sns.boxplot(x=&quot;Pclass&quot;,y=&quot;Age&quot;,data=titanic_data)
 
first = titanic_data[&quot;Age&quot;][titanic_data[&quot;Pclass&quot;]==1].mean()
second = titanic_data[&quot;Age&quot;][titanic_data[&quot;Pclass&quot;]==2].mean()
third = titanic_data[&quot;Age&quot;][titanic_data[&quot;Pclass&quot;]==3].mean()
print(&quot;Average age for First class Passenger : &quot;, first)
print(&quot;Average age for Second class Passenger : &quot;, second)
print(&quot;Average age for Third class Passenger : &quot;, third)
 
for data in combined_data:
    data[&quot;Age&quot;] = data[&quot;Age&quot;].fillna(0)
 
for i in range(891):
    if(titanic_data.at[i,&quot;Age&quot;]==0):
        if(titanic_data.at[i,&quot;Pclass&quot;]==1):
            titanic_data.at[i,&quot;Age&quot;]= first
        elif(titanic_data.at[i,&quot;Pclass&quot;]==2):
            titanic_data.at[i,&quot;Age&quot;]= second
        else:
            titanic_data.at[i,&quot;Age&quot;]= third
for i in range(418):
    if(titanic_test.at[i,&quot;Age&quot;]==0):
        if(titanic_test.at[i,&quot;Pclass&quot;]==1):
            titanic_test.at[i,&quot;Age&quot;]= first
        elif(titanic_test.at[i,&quot;Pclass&quot;]==2):
            titanic_test.at[i,&quot;Age&quot;]= second
        else:
            titanic_test.at[i,&quot;Age&quot;]= third
 
titanic_data.describe()
 
titanic_test.describe()
 
fac = sns.FacetGrid(titanic_data, hue = &quot;Survived&quot;, aspect = 5)
fac.map(sns.kdeplot,'Age',shade=True)
fac.set(xlim=(0, titanic_data[&quot;Age&quot;].max()))
fac.add_legend()
 
# Looking at the age range. 
# Age range : 0 - 20 
 
fac = sns.FacetGrid(titanic_data,hue = &quot;Survived&quot;, aspect = 5)
fac.map(sns.kdeplot,'Age',shade=True)
fac.set(xlim=(0,titanic_data[&quot;Age&quot;].max()))
fac.add_legend()
plt.xlim(0,20)
 
# Age range : 20 - 30
 
fac = sns.FacetGrid(titanic_data,hue = &quot;Survived&quot;, aspect = 5)
fac.map(sns.kdeplot,'Age',shade=True)
fac.set(xlim=(0,titanic_data[&quot;Age&quot;].max()))
fac.add_legend()
plt.xlim(20,30)
 
# Age range : 30 above
 
fac = sns.FacetGrid(titanic_data,hue = &quot;Survived&quot;, aspect = 5)
fac.map(sns.kdeplot,'Age',shade=True)
fac.set(xlim=(0,titanic_data[&quot;Age&quot;].max()))
fac.add_legend()
plt.xlim(30,75)
 
# Observation : 
 
# age 0 - 20 are more likely to survive than to die
#  age 20 - 30 are more likely to die
# Elderly people are more likely to survive 
 
 
 
# 5. Sex and Survival Rate :
 
survived = titanic_data[titanic_data[&quot;Survived&quot;]==1][&quot;Sex&quot;].value_counts()
dead = titanic_data[titanic_data[&quot;Survived&quot;]==0][&quot;Sex&quot;].value_counts()
df_sex = pd.DataFrame([survived,dead])
df_sex.index = [&quot;Survived&quot;,&quot;Dead&quot;]
df_sex.plot(kind=&quot;bar&quot;,stacked = True, figsize = (8,4))
 
 
# Female passengers have a higher chance of survival than their male counterpart. 
 
# Male = 0 
# Female = 1 
 
 
dummy = pd.get_dummies(titanic_data[&quot;Sex&quot;])
dummy.head()
 
titanic_data[&quot;Sex&quot;] = dummy[&quot;female&quot;]
titanic_data.head()
 
dummy2 = pd.get_dummies(titanic_test[&quot;Sex&quot;])
titanic_test[&quot;Sex&quot;] = dummy2[&quot;female&quot;]
titanic_test.head()
 
# Embarked and Survival Rate 
 
emb_dummies = pd.get_dummies(titanic_data[&quot;Embarked&quot;])
emb_dummies.head()
 
titanic_data[&quot;Q&quot;] = emb_dummies[&quot;Q&quot;]
titanic_data[&quot;S&quot;] = emb_dummies[&quot;S&quot;]
titanic_data.drop(columns=&quot;Embarked&quot;,inplace = True)
titanic_data.head()
 
emb_dumm = pd.get_dummies(titanic_test[&quot;Embarked&quot;])
titanic_test[&quot;Q&quot;] = emb_dumm[&quot;Q&quot;]
titanic_test[&quot;S&quot;] = emb_dumm[&quot;S&quot;]
titanic_test.drop(columns=&quot;Embarked&quot;,inplace = True)
titanic_test.head()
 
# PassengerId and Survival Rate
 
titanic_data.drop(columns = &quot;PassengerId&quot;, inplace = True)
titanic_test.drop(columns = &quot;PassengerId&quot;, inplace = True)
titanic_data.head()
 
# Ticket and Survival Rate  
 
titanic_data.drop(columns = &quot;Ticket&quot;, inplace = True)
titanic_test.drop(columns = &quot;Ticket&quot;, inplace = True)
titanic_data.head()
 
# Fare and Survival Rate 
 
plt.figure(figsize=(5,8))
sns.boxplot(x=&quot;Survived&quot;,y=&quot;Fare&quot;,data=titanic_data)
 
# Observation: 
# The average money spent on the ticket was more for the passengers that survived.
 
titanic_data[&quot;Fare&quot;].isnull().value_counts()
 
titanic_test[&quot;Fare&quot;].isnull().value_counts()
 
 
# The titanic_test dataset has one null value. 
# replace this with the average fare of the people in the same passenger class
 
 
first_fare = titanic_data[&quot;Fare&quot;][titanic_data[&quot;Pclass&quot;]==1].mean()
second_fare = titanic_data[&quot;Fare&quot;][titanic_data[&quot;Pclass&quot;]==2].mean()
third_fare = titanic_data[&quot;Fare&quot;][titanic_data[&quot;Pclass&quot;]==3].mean()
print(&quot;Average Fare for First class Passenger : &quot;, first_fare)
print(&quot;Average Fare for Second class Passenger : &quot;, second_fare)
print(&quot;Average Fare for Third class Passenger : &quot;, third_fare)
 
titanic_test[&quot;Fare&quot;] = titanic_test[&quot;Fare&quot;].fillna(0)
 
for i in range(418):
    if(titanic_test.at[i,&quot;Fare&quot;]==0):
        if(titanic_test.at[i,&quot;Pclass&quot;]==1):
            titanic_test.at[i,&quot;Fare&quot;]= first_fare
        elif(titanic_test.at[i,&quot;Pclass&quot;]==2):
            titanic_test.at[i,&quot;Fare&quot;]= second_fare
        else:
            titanic_test.at[i,&quot;Fare&quot;]= third_fare
 
titanic_test[&quot;Fare&quot;].isnull().value_counts()
 
fac = sns.FacetGrid(titanic_data,hue = &quot;Survived&quot;, aspect = 5)
fac.map(sns.kdeplot,'Fare',shade=True)
fac.set(xlim=(0,titanic_data[&quot;Fare&quot;].max()))
fac.add_legend()
 
fac = sns.FacetGrid(titanic_data,hue = &quot;Survived&quot;, aspect = 5)
fac.map(sns.kdeplot,'Fare',shade=True)
fac.set(xlim=(0,titanic_data[&quot;Fare&quot;].max()))
fac.add_legend()
plt.xlim(0,100)
 
# Passengers who paid a lower fare were most likely from Second or Third class and therefore had a lesser chance of surviving.
 
 
 
titanic_data[&quot;Family&quot;] = titanic_data[&quot;SibSp&quot;] + titanic_data[&quot;Parch&quot;] + 1
titanic_test[&quot;Family&quot;] = titanic_test[&quot;SibSp&quot;] + titanic_test[&quot;Parch&quot;] + 1
 
for data in combined_data:
    data.drop(columns = [&quot;SibSp&quot;,&quot;Parch&quot;],inplace =True)
 
titanic_data.head()
 
titanic_test.head()
 
plt.figure(figsize = (12,6))
sns.set_style(&quot;whitegrid&quot;)
sns.countplot(x = &quot;Survived&quot;, hue = &quot;Family&quot;, data = titanic_data)
 
# Observation 
# Those with companions had higher chances of survival than those who travelled alone. 
 
 
#   Feature Scaling and Titanic_data - Titanic_test - Split
 
# We should scale the data before making our model with any of the various algorithms. 
 
 
 
X = titanic_data[[&quot;Pclass&quot;,&quot;Sex&quot;,&quot;Age&quot;,&quot;Fare&quot;,&quot;Cabin&quot;,&quot;Prefix&quot;,&quot;Q&quot;,&quot;S&quot;,&quot;Family&quot;]]
Y = titanic_data[&quot;Survived&quot;]
X_TEST = titanic_test[[&quot;Pclass&quot;,&quot;Sex&quot;,&quot;Age&quot;,&quot;Fare&quot;,&quot;Cabin&quot;,&quot;Prefix&quot;,&quot;Q&quot;,&quot;S&quot;,&quot;Family&quot;]]
 
print(X)
 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)
X_TEST =  sc.transform(X_TEST)
X
 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2, random_state=1)
 
# MAKING OUR MODEL 
 
# K - Nearest Neighbor Algorithm
 
 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
 
acc = []
 
for i in range(1,20):
    knn = KNeighborsClassifier(n_neighbors = i)
    knn.fit(X_train,y_train)
    yhat = knn.predict(X_test)
    acc.append(accuracy_score(y_test,yhat))
    print(&quot;For k = &quot;,i,&quot; : &quot;,accuracy_score(y_test,yhat))
 
plt.figure(figsize=(8,6))
plt.plot(range(1,20),acc, marker = &quot;o&quot;)
plt.xlabel(&quot;Value of k&quot;)
plt.ylabel(&quot;Accuracy Score&quot;)
plt.title(&quot;Finding the right k&quot;)
plt.xticks(range(1,20))
plt.show()
 
# From the graph above, the preferred value for k that gives us the highest accuracy is k = 9
 
 
    KNN = KNeighborsClassifier(n_neighbors = 9)
    KNN.fit(X,Y)
    y_pred = KNN.predict(X_TEST)
 
    y_pred
 
df_KNN = pd.DataFrame()
df_KNN[&quot;PassengerId&quot;] = test2[&quot;PassengerId&quot;]
df_KNN[&quot;Survived&quot;] = y_pred
df_KNN.head()


```


I dont know what when wrong with my code. I was trying to show accuracy of the survival rate from the titanic dataset but I got a NameError at the end of end of it all. Every part of my code is correct apart from the last three lines. 
</code></pre>
",19435439.0,19435439.0,2022-10-28 13:54:40,2022-10-28 13:54:40,The Last part of titanic data modelling has refused to give the desired result and I dont know why. Error is from the last three lines of the code,<python><pandas><machine-learning><dataset><data-science>,0,5,N/A,CC BY-SA 4.0
71486471,1,71486773.0,2022-03-15 17:24:45,0,160,"<p>I have two data frames.</p>
<p><code>first_dataframe</code></p>
<pre><code>id
9
8
6
5
7
4
</code></pre>
<p><code>second_dataframe</code></p>
<pre><code>id
6
4
1
5
2
3
</code></pre>
<p><strong>Note:</strong> My dataframe has many columns, but I need to compare only based on ID |
I need to find:</p>
<ol>
<li>ids that are in first dataframe and not in second <code>[1,2,3]</code></li>
<li>ids that are in second dataframe and not in first <code>[7,8,9]</code></li>
</ol>
<p>I have searched for an answer, but all solutions that I've found doesn't seem to work for me, because they look for changes based on index.</p>
",17583214.0,17583214.0,2022-03-15 17:56:29,2022-03-15 17:56:29,"How to find elements that are in first pandas Data frame and not in second, and viceversa. python",<python><python-3.x><pandas><dataframe><data-science>,1,0,2022-03-15 17:47:56,CC BY-SA 4.0
74241047,1,-1.0,2022-10-28 21:23:14,0,560,"<p>I want to use the pyodide package on a data analysis project on vs code but i can't install this packcage , I need help please</p>
<pre><code>I tried to run pip install pyodide but i have this problem :
</code></pre>
<p>Collecting pyodide
Using cached pyodide-0.0.2.tar.gz (19 kB)
Preparing metadata (setup.py) ... error
error: subprocess-exited-with-error</p>
<p>× python setup.py egg_info did not run successfully.
│ exit code: 1
╰─&gt; [7 lines of output]
Traceback (most recent call last):
File &quot;&quot;, line 2, in 
File &quot;&quot;, line 34, in 
File &quot;C:\Users\Mouad\AppData\Local\Temp\pip-install-hytn_jkn\pyodide_d2f2163c84794780a8a92f7a373c9c76\setup.py&quot;, line 7, in 
raise ValueError(
ValueError: Pyodide is a Python distribution that runs in the browser or Node.js. It cannot be installed from PyPi.
See <a href=""https://github.com/pyodide/pyodide"" rel=""nofollow noreferrer"">https://github.com/pyodide/pyodide</a> for how to use Pyodide.
[end of output]</p>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed</p>
<p>× Encountered error while generating package metadata.
╰─&gt; See above for output.</p>
<p>note: This is an issue with the package mentioned above, not pip.
hint: See above for details.</p>
<pre><code>
</code></pre>
",19421495.0,-1.0,N/A,2022-10-28 21:23:14,How to install pyodide package on vs code,<python><visual-studio-code><data-science><data-analysis><pyodide>,0,5,N/A,CC BY-SA 4.0
74230856,1,-1.0,2022-10-28 05:02:51,0,34,"<p><a href=""https://i.stack.imgur.com/uZhw8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uZhw8.png"" alt=""This is the Column, how can we get it the same way as some other Columns are in the format of Month-Year"" /></a></p>
<p>I am thinking of trying if condition, but is there any library or method which I don't know about can solve this?</p>
",16142496.0,-1.0,N/A,2022-10-28 05:31:08,"I am working with a Dataset, where few values in the Date Column are like ' 2 Months Ago', ' 28 Days Ago', how can we change these dates to Month-Year",<python><pandas><date><dataset><data-science>,2,1,N/A,CC BY-SA 4.0
74238871,1,74241268.0,2022-10-28 17:24:03,1,84,"<p>I have huge Sensor log data which is in form of [key=value] pair I need to parse the data column wise
i found this code for my problem</p>
<pre><code>import pandas as pd

lines = []
with open('/path/to/test.txt', 'r') as infile:
    for line in infile:
        if &quot;,&quot; not in line:
            continue
        else:
            lines.append(line.strip().split(&quot;,&quot;))

row_names = []
column_data = {}

max_length = max(*[len(line) for line in lines])

for line in lines:
    while(len(line) &lt; max_length):
        line.append(f'{len(line)-1}=NaN')

for line in lines:
    row_names.append(&quot; &quot;.join(line[:2]))
    for info in line[2:]:
        (k,v) = info.split(&quot;=&quot;)
        if k in column_data:
            column_data[k].append(v)
        else:
            column_data[k] = [v]

df = pd.DataFrame(column_data)
df.index = row_names
print(df)

df.to_csv('/path/to/test.csv')
</code></pre>
<p>the above code is suitable when the data is in form of &quot;Priority=0, X=776517049&quot; but my data is something like this [Priority=0][X=776517049] and there is no separator in between two columns how can i do it in python and i am sharing the link of sample data here raw data and bilow that expected parsed data which i done manually https://docs.google.com/spreadsheets/d/1EVTVL8RAkrSHhZO48xV1uEGqOzChQVf4xt7mHkTcqzs/edit?usp=sharing kindly check this link</p>
",19419337.0,-1.0,N/A,2022-10-28 21:53:52,How to parse the log data which is in form of nested [key=value] format using python pandas,<python><pandas><dictionary><parsing><data-science>,1,0,N/A,CC BY-SA 4.0
74241439,1,-1.0,2022-10-28 22:22:46,0,41,"<p>I have a dataframe &quot;Upton of the kinetics of the anti-asthmatic drug theophylline&quot; with tasks.<a href=""https://www.key2stats.com/data-set/view/441"" rel=""nofollow noreferrer"">link to dataset</a>
A piece of dataframe is in the picture. One of the tasks is &quot;Only average concentration per subject should be reported&quot;. I wanted to calculate average concentration for each subject (there are 12 factors in total) and then replace the value of conc to these average value.
<a href=""https://i.stack.imgur.com/X1fvJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X1fvJ.png"" alt="" A piece of dataframe"" /></a></p>
<p>I tried to create a function:</p>
<blockquote>
</blockquote>
<pre><code>conc_mean &lt;- function(x, df){
  mean_x = apply(subset(df, Subject==x, select = conc), 2, FUN=mean)
  apply(df$Subject, MARGIN = 1, 
        FUN = function(x)
        {
          if(Subject == x)
            conc=mean_x
        }
  )
}
</code></pre>
<p>Mean is counted right, but apply() doesn`t work :</p>
<pre><code>apply(df$Subject, MARGIN = 1, FUN = function(x) { :
  dim(X) must have a positive length 
</code></pre>
<p>How can these problem be solved?</p>
",20361895.0,17303805.0,2022-10-28 22:54:15,2022-10-28 22:54:15,Is there an R function which calculates a average value by another column for each factor?,<r><data-science><apply>,1,3,2022-11-03 19:48:52,CC BY-SA 4.0
74246219,1,-1.0,2022-10-29 14:41:52,2,241,"<p>I have a task: &quot;Let's consider 'today' is 2000-01-01. Convert amount of months applicant living at his/her current address to days&quot;.The image shows the columns of my dataframe <a href=""https://i.stack.imgur.com/ow0hT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ow0hT.jpg"" alt=""enter image description here"" /></a></p>
<p>I tried to change the entire column in this way, but I got an error</p>
<pre><code>d &lt;- as.Date(&quot;2000-01-01&quot;)
colnames(creditcard)[which(colnames(creditcard) == 'months')] &lt;- 'days'
creditcard&lt;-transform(creditcard, days=as.integer(d - (d - months(days))))
creditcard$days&lt;-sapply(creditcard$days, function(x) as.integer(d - (d - months(x))))
#no applicable method for 'months' applied to an object of class &quot;c('integer', 'numeric')&quot;

&gt; dput(head(creditcard))
structure(list(card = structure(c(2L, 2L, 2L, 2L, 2L, 2L), levels = c(&quot;no&quot;, 
&quot;yes&quot;), class = &quot;factor&quot;), reports = c(0L, 0L, 0L, 0L, 0L, 0L
), age = c(37.66667, 33.25, 33.66667, 30.5, 32.16667, 23.25), 
    income = c(4.52, 2.42, 4.5, 2.54, 9.7867, 2.5), share = c(0.03326991, 
    0.005216942, 0.004155556, 0.06521378, 0.06705059, 0.0444384
    ), expenditure = c(124.9833, 9.854167, 15, 137.8692, 546.5033, 
    91.99667), owner = structure(c(2L, 1L, 2L, 1L, 2L, 1L), levels = c(&quot;no&quot;, 
    &quot;yes&quot;), class = &quot;factor&quot;), selfemp = structure(c(1L, 1L, 
    1L, 1L, 1L, 1L), levels = c(&quot;no&quot;, &quot;yes&quot;), class = &quot;factor&quot;), 
    dependents = c(3L, 3L, 4L, 0L, 2L, 0L), days = c(54L, 34L, 
    58L, 25L, 64L, 54L), majorcards = c(1L, 1L, 1L, 1L, 1L, 1L
    ), active = c(12L, 13L, 5L, 7L, 5L, 1L), income_fam = c(1.13, 
    0.605, 0.9, 2.54, 3.26223333333333, 2.5)), row.names = c(&quot;1&quot;, 
&quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;), class = &quot;data.frame&quot;)
</code></pre>
",20366209.0,20366209.0,2022-10-29 17:47:55,2022-10-29 18:35:37,How to convert months to days in R?,<r><function><data-science>,1,2,N/A,CC BY-SA 4.0
74237093,1,-1.0,2022-10-28 14:47:40,-3,72,"<p>I know this is an old question. You will probably conclude that Average of the Average is always wrong. Consider the following example:
You want to know the purchasing behaviour for a supermarket by understanding the share% of the baskeket. For each order, you can have a share% across product categories. The dataset can be like this:
order_id, grocery%, tabacco%, cloth%, etc. The share% is based on the order amount. Each row is a unique order_id.</p>
<p>If you are summing up all grocery amount and divided by total order amount, you can indeed get the average grocery share. If given more contexts, let's say, the VIP in this supermarket accounts for 10% and each order they can spend 1 million (just assumption). So it is quite possible that the result tends to be close to the VIP result.</p>
<p>If I am more interested in the average player behaviour, it seems to use the average of the average metric, which is this one: (grocery% + grocery% + ...)/order number.</p>
<p>Any thoughts?</p>
",12343115.0,-1.0,N/A,2022-11-02 18:56:12,Debating: Average of the Average vs Average,<statistics><data-science><cluster-analysis><average>,1,1,N/A,CC BY-SA 4.0
71488661,1,71488720.0,2022-03-15 20:31:17,0,483,"<p>I would like duplicate the first row of this dataframe but keep the index and the first column the same. I cant find a way of leaving the first column the same.</p>
<p>This is just a minimized example I have 1000 rows that I am working with.</p>
<p>This is what is tried:</p>
<pre><code>import numpy as np
import pandas as pd
df = pd.DataFrame({'X':[1,2,3,4,5], 'Y':[84,94,89,83,86],'Z':[86,97,96,72,83]});

df.iloc[np.arange(1).repeat(len(df))].reset_index(drop=True)
</code></pre>
<p>This is what I am looking for</p>
<pre><code>({'X':[1,2,3,4,5], 'Y':[84,84,84,84,84],'Z':[86,86,86,86,86]});
</code></pre>
",18302449.0,-1.0,2022-03-15 20:32:06,2022-03-15 20:38:15,How can I duplicate the first row of a dataframe to the length of the dataframe using Pandas,<python><pandas><dataframe><data-science>,3,0,N/A,CC BY-SA 4.0
71491847,1,-1.0,2022-03-16 04:34:38,1,389,"<p>I hope someone can help me. I'm doing a data science project (as a beginner) and I'm using k-means clustering as an image segmentation tool to count the number of fruit on a tree displayed in an image.
I was able to read the image, turn it into a numpy array, and clustered the pixels (I did this by  creating a data matrix of the number of pixels in the image x 3 and then used k-means). I then used model.cluster_centers to get the cluster centers. I then compressed the image with each pixel location being the value of the cluster center.</p>
<p>What I'm struggling with now is how to segment the image into two: an image with just the fruit and an image with everything but the fruit. Did I not create a binary mask already?</p>
<p>I am also tasked with counting the amount of fruit displayed in the image. I've been told I have to use measure.label from skimage but I am not sure where I would even go from my segmentation to that.</p>
<p>Any help or guidance would be appreciated. I tried looking up image segmentation examples or documentation but I didn't find anything useful. Also haven't learned OpenCV so I'd rather just focus on sklearn, PIL, np, and matplot.plt. My code is below. Thanks!</p>
<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from PIL import Image              
from IPython.display import display 

im = Image.open(&quot;Fruit.jpg&quot;)
display(im)

from numpy import asarray
data = asarray(im)
#print(data.shape)
#Data was (1000, 649, 3)
plt.imshow(data)

k = 8
X = data.reshape((-1, 3))
kmeans = KMeans(n_clusters=k)
kmeans.fit(X)
pred=kmeans.predict(X)

c=kmeans.cluster_centers_.reshape(1,8,3).astype(int)
plt.imshow(c)

X2=np.copy(X)
for x in range(X.shape[0]):
    X2[x]=c[0,pred[x]]
X2=X2.reshape(1000,649,3)
plt.imshow(X2)
</code></pre>
",16791092.0,-1.0,N/A,2022-03-18 20:44:16,PIL/SciKitLearn/Cluster Image Segmentation and Clustering,<python><python-imaging-library><data-science><k-means><scikit-image>,0,0,N/A,CC BY-SA 4.0
74187497,1,-1.0,2022-10-24 22:39:00,2,40,"<p>hey I am trying to rearrange a dataframe into one line with columns and subcolumns but i am having some trouble doing it.</p>
<p>I have input statistics code that looks like this :</p>
<pre><code>                          Characteristic     Total     Male   Female
0         Total population by age groups  105145.0  50080.0  55060.0
1                           0 to 4 years    5660.0   2975.0   2685.0
2                           5 to 9 years    4105.0   2055.0   2050.0
3                         10 to 14 years    4025.0   1980.0   2040.0
4                         15 to 19 years    5105.0   2555.0   2545.0
5                               15 years     925.0    455.0    470.0
</code></pre>
<p>but I want to rearrange to be more like</p>
<pre><code>   Total population by age groups                0 to 4 years                5 to 9 years                10 to 14 years                15 to 19 years                   15 years                                    
                            Total Male Female           Total Male Female           Total Male Female             Total Male Female             Total Male Female          Total Male Female   
0                             105145.0  50080.0    55060.0             5660.0  2975.0    2685.0             4105.0  2055.0   2050.0                4025.0   1980.0   2040.0               5105.0   2555.0   2545.0            925.0    455.0    470.0      

</code></pre>
<p>i have some code but im super inexperienced with Pandas and how to use them.
Thank you</p>
",13789472.0,-1.0,N/A,2022-10-24 23:37:06,Rearrange Data frame to have subcolumns in python,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
70750724,1,-1.0,2022-01-18 05:34:26,0,99,"<p>I've just begun my career transition to tech (Data Science) and want to create a smooth workflow using Jupyter, PyCharm, and Github, specifically maximizing the features of each platform.</p>
<p>For example, I run my Python code on Jupyter (.ipynb) but I cannot run the file on PyCharm CE. Likewise, I cannot run or edit .py files in Jupyter. What's the workaround, or is there a better way of doing this?</p>
<p>This is my first question here so please don't be too harsh. I'm looking forward to learning from all of you.</p>
",17950134.0,1672826.0,2022-01-18 17:39:47,2022-01-18 17:39:47,"How to have an integrated workflow using Jupyter, PyCharm CE, and Github and maximize their features?",<python><github><pycharm><data-science><jupyter>,0,2,N/A,CC BY-SA 4.0
74211314,1,74211346.0,2022-10-26 16:37:38,0,36,"<p>I have a database where four of its columns are: score_home, score_away, home_id and away_id.</p>
<p>I expect to get a variable whose rows contain the winning ID in each game.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>gRes</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>GB</td>
</tr>
<tr>
<td>1</td>
<td>GB</td>
</tr>
</tbody>
</table>
</div>
<p>For that, I tried with the following code</p>
<pre><code>team_f['gRes'] = 0
if team_f['score_home'] &gt; team_f['score_away']:
    team_f['gRes'] = team_f['home_id']
else:
    team_f['gRes'] = team_f['away_id']
</code></pre>
<p>and i get the following error</p>
<pre><code>The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>
<p>Could you suggest how to correct the error or, failing that, any alternatives to build the variable?</p>
",19778814.0,-1.0,N/A,2022-10-26 16:40:48,A variable that indicates who win,<python><pandas><dataframe><data-science>,2,1,2022-10-26 16:40:41,CC BY-SA 4.0
74248004,1,-1.0,2022-10-29 18:56:52,2,151,"<p>I'm working on document clustering where I first build a distance matrix from the tf-idf results. I use the below code to get my tf-idf matrix:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words={'english'})
X = vectorizer.fit_transform(models)
</code></pre>
<p>This results in a matrix of (9069, 22210). Now I want to build a distance matrix from this (9069*9069). I'm using the following code for that:</p>
<pre><code>import numpy as np
import pandas as pd
from scipy.spatial import distance_matrix
from scipy.spatial import distance

arrX = X.toarray()

rowSize = X.shape[0]
distMatrix = np.zeros(shape=(rowSize, rowSize))


#build distance matrix
for i, x in enumerate(arrX):
    for j, y in enumerate(arrX):   
        distMatrix[i][j] = distance.braycurtis(x, y)

np.savetxt(&quot;dist.csv&quot;, distMatrix, delimiter=&quot;,&quot;)
</code></pre>
<p>The problem with this code is that it's extremely slow for this matrix size. Is there a faster way of doing this?</p>
",1789591.0,-1.0,N/A,2022-10-30 23:02:33,How to efficiently iterate through rows in a large matrix with too many columns?,<python><numpy><loops><data-science><distance-matrix>,2,1,N/A,CC BY-SA 4.0
71490495,1,-1.0,2022-03-16 00:26:29,0,428,"<p>Hello how can I convert from wide format to long format with this sample spatial data with R</p>
<p>Sample Data:</p>
<pre><code>origin  origin_x    origin_y destination      dest_x       dest_y       n
Paris   6587.54     44547.58    London     456474.5   8346585.4       1577
Milan   3949.45    5406644.6    Manchester 5437374.464  846347.99     8436
Toronto 86866.44    886254.4    Alberta   8327646.3   676442.98       2000
    
</code></pre>
<p>Expected output:</p>
<pre><code>Key                X              Y                     n           
Paris          6587.54          44547.58               1577         
London         456474.5        8346585.4               1577         
Milan          3949.45         5406644.6               8436         
Manchester     5437374.46      846347.99               8436         
Toronto        86866.44        886254.4                2000         
Alberta        8327646.3       8327646.3               2000 
</code></pre>
<p><img src=""https://i.stack.imgur.com/F6jiw.jpg"" alt=""Image attached for further insight"" /></p>
",18477482.0,10744082.0,2022-03-16 15:05:01,2022-03-16 15:05:01,Convert from wide to long format,<r><tidyverse><data-science><tidyr>,1,2,N/A,CC BY-SA 4.0
74247128,1,74256385.0,2022-10-29 16:53:41,1,166,"<p>I have a dynamic Plotly table. She get dataframe from SQL query using Pandas. Height of table constantly changing and i don't set current value. After setting table she must send as photo using aiogram Telegram bot. How to remove whitespace under table, which arises if i don't set current height?</p>
<pre><code>figure = go.Figure(data=[go.Table(
    columnwidth=[300, 200, 500, 500, 150, 400, 500, 150],
    header=dict(
        values=['&lt;b&gt;Date&lt;/b&gt;', '&lt;b&gt;Time&lt;/b&gt;', '&lt;b&gt;Client&lt;/b&gt;', '&lt;b&gt;Service&lt;/b&gt;', '&lt;b&gt;Price&lt;/b&gt;', '&lt;b&gt;Tips&lt;/b&gt;', '&lt;b&gt;Additional&lt;/b&gt;', '&lt;b&gt;Work time&lt;/b&gt;'],
        align=['center'],
        height=40
    ),
    cells=dict(
        values=dataframe.transpose().values.tolist(),
        align=['center'],
        height=30
    ),
)])
figure.update_layout(autosize=False, margin={'l': 0, 'r': 0, 't': 0, 'b': 0}, width=1280)
figure.write_image(&quot;db/records.png&quot;)
</code></pre>
<p>UPD: <code>dataframe.head().to_dict()</code> reuslt</p>
<pre><code>{'0': {0: '2022-10-24'}, '1': {0: '15:00'}, '2': {0: 'Bob'}, '3': {0: 'Radiator replacement'}, '4': {0: '16'}, '5': {0: '4'}, '6': {0: 'No'}, '7': {0: '2.0 h'}}
</code></pre>
",13471945.0,13471945.0,2022-10-30 03:05:09,2022-10-30 20:05:53,How to remove whitespace under dynamic Plotly table?,<python><plotly><data-science><plotly-python>,1,4,N/A,CC BY-SA 4.0
74249544,1,74272777.0,2022-10-29 23:23:46,1,181,"<p>I am a newbie in the field of data science and I would like to identify the reason why I have been facing the following error:</p>
<pre><code>Error in grid_latin_hypercube(): these arguments contain unknowns: `mtry`. See the finalize() function. 
</code></pre>
<p>My computational routine is structured as follows:</p>
<pre><code>datatrain &lt;- training(data)
rf_mod &lt;- rand_forest(
trees = tune(),
min_n = tune(),
mtry = tune()
) %&gt;% set_engine(&quot;randomForest&quot;) %&gt;% set_mode(&quot;regression&quot;)

tuneargs &lt;- rf_mod 

reci &lt;- recipe(Response ~.,datatrain)

workf &lt;- workflow() %&gt;%
add_model(tuneargs) %&gt;%
add_recipe(reci)

rand_grid &lt;- grid_latin_hypercube(trees(),
                                  min_n(),
                                  mtry(),
                                  size = 100)

</code></pre>
<p>After that the error described above appears.</p>
<p>I think the error may be associated with the fact that I'm considering a randomForest algorithm and I'm using a grid_latin_hypercube. Therefore, it may be that the parameter specifications may not be in agreement.</p>
<p>In this case, how could I solve it?</p>
",10478454.0,-1.0,N/A,2022-11-01 10:10:44,Error in grid_latin_hypercube() for randomForest model in R,<r><machine-learning><data-science><random-forest>,1,0,N/A,CC BY-SA 4.0
74270985,1,-1.0,2022-11-01 02:42:08,0,173,"<p>I have made the following code for a static Gauge chart.</p>
<pre><code>import plotly.graph_objects as go
fig = go.Figure(go.Indicator(
        mode = &quot;gauge+number+delta&quot;,
        value=180,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': &quot;Speed&quot;}))
        # delta={'reference':380}
        fig
</code></pre>
<p>Please help me in making a dynamic Gauge chart where <code>value</code> changes.</p>
<p>Thanks in advance.</p>
",19322508.0,-1.0,N/A,2022-11-01 02:42:08,How to make a gauge chart dynamic?,<python><plotly><data-science><data-analysis><streamlit>,0,2,N/A,CC BY-SA 4.0
71508265,1,71508408.0,2022-03-17 07:07:14,1,60,"<p>I am having a dataset of all the abstracts and the author gender. Now i want to get the all the repetitions of words gender wise so that i can plot it as a graph number of repetition of words with respect to gender.</p>
<pre class=""lang-py prettyprint-override""><code>data_path = '/content/digitalhumanities - forum-and-fiction.csv'
def change_table(data_path):
  df = pd.read_csv(data_path)
  final = df.drop([&quot;Title&quot;, &quot;Author&quot;, &quot;Season&quot;, &quot;Year&quot;, &quot;Keywords&quot;, &quot;Issue No&quot;, &quot;Volume&quot;], axis=1)
  fin = final.set_index('Gender')
  return fin
change_table(data_path).T
</code></pre>
<pre><code>This is the out put i got 
| Gender   | None                                              | Female                                            | Male                                              | None       | None                                  | Male                                              ,Female                                            |None                                              | Male                                             ,Female                                            |
|:----------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|------------|---------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------:|
| Abstract | This article describes Virginia Woolf's preocc... | The Amazonian region occupies a singular place... | This article examines Kipling's 1901 novel Kim... | Pamela; or | Virtue Rewarded uses a literary fo... | This article examines Nuruddin Farah's 1979 no... | Ecological catastrophe has challenged the cont... | British political fiction was a satirical genr... | The Lydgates have bought too much furniture an... 
</code></pre>
<p>Now how can i get the repetition of each word in the abstract with respect to gender and append to the data frame.</p>
<p>Expecting output example</p>
<pre><code>|gender|male|female|none|
|------|----|------|----|
| This    |    3|     0|   0|
|   occupies  |    5|     3|   0|
| examines    |    6|      0|   0|
|   British  |    0|      0|    7|
</code></pre>
<p>.
.
.
<a href=""https://i.stack.imgur.com/BUFJW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BUFJW.png"" alt=""enter image description here"" /></a></p>
",-1.0,-1.0,2022-03-17 08:30:38,2022-03-17 09:14:10,how to count the number of repetation of words and assign a number and append into dataframe,<python><pandas><dataframe><pandas-groupby><data-science>,1,3,N/A,CC BY-SA 4.0
74281224,1,-1.0,2022-11-01 20:03:34,0,44,"<p>I have a txt file like this:
text='{&quot;id&quot;: &quot;a364ebd4&quot;, &quot;reason&quot;: &quot;delay&quot;, &quot;description&quot;: &quot;I bought a tv. It's not working and the manager said it was &quot;the best&quot; &quot;}'</p>
<p>When I try to convert the string into a dict using json.loads()
textf = json.loads(text)
textf</p>
<p>I get the error JSONDecodeError: Expecting ',' delimiter: line 1 column 3290 (char 3289).</p>
<p>I think the problem is the quotation marks at &quot;the best&quot; but I don't know how to remove them without disturbing the quotation marks at &quot;id&quot;,&quot;reason&quot;, etc., which I need to process the information.</p>
",20391520.0,-1.0,N/A,2022-11-01 20:39:32,"JSONDecodeError: Expecting ',' delimiter: Error while trying to convert string to dictionary",<python><nlp><data-science>,1,1,N/A,CC BY-SA 4.0
74252883,1,-1.0,2022-10-30 12:04:10,0,54,"<pre><code>from sys import stdin,stdout

def readln(): 
    return stdin.readline().rstrip()

def outln(n): 
    stdout.write(str(n)) stdout.write(&quot;\n&quot;)

m = 10 
n = random.sample(range(1, 1000000),m)

def particao(n, inicio, fim): 
lista = [] 
primeiro = n[inicio] 
lista.append(primeiro) 
pos_meio = (inicio+fim)//2 
meio = n[pos_meio] 
lista.append(meio) 
ultimo = n[fim] 
lista.append(ultimo) 
lista.sort() 
pivot = lista[1] 
if lista[1] == primeiro: 
    indice_pivot = inicio 
if lista[1] == meio: 
    indice_pivot = pos_meio 
if lista[1] == ultimo: 
    indice_pivot = fim

while inicio &lt; fim:
    while inicio &lt; len(n) and n[inicio] &lt;= pivot:
        inicio+=1

    while n[fim] &gt; pivot:
        fim-=1

    if inicio &lt; fim:
        hold1 = n[inicio]
        n[inicio] = n[fim]
        n[fim] = hold1

hold2 = n[indice_pivot]
n[indice_pivot] = n[fim]
n[fim] = hold2
return fim

def quicksort(n, inicio, fim): 
    if inicio &lt; fim: 
        part = particao(n, inicio, fim) 
        quicksort(n, inicio, part-1) 
        quicksort(n, part+1, fim)

quicksort(n,0,len(n)-1) 
print(n)
</code></pre>
<p>I expected the final result to be sorted but most of the times it is not sorted, i tried with an  array of 10 elements but it should be done with 1000+, i think it's an error with the pivot but i tried everything i know and could and wasn't able to make it work.</p>
<pre><code></code></pre>
",20371359.0,-1.0,N/A,2022-10-30 21:52:28,"Implementation of quicksort with pivot as median of first, middle and last elements",<algorithm><sorting><data-science><quicksort><median>,1,3,N/A,CC BY-SA 4.0
71511842,1,-1.0,2022-03-17 11:51:07,0,1274,"<p>I am trying to use node2vec in Neo4j Desktop. My DB is v. 4.4.4. I have installed Graph Data Science Library (1.8.5) from the plugins tab. But when I try to use &quot;CALL gds.beta.node2vec.stream&quot; I get</p>
<blockquote>
<p>There is no procedure with the name <code>gds.beta.node2vec.stream</code>
registered for this database instance. Please ensure you've spelled
the procedure name correctly and that the procedure is properly
deployed.</p>
</blockquote>
<p>Any ideas?</p>
",2168690.0,-1.0,N/A,2022-03-17 12:30:11,No procedure with the name `gds.beta.node2vec.stream`,<neo4j><graph-data-science>,1,2,N/A,CC BY-SA 4.0
74281033,1,-1.0,2022-11-01 19:41:50,0,61,"<p>I'm trying to pass <code>normalize</code> in parameter search to pass it to GridSearchCV.
I'm getting a warning that normalize is depreciated and that I should use StandardScalar instead.
I can't just add StandardScalar to the pipeline because that would apply it all the time when I want it to be applied once and not applied the second time.</p>
<p>This is my code:</p>
<pre><code>        'params': {
        'normalize': [True, False]
        }
</code></pre>
<p>How can I use <code>StandardScalar</code> here instead of <code>normalize</code>?</p>
",7049845.0,-1.0,N/A,2022-11-01 20:21:41,StandardScalar instead of normalize in parameter_grid,<python-3.x><machine-learning><scikit-learn><data-science><gridsearchcv>,1,0,N/A,CC BY-SA 4.0
74284250,1,-1.0,2022-11-02 04:07:48,-1,131,"<p>I have a thesis paper that focuses on using Bilingual LDA and a modified version (modified for runtime) of K-Means for Sentiment Analysis (using Multinomial NB) on Filipino and English COVID-19 Tweets.</p>
<p>I have the files that came from my Bi-LDA from <a href=""https://github.com/1991wzc/python-LDA-and-BiLDA/blob/master/BiLDA.py"" rel=""nofollow noreferrer"">https://github.com/1991wzc/python-LDA-and-BiLDA/blob/master/BiLDA.py</a> which made text files like theta values, phi values, topic of words and a wordmap (photo of the files attached) <a href=""https://i.stack.imgur.com/7On0l.png"" rel=""nofollow noreferrer"">![Bi-LDA outputs] (https://i.stack.imgur.com/7On0l.png)</a> from the tweets that were tokenized and lemmatized, basically preprocessed, however, I cannot seem to apply K-Means that came from my Bi-LDA files since I do not know what to do next.</p>
<p>I will also attach the Google Colab of the .ipynb file so you can see what is needed to be put for K-Means:
<a href=""https://colab.research.google.com/drive/1FE4WkG-cEe1SPmFm49Z6ovA7oREg17VT?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1FE4WkG-cEe1SPmFm49Z6ovA7oREg17VT?usp=sharing</a></p>
<p>Thank you so much, a little help will surely make a difference for me and my group.</p>
<p>What I did before running the Bi-LDA algorithm is to change the value of K which is equal to the value of K in my K-Means so that it only needs six (6) values once each topic is clustered.</p>
<p>I do not know what to expect, really since I don't know which values are to be put in a K-Means algorithm, but I am expecting after K-Means, I can get to statistical treatments.</p>
",20265846.0,4685471.0,2022-11-02 14:41:59,2022-11-02 16:22:28,Bilingual Latent Dirichlet Allocation into [a Modified] K-Means Clustering Algorithm,<machine-learning><data-science><k-means><lda><naivebayes>,1,0,N/A,CC BY-SA 4.0
74255852,1,-1.0,2022-10-30 18:49:54,0,11,"<p>I've been trying for 2 days now I cant seem to figure out how do I check it and return values if its empty. For example if a roll no. in roll number column is filled I want to check the same row has a name under name column. If not, I want to return &quot;NO&quot;.</p>
<pre><code>value2=u_op.loc[u_op['RollNo'].astype(str).notnull()!=u_op['Name'].astype(str).notnull(), 'Name']
</code></pre>
<p>trying to get nan as an output if roll no is not empty and name is empty but not working</p>
",18486294.0,-1.0,N/A,2022-10-30 19:05:36,How do i check an adjacent cell is non empty if one cell on the same row is empty pandas,<pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
71497908,1,-1.0,2022-03-16 13:26:45,0,537,"<p>I'm running into the error  <code>Shape of passed values is (2549950, 1), indices imply (2549950, 13)</code> when trying to create pandas dataframe out of a One Hot encoded column.</p>
<p>Here's my code snippet.</p>
<pre><code>ohe_df = pd.DataFrame(transformed, columns=enc.get_feature_names())
</code></pre>
<p>The shape of transformed is (2549950, 13) and shape of enc.get_feature_names() is (13,)</p>
<p>Thanks</p>
",15323870.0,-1.0,N/A,2022-03-16 14:23:39,"Error : Shape of passed values is (2549950, 1), indices imply (2549950, 13)",<python><pandas><dataframe><data-science>,1,5,N/A,CC BY-SA 4.0
74259141,1,74259361.0,2022-10-31 05:39:44,1,510,"<p>I am trying to auto-adjust the column cells to the excel data i used this code which i found in stack overflow</p>
<pre><code>df=pd.read_excel(r&quot;location of the file&quot;)
writer = pd.ExcelWriter(r'C:\Users\Aparna\Downloads\columnwidth1.xlsx') 
df.to_excel(writer, sheet_name='my_analysis', index=False, na_rep='NaN')

# Auto-adjust columns' width
for column in df:
    column_width = max(df[column].astype(str).map(len).max(), len(column))
    col_idx = df.columns.get_loc(column)
    writer.sheets['my_analysis'].set_column(col_idx, col_idx, column_width)

writer.save()
</code></pre>
<p>i used this code for increasing the column width but it throws attribute error
<code>Attribute Error: 'DataFrame' object has no attribute 'map'</code>
and the saved excel sheet is not able to open getting file extention is not valid how to over come this here I am sharing the raw data along with expected output <a href=""https://docs.google.com/spreadsheets/d/1_iemqwkTYdUCoXKj3O8JNnq3V1ePY5sSRuFD9zDJRPA/edit?usp=sharing"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/1_iemqwkTYdUCoXKj3O8JNnq3V1ePY5sSRuFD9zDJRPA/edit?usp=sharing</a> this is the link for data.</p>
",19419337.0,-1.0,N/A,2022-10-31 06:32:54,How to auto adjust the column width of a excel sheet using pandas,<python><pandas><data-science><xlsxwriter>,1,2,N/A,CC BY-SA 4.0
74286527,1,74288854.0,2022-11-02 08:50:19,0,1013,"<p>I would like to extend a <a href=""https://huggingface.co/cross-encoder/nli-deberta-v3-base"" rel=""nofollow noreferrer"">zero-shot text classification (NLI) model</a>'s vocabulary, to include domain-specific vocabulary or just to keep it up-to-date. For example, I would like the model to know the names of the latest COVID-19 variants are related to the topic 'Healthcare'.</p>
<p>I've added the tokens to the tokenizer and resized the token embeddings. However, I don't know how to finetune the weights in the embedding layer, as suggested <a href=""https://github.com/huggingface/transformers/issues/1413#issuecomment-608061516"" rel=""nofollow noreferrer"">here</a>.</p>
<p>To do the finetuning, can I use simply use texts containing a mixture of new vocabulary and existing vocabulary, and have the tokenizer recognise the relations between tokens through co-occurrences in an unsupervised fashion?</p>
<p>Any help is appreciated, thank you!</p>
",10939465.0,-1.0,N/A,2022-11-02 12:04:04,How to extend the vocabulary of a pretrained transformer model?,<nlp><data-science><huggingface-transformers><huggingface-tokenizers><fine-tune>,1,0,N/A,CC BY-SA 4.0
74297283,1,74297310.0,2022-11-03 01:27:12,-1,115,"<p>I'm starting to learn about cdfs and am experimenting with this sample code:</p>
<pre><code>xs = np.linspace(-3,3)
ys = norm(0,1).cdf(XS)
</code></pre>
<p>While xs gives me 50 values between -3 and 3, the highest value is 3. In turn, ys gives a set of 50 incremental probabilities between 0 and 1, seemingly representing the probability of getting the value at or below each xs value. It's curious to me though that the top value of ys is 0.9986501, instead of 1. It would seem to me that you would always get a value of 3 or less. So why is the highest value (marginally) less than 1?</p>
",19176762.0,-1.0,N/A,2022-11-03 01:34:07,Why isn't the highest probability 1 for a cumulative distribution function when applied to a normal distribution?,<python><numpy><data-science><distribution><normal-distribution>,1,0,N/A,CC BY-SA 4.0
74295038,1,-1.0,2022-11-02 20:00:24,1,179,"<p>I am conducting PCA on a dataset. I am attempting to add a line in my 3d graph which shows the first principal component. I have tried a few methods but have not been able to display the first principal component as a line in my 3d graph. Any help is greatly appreciated. My code is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
np.set_printoptions (suppress=True, precision=5, linewidth=150)
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

file_name = 'C:/Users/data'
input_data = pd.read_csv (file_name + '.csv', header=0, index_col=0)
A = input_data.A.values.astype(float)
B = input_data.B.values.astype(float)
C = input_data.C.values.astype(float)
D = input_data.D.values.astype(float)
E = input_data.E.values.astype(float)
F = input_data.F.values.astype(float)
X = np.column_stack((A, B, C, D, E, F))

ncompo = int (input (&quot;Number of components to study: &quot;))
print(&quot;&quot;)
pca = PCA (n_components = ncompo)
pcafit = pca.fit(X)
cov_mat = np.cov(X, rowvar=0)
eig_vals, eig_vecs = np.linalg.eig(cov_mat)

perc = pcafit.explained_variance_ratio_
perc_x = range(1, len(perc)+1)
plt.plot(perc_x, perc)
plt.xlabel('Components')
plt.ylabel('Percentage of Variance Explained')
plt.show()

#3d Graph
plt.clf()
le = LabelEncoder()
le.fit(input_data.Grade)
number = le.transform(input_data.Grade)
colormap = np.array(['green', 'blue', 'red', 'yellow'])

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(D, E, F, c=colormap[number])

ax.set_xlabel('D')
ax.set_ylabel('E')
ax.set_zlabel('F')

plt.title('PCA')
plt.show()
</code></pre>
",19409359.0,19409359.0,2022-11-02 21:27:06,2022-11-02 22:05:24,"Principle Component Analysis, add a line to the 3d graph showing the first principal component",<python><components><data-science><pca><principal>,1,2,N/A,CC BY-SA 4.0
71530121,1,-1.0,2022-03-18 16:18:59,0,117,"<p>I am a beginner in the field of python. I am trying to plot a 3d projection from a text file containing multiple tables. The sample of a file is as follows:</p>
<pre><code> old parameters found on file WAVECAR:
 energy-cutoff  :      900.00
 volume of cell :      199.05
  k-point     1 :       0.0000    0.0000            0.0000
  band No.  band energies     occupation 
      1     -25.6663      2.00000
      2     -25.6144      2.00000
      3     -11.1229      2.00000
      4     -10.9309      2.00000
      5     -10.9048      2.00000
      6     -10.8874      2.00000
      7     -10.7276      2.00000
      8     -10.6629      2.00000
      9      -4.3196      2.00000
     10      -3.7248      2.00000
     11       1.1756      2.00000
     12       1.1796      2.00000
     13       1.6655      2.00000
     14       1.9005      2.00000
     15       1.9115      2.00000
     16       2.3948      1.90818
     17       2.7068      0.60269
     18       3.2113      0.00004
     19       3.9517      0.00000
     20       3.9902      0.00000
     21       4.5536      0.00000

 k-point     2 :       0.0000    0.0000    0.1250
 band No.  band energies     occupation 
      1     -25.6662      2.00000
      2     -25.6144      2.00000
      3     -11.1228      2.00000
      4     -10.9338      2.00000
      5     -10.9045      2.00000
      6     -10.8880      2.00000
      7     -10.7276      2.00000
      8     -10.6632      2.00000
      9      -4.2799      2.00000
     10      -3.7204      2.00000
     11       0.9818      2.00000
     12       1.0592      2.00000
     13       1.6660      2.00000
     14       1.8142      2.00000
     15       1.8155      2.00000
     16       2.5756      1.31601
     17       2.7520      0.40073
     18       3.3411      0.00000
     19       3.9921      0.00000
     20       4.2573      0.00000
     21       4.6058      0.00000

 k-point     3 :       0.0000    0.0000    0.2500
 band No.  band energies     occupation 
      1     -25.6662      2.00000
      2     -25.6144      2.00000
      3     -11.1225      2.00000
      4     -10.9406      2.00000
      5     -10.9038      2.00000
      6     -10.8896      2.00000
      7     -10.7277      2.00000
      8     -10.6638      2.00000
      9      -4.1713      2.00000
     10      -3.7204      2.00000
     11       0.6623      2.00000
     12       0.8871      2.00000
     13       1.2141      2.00000
     14       1.6374      2.00000
     15       2.0643      1.99994
     16       2.7959      0.24996
     17       2.8879      0.07166
     18       3.6028      0.00000
     19       4.1048      0.00000
     20       4.7052      0.00000
     21       4.8484      0.00000
   k-point   410 :       0.6250    0.0000    0.5000
   band No.  band energies     occupation 
      1     -25.6503      2.00000
      2     -25.6304      2.00000
      3     -11.0737      2.00000
      4     -10.9810      2.00000
      5     -10.8498      2.00000
      6     -10.8486      2.00000
      7     -10.7829      2.00000
      8     -10.7454      2.00000
      9      -3.8153      2.00000
     10      -3.7466      2.00000
     11      -0.0226      2.00000
     12       0.3733      2.00000
     13       1.6915      2.00000
     14       1.9180      2.00000
     15       2.0302      1.99998
     16       2.1485      1.99939
     17       3.3140      0.00000
     18       3.5828      0.00000
     19       4.6023      0.00000
     20       5.0997      0.00000
     21       5.1853      0.00000
</code></pre>
<p>The complete text file has about 729 k_points tables.
Some of them are presented here (k-point 1,k-point 2,k-point 3,k-point 410). Values after k-points like 0.6250,    0.0000,    0.5000 are the value of kx, ky &amp; kz, respectively, as the value on the x-axis, y-axis, z-axis. The column is given in the table named 'band energies' will be reflected as density/colors in the plot. There is no need for column 'occupation' in plotting.</p>
<p>I have been trying for a month but could not create a program that can plot a 3d projection. I tried to convert the given table into NumPy arrays but did not get a satisfactory result. Then I tried to convert this into a data frame using pandas but failed. I used everything I knew.</p>
",18485753.0,1910483.0,2022-03-18 16:25:44,2022-03-19 02:04:16,How can i plot 3d projection using python from a text file...?,<python><pandas><matplotlib><plot><data-science>,1,2,N/A,CC BY-SA 4.0
71530946,1,71532271.0,2022-03-18 17:25:21,0,204,"<p>I am trying to write CSV files into elasticsearch database, but first I want to pass it as json and I keep getting this error, and I don't know how to fix it...</p>
<p>Here is the code below</p>
<pre><code>from haystack.document_store.elasticsearch import ElasticsearchDocumentStore
document_store = ElasticsearchDocumentStore(host=&quot;localhost&quot;, username=&quot;&quot;, password=&quot;&quot;, index=&quot;document&quot;)

import pandas as pd
df = pd.read_csv('Data/FINAL_CORD_DATA_0.csv')

dicts = df.to_dict('records')

final_dicts = []
for each in dicts:
    tmp = {}
    tmp['text'] = each.pop('body_text')
    tmp['meta'] = each
    final_dicts.append(tmp)
</code></pre>
<p>Here is the error message I receive when I run the last cell</p>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-13-e5e7b4b7ff5a&gt; in &lt;module&gt;
      2 for each in dicts:
      3     tmp = {}
----&gt; 4     tmp['text'] = each.pop('body_text')
      5     tmp['meta'] = each
      6     final_dicts.append(tmp)

KeyError: 'body_text'
</code></pre>
",12401891.0,-1.0,N/A,2022-03-18 19:26:49,How can i fix this error when converting csv to json,<python><elasticsearch><machine-learning><nlp><data-science>,1,0,N/A,CC BY-SA 4.0
74298828,1,74298992.0,2022-11-03 06:26:40,0,34,"<p>I came across a solution for <a href=""https://stackoverflow.com/a/61482149/15162906"">how to map non factorial data into a correlation matrix</a> in which the solution uses an argument <code>~0+.</code>. For reference, here's the code :</p>
<pre><code>model.matrix(~0+., data=df) %&gt;% 
  cor(use=&quot;pairwise.complete.obs&quot;) %&gt;% 
  ggcorrplot(show.diag = F, type=&quot;lower&quot;, lab=TRUE, lab_size=2)
</code></pre>
<p>What does the object ~0+. do in this scenario?</p>
<p>I tried looking up the meaning of the operator ~ and tried to make sense of the formula but I cannot understand how it this formula helping us create a mapping of factors.</p>
",15162906.0,-1.0,N/A,2022-11-03 06:48:06,~0+. object in R programming,<r><graph-data-science>,1,0,N/A,CC BY-SA 4.0
74252978,1,-1.0,2022-10-30 12:18:06,0,85,"<p>I have a BiqQuery table (Table A) that has around 1,000 records containing an ID and 15 datapoints that range between 0 - 100. Imagine its like a top-trumps card but with 15 attributes. Here's an example:</p>
<pre><code>Record_ID = 0001
Size = 56
Height = 34
Width = 23
Weight = 78
Color = 42
Volume = 8
Density = 77
Smell = 23
Touch = 67
Hearing = 52
Power = 87
Sensitivity = 3
Strength = 78
Endurance = 45
Reliability = 87
</code></pre>
<p>And I have a separate table (Table B) that has exactly the same schema with around 5,000 different records</p>
<p>I need to take each Record_ID from Table A and then somehow rank the records in Table B that most closely match across all attributes. If I were just trying to rank records based on a single attribute such as Size then this would be really easy but I don't know where to start when I'm trying to find the closest matches and rankings across all attributes.</p>
<p>Is there any kind of model or approach that might help me achieve this? I have been reading up on clustering and K-means nearest neighbor but these don't seem to help.</p>
",13547768.0,-1.0,N/A,2022-10-30 19:26:38,How to rank the closest matches to a record of attributes in Google BigQuery,<google-bigquery><data-science>,2,0,N/A,CC BY-SA 4.0
71502342,1,-1.0,2022-03-16 18:11:20,1,93,"<p>Since the features for the MNIST digits dataset (the pixels) are all expressed in the same units and are comprised between [0:255], is it relevant to standardize them (apply StandardScaler from scikitlearn) or a simple normalization would be sufficient (like minmaxscaler)?</p>
",11304490.0,-1.0,N/A,2022-03-16 18:11:20,Is it relevant to standardize data before PCA on MNIST digits dataset?,<python><data-science>,0,0,N/A,CC BY-SA 4.0
74288691,1,-1.0,2022-11-02 11:49:21,1,68,"<p>I have a value in a column 'ACCOUNT_N0', it consists of 14 digits I want them to be 16 digits by inserting 2 zeros in the middle, one in the 4th position and the other in the 8th position
example:
This is the value: 33322288888888
The output: 3330222088888888</p>
<p>What i have found is inserting zeros at the beginning of the number, using:</p>
<pre><code>df['ACCOUNT_NO'].astype(str).str.zfill(16)
</code></pre>
<p>I want to know how to insert in the 4th position and the 8th position</p>
",20397196.0,-1.0,N/A,2022-11-02 13:58:04,How to loop on value in a data frame and insert zeros in the middle,<python><validation><data-science><data-analysis><data-cleaning>,2,2,N/A,CC BY-SA 4.0
71534239,1,71534303.0,2022-03-18 23:41:23,1,58,"<p>I was using Hub, a <a href=""https://github.com/activeloopai/hub"" rel=""nofollow noreferrer"">dataset format for AI</a> that allows data streaming to GPUs without sacrificing performance.</p>
<p>I have been using Hub for image datasets and would like to try to use the product for other data types.</p>
<p>How would Hub work for different data types such as 3D objects, audio, video, etc?</p>
<p>The following Activeloop Hub doc has an example of how to <a href=""https://docs.activeloop.ai/getting-started/creating-datasets-manually"" rel=""nofollow noreferrer"">upload image datasets to Hub</a> and I am using a similar approach for working with my image dataset.</p>
",18508132.0,-1.0,N/A,2022-03-18 23:55:44,"How do non-image datatypes such as 3D objects, audio, video, etc work with Activeloop Hub?",<python><tensorflow><pytorch><data-science><hub>,1,0,N/A,CC BY-SA 4.0
74303148,1,-1.0,2022-11-03 12:33:26,0,54,"<p>I want to clean up this date column inside of a csv file using python pandas.</p>
<p>Let's say my code is:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    'name': ['alice','bob','charlie'],
    'date_of_birth': ['10/25/2005 R','10/29/2002','01/01/2001 BD']
})
</code></pre>
<p><strong>How can I clean up this mess for thousands of rows?</strong></p>
<p>I thought of using:</p>
<pre><code>df['date of birth'] = df['new date'].str[0,10]
</code></pre>
<p>but it does not work.</p>
",20407589.0,8528014.0,2022-11-05 13:01:34,2022-11-05 13:09:31,How can I only get the first N numbers in a Date column in Pandas,<pandas><data-science><data-cleaning>,2,1,N/A,CC BY-SA 4.0
74296739,1,-1.0,2022-11-02 23:41:50,1,469,"<p>I have a data frame and one column consists of list value. I have attached the picture in excel format and data frame as well.</p>
<pre><code>column
&quot;[
&quot;&quot;Hello&quot;&quot;
]&quot;
&quot;[
&quot;&quot;Hello&quot;&quot;, 
 &quot;&quot;Hi&quot;&quot;
]&quot;
&quot;[
&quot;&quot;Hello&quot;&quot;, 
 &quot;&quot;Hi&quot;&quot;,
 &quot;&quot;&quot;&quot;
]&quot;
&quot;[
&quot;&quot;&quot;&quot;,
&quot;&quot;Hello&quot;&quot;, 
 &quot;&quot;Hi&quot;&quot;
]&quot;
&quot;[
&quot;&quot;Hello&quot;&quot;,
&quot;&quot;&quot;&quot;
]&quot;
&quot;[
&quot;&quot;&quot;&quot;,
&quot;&quot;Hello&quot;&quot;

]&quot;
</code></pre>
<p><a href=""https://i.stack.imgur.com/SPkrk.png"" rel=""nofollow noreferrer"">1</a>][1]<a href=""https://i.stack.imgur.com/PLWKJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PLWKJ.png"" alt=""enter image description here"" /></a>The column value looks like</p>
<pre><code>column
------
[\n &quot;Hello&quot; \n]
[\n &quot;Hello&quot;, \n &quot;Hi&quot;\n]
[\n &quot;Hello&quot;, \n &quot;Hi&quot;\n, \n &quot;&quot;\n]
[\n &quot;&quot;\n, \n &quot;Hello&quot;, \n &quot;Hi&quot;\n]
[\n &quot;Hello&quot; \n, \n &quot;&quot;\n]
[\n &quot;&quot;\n, \n &quot;Hello&quot; \n]
</code></pre>
<p>So, I want to remove <code>\n</code> and <code>&quot;&quot;</code> from the list and have value as</p>
<pre><code>column
------
[&quot;Hello&quot;]
[&quot;Hello&quot;, &quot;Hi&quot;]
[&quot;Hello&quot;, &quot;Hi&quot;]
[&quot;Hello&quot;, &quot;Hi&quot;]
[&quot;Hello&quot;]
[&quot;Hello&quot;]
</code></pre>
<p>So, how can we obtain following result using pandas and python?</p>
",16875907.0,16875907.0,2022-11-03 14:53:42,2023-06-04 22:28:20,How to remove \n and empty string in a column in a dataframe?,<python><pandas><list><dataframe><data-science-experience>,2,0,N/A,CC BY-SA 4.0
71532962,1,-1.0,2022-03-18 20:41:41,0,35,"<p>I am new to R so apologies if this is obvious. I am trying to manipulate daily data into monthly totals and I have managed to do so with the below code. However, I am accidentally applying some of my code to the new column name.</p>
<p>This is my code:</p>
<pre><code>monthly_df &lt;- daily_df %&gt;%                      
  group_by(lubridate::floor_date(daily_df$daily_col_name, 'new_col_name')) %&gt;%
  dplyr::summarise(daily_value = sum(daily_value))
</code></pre>
<p>I am trying to name this new column 'new_col_name', however the column name is this entire line of code:</p>
<p>'lubridate::floor_date(daily_df$daily_col_name, 'new_col_name')'</p>
<p>Any help as to what I am doing wrong here would be really appreciated.</p>
<p>Thanks all</p>
",18507116.0,6574038.0,2022-03-18 21:09:42,2022-03-18 21:09:42,Why is my R code geting applied to a column name?,<r><data-science><data-analysis>,1,3,N/A,CC BY-SA 4.0
74304188,1,-1.0,2022-11-03 13:48:46,0,187,"<p>I know we can solve this error using <code>on_bad_lines = 'skip'</code>. This solves the error when we are having more no. of values than columns then those lines are skipped while reading into dataframe but it doesn't skip the rows having less no. of values than the columns</p>
<p>e.g. Consider a dataset like below</p>
<pre><code>c1, c2, c3
1, 2, 3
4, 34, 56
34, 45, 23, 32
2, 3
21, 32, 324,34
1
12, 23, 233
</code></pre>
<p>when I used <code>df = pd.read_csv(dataset, sep= ',', encoding= 'unicode_escape', on_bad_lines = 'skip')</code></p>
<p>It skipped the lines having more than 3 values but threw an error at the line which has less no. of values.</p>
<p>How can we skip the lines those are having less number of values than the columns?</p>
<p>So that the df should look like this</p>
<pre><code>c1, c2, c3
1, 2, 3
4, 34, 56
12, 23, 233
</code></pre>
",17033722.0,-1.0,N/A,2022-11-03 13:48:46,How can we skip bad lines(expected more fields saw less) from a dataset using pandas,<python><pandas><numpy><dataset><data-science>,0,9,N/A,CC BY-SA 4.0
71538418,1,71539228.0,2022-03-19 13:10:00,0,2915,"<p>I have a DataFrame <code>df2</code> with columns <code>currencies</code> and <code>c_codes</code>.</p>
<p>Each row in the <code>currencies</code> column is a list of one or more dictionaries. I want to extract the value for the key <code>code</code> in each dictionary within each list in each row of <code>currencies</code> and transfer the <code>code</code> values to a different column of the DataFrame <code>c_codes</code></p>
<p>Case in point:</p>
<p><img src=""https://i.stack.imgur.com/C6E2B.png"" alt=""Image 1"" /></p>
<p>From <code>df2['currencies'][0]</code> I want to extract the <code>code</code> value <code>AFN</code> and transfer it to <code>df2['c_codes'][0]</code></p>
<p>Similarly, if there are multiple <code>code</code> values for a row, such as <code>df2['currencies'][8] </code>then I want to extract a list of <code>code</code> values <code>['AUD','GBP']</code> and transfer the list to <code>df2['c_codes][8]</code></p>
<p>Each entry in <code>c_codes</code> can be a list for this purpose.</p>
<p>Here's my code:</p>
<p><img src=""https://i.stack.imgur.com/UdnPH.png"" alt=""Image 2"" /></p>
<p>I have tried using a loop to grab the <code>code</code> values in each dict. in each row and append them to a list <code>temp</code>. Then append the list <code>temp</code> to a bigger list <code>list_of_currencies</code> so I get a list with lists of codes corresponding to each row. Then I clear the <code>temp</code> list so it can grab the next row of <code>code</code> and so on.</p>
<p>However, the code returns the <code>list_of_currencies</code> as empty. I have tried playing around with the looping, <code>temp</code>, lists, etc. but it just returns empty list or else a list of all appended <code>codes</code> without sub-lists.</p>
<p>I want a list returned with sub-lists of <code>codes</code> so I can assign each sub-list to a corresponding row in <code>c_codes</code> column.</p>
<p>What am I doing wrong? Is there a simpler way to do this?</p>
",18512582.0,-1.0,2022-03-19 14:52:34,2022-03-19 15:01:36,How do I extract multiple values from each row of a DataFrame in Python?,<python><pandas><dataframe><data-science>,2,2,N/A,CC BY-SA 4.0
71539821,1,71539830.0,2022-03-19 16:11:17,1,64,"<p>I was using <a href=""https://github.com/activeloopai/hub"" rel=""nofollow noreferrer"">Hub the Dataset format for AI</a> and I ran <code>function().eval(ds.tensor[:].numpy(), ds</code> which gave me a zero division error.</p>
<p>However when I ran <code>function().eval(ds.tensor[:].numpy(), ds, num_workers=2)</code> I did not get the error.</p>
<p>I was using Hub version: 2.2.4.</p>
",18513827.0,18513827.0,2022-03-22 01:50:43,2022-03-22 01:50:43,ZeroDivisionError in the eval function for hub.compute when kept a default value of 1 for the num_workers parameter,<python><machine-learning><dataset><data-science><hub>,1,0,N/A,CC BY-SA 4.0
71539946,1,71539981.0,2022-03-19 16:25:55,2,182,"<p>I was taking a look at Hub—<a href=""https://github.com/activeloopai/hub"" rel=""nofollow noreferrer"">the dataset format for AI</a>—and noticed that hub integrates with GCP and AWS. I was wondering if it also supported integrations with MinIO.</p>
<p>I know that Hub allows you to directly stream datasets from cloud storage to ML workflows but I’m not sure which ML workflows it integrates with.</p>
<p>I would like to use MinIO over S3 since my team has a self-hosted MinIO instance (aka it's free).</p>
",18513914.0,-1.0,N/A,2022-03-19 16:28:56,"Does Hub support integrations for MinIO, AWS, and GCP? If so, how does it work?",<amazon-s3><dataset><data-science><minio><hub>,1,0,N/A,CC BY-SA 4.0
74311522,1,-1.0,2022-11-04 02:24:56,0,472,"<p>I want to extract data present inside a rectangle box in a PDF file to a CSV file with corresponding columns and rows.
I tried using Camelot, PyPdf2, Tabula libraries etc, but I couldn't get the desired outcome in a CSV file. Could anyone help me here ?
I want this data to be published into a CSV file with respective rows and columns.</p>
<p>Below is the data present inside a rectangle box inside a PDF file and link to input PDF file is attached as well:
<a href=""https://i.stack.imgur.com/n2F5h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n2F5h.png"" alt=""enter image description here"" /></a></p>
<p>[enter link description here][2]</p>
<p>[2]: [enter link description here][2]</p>
<p>Below is the code, which I have tried :</p>
<pre><code>import PyPDF2
pdf_file_obj = open('Rectangle_Box_PDF_2021_v2.pdf', 'rb')
pdf_read = PyPDF2.PdfFileReader(pdf_file_obj)
print(&quot;The total number of pages : &quot; +str(pdf_read.numPages))
page_obj = pdf_read.getPage(0)
cont = []
pdf_list = [page_obj.extractText()]
print(pdf_list)
list1 = []
pdf_list = [page_obj.extractText()]
for i in range(0, len(pdf_list)):
    list1.append(pdf_list[i].split('\n'))
flatList = sum(list1, [])
print(flatList)
</code></pre>
<p>[2]: The pdf file link : <a href=""https://drive.google.com/file/d/1m1mwO6V9UMuXTddXdkAf0Bx88l9zudcB/view?usp=share_link"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1m1mwO6V9UMuXTddXdkAf0Bx88l9zudcB/view?usp=share_link</a></p>
",13540185.0,13540185.0,2022-11-04 04:52:34,2022-11-08 04:17:46,Python - Extract data inside a Rectangle Box from a PDF file to CSV file,<python><data-science><pypdf><python-camelot><tabula-py>,1,11,N/A,CC BY-SA 4.0
74312475,1,-1.0,2022-11-04 05:07:50,1,177,"<p>I have a model whose training accuracy is 95-100 % and I believe there is overfitting. So, I want to avoid overfitting in my model. One way to avoid overfitting is to do k-fold cross-validation. So, while performing cross-validation there are several results for each iteration. So, how to choose the best result from different results and predict unseen data?</p>
<pre><code>
from sklearn.model_selection import train_test_split
train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)


from sklearn.ensemble import RandomForestClassification

rf = RandomForestClassification(random_state = 42)
rf.fit(train_features, train_labels)

predictions = rf.predict(test_features)
</code></pre>
<p>Cross-validation sample from sklearn is</p>
<pre><code>from sklearn.model_selection import cross_val_score
clf = RandomForestClassification(random_state = 42)
scores = cross_val_score(clf, X, y, cv=5)
</code></pre>
",16875907.0,16875907.0,2022-11-05 20:50:51,2022-11-05 20:50:51,How to do k-fold cross validation and use the model to predict unseen data?,<python><machine-learning><artificial-intelligence><data-science-experience><overfitting-underfitting>,0,4,N/A,CC BY-SA 4.0
74309426,1,-1.0,2022-11-03 20:52:30,0,208,"<p>Code:</p>
<pre><code>import graphing

graphing.scatter_2D(dataset,    label_x=&quot;harness_size&quot;, 
                                label_y=&quot;boot_size&quot;,
                                trendline=lambda x: fitted_model.params[1] * x + fitted_model.params[0]
                              )
</code></pre>
<p>Why do I have this error:</p>
<pre><code>module 'graphing' has no attribute 'scatter_2D'
</code></pre>
<p>It's from an Azure course.</p>
<p>Try to install the library, it's okay for this.</p>
",20411808.0,472495.0,2022-11-03 22:21:03,2023-02-14 05:31:42,"Error with the graphing library: ""module 'graphing' has no attribute 'scatter_2D'""",<python><data-science><graphing>,1,1,N/A,CC BY-SA 4.0
74310659,1,74310720.0,2022-11-03 23:38:27,1,49,"<p>How do I replace multiple column names with different values? Ideally I want to remove certain characters in column names and replace others.</p>
<p>I have to run my jupyter notebook twice in order to get this code to work. Does anyone know the reason for this? Also, how would I go about simplifying this code (I am aware of just nesting .replace(), however that doesn't solve my problem). The snippet posted below may not be enough to go off of; please view the following link to my notebook if needed: <a href=""https://datalore.jetbrains.com/notebook/iBhSV0RbfC66p84tZsnC24/w3Z6tCriPC5v5XwqCDQpWf/"" rel=""nofollow noreferrer"">https://datalore.jetbrains.com/notebook/iBhSV0RbfC66p84tZsnC24/w3Z6tCriPC5v5XwqCDQpWf/</a></p>
<pre><code>for col in df.columns:
    df.rename(columns={col: col.replace('Deaths - ', '').replace(' - Sex: Both - Age: All Ages (Number)', '')}, inplace=True)
    df.rename(columns={col: col.replace(' (deaths)', '')}, inplace=True)
    df.rename(columns={col: col.replace(' ', '_')}, inplace=True)
    
for col in df.columns:
    df.rename(columns={col: col.lower()}, inplace=True)
</code></pre>
",20412560.0,-1.0,N/A,2022-11-04 02:43:19,Replace method in Pandas,<python><pandas><replace><data-science>,3,0,N/A,CC BY-SA 4.0
74311707,1,-1.0,2022-11-04 02:59:51,0,20,"<p>so I'm working on an optimization problem and wanted to create a function that, given a client X will give me the cities under a certain distance radius. <a href=""https://i.stack.imgur.com/us5lq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/us5lq.png"" alt=""enter image description here"" /></a></p>
<p>Say you pick client A. I want to get a list of all the clients that are less than 2 miles away. If you see the image you can see that I should get a list that looks like this:
[A,B,C,D,E,F,I,M,O]
which are the cities that meet the criteria.
Thanks!</p>
<p>I don't know how to filter columns by row condition, and get a list of the column headers that meet the criteria</p>
",20413447.0,-1.0,N/A,2022-11-04 03:33:14,Filtering a distance matrix,<python><pandas><optimization><data-science>,1,0,N/A,CC BY-SA 4.0
71545326,1,-1.0,2022-03-20 08:50:21,-1,32,"<p>I have read a few similar questions on counting consecutive rows, but none of them gave me a clear answer. I hope someone could give me some help with my problem. I have the following table data:
<code>
ID       TEST_VALUES
1       A
2       B
3       C
4       C
5       C
6       C
7       A
8       D
9       D
10      D
11      B
12      C
13      C
14      C
15      C
</code></p>
<p>now I want to find if there  three consecutive rows with the same value how i can do it ?</p>
",18351525.0,18351525.0,2022-03-20 15:05:24,2022-03-20 15:44:01,how i can check if there is a repetition of the same results in a table?,<python><data-science><grouping>,1,1,N/A,CC BY-SA 4.0
74320528,1,74320565.0,2022-11-04 16:58:22,1,99,"<p>I have a date-time stamp column and i was able to convert the column to R date-time format, in a bid for me to split the column, i created 2 new columns with the date information in one and time information in the order. Now I have found a way to split the original Date...Timestamp column so i want to drop the 2 new columns but I'm getting this error.</p>
<p>I tried this code:</p>
<pre><code>newpHdata = subset(newpHdata, select = -c(newpHdata$Dates))
newpHdata = subset(newpHdata, select = -c(newpHdata$Time))
</code></pre>
<p>And I'm getting this error</p>
<pre><code>&gt; newpHdata = subset(newpHdata, select = -c(newpHdata$Dates))
Error in `-.Date`(c(newpHdata$Dates)) : 
  unary - is not defined for &quot;Date&quot; objects
&gt; newpHdata = subset(newpHdata, select = -c(newpHdata$Time))
Error in -c(newpHdata$Time) : invalid argument to unary operator
</code></pre>
",6686671.0,680068.0,2022-11-07 09:10:50,2022-11-07 09:10:50,How to drop a Date column in R,<r><dataframe><datetime><data-science>,1,0,2022-11-07 09:12:12,CC BY-SA 4.0
71531098,1,-1.0,2022-03-18 17:38:46,0,375,"<p>I have experience with Neo4j and Cypher, but still struggle with aggregate functions. I'm trying to pull a CSV out of Neo4j that should look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Location</th>
<th>Number of Node X at Location</th>
<th>Number of Node Y at Location</th>
</tr>
</thead>
<tbody>
<tr>
<td>ABC</td>
<td>5</td>
<td>20</td>
</tr>
<tr>
<td>DEF</td>
<td>39</td>
<td>4</td>
</tr>
<tr>
<td>Etc.</td>
<td>#</td>
<td>#</td>
</tr>
</tbody>
</table>
</div>
<p>My current query looks like this:</p>
<pre><code>MATCH (loc:Location)--(x:Node_X)
RETURN loc.key AS Location, count(x) AS `Number of Node X at Location`, 0 AS `Number of Node Y at Location`
UNION
MATCH (loc:Location)--(y:Node_Y)
RETURN loc.key AS Location, 0 AS `Number of Node X at Location`, count(y) AS `Number of Node Y at Location`
</code></pre>
<p>Which yields a table like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Location</th>
<th>Number of Node X at Location</th>
<th>Number of Node Y at Location</th>
</tr>
</thead>
<tbody>
<tr>
<td>ABC</td>
<td>5</td>
<td>0</td>
</tr>
<tr>
<td>DEF</td>
<td>39</td>
<td>0</td>
</tr>
<tr>
<td>Etc.</td>
<td>#</td>
<td>#</td>
</tr>
<tr>
<td>ABC</td>
<td>0</td>
<td>20</td>
</tr>
<tr>
<td>DEF</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>Etc.</td>
<td>#</td>
<td>#</td>
</tr>
</tbody>
</table>
</div>
<p>I think I'm close, but I have double the number of Location rows as I need, and am not sure how to make the results more succinct. Suggestions on this and generally tips for aggregate functions are appreciated!</p>
",14677129.0,-1.0,N/A,2022-03-18 20:45:55,Neo4j Cypher Query - return counts of relationships in separate columns,<neo4j><cypher><graph-databases><graph-data-science>,4,0,N/A,CC BY-SA 4.0
71535385,1,-1.0,2022-03-19 04:32:47,0,74,"<p>I am currently working on a deep learning project. My input is of the shape (20,938). I'm trying to normalize the data and then scale them between 0 and 1. Now my dataset contains around 53k examples.</p>
<p>It is computationally expensive for me to read every data point and calculate the overall min, max, mean and standard deviation.</p>
<p>Does anyone know a better approach ?</p>
",12651980.0,-1.0,N/A,2022-03-19 04:32:47,Normalizing my data without knowing population estimates,<machine-learning><deep-learning><statistics><data-science><data-analysis>,0,3,N/A,CC BY-SA 4.0
74312830,1,-1.0,2022-11-04 06:07:57,0,40,"<p>I have installed tools but could not find a way to install tools.common.misc</p>
<p>I tried pip installing the modules but it did not work</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
Input In [4], in &lt;cell line: 2&gt;()
      1 import numpy as np
----&gt; 2 from tools.common_misc import gen_obs, rmse_spread, createH, getBsimple
      3 from tools.common_plots import plotRMSP
      5 from tools.L63_model import lorenz63

ModuleNotFoundError: No module named 'tools.common_misc'
</code></pre>
",20414480.0,-1.0,N/A,2022-11-05 11:11:03,How do I install the tools packages in python?,<python><jupyter-notebook><data-science>,0,4,N/A,CC BY-SA 4.0
71544762,1,71545048.0,2022-03-20 07:08:29,0,706,"<p>In the data set of a market, each data belongs to a product group. I want to group this data group by fische number and find the total number of male and female customers.
The data set is as in the picture. <a href=""https://i.stack.imgur.com/YKZgm.png"" rel=""nofollow noreferrer"">dataset</a> The number of unique fischeno is 141783. Therefore, the total number of customers should be 141783.</p>
<p><a href=""https://i.stack.imgur.com/8mFZB.png"" rel=""nofollow noreferrer"">For example</a></p>
",18516004.0,18516004.0,2022-03-20 07:24:20,2022-03-20 09:31:24,How to determine gender count in a data grouped with groupby in python programming language?,<python><pandas-groupby><data-science><filtering>,1,2,N/A,CC BY-SA 4.0
70769606,1,-1.0,2022-01-19 11:15:33,-1,466,"<p>I will get condition from json file where the condition is '&lt;=' and need to compare two columns containing dates.</p>
<p>So I tried as below:</p>
<pre><code>                left = y['predicate']['left']
                right = y['predicate']['right']
                grp1 = y['groupby']['groupByFields']
                grp2 = y['groupby']['aggregateField']
                ope = y['groupby']['aggregateOperation']
                res = y['groupby']['result']
                logic = y['predicate']['logic']
                result[left] = pd.to_datetime(result[left])
                result[right] = pd.to_datetime(result[right])
                res = result.loc[result[left] + eval(logic) + result[right]]
</code></pre>
<p>By using the following code it is working perfectly</p>
<pre><code>res = result.loc[result[left] &lt;= result[right]]
</code></pre>
<p>and i tried these also</p>
<pre><code>res = result.loc[eval(str(result[left]) +&quot; &quot; + logic +&quot; &quot; + str(result[right]))]

                  
</code></pre>
<p>I am getting error.</p>
",3655069.0,472495.0,2022-01-22 09:50:08,2022-01-22 09:50:08,Dynamic expression in Python,<python><pandas><data-science>,3,1,2022-01-22 10:36:08,CC BY-SA 4.0
74320529,1,74321608.0,2022-11-04 16:58:25,1,22,"<p>I have a dataframe with 6 columns:</p>
<pre><code>Id  count1  Value1  count2  Value2  Value3  
1    1200      3     2000       4       6     
2    100       2     400        5       8     
3    800       4     1100       7       9  
</code></pre>
<p>I need to get a new column with following conditions:
if count 1 &gt; 1000, new column should take values from Value1 column.
elif count 2 &gt; 1000, new column should take values from Value2 column.
else, new column should be filled with values from Value 3.</p>
<p>Output should look like:</p>
<pre><code>Id  count1  Value1  count2  Value2  Value3  New Column
1    1200      3     2000       4       6     3
2    100       2     400        5       8     8
3    800       4     1100       7       9     7
</code></pre>
",15565186.0,15565186.0,2022-11-04 18:37:39,2022-11-04 18:39:06,How to get another column in a dataframe filled with values from another columns based on multiple conditions?,<python><pandas><dataframe><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
74324054,1,74326257.0,2022-11-04 23:54:39,0,152,"<p>I'm working on a simple project in which I'm trying to describe the relationship between two positively correlated variables and determine if that relationship is changing over time, and if so, to what degree. I feel like this is something people probably do pretty often, but maybe I'm just not using the correct terminology because google isn't helping me very much.</p>
<p>I've plotted the variables on a scatter plot and know how to determine the correlation coefficient and plot a linear regression. I thought this may be a good first step because the linear regression tells me what I can expect y to be for a given x value. This means I can quantify how &quot;far away&quot; each data point is from the regression line (I think this is called the squared error?). Now I'd like to see what the error looks like for each data point over time. For example, if I have 100 data points and the most recent 20 are much farther away from where the regression line/function says it should be, maybe I could say that the relationship between the variables is showing signs of changing? Does that make any sense at all or am I way off base?</p>
<p>I have a suspicion that there is a much simpler way to do this and/or that I'm going about it in the wrong way. I'd appreciate any guidance you can offer!</p>
",18283252.0,-1.0,N/A,2022-11-05 08:31:25,How do I analyze the change in the relationship between two variables?,<statistics><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
71550416,1,71550458.0,2022-03-20 20:30:50,-1,123,"<p>I am working with the places365(resized) dataset. It is a classification dataset with around 2.7 million images and it is 131GB.</p>
<p>I am trying to upload this dataset to Hub—<a href=""https://github.com/activeloopai/hub"" rel=""nofollow noreferrer"">the dataset format for AI</a>—and the dataset was uploading at around 5MB/s. After doing so I was able to load the dataset and around 2.4 million images were there.</p>
<p>Is it possible to make the uploading process faster?</p>
<p>I used the following code to try and upload the dataset:</p>
<pre><code>import hub
import numpy as np
from PIL import Image
import argparse
import tqdm
import time

import traceback
import sys

import logging

import torchvision.datasets as datasets

NUM_WORKERS = 1
DS_OUT_PATH = &quot;./data/places365&quot;  # optionally s3://, gcs:// or hub:// path
DOWNLOAD = False
splits = [
    &quot;train-standard&quot;,
    # &quot;val&quot;,
    # &quot;train-challenge&quot;
]

parser = argparse.ArgumentParser(description=&quot;Hub Places365 Uploading&quot;)
parser.add_argument(&quot;data&quot;, metavar=&quot;DIR&quot;, help=&quot;path to dataset&quot;)
parser.add_argument(
    &quot;--num_workers&quot;,
    type=int,
    default=NUM_WORKERS,
    metavar=&quot;O&quot;,
    help=&quot;number of workers to allocate&quot;,
)
parser.add_argument(
    &quot;--ds_out&quot;,
    type=str,
    default=DS_OUT_PATH,
    metavar=&quot;O&quot;,
    help=&quot;dataset path to be transformed into&quot;,
)

parser.add_argument(
    &quot;--download&quot;,
    type=bool,
    default=DOWNLOAD,
    metavar=&quot;O&quot;,
    help=&quot;Download from the source http://places2.csail.mit.edu/download.html&quot;,
)

args = parser.parse_args()


def define_dataset(path: str, class_names: list = []):
    ds = hub.empty(path, overwrite=True)

    ds.create_tensor(&quot;images&quot;, htype=&quot;image&quot;, sample_compression=&quot;jpg&quot;)
    ds.create_tensor(&quot;labels&quot;, htype=&quot;class_label&quot;, class_names=class_names)

    return ds


@hub.compute
def upload_parallel(pair_in, sample_out):
    filepath, target = pair_in[0], pair_in[1]
    try:
        img = Image.open(filepath)
        if len(img.size) == 2:
            img = img.convert(&quot;RGB&quot;)
        arr = np.asarray(img)
        sample_out.images.append(arr)
        sample_out.labels.append(target)
    except Exception as e:
        logging.error(f&quot;failed uploading {filepath} with target {target}&quot;)


def upload_iteration(filenames_target: list, ds: hub.Dataset):
    with ds:
        for filepath, target in tqdm.tqdm(filenames_target):
            try:
                img = Image.open(filepath)
                if len(img.size) == 2:
                    img = img.convert(&quot;RGB&quot;)
                arr = np.asarray(img)
                ds.images.append(arr)
                ds.labels.append(target)
            except Exception as e:
                logging.error(f&quot;failed uploading {filepath} with target {target}&quot;)


if __name__ == &quot;__main__&quot;:

    for split in splits:
        torch_dataset = datasets.Places365(
            args.data,
            split=split,
            download=args.download,
        )
        categories = torch_dataset.load_categories()[0]
        categories = list(map(lambda x: &quot;/&quot;.join(x.split(&quot;/&quot;)[2:]), categories))
        ds = define_dataset(f&quot;{args.ds_out}-{split}&quot;, categories)
        filenames_target = torch_dataset.load_file_list()

        print(f&quot;uploading {split}...&quot;)
        t1 = time.time()
        if args.num_workers &gt; 1:

            upload_parallel().eval(
                filenames_target[0],
                ds,
                num_workers=args.num_workers,
                scheduler=&quot;processed&quot;,
            )
        else:
            upload_iteration(filenames_target[0], ds)
        t2 = time.time()
        print(f&quot;uploading {split} took {t2-t1}s&quot;)
</code></pre>
<p>I'm using Hub v2.2.2</p>
",18514010.0,18514010.0,2022-03-24 00:54:38,2022-03-24 00:56:52,My uploading to Activeloop Hub is slow. How to make Hub dataset uploading faster?,<machine-learning><upload><dataset><data-science><hub>,1,1,2022-03-21 00:29:29,CC BY-SA 4.0
71553067,1,-1.0,2022-03-21 05:11:06,0,51,"<p><a href=""https://i.stack.imgur.com/Yw3ka.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I am a new programmer in the Data field and need help to write in Python</p>
",18525617.0,-1.0,N/A,2022-03-21 05:11:06,"How to write a double summation in Python with multiple variables , I am a new programmer in the data science field",<python-3.x><jupyter-notebook><formula><equation><data-science-experience>,0,4,N/A,CC BY-SA 4.0
71569134,1,-1.0,2022-03-22 08:44:21,0,86,"<p>So, I am playing around in with openpyxl because I want to automate a Excel huge file ( filter, delete rows, merge and stuff like that ) in another better Excel and cleaner so I can import it in PowerBi.
But anytime I use for ex. ws.insert_cols or rows or ws.merge_cells, goes after I hit save and run into this error &quot;AttributeError: 'NoneType' object has no attribute&quot; after run.
I use Anaconda3 and I have installed the lasted version of openpyxl.
Thanks a lot in advanced</p>
<p><a href=""https://i.stack.imgur.com/yFoxY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yFoxY.png"" alt="""" /></a>   I have tried updating the the openpyxl but it does not fix it.</p>
",18539989.0,-1.0,N/A,2022-03-22 08:44:21,"Why what ever function I use in Anaconda 3 IDE from openpyxl goes into ""AttributeError: 'NoneType' object has no attribute"" after run?",<python><data-science><openpyxl><powerbi-desktop>,0,6,N/A,CC BY-SA 4.0
74323692,1,-1.0,2022-11-04 22:47:42,0,74,"<p>I have data detailing specific projects going on and when they achieved certain milestones. I'm trying to predict the end date of the project based on these milestone dates. An example of the dates would be:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Milestone 1</th>
<th>Milestone 2</th>
<th>Milestone 3</th>
<th>End Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>08-10-2022</td>
<td>08-24-2022</td>
<td>09-25-2022</td>
<td>10-08-2022</td>
</tr>
<tr>
<td>2</td>
<td>09-15-2022</td>
<td>10-10-2022</td>
<td>10-25-2022</td>
<td>11-04-2022</td>
</tr>
<tr>
<td>3</td>
<td>08-17-2022</td>
<td>9-10-2022</td>
<td>10-01-2022</td>
<td>10-26-2022</td>
</tr>
</tbody>
</table>
</div>
<p>I have other variables available as well however such as the location of the project but these have shown to be far less powerful.</p>
<p>The difficulty comes in when putting this model into practice. When predicting the end date on projects in progress then they may not have reached all the milestones. In order to simulate this, I have found the average durations between the milestones, found the proportion of missing milestones in my live data, and then artificially removed that amount from my modeling data, and inserted the sum of the previous non-missing milestone dates plus the average durations to create imputed milestone dates for those that are missing. I then converted all of these dates to ordinal and created several models.</p>
<p>The results were pretty good, and I achieved a mean absolute error averaged across 3 folds of cross-validation of about 12 days. However, when I went to put this into practice on the live (not the modeling) data I noticed that the vast majority of my predicted End Dates were all falling behind my milestone 3 and all were behind todays date despite these projects being in progress.</p>
<p>This made me realize the naivety of my process... By using modeling data where the largest end date can only be up to todays date then my predictions on live data will not likely exceed this which is the entire point of the model.</p>
<p>So, now that I've realized my error, what would you all suggest I do instead? I've considered using the duration between the milestones but that would likely just end up being the last milestone date that occurred, plus the average duration to the end date. This doesn't produce phenomenal results in my testing, is that all I can hope to achieve? Any alternative approaches would be greatly appreciated.</p>
",17824363.0,-1.0,N/A,2022-11-04 22:47:42,Forecasting a date based on a sequence of prior dates,<date><machine-learning><statistics><data-science><forecasting>,0,3,N/A,CC BY-SA 4.0
71589917,1,71590010.0,2022-03-23 15:31:10,0,270,"<p>This is my code.</p>
<p>Here My dataFrame is df and I am trying to convert this data frame into excel sheet.
I am using ExcelWriter to convert dataframe to Excel sheet</p>
<pre><code>#python
    
dataxlsx=pd.ExcelWriter(&quot;FromPython.xlsx&quot;,engine='xlsxwriter')
#dataxlsx.book.use_zip64()
df.to_excel(dataxlsx,sheet_name=&quot;df_sheet1&quot;)
dataxlsx.save()
</code></pre>
<p>so far I got the following</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-40-3d2ff9cdbba5&gt; in &lt;module&gt;
      2 dataxlsx=pd.ExcelWriter(&quot;FromPython.xlsx&quot;,engine='xlsxwriter')
      3 #dataxlsx.book.use_zip64()
----&gt; 4 df.to_excel(dataxlsx,sheet_name=&quot;df_sheet1&quot;)
      5 dataxlsx.save()

~\Anaconda3\lib\site-packages\pandas\core\generic.py in to_excel(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)
   2187             inf_rep=inf_rep,
   2188         )
-&gt; 2189         formatter.write(
   2190             excel_writer,
   2191             sheet_name=sheet_name,

~\Anaconda3\lib\site-packages\pandas\io\formats\excel.py in write(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)
    801         num_rows, num_cols = self.df.shape
    802         if num_rows &gt; self.max_rows or num_cols &gt; self.max_cols:
--&gt; 803             raise ValueError(
    804                 f&quot;This sheet is too large! Your sheet size is: {num_rows}, {num_cols} &quot;
    805                 f&quot;Max sheet size is: {self.max_rows}, {self.max_cols}&quot;

ValueError: This sheet is too large! Your sheet size is: 5704247, 5 Max sheet size is: 1048576, 16384
</code></pre>
<p>give me proper solution please</p>
",18491069.0,355230.0,2022-03-23 16:29:12,2022-03-23 16:29:12,I want convert my dataframes into excel sheets but I got dataframe is too large error!1,<python><dataframe><data-science>,1,5,N/A,CC BY-SA 4.0
74314745,1,-1.0,2022-11-04 09:26:16,1,337,"<p>I have a table that has columns Name, Series and Season.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>series</th>
<th>season</th>
</tr>
</thead>
<tbody>
<tr>
<td>abc</td>
<td>alpha</td>
<td>s1</td>
</tr>
<tr>
<td>abc</td>
<td>alpha</td>
<td>s2</td>
</tr>
<tr>
<td>pqr</td>
<td>alpha</td>
<td>s1</td>
</tr>
<tr>
<td>xyz</td>
<td>beta</td>
<td>s2</td>
</tr>
<tr>
<td>xyz</td>
<td>gamma</td>
<td>s3</td>
</tr>
<tr>
<td>abc</td>
<td>theta</td>
<td>s1</td>
</tr>
</tbody>
</table>
</div>
<p>I am trying to extract the number of people who have watched only the series 'alpha', and not any other series.
How to get this count?</p>
<p>On giving the &quot;where series='alpha' &quot; condition, I get the counts of people who watched alpha, but not the counts of those who watched <em>only alpha</em> eg: abc has watched alpha as well as theta, but pqr has watched only alpha.</p>
",20415890.0,-1.0,N/A,2022-11-04 10:44:57,"How to give "" Only in "" condition in SQL?",<mysql><sql><data-science><data-analysis>,5,2,N/A,CC BY-SA 4.0
71549183,1,-1.0,2022-03-20 17:46:42,0,75,"<p>I have 2 datasets and each has 2 columns namely code, description. I want to read both the files search for similar text and map the codes of dataset 1 and dataset 2 together. For example.</p>
<p>file1.csv</p>
<pre><code>code, description
111, Milk producer
112, IT specialist
</code></pre>
<p>file2.csv</p>
<pre><code>code, description
001, Milkman
002, Driver
</code></pre>
<p>Now the combined dataset would be,
file_combined.csv</p>
<pre><code>code1, description1, code2, description2
111,milk producer,001,milk man
112,IT specialist,002,drvier
</code></pre>
",7640263.0,2015686.0,2022-03-21 03:52:32,2022-03-21 11:05:56,Combine Two Data Set CSV Files in Python,<python><nlp><data-science>,1,5,N/A,CC BY-SA 4.0
71565780,1,71565797.0,2022-03-22 00:56:06,1,140,"<p>Suppose I have a df with 2 columns A and B and a class Foo() which can be instantiated by A's value. I'd like to get a unique series of B which satisfy Foo(A).item.price == 0.
I tried the following code which obviously doesn't work.</p>
<pre><code>unique_B = df.loc[Foo(df.A.str).item.price == 0, &quot;B&quot;].unique()
</code></pre>
<p>I guess I could create a third column to store the value of foo(df.A), is there a simpler solution?</p>
",3281900.0,3281900.0,2022-03-22 01:01:40,2022-03-22 01:11:05,Pandas: Filter a column by the value from applying a function to another column?,<python><pandas><dataframe><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
70775344,1,-1.0,2022-01-19 17:42:37,-1,60,"<blockquote>
<pre><code>#counters
</code></pre>
</blockquote>
<pre><code>counter_setosa = 0
counter_versicolor = 0
counter_virginica = 0
</code></pre>
<blockquote>
<pre><code>#checking the name of the flowers and adding them to their counters
</code></pre>
</blockquote>
<pre><code>for item in df['species']:
  
</code></pre>
<blockquote>
<p>#for setosas(in loop)</p>
</blockquote>
<pre><code>  if item == 'setosa': 
    counter_setosa +=1
  
</code></pre>
<blockquote>
<p>#for versicolor(in loop)</p>
</blockquote>
<pre><code>  if item == 'versicolor':
    counter_versicolor +=1
  
</code></pre>
<blockquote>
<p>#for virginica(in loop)</p>
</blockquote>
<pre><code>  if item == 'virginica':
    counter_virginica +=1
</code></pre>
<blockquote>
<p>#priniting the counters and their names</p>
</blockquote>
<pre><code>print(&quot;setosa: &quot;, counter_setosa)
print(&quot;versicolor: &quot;, counter_versicolor)
print(&quot;virginica: &quot;, counter_virginica)
</code></pre>
",17976692.0,-1.0,N/A,2022-01-19 20:28:44,how can i print a histogram based on the counters of each flower species?(x axis- name of the flower y axis-the amount),<python><matplotlib><data-science><counter><pie-chart>,1,3,N/A,CC BY-SA 4.0
72820094,1,-1.0,2022-06-30 17:52:58,0,829,"<p>I need help writing a function that compares two dataframes and returns the cells that are different.</p>
<p>The column names will be the same across the two dataframes, but the length might not be.</p>
<p>I have example code:</p>
<pre><code>import pandas as pd

data = [('11','12','13'),
        ('21','22','23'),
        ('31','32','33')]

df = pd.DataFrame(data,
                 columns = ('col_1', 'col_2', 'col_3' ))


data2 = [('11','12','13'),
        ('21','22','23')]

df2 = pd.DataFrame(data2,
                 columns = ('col_1', 'col_2', 'col_3' ))


df_mask = df.compare(df2, keep_shape=True).notnull().astype('int')

df_compare = df.compare(df2, keep_shape=True, keep_equal=True)

def apply_color(x):

    colors = {1: 'lightblue', 0: 'white'}

    return df_mask.applymap(lambda val: 'background-color: {}'.format(colors.get(val,'')))

df_compare.style.apply(apply_color, axis=None)
</code></pre>
<p>This will throw an error:
ValueError: Can only compare identically-labeled Dataframe objects.</p>
",18553037.0,18553037.0,2022-06-30 18:15:20,2022-06-30 19:02:45,Comparison of two dataframes with different Lengths:,<python><pandas><dataframe><compare><data-science>,1,1,N/A,CC BY-SA 4.0
71565256,1,71565335.0,2022-03-21 23:18:58,0,60,"<p>I am working on a research project and a while back I asked <a href=""https://math.stackexchange.com/questions/4387891/calculate-variance-of-period-to-period-change-of-markov-chain-given-transition-m?noredirect=1#comment9194183_4387891"">this question</a> on Mathematics Stack Exchange, where I was looking for a way to calculate the variance of the period-to-period <strong>change</strong> in income given a transition matrix, where each state corresponds to a log level of income in a vector, which is given. I want to calculate what the variance of an individual's change in income is over some n number of periods given that they began in each state. My state space consists of 11 states, so I hope to end up with a vector consisting of 11 different variances. When I asked the question, I received a satisfactory answer, but I am running into some issues when trying to code it in R I was hoping to receive help with.</p>
<p>I have created this piece of code to calculate the variances:</p>
<pre><code>install.packages(&quot;expm&quot;)
library(expm)

# creating standard basis vectors
e &lt;- function(i) {
  e_i = rep(0, length(alpha))
  e_i[i] = 1
  return(e_i)
}

# compute variances
p2p_variance = function(n, alpha, P) {
  variance = list()
  pi_n = list()
  for (i in 1:length(alpha)) {
    pi_n[[i]] = e(i) %*% (P %^% n)
    beta = (t(alpha) - t(alpha)[i])^2
    variance[[i]] = (pi_n[[i]] %*% t(beta)) - (((pi_n[[i]] %*% alpha) - alpha[i]) %^% 2)
  }
  return(t(variance))
}
</code></pre>
<p>And for my values of alpha (vector of log levels of income) and P (transition matrix) I use:</p>
<pre><code>alpha = c(3.4965, 3.5835, 3.6636, 3.7377, 3.8067, 3.8712, 3.9318,  3.9890, 4.0431, 4.0943, 4.1431)
P = rbind(c(0.9004, 0.0734, 0.0203, 0.0043, 0.0010, 0.0003, 0.0001, 0.0001, 0.0000, 0.0000, 0.0000),
          c(0.3359, 0.3498, 0.2401, 0.0589, 0.0115, 0.0026, 0.0007, 0.0003, 0.0001, 0.0001, 0.0000),
          c(0.1583, 0.1538, 0.3931, 0.2346, 0.0481, 0.0090, 0.0021, 0.0007, 0.0003, 0.0001, 0.0001),
          c(0.0746, 0.0609, 0.1600, 0.4368, 0.2178, 0.0397, 0.0073, 0.0019, 0.0006, 0.0002, 0.0001),
          c(0.0349, 0.0271, 0.0559, 0.1724, 0.4628, 0.2031, 0.0344, 0.0067, 0.0018, 0.0006, 0.0003),
          c(0.0155, 0.0122, 0.0230, 0.0537, 0.1817, 0.4870, 0.1860, 0.0316, 0.0066, 0.0018, 0.0009),
          c(0.0066, 0.0054, 0.0100, 0.0204, 0.0529, 0.1956, 0.4925, 0.1772, 0.0307, 0.0064, 0.0023),
          c(0.0025, 0.0023, 0.0043, 0.0084, 0.0186, 0.0530, 0.2025, 0.4980, 0.1760, 0.0275, 0.0067),
          c(0.0009, 0.0009, 0.0017, 0.0035, 0.0072, 0.0168, 0.0490, 0.2025, 0.5194, 0.1721, 0.0260),
          c(0.0003, 0.0003, 0.0007, 0.0013, 0.0029, 0.0061, 0.0142, 0.0430, 0.2023, 0.5485, 0.1804),
          c(0.0001, 0.0001, 0.0002, 0.0003, 0.0008, 0.0017, 0.0032, 0.0068, 0.0212, 0.1079, 0.8578))
</code></pre>
<p>For instance, a call of <code>p2p_variance(100, alpha, P)</code> (calculating the variance over 100 periods) results in the following vector of variances:</p>
<pre><code>0.04393012 0.04091066 0.03856503 0.03636202 0.03472286 0.03331921 0.03213084 0.03068901 0.03143765 0.03255994 0.03522346
</code></pre>
<p>Which seem plausible. However, If I run <code>p2p_variance(1000, alpha, P)</code>, it results in:</p>
<pre><code>0.06126449 0.03445073 0.009621497 -0.01447615 -0.03652425 -0.05752316 -0.07753646 -0.09726683 -0.1134972 -0.1287498 -0.141676
</code></pre>
<p>This is obviously not correct, since we cannot have negative variance. I cannot figure out why simply increasing n to 1000 is resulting in negative variance here. I have most likely coded my p2p_variance function incorrectly, but I cannot for the life of me find the issue. Or perhaps is the process I am using to find these variances flawed somehow? I would really appreciate if anyone could look over this code and help me diagnose the issue</p>
",6918295.0,6918295.0,2022-03-21 23:26:38,2022-03-21 23:30:55,Implementing a function to calculate variance of period-to-period change of markov chain,<r><statistics><data-science><data-analysis><markov-chains>,1,0,N/A,CC BY-SA 4.0
71581585,1,71581645.0,2022-03-23 03:48:16,0,51,"<pre><code>col A
28
45
67
A
67
C
D
78
89
</code></pre>
<p>I want to remove row containing characters(i.e) A, B, C...(can be any from A-Z)
I was able to remove A,B,C using the below code</p>
<pre><code>new_df = df[(df['colA'] != 'A') &amp; (df['colA'] != 'B') &amp; (df['colA'] != 'C')] 
</code></pre>
<p>I feel this is hardcoded as I know the column contains A,B,C. Some other column might contain F,G or something.
Any better approach to removing the characters from column.</p>
<p><strong>Note:- colA is of type object in dataframe</strong></p>
<p><strong>Output should be column containing numbers only and datatype should be changed from object to int</strong></p>
",18124427.0,18124427.0,2022-03-25 01:10:26,2022-03-25 01:20:02,Removing alphabets from column,<python-3.x><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
72820886,1,72833461.0,2022-06-30 19:06:42,0,111,"<p>Started learning pandas and maybe got lost with it so just need some assistance.</p>
<p>I am trying to automate a process for editing a csv file. I am receiving unsorted ones and trying to get them ready to go for some bulk updates.</p>
<p>So, what I need to do is:
1.Rename the needed columns (which I did okay)
2.Replace existing double quotes for two single ones in title column
3.Add double quotes to all the titles (that's first column)
4.drop columns (not sure what's best - del or df.drop)
5.Save changes to a new file somewhere defined</p>
<p>This is the code I have tried now and it worked with:</p>
<pre><code>import pandas as pd

df = pd.read_csv('Sheet1.csv')
droped = df.drop(df.columns[[2,3,4,5,6,7,8,9,10]],axis = 1,inplace=True)
renamed = df.rename(columns={df.columns[0]: 'title', df.columns[1]: 'product_id'})
swapped = renamed[[0]].str.replace(r'\&quot;', r&quot;\''&quot;)
updated = swapped.update('&quot;' + df.columns[[0]].astype(str) + '&quot;')

print(renamed.head())
</code></pre>
<p>These quotes are headache to me now - some titles have double quotes inside - I need to remove them and then add double quotes around every title.</p>
<p>First three lines work fine, I am able to drop the unnecessary columns and rename the remaining ones, but last two operations are not well written. I am trying to figure out googling around but no luck so far.</p>
<p>Example for this quote swap:</p>
<p>Now:Banini Movie&quot; Teather
Desired:&quot;Banini Movie'' Theater&quot;</p>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;Desktop/testing_scripts/bulk_script.py&quot;, line 6, in &lt;module&gt;
    swapped = renamed[[0]].str.replace(r'\&quot;', r&quot;\''&quot;)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py&quot;, line 3511, in __getitem__
    indexer = self.columns._get_indexer_strict(key, &quot;columns&quot;)[1]
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py&quot;, line 5782, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py&quot;, line 5842, in _raise_if_missing
    raise KeyError(f&quot;None of [{key}] are in the [{axis_name}]&quot;)
KeyError: &quot;None of [Int64Index([0], dtype='int64')] are in the [columns]&quot;
</code></pre>
",17326438.0,17326438.0,2022-07-01 11:15:36,2022-07-01 18:43:07,"How to correct the pandas script to edit csv file - removing single quotes, adding double ones, deleting the unnecessary columns",<python><pandas><csv><data-science>,1,6,N/A,CC BY-SA 4.0
71595867,1,71595880.0,2022-03-24 01:03:31,2,49,"<p>I am getting the following error while trying to upload a dataset to <a href=""https://github.com/activeloopai/Hub"" rel=""nofollow noreferrer"">Hub (dataset format for AI)</a> <code>S3SetError: Connection was closed before we received a valid response from endpoint URL: &quot;&lt;...&gt;&quot;.</code></p>
<p>So, I tried to delete the dataset and it is throwing this error below.</p>
<p><code>CorruptedMetaError: 'boxes/tensor_meta.json' and 'boxes/chunks_index/unsharded' have a record of different numbers of samples. Got 0 and 6103 respectively.</code></p>
<p>Using Hub version: v2.3.1</p>
",18508132.0,-1.0,N/A,2022-03-24 01:06:20,What does stopping the runtime while uploading a dataset to Hub cause?,<database><machine-learning><data-science><artificial-intelligence><hub>,1,0,N/A,CC BY-SA 4.0
71592793,1,71594130.0,2022-03-23 19:12:06,0,855,"<p>I want to quantify some geolocations with osmnx using the nearest_edges-function. I get a value error message when running this code and don't know what I'm doing wrong:</p>
<pre><code># project graph and points
G_proj = ox.project_graph(G)
gdf_loc_p = gdf_loc[&quot;geometry&quot;].to_crs(G_proj.graph[&quot;crs&quot;])

ne, d = ox.nearest_edges(
    G_proj, X=gdf_loc_p.x.values, Y=gdf_loc_p.y.values, return_dist=True
)

# reindex points based on results from nearest_edges
gdf_loc = (
    gdf_loc.set_index(pd.MultiIndex.from_tuples(ne, names=[&quot;u&quot;, &quot;v&quot;, &quot;key&quot;]))
    .assign(distance=d)
    .sort_index()
)

# join geometry from edges back to points
# aggregate so have number of accidents on each edge
gdf_bad_roads = (
    gdf_edges.join(gdf_loc, rsuffix=&quot;_loc&quot;, how=&quot;inner&quot;)
    .groupby([&quot;u&quot;, &quot;v&quot;, &quot;key&quot;])
    .agg(geometry = (&quot;geometry&quot;, &quot;first&quot;), number=(&quot;osmid&quot;, &quot;size&quot;))
    .set_crs(gdf_edges.crs)
)
</code></pre>
<p>When running it tells me in the line .agg(geometry)<code># we require a list, but not a 'str'</code> and from there on couple more issues leading to a value error <code>data' should be a 1-dimensional array of geometry objects</code>. I attached the whole Traceback. Thanks for your help!</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/var/folders/jy/1f2tlvb965g30zhw9q3cvdw07r5rb_/T/ipykernel_82991/3621029527.py in &lt;module&gt;
      2 # aggregate so have number of accidents on each edge
      3 gdf_bad_roads = (
----&gt; 4     gdf_edges.join(gdf_loc, rsuffix=&quot;_loc&quot;, how=&quot;inner&quot;)
      5     .groupby([&quot;u&quot;, &quot;v&quot;, &quot;key&quot;])
      6     .agg(geometry = (&quot;geometry&quot;, &quot;first&quot;), number=(&quot;osmid&quot;, &quot;size&quot;))

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/groupby/generic.py in aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
    977 
    978         op = GroupByApply(self, func, args, kwargs)
--&gt; 979         result = op.agg()
    980         if not is_dict_like(func) and result is not None:
    981             return result

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/apply.py in agg(self)
    159 
    160         if is_dict_like(arg):
--&gt; 161             return self.agg_dict_like()
    162         elif is_list_like(arg):
    163             # we require a list, but not a 'str'

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/apply.py in agg_dict_like(self)
    457 
    458             axis = 0 if isinstance(obj, ABCSeries) else 1
--&gt; 459             result = concat(
    460                 {k: results[k] for k in keys_to_use}, axis=axis, keys=keys_to_use
    461             )

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    309                     stacklevel=stacklevel,
    310                 )
--&gt; 311             return func(*args, **kwargs)
    312 
    313         return wrapper

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/reshape/concat.py in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    305     )
    306 
--&gt; 307     return op.get_result()
    308 
    309 

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/reshape/concat.py in get_result(self)
    537 
    538             cons = sample._constructor
--&gt; 539             return cons(new_data).__finalize__(self, method=&quot;concat&quot;)
    540 
    541     def _get_result_dim(self) -&gt; int:

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/geopandas/geodataframe.py in __init__(self, data, geometry, crs, *args, **kwargs)
    155             try:
    156                 if (
--&gt; 157                     hasattr(self[&quot;geometry&quot;].values, &quot;crs&quot;)
    158                     and self[&quot;geometry&quot;].values.crs
    159                     and crs

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/geopandas/geodataframe.py in __getitem__(self, key)
   1325         GeoDataFrame.
   1326         &quot;&quot;&quot;
-&gt; 1327         result = super().__getitem__(key)
   1328         geo_col = self._geometry_column_name
   1329         if isinstance(result, Series) and isinstance(result.dtype, GeometryDtype):

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/frame.py in __getitem__(self, key)
   3424             if self.columns.is_unique and key in self.columns:
   3425                 if isinstance(self.columns, MultiIndex):
-&gt; 3426                     return self._getitem_multilevel(key)
   3427                 return self._get_item_cache(key)
   3428 

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/frame.py in _getitem_multilevel(self, key)
   3511             result_columns = maybe_droplevels(new_columns, key)
   3512             if self._is_mixed_type:
-&gt; 3513                 result = self.reindex(columns=new_columns)
   3514                 result.columns = result_columns
   3515             else:

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    322         @wraps(func)
    323         def wrapper(*args, **kwargs) -&gt; Callable[..., Any]:
--&gt; 324             return func(*args, **kwargs)
    325 
    326         kind = inspect.Parameter.POSITIONAL_OR_KEYWORD

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs)
   4770         kwargs.pop(&quot;axis&quot;, None)
   4771         kwargs.pop(&quot;labels&quot;, None)
-&gt; 4772         return super().reindex(**kwargs)
   4773 
   4774     @deprecate_nonkeyword_arguments(version=None, allowed_args=[&quot;self&quot;, &quot;labels&quot;])

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs)
   4816 
   4817         # perform the reindex on the axes
-&gt; 4818         return self._reindex_axes(
   4819             axes, level, limit, tolerance, method, fill_value, copy
   4820         ).__finalize__(self, method=&quot;reindex&quot;)

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)
   4589         columns = axes[&quot;columns&quot;]
   4590         if columns is not None:
-&gt; 4591             frame = frame._reindex_columns(
   4592                 columns, method, copy, level, fill_value, limit, tolerance
   4593             )

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/frame.py in _reindex_columns(self, new_columns, method, copy, level, fill_value, limit, tolerance)
   4634             new_columns, method=method, level=level, limit=limit, tolerance=tolerance
   4635         )
-&gt; 4636         return self._reindex_with_indexers(
   4637             {1: [new_columns, indexer]},
   4638             copy=copy,

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups)
   4895             new_data = new_data.copy()
   4896 
-&gt; 4897         return self._constructor(new_data).__finalize__(self)
   4898 
   4899     def filter(

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/geopandas/geodataframe.py in __init__(self, data, geometry, crs, *args, **kwargs)
    162                     _crs_mismatch_warning()
    163                     # TODO: raise error in 0.9 or 0.10.
--&gt; 164                 self[&quot;geometry&quot;] = _ensure_geometry(self[&quot;geometry&quot;].values, crs)
    165             except TypeError:
    166                 pass

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/geopandas/geodataframe.py in _ensure_geometry(data, crs)
     44             return GeoSeries(out, index=data.index, name=data.name)
     45         else:
---&gt; 46             out = from_shapely(data, crs=crs)
     47             return out
     48 

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/geopandas/array.py in from_shapely(data, crs)
    149 
    150     &quot;&quot;&quot;
--&gt; 151     return GeometryArray(vectorized.from_shapely(data), crs=crs)
    152 
    153 

~/opt/anaconda3/envs/pyproj_env/lib/python3.10/site-packages/geopandas/array.py in __init__(self, data, crs)
    278             )
    279         elif not data.ndim == 1:
--&gt; 280             raise ValueError(
    281                 &quot;'data' should be a 1-dimensional array of geometry objects.&quot;
    282             )

ValueError: 'data' should be a 1-dimensional array of geometry objects.
</code></pre>
<p><strong>Edit:</strong> thank you! Unfortunately it doesnt work. I downgraded Python to 3.9 (and upgraded Panda to 1.4 but have same issue). I added the Traceback of the other code as well.</p>
<pre><code>
----
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [4], in &lt;cell line: 4&gt;()
      2 gdf_bad_roads = gdf_edges.join(gdf_loc, rsuffix=&quot;_loc&quot;, how=&quot;inner&quot;)
      3 # aggregate so have number of accidents on each edge
----&gt; 4 gdf_bad_roads_agg = gdf_bad_roads.groupby([&quot;u&quot;, &quot;v&quot;, &quot;key&quot;]).agg(
      5     geometry=(&quot;geometry&quot;, &quot;first&quot;), number=(&quot;osmid&quot;, &quot;size&quot;)
      6 ).set_crs(gdf_edges.crs)
      8 print(f&quot;&quot;&quot;
      9 pandas: {pd.__version__}
     10 geopandas: {gpd.__version__}
     11 osmnx: {ox.__version__}&quot;&quot;&quot;)

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/groupby/generic.py:869, in DataFrameGroupBy.aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
    866 func = maybe_mangle_lambdas(func)
    868 op = GroupByApply(self, func, args, kwargs)
--&gt; 869 result = op.agg()
    870 if not is_dict_like(func) and result is not None:
    871     return result

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/apply.py:168, in Apply.agg(self)
    165     return self.apply_str()
    167 if is_dict_like(arg):
--&gt; 168     return self.agg_dict_like()
    169 elif is_list_like(arg):
    170     # we require a list, but not a 'str'
    171     return self.agg_list_like()

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/apply.py:498, in Apply.agg_dict_like(self)
    495         keys_to_use = ktu
    497     axis = 0 if isinstance(obj, ABCSeries) else 1
--&gt; 498     result = concat(
    499         {k: results[k] for k in keys_to_use}, axis=axis, keys=keys_to_use
    500     )
    501 elif any(is_ndframe):
    502     # There is a mix of NDFrames and scalars
    503     raise ValueError(
    504         &quot;cannot perform both aggregation &quot;
    505         &quot;and transformation operations &quot;
    506         &quot;simultaneously&quot;
    507     )

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/util/_decorators.py:311, in deprecate_nonkeyword_arguments.&lt;locals&gt;.decorate.&lt;locals&gt;.wrapper(*args, **kwargs)
    305 if len(args) &gt; num_allow_args:
    306     warnings.warn(
    307         msg.format(arguments=arguments),
    308         FutureWarning,
    309         stacklevel=stacklevel,
    310     )
--&gt; 311 return func(*args, **kwargs)

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/reshape/concat.py:359, in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    155 &quot;&quot;&quot;
    156 Concatenate pandas objects along a particular axis with optional set logic
    157 along the other axes.
   (...)
    344 ValueError: Indexes have overlapping values: ['a']
    345 &quot;&quot;&quot;
    346 op = _Concatenator(
    347     objs,
    348     axis=axis,
   (...)
    356     sort=sort,
    357 )
--&gt; 359 return op.get_result()

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/reshape/concat.py:599, in _Concatenator.get_result(self)
    596     new_data._consolidate_inplace()
    598 cons = sample._constructor
--&gt; 599 return cons(new_data).__finalize__(self, method=&quot;concat&quot;)

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/geopandas/geodataframe.py:157, in GeoDataFrame.__init__(self, data, geometry, crs, *args, **kwargs)
    154 index = self.index
    155 try:
    156     if (
--&gt; 157         hasattr(self[&quot;geometry&quot;].values, &quot;crs&quot;)
    158         and self[&quot;geometry&quot;].values.crs
    159         and crs
    160         and not self[&quot;geometry&quot;].values.crs == crs
    161     ):
    162         _crs_mismatch_warning()
    163         # TODO: raise error in 0.9 or 0.10.

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/geopandas/geodataframe.py:1327, in GeoDataFrame.__getitem__(self, key)
   1321 def __getitem__(self, key):
   1322     &quot;&quot;&quot;
   1323     If the result is a column containing only 'geometry', return a
   1324     GeoSeries. If it's a DataFrame with a 'geometry' column, return a
   1325     GeoDataFrame.
   1326     &quot;&quot;&quot;
-&gt; 1327     result = super().__getitem__(key)
   1328     geo_col = self._geometry_column_name
   1329     if isinstance(result, Series) and isinstance(result.dtype, GeometryDtype):

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/frame.py:3473, in DataFrame.__getitem__(self, key)
   3471     if self.columns.is_unique and key in self.columns:
   3472         if isinstance(self.columns, MultiIndex):
-&gt; 3473             return self._getitem_multilevel(key)
   3474         return self._get_item_cache(key)
   3476 # Do we have a slicer (on rows)?

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/frame.py:3560, in DataFrame._getitem_multilevel(self, key)
   3558 result_columns = maybe_droplevels(new_columns, key)
   3559 if self._is_mixed_type:
-&gt; 3560     result = self.reindex(columns=new_columns)
   3561     result.columns = result_columns
   3562 else:

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/util/_decorators.py:324, in rewrite_axis_style_signature.&lt;locals&gt;.decorate.&lt;locals&gt;.wrapper(*args, **kwargs)
    322 @wraps(func)
    323 def wrapper(*args, **kwargs) -&gt; Callable[..., Any]:
--&gt; 324     return func(*args, **kwargs)

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/frame.py:4798, in DataFrame.reindex(self, *args, **kwargs)
   4796 kwargs.pop(&quot;axis&quot;, None)
   4797 kwargs.pop(&quot;labels&quot;, None)
-&gt; 4798 return super().reindex(**kwargs)

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/generic.py:4974, in NDFrame.reindex(self, *args, **kwargs)
   4971     return self._reindex_multi(axes, copy, fill_value)
   4973 # perform the reindex on the axes
-&gt; 4974 return self._reindex_axes(
   4975     axes, level, limit, tolerance, method, fill_value, copy
   4976 ).__finalize__(self, method=&quot;reindex&quot;)

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/frame.py:4611, in DataFrame._reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)
   4609 columns = axes[&quot;columns&quot;]
   4610 if columns is not None:
-&gt; 4611     frame = frame._reindex_columns(
   4612         columns, method, copy, level, fill_value, limit, tolerance
   4613     )
   4615 index = axes[&quot;index&quot;]
   4616 if index is not None:

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/frame.py:4656, in DataFrame._reindex_columns(self, new_columns, method, copy, level, fill_value, limit, tolerance)
   4643 def _reindex_columns(
   4644     self,
   4645     new_columns,
   (...)
   4651     tolerance=None,
   4652 ):
   4653     new_columns, indexer = self.columns.reindex(
   4654         new_columns, method=method, level=level, limit=limit, tolerance=tolerance
   4655     )
-&gt; 4656     return self._reindex_with_indexers(
   4657         {1: [new_columns, indexer]},
   4658         copy=copy,
   4659         fill_value=fill_value,
   4660         allow_dups=False,
   4661     )

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/pandas/core/generic.py:5054, in NDFrame._reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups)
   5051 if copy and new_data is self._mgr:
   5052     new_data = new_data.copy()
-&gt; 5054 return self._constructor(new_data).__finalize__(self)

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/geopandas/geodataframe.py:164, in GeoDataFrame.__init__(self, data, geometry, crs, *args, **kwargs)
    162         _crs_mismatch_warning()
    163         # TODO: raise error in 0.9 or 0.10.
--&gt; 164     self[&quot;geometry&quot;] = _ensure_geometry(self[&quot;geometry&quot;].values, crs)
    165 except TypeError:
    166     pass

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/geopandas/geodataframe.py:46, in _ensure_geometry(data, crs)
     44     return GeoSeries(out, index=data.index, name=data.name)
     45 else:
---&gt; 46     out = from_shapely(data, crs=crs)
     47     return out

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/geopandas/array.py:151, in from_shapely(data, crs)
    135 def from_shapely(data, crs=None):
    136     &quot;&quot;&quot;
    137     Convert a list or array of shapely objects to a GeometryArray.
    138 
   (...)
    149 
    150     &quot;&quot;&quot;
--&gt; 151     return GeometryArray(vectorized.from_shapely(data), crs=crs)

File ~/opt/anaconda3/envs/pyproj_env/lib/python3.9/site-packages/geopandas/array.py:280, in GeometryArray.__init__(self, data, crs)
    275     raise TypeError(
    276         &quot;'data' should be array of geometry objects. Use from_shapely, &quot;
    277         &quot;from_wkb, from_wkt functions to construct a GeometryArray.&quot;
    278     )
    279 elif not data.ndim == 1:
--&gt; 280     raise ValueError(
    281         &quot;'data' should be a 1-dimensional array of geometry objects.&quot;
    282     )
    283 self.data = data
    285 self._crs = None

ValueError: 'data' should be a 1-dimensional array of geometry objects.
</code></pre>
<pre><code>pandas: 1.4.1
geopandas: 0.10.2
osmnx: 1.1.2
</code></pre>
",17891886.0,17891886.0,2022-03-23 22:44:08,2022-03-24 07:35:12,GeoDataFrame Value Error: 'data' should be a 1-dimensional array of geometry objects',<python><data-science><osmnx>,1,0,N/A,CC BY-SA 4.0
71601370,1,71617260.0,2022-03-24 11:07:31,-1,920,"<p>the data:
consider this sample dataset
<a href=""https://docs.google.com/spreadsheets/d/17Xjc81jkjS-64B4FGZ06SzYDRnc6J27m/edit#gid=1176233701"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/17Xjc81jkjS-64B4FGZ06SzYDRnc6J27m/edit#gid=1176233701</a></p>
<p>How to delete rows rows based on multiple columns condition?</p>
<p>i am filtering the data based on my thread i asked earlier. <a href=""https://stackoverflow.com/questions/71585999/how-to-filter-this-dataframe"">How to filter this dataframe?</a>
The solution in this thread ended up with errors</p>
<p>I want to filter the data based on the Edit section in the above thread?</p>
",18503721.0,18503721.0,2022-03-25 07:46:08,2022-03-25 12:49:31,how to delete rows based on multiple columns and condition with pandas and python?,<python><pandas><dataframe><numpy><data-science>,1,1,2022-03-31 10:07:21,CC BY-SA 4.0
74326969,1,-1.0,2022-11-05 10:32:22,1,40,"<p>how can I find <em>5th</em> or <em>7th</em>, or <em>15th</em> biggest element in multidimensional array without existing methods (like <code>list.Add</code>)
I will be pleased if you write it in c#</p>
<pre><code>int[,,,] x =new int[100, 20, 35, 200];

...

int indis  = 0;
int toplam = 0;
int enss   = 0;

for (int i = 0; i &lt; 100; i++) {
  for (int j = 0; j &lt; 20; j++) {
    toplam = 0;

    for (int k = 0; k &lt; 35; k++) {
      for (int l = 0; l &lt; 200; l++) {
        toplam += x[i, j, k, l];
      }
    }

    if (toplam &gt; enss) {
      enss  = toplam; 
      indis = j;
    }
  }
}
</code></pre>
",16420414.0,2319407.0,2022-11-05 11:04:17,2022-11-05 11:20:57,finding nth biggest element in multidimensional array,<c#><arrays><algorithm><multidimensional-array><data-science>,2,1,N/A,CC BY-SA 4.0
71605354,1,-1.0,2022-03-24 15:36:59,1,43,"<p>I am training on some data via gridsearch and I noticed that the best score is coming out to be way off base from what itd be for the test set:</p>
<pre><code>custom_scorer = make_scorer(f1_score, greater_is_better=True,  pos_label=1)

rf_params = {
    'max_depth': [20,50,100,150],
    'min_samples_split' : [10, 20, 50, 100],
}

rf = RandomForestClassifier(random_state=42)

rf_grid = GridSearchCV(rf, param_grid = rf_params, cv = 5, scoring = custom_scorer)

rf_grid.fit(X_train, y_train)
print( &quot;Best Score: {}&quot;.format(rf_grid.best_score_) )
&gt;&gt; Best Score: 0.9616742738181994
</code></pre>
<p>When I run on the test set its looking like this:</p>
<pre><code>y_preds = rf_grid.predict(X_test)
print(metrics.classification_report(y_test, y_preds))

                precision    recall  f1-score   support

           0       0.93      1.00      0.96      2308
           1       0.88      0.07      0.13       192

    accuracy                           0.93      2500
   macro avg       0.90      0.54      0.55      2500
weighted avg       0.92      0.93      0.90      2500
</code></pre>
<p>As you can see the F1 score on the positive class is 0.13 which is very different from the <code>best_score_</code> on GridSearchCV. I know that they should be different because its different data sets but this is just confusing.</p>
<p>I tried a lot of variations on this testing including upsampling the minority class, enhancing/reducing the params. Not sure what else.</p>
",5112032.0,4685471.0,2022-03-24 15:37:33,2022-03-24 17:50:41,GridSearchCV Best Score Way off Base,<python><machine-learning><data-science><random-forest>,1,2,N/A,CC BY-SA 4.0
71607529,1,-1.0,2022-03-24 18:14:38,0,326,"<p>I'm trying to count how many times an organization was cited, but am coming across this problem:</p>
<p>('The Regents Of The University Of California', 468), (' The Regents Of The University Of California', 64)</p>
<p>The 2 organizations are clearly the same but I am unable to clean the data to merge the count. I came across the fuzzy wuzzy library but am unable to get it to work (nor understand how it works as it parses through data).</p>
<p>I need to replace the text so that it can get the count correct.</p>
<p>The code I have thus far:</p>
<pre><code>raw_assignee = list(clean1.iloc[:,1])
assignee_list_example = ['The Regents of the University of California', 'llc', 'ltd', 'inc']
deduplicated_assignee = process.dedupe(raw_assignee, threshold=80)
print(deduplicated_assignee)
</code></pre>
<p>I'm new to data science and have no idea what's going on - appreciate your help!</p>
",10955096.0,-1.0,N/A,2022-03-24 18:14:38,Deduplication and Replacement Using Fuzzywuzzy,<python><data-science><data-cleaning><fuzzy-search><fuzzywuzzy>,0,2,N/A,CC BY-SA 4.0
71604538,1,-1.0,2022-03-24 14:43:01,0,163,"<p>i am trying to scrape amazon's products using scrapy with the crawl template, but i found the amazon use some javascript to get some block of the product details, so i decide to use splash to render javascript, it works fine in the shell command, but i can't figure out how to implement it in my code.</p>
<pre><code>import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class AmazonCrawlerSpider(CrawlSpider):
    name = 'amazon_Crawler'
    allowed_domains = ['amazon.com']
    start_urls = ['https://www.amazon.com/s?i=specialty-aps&amp;bbn=16225009011&amp;rh=n%3A%2116225009011%2Cn%3A502394&amp;ref=nav_em__nav_desktop_sa_intl_camera_and_photo_0_2_5_3']

    len_product_details = LinkExtractor(restrict_css='h2 &gt; a')
    product_details = Rule(len_product_details,
                           callback='parse_item', follow=False)

    len_products_pagination = LinkExtractor(
        restrict_xpaths='//*[@id=&quot;search&quot;]/div[1]/div[1]/div/span[3]/div[2]/div[37]/div/div/span/a[3]')
    products_pagination = Rule(len_products_pagination, follow=True)
    rules = (
        product_details, products_pagination
    )

    def parse_item(self, response):

        data = {

            &quot;categorie_0&quot;: response.xpath('//*[@id=&quot;wayfinding-breadcrumbs_feature_div&quot;]/ul/li[1]/span/a/text()').get(),
            &quot;categorie_1&quot;: response.xpath('//*[@id=&quot;wayfinding-breadcrumbs_feature_div&quot;]/ul/li[3]/span/a/text()').get(),
            &quot;title&quot;: response.css('h1 &gt; span ::text').get(),
            &quot;price&quot;: response.xpath('//div[@id=&quot;corePrice_feature_div&quot;]/div/span/span[1]//text()').get(),
            &quot;amazon_globale_shiping&quot;: response.xpath('//*[@id=&quot;a-popover-content-2&quot;]/table/tbody/tr[2]/td[3]/span/text()').get(),
            &quot;estimated_import_fees_deposit&quot;: response.xpath('//*[@id=&quot;a-popover-content-2&quot;]/table/tbody/tr[3]/td[3]/span/text()').get(),
            &quot;total&quot;: response.xpath('//*[@id=&quot;a-popover-content-2&quot;]/table/tbody/tr[5]/td[3]/span/text()').get(),
            &quot;delevery_period&quot;: response.xpath('//*[@id=&quot;mir-layout-DELIVERY_BLOCK-slot-PRIMARY_DELIVERY_MESSAGE_LARGE&quot;]/span/span/text()').get(),
            &quot;delevery_destination&quot;: response.xpath('//*[@id=&quot;contextualIngressPtLabel_deliveryShortLine&quot;]/span[2]/text()').get(),
            &quot;in_stock&quot;: response.xpath('//*[@id=&quot;availability&quot;]/span/text()').get(),
            &quot;quantity&quot;: &quot;not_exist&quot;,
            &quot;ship_from&quot;: response.xpath('//*[@id=&quot;tabular-buybox&quot;]/div[1]/div[2]/div/span/text()').get(),
            &quot;sold_by&quot;: {
                &quot;name&quot;: response.xpath('//*[@id=&quot;sellerProfileTriggerId&quot;]/text()').get(),
                'store_url': response.xpath('//*[@id=&quot;sellerProfileTriggerId&quot;]/@href').get(),
                'packaging': response.xpath('//*[@id=&quot;tabular-buybox&quot;]/div[1]/div[6]/div/span/text()').get()
            },
            &quot;description&quot;: response.xpath('//*[@id=&quot;productDescription&quot;]/p/text()').get(),
            # &quot;brand&quot;: response.xpath('//*[@id=&quot;productOverview_feature_div&quot;]/div/table/tbody/tr[1]/td[2]/span/text()').get(),
            &quot;is_returned&quot;: response.xpath('//*[@id=&quot;productSupportAndReturnPolicy-return-policy-popover-celWidget&quot;]/div/div[1]/text()').get(),
            &quot;extra_info&quot;: [],
            &quot;details&quot;: [],
            &quot;about_this_item&quot;: [],
            &quot;note&quot;: response.xpath('//*[@id=&quot;universal-product-alert&quot;]/div/span[2]/text()').get(),
            &quot;Q_AW&quot;: [],
            &quot;Customer_reviews&quot;: {
                &quot;customer_rate&quot;: response.xpath('//*[@id=&quot;reviewsMedley&quot;]/div/div[1]/div[2]/div[1]/div/div[2]/div/span/span/text()').get(),
                &quot;total_rate&quot;: response.xpath('//*[@id=&quot;reviewsMedley&quot;]/div/div[1]/div[2]/div[2]/span/text()').get(),
                &quot;global_rate&quot;: {
                    &quot;1_star&quot;: response.xpath('//*[@id=&quot;histogramTable&quot;]/tbody/tr[5]/td[3]/span[2]/a/text()').get(),
                    &quot;2_star&quot;: response.xpath('//*[@id=&quot;histogramTable&quot;]/tbody/tr[4]/td[3]/span[2]/a/text()').get(),
                    &quot;3_star&quot;: response.xpath('//*[@id=&quot;histogramTable&quot;]/tbody/tr[3]/td[3]/span[2]/a/text()').get(),
                    &quot;4_star&quot;: response.xpath('//*[@id=&quot;histogramTable&quot;]/tbody/tr[2]/td[3]/span[2]/a/text()').get(),
                    &quot;5_star&quot;: response.xpath('//*[@id=&quot;histogramTable&quot;]/tbody/tr[1]/td[3]/span[2]/a/text()').get(),
                },
                &quot;rate_by_feature&quot;: [],
                &quot;product_reviews&quot;: []

            },
            &quot;url&quot;: response.url

        }
        for reveiw in response.xpath('//*[@id=&quot;cm-cr-dp-review-list&quot;]/div'):
            data[&quot;Customer_reviews&quot;][&quot;product_reviews&quot;].append(
                {
                    &quot;rate&quot;: reveiw.xpath('/div/div/div[2]/a/i/span/text()').get(),
                    &quot;feature&quot;: reveiw.xpath('div/div/div[2]/a[2]/span/text()').get(),
                    &quot;date_from&quot;: reveiw.xpath('div/div/span/text()').get(),
                    &quot;verified&quot;: reveiw.xpath('div/div/div[3]/span[2]/text()').get(),
                    &quot;review&quot;: reveiw.xpath('div/div/div[4]/span/div/div[1]/span/text()').get(),
                    'view_reaction': reveiw.xpath('div/div/div[5]/span[1]/div[1]/span/text()').get()
                }
            )

        for cr_rf in response.xpath('//*[@id=&quot;cr-summarization-attributes-list&quot;]/div'):
            data[&quot;Customer_reviews&quot;][&quot;rate_by_feature&quot;].append(
                {
                    &quot;key&quot;: cr_rf.xpath('div/div/div/div/span/text()').get(),
                    &quot;value&quot;: response.xpath('div/div/div[2]/span[2]/text()').get()
                }
            )

        for Q_AW in response.xpath('//*[@id=&quot;ask-btf-container&quot;]/div/div/div[2]/span/div/div'):
            data[&quot;Q_AW&quot;].append(
                {
                    &quot;Question&quot;: Q_AW.xpath('div/div[2]/div/div/div[2]/a/span/text()').get(),
                    &quot;Answer&quot;:  Q_AW.xpath('div/div[2]/div[2]/div/div[2]/span/span[2]/text()').get(),
                    &quot;vote&quot;: Q_AW.xpath('div/div/ul/li[2]/span[1]/text()').get(),
                    &quot;date_answer&quot;: Q_AW.xpath('div/div[2]/div[2]/div/div[2]/span[3]/text()').get()
                }
            )

        for extra_info in response.xpath('//*[@id=&quot;productDetails_detailBullets_sections1&quot;]/tbody/tr'):
            data[&quot;extra_info&quot;].append(
                {
                    &quot;1&quot;: extra_info.css('th::text').get(),
                    &quot;2&quot;: extra_info.css('td::text').get()
                }
            )
        for index, about_this_item in enumerate(response.xpath('//*[@id=&quot;feature-bullets&quot;]/ul/li')):
            data[&quot;about_this_item&quot;].append(
                {
                    index+1: about_this_item.xpath('span/text()').get(),

                }
            )
        for extra in response.xpath('//*[@id=&quot;productOverview_feature_div&quot;]/div/table/tbody/tr'):
            data['details'].append(
                {
                    extra.xpath('td[1]/span/text()').get(): extra.css('td[2]/span/text()').get()
                }
            )

        yield data

</code></pre>
",14854833.0,-1.0,N/A,2022-03-24 14:51:32,how to use scrapy with the crawler template and scrapy-splash to parse javascript,<python><selenium><scrapy><data-science><scrapy-splash>,1,1,N/A,CC BY-SA 4.0
74329144,1,74537577.0,2022-11-05 15:39:17,0,710,"<p>Given this example code:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt


data = 'https://raw.githubusercontent.com/marsja/jupyter/master/flanks.csv'

df = pd.read_csv(data, index_col=0)

# Subsetting using Pandas query():
congruent = df.query('TrialType == &quot;congruent&quot;')['RT']
incongruent = df.query('TrialType == &quot;incongruent&quot;')['RT']

# Combine data
plot_data = list([incongruent, congruent])

fig, ax = plt.subplots()

xticklabels = ['Incongruent', 'Congruent']
ax.set_xticks([1, 2])
ax.set_xticklabels(xticklabels)

ax.violinplot(plot_data, showmedians=True)
</code></pre>
<p>Which results in the following plot:</p>
<p><a href=""https://i.stack.imgur.com/OiPpa.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OiPpa.jpg"" alt=""enter image description here"" /></a></p>
<p>How can I annotate the min, max, and mean lines with their respective values?</p>
<p>I haven't been able to find examples online that allude to how to annotate violin plots in this way. If we set <code>plot = ax.violinplot(plot_data, showmedians=True)</code> then we can access attributes like <code>plot['cmaxes']</code> but I cant quite figure out how to use that for annotations.</p>
<p>Here is an example of what I am trying to achieve:</p>
<p><a href=""https://i.stack.imgur.com/DLDN2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DLDN2.png"" alt=""enter image description here"" /></a></p>
",19644106.0,19644106.0,2022-11-21 20:16:59,2022-11-22 18:39:52,Annotate Min/Max/Median in Matplotlib Violin Plot,<python><matplotlib><data-science><visualization><violin-plot>,1,0,N/A,CC BY-SA 4.0
71606863,1,-1.0,2022-03-24 17:24:00,0,242,"<p>on Stackoverflow I have found many posts about how to find the prevalent color of an image in python. However, I have not found anything about videos. Maybe because it could be possible to process each frame, but I was wondering if there is any easier method. In particular, I would like to study the dominant color of a whole movie.
Thank you a lot in advance.</p>
",18571016.0,-1.0,N/A,2022-03-24 17:55:38,Find dominant color of a video,<python><machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
71607966,1,-1.0,2022-03-24 18:51:13,0,180,"<p><strong>Help Me, please...... ValueError: Unable to coerce to Series, the length must be 1: given 300</strong></p>
<p><strong>Splitting Data</strong></p>
<pre><code>from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest = train_test_split(feature,target, test_size=0.3,random_state=101)
</code></pre>
<p><strong>Modeling</strong></p>
<pre><code>from sklearn.neighbors import KNeighborsClassifier
knm = KNeighborsClassifier(n_neighbors=1)
knm.fit(xtrain,ytrain.values.ravel())
</code></pre>
<p><strong>Predictions</strong></p>
<pre><code>Predictions = knm.predict(xtest)
</code></pre>
<p><strong>Conclutions</strong></p>
<pre><code>from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(ytest,Predictions))
print(confusion_matrix(ytest,Predictions))
</code></pre>
<p><strong>Everything is ok but I got an error from here</strong></p>
<p>error_rate = []</p>
<p>for i in range(1,40):</p>
<pre><code>knm = KNeighborsClassifier(n_neighbors=i)
knm.fit(xtrain,ytrain.values.ravel())
pred_i = knm.predict(xtest)
error_rate.append(np.mean(pred_i != ytest))
</code></pre>
",12739165.0,12739165.0,2022-03-25 05:16:09,2022-03-25 12:59:40,"ValueError: Unable to coerce to Series, length must be 1: given 300",<python><machine-learning><deep-learning><data-science><knn>,1,0,N/A,CC BY-SA 4.0
71608038,1,71608269.0,2022-03-24 18:58:39,0,191,"<p><em><strong>Edit: Solutions posted in <a href=""https://github.com/ghayward/pandas_apply_lambda_or_not/blob/main/Pandas%20Apply%20and%20Lambda%20Usage.ipynb"" rel=""nofollow noreferrer"">this notebook</a>. Special thanks to Étienne Célèry and ifly6!</strong></em></p>
<hr />
<p>I am trying to figure out how to beat the feared error:</p>
<blockquote>
<p>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</p>
</blockquote>
<pre><code>d = {
    'nickname': ['bobg89', 'coolkid34','livelaughlove38'], 
    'state': ['NY', 'CA','TN'],
    'score': [100, 200,300]
}
df = pd.DataFrame(data=d)
df_2 = df.copy() #for use in the second non-lambda part
print(df)
</code></pre>
<p>And this outputs:</p>
<pre><code>          nickname state  score
0           bobg89    NY    100
1        coolkid34    CA    200
2  livelaughlove38    TN    300
</code></pre>
<p>Then the goal is to add 50 to the score if they are from NY.</p>
<pre><code>def add_some_love(state_value,score_value,name):
     if state_value == name:
          return score_value + 50
     else:
          return score_value
</code></pre>
<p>Then we can apply that function with a <code>lambda</code> function.</p>
<pre><code>df['love_added'] = df.apply(lambda x: add_some_love(x.state, x.score, 'NY'), axis=1)
print(df)
</code></pre>
<p>And that gets us:</p>
<pre><code>          nickname state  score  love_added
0           bobg89    NY    100         150
1        coolkid34    CA    200         200
2  livelaughlove38    TN    300         300
</code></pre>
<p>And here is where I tried writing it, without the lambda, and that's where I get the error.</p>
<p>It seems like @MSeifert's <a href=""https://stackoverflow.com/a/36922103/11736959"">answer here</a> explains why this happens (that the function is looking at a whole column instead of a row in a column, but I also thought passing <code>axis = 1</code> into the <code>.apply()</code> method would apply the function row-wise, and fix the problem).</p>
<p>So I then do this:</p>
<pre><code>df2['love_added'] = df2.apply(add_some_love(df2.state, df2.score, 'NY'), axis=1)
print(df2)
</code></pre>
<p>And then you get the error:</p>
<blockquote>
<p>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</p>
</blockquote>
<p>So I've tried these solutions, but I can't seem to figure out how to rewrite the <code>add_some_love()</code> function so that it can run properly without the lambda function.</p>
<p>Does anyone have any advice?</p>
<p>Thanks so much for your time and consideration.</p>
",11736959.0,11736959.0,2022-03-24 20:18:48,2023-05-19 20:38:53,"How to Re-Write Lambda Function (via Pandas .apply() method) to Beat Famous ""ValueError: The truth value of a Series is ambiguous.""?",<python><pandas><dataframe><lambda><data-science>,2,0,N/A,CC BY-SA 4.0
71600115,1,-1.0,2022-03-24 09:43:26,0,101,"<p>I have tried to perform a two-way repeated-measures analysis, but I cannot get the p-values with my R code. Do you have any ideas about what is wrong?</p>
<pre><code>myData.mean &lt;- aggregate(main$ER,
                         by = list(main$PID, main$Mode, main$Task),
                         FUN = &quot;mean&quot;)
colnames(myData.mean) &lt;- c(&quot;PID12&quot;,&quot;Mode12&quot;,&quot;Task12&quot;,&quot;ER12&quot;)
myData.mean &lt;- myData.mean %&gt;% arrange(PID12)

stress.aov &lt;- with(myData.mean,
                   aov(ER12 ~ Mode12 + Task12 + 
                         Error(PID12 / Mode12 * Task12)))
summary(stress.aov)
</code></pre>
<p>Do you have any ideas why this could be?
Thank you in advance for your answers! Of course, I am happy to answer any questions regarding the problem.</p>
",18545075.0,18545075.0,2022-03-27 14:21:55,2022-03-27 14:21:55,R: I do not get P-Values for rmANOVA using aov(),<r><statistics><data-science><analysis>,2,2,N/A,CC BY-SA 4.0
71604599,1,-1.0,2022-03-24 14:46:53,0,22,"<p><img src=""https://i.stack.imgur.com/Yfdmi.png"" alt=""enter image description here"" /></p>
<p>Can some one help me,i have created one function which can plot scatter plot.If I chnage the agruments in function ,i get different subplots.Now,How can i put this function so that i can put in subplots changing different arguments.</p>
<p>I tried to put function in subplots, but  throws error</p>
",16515770.0,3528321.0,2023-01-07 05:46:44,2023-01-07 05:46:44,how to make subplots using user defined function,<matplotlib><data-science>,0,2,2022-03-24 15:12:11,CC BY-SA 4.0
71609657,1,-1.0,2022-03-24 21:29:00,0,75,"<p>Please How do I split one column into multiple columns with delimiter
starting from the end of the string? e.g</p>
<blockquote>
<p>2014_FIFA_World_Cup_en.wikipedia.org_all-access_all-agents
2015_Copa_AmÃ©rica_en.wikipedia.org_all-access_all-agents
2016_Summer_Olympics_en.wikipedia.org_all-access_all-agents
2018_FIFA_World_Cup_en.wikipedia.org_all-access_all-agents
2014_FIFA_World_Cup_en.wikipedia.org_mobile-web_all-agents
A_Song_of_Ice_and_Fire_en.wikipedia.org_desktop_all-agents</p>
</blockquote>
<pre><code>enter code here
</code></pre>
",15751126.0,-1.0,N/A,2022-03-24 21:59:29,Split one column into multiple columns with delimiter starting from the end of the string - Python Data science,<python><data-science>,1,2,N/A,CC BY-SA 4.0
74332364,1,74332439.0,2022-11-06 00:06:10,0,29,"<p>When I run this script I can verify that it loops through all of the values, but not all of them get passed into my dictionary</p>
<pre><code>file = open('path', 'rb')
readFile = PyPDF2.PdfFileReader(file)

lineData = {}

totalPages = readFile.numPages

for i in range(totalPages):
    pageObj = readFile.getPage(i)
    pageText = pageObj.extractText
    newTrans = re.compile(r'Jan \d{2,}')
    for line in pageText(pageObj).split('\n'):
        if newTrans.match(line):
            newValue = re.split(r'Jan \d{2,}', line)
            newValueStr = ' '.join(newValue)
            newKey = newTrans.findall(line)
            newKeyStr = ' '.join(newKey)
            print(newKeyStr + newValueStr)
            lineData[newKeyStr] = newValueStr
print(len(lineData))

</code></pre>
<p>There are 80+ data pairs but when I run this the dict only gets 37</p>
",20429109.0,-1.0,N/A,2022-11-06 00:23:11,Why is only half my data being passed into my dictionary?,<python-3.x><dictionary><data-science><nested-loops>,1,0,N/A,CC BY-SA 4.0
71616676,1,-1.0,2022-03-25 12:02:09,-1,146,"<p>I am getting output in this format
<img src=""https://i.stack.imgur.com/KRzIS.png"" alt=""enter image description here"" />
But I want output in this format
<img src=""https://i.stack.imgur.com/bpDj3.png"" alt=""enter image description here"" /></p>
<p>Any Help will be appreciated
Thankyou in Advance</p>
<p>I've tried to convert my data into an array but it doesn't work as i want</p>
<p>This is my output :</p>
<p>{'date': '2021-12-30 17:31:05.865139', 'sub_data': [{'key': 'day0', 'value': 255}, {'key': 'day1', 'value': 1}, {'key': 'day3', 'value': 8}, {'key': 'day7', 'value': 2}, {'key': 'day15', 'value': 3}, {'key': 'day30', 'value': 5}]}</p>
<p>{'date': '2021-12-31 17:31:05.907697', 'sub_data': [{'key': 'day0', 'value': 222}, {'key': 'day1', 'value': 1}, {'key': 'day3', 'value': 0}, {'key': 'day7', 'value': 0}, {'key': 'day15', 'value': 1}, {'key': 'day30', 'value': 0}]}]</p>
",18578575.0,-1.0,N/A,2022-03-27 14:34:56,How can i change the dictionary output format in python,<python><dataframe><dictionary><key><data-science>,1,1,N/A,CC BY-SA 4.0
71570565,1,71570914.0,2022-03-22 10:35:56,0,427,"<p>I have to series, where every element of the series is a list:</p>
<pre><code>s1 = pd.Series([[1,2,3],[4,5,6],[7,8,9]])
s2 = pd.Series([[1,2,3],[1,1,1],[7,8,8]])
</code></pre>
<p>And I want to calculate element-wise <code>sklearn.metrics.mean_squared_error</code>, so I will get:</p>
<pre><code>[0, 16.666, 0.33]
</code></pre>
<p>What is the best way to do it?</p>
",6057371.0,17562044.0,2022-11-23 12:38:09,2022-11-23 12:38:09,Python apply element-wise operation between to series where every element is a list,<python><pandas><dataframe><data-science><series>,2,0,N/A,CC BY-SA 4.0
71594551,1,-1.0,2022-03-23 21:57:26,0,266,"<p>The following code has generated this error message:</p>
<pre><code>(dd 
   [['cfs']]
  .loc['2018/3':'2019/5']

  .query('cfs.isna()')
)
---&gt;TypeError: unhashable type: 'Series'
</code></pre>
<p>Can somebody help me in figuring out the problem?</p>
",4312198.0,494134.0,2022-03-23 22:04:42,2022-12-27 12:40:16,How can I fix for unhashable type series problem,<python><pandas><numpy><time-series><data-science>,1,0,N/A,CC BY-SA 4.0
71595945,1,-1.0,2022-03-24 01:17:31,0,50,"<p>I am trying to get the outliers of a column (with IQR), once I get the outliers I want to set the values where the outliers are in my main dataframe to null in order to impute them afterwards. This is the way I implemeted it:</p>
<pre><code> df_outliers_detected = detect_outliers_IQR(df['Outliers'])
 df_outliers_detected = pd.DataFrame(df_outliers_detected)
 print(df_outliers_detected)

 for i in range(len(df)):
  for j in range(len df_outliers_detected)):
     if(df.loc[i, &quot;Outliers&quot;] ==  df_outliers_detected.iloc[j,0]):
       df.loc[i,'Outliers'] = None
                    
 print(df['Outliers'].head(100))




</code></pre>
<p>This 2 for loops makes the program really slow, is their a better way to implement this?</p>
<p>The function code of &quot;remove_outliers_IQR&quot;:</p>
<pre><code>def detect_outliers_IQR(df):

    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    
    print(df)
    print(&quot;\n&quot;)
    df_outlier = df[((df&lt;(Q1-1.5*IQR)) | (df&gt;(Q3+1.5*IQR)))]
    print(len(df_outlier))
    return df_outlier
</code></pre>
",13535093.0,-1.0,N/A,2022-03-24 02:06:53,How can i make this algorithm more efficient using dataframes?,<python><pandas><dataframe><bigdata><data-science>,1,0,N/A,CC BY-SA 4.0
71607789,1,71607930.0,2022-03-24 18:36:47,0,1205,"<p>I'm currently working with the Python framework - Prefect (prefect.io)
I wrote the code below</p>
<pre><code>from prefect import Flow, task
@task
def say_hello():
    print('Hello')

@task
def say_how_a_u():
    print('How are you?')

@task
def say_bye():
    print('Bye Bye')

with Flow('Test') as flow:
   say_hello()
   say_how_a_u()
   say_bye()

flow.run()
</code></pre>
<p>The fact is that all functions are called in parallel. How to make one function call after another and waited for the previous function? hello -&gt; how_a_u -&gt; bye</p>
<p>I work with triggers, but it fail</p>
",18571513.0,4294399.0,2022-04-15 12:24:56,2022-04-15 12:24:56,Tasks sequence in Prefect Flow Python,<python><data-science><workflow><prefect>,1,0,N/A,CC BY-SA 4.0
74330003,1,-1.0,2022-11-05 17:30:24,-1,97,"<p>I have a dataframe with different weather variables and datetime variables (month, day, hour). I'm going to use the variables to predict number of cyclist.
I want to scale the variables with StandardScaler, but should I include the datetime variables as well, or just scale the weather variables? It just feels wrong to scale the datetime variables.</p>
",20000852.0,-1.0,N/A,2022-11-05 18:02:38,Should I scale datetime variables?,<python><data-science>,1,0,N/A,CC BY-SA 4.0
71624652,1,71628645.0,2022-03-26 01:29:00,0,921,"<p>I'm new to Python, and so I'm struggling a bit with this. Basically, the code below gets the text of tweets with the hashtag bitcoin in it, and I want to extract the date and author as well as the text. I've tried different things, but stuck rn.
Greatly appreciate any help with this.</p>
<pre><code>import pandas as pd
import numpy as np
import tweepy

api_key = '*'
api_secret_key = '*'
access_token = '*'
access_token_secret = '*'

authentication = tweepy.OAuthHandler(consumer_key, consumer_secret_key)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(authentication, wait_on_rate_limit=True)

#Get tweets about Bitcoin and filter out any retweets
search_term = '#bitcoin -filter:retweets'
tweets = tweepy.Cursor(api.search_tweets, q=search_term, lang='en', since='2018-11-01', tweet_mode='extended').items(50)
all_tweets = [tweet.full_text for tweet in tweets]


df = pd.DataFrame(all_tweets, columns=['Tweets'])
df.head()
</code></pre>
",13142193.0,13142193.0,2022-03-26 12:03:09,2022-03-26 13:59:12,"Extract date from tweets (Tweepy, Python)",<python><date><twitter><data-science><tweepy>,1,3,N/A,CC BY-SA 4.0
71616951,1,-1.0,2022-03-25 12:23:26,-1,612,"<p>In random forest we actually use bootstrap aggregation,actually we are following two steps like Row sampling with replacement and feature sampling for creating bootstrap sample,Actually my questions are</p>
<p>1)what is actual purpose of this row sampling and feature sampling?</p>
<p>2)In row sampling with replacement same rows can be repeated in the bootstrap sample right(correct me if i am wrong),if same rows are appearing twice how will it impact our final prediction is there any need of that replacement / Is that row sampling with replacement necessary?</p>
<p>3)Say if a dataset consists of 7 features, In feature sampling we actually select some of the columns
right if we select 4 columns only from our dataset will it affect our result?</p>
<p>please correct if i am wrong and help me by answering the correct method of bootstrap sampling in random Forest</p>
<p><img src=""https://i.stack.imgur.com/KarEU.png"" alt="""" />Is this bootstrap sample  created corrrect?(First data is our actual data other two are bootstrap samplees</p>
",18203528.0,-1.0,N/A,2022-03-25 13:33:53,Purpose of Row sampling with replacement and feature sampling in Random Forest?,<python><data-science><random-forest>,1,0,N/A,CC BY-SA 4.0
71620892,1,71622480.0,2022-03-25 17:25:16,1,586,"<p>I am moving a visualization from the seaborn library to the plotly library.
I want a scatterplot with marginal histograms for my x and y variables.
I want to show a vertical and horizontal line for the average of my x and y variables.</p>
<p>To show my problem I created a dummy dataframe of random X and Y values</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import seaborn as sns
import plotly.express as px

df = pd.DataFrame(np.random.randint(0,100,size=(100, 2)), columns=list('XY'))
</code></pre>
<p>I want to replicate the seaborn library output on plotly.</p>
<pre class=""lang-py prettyprint-override""><code>g = sns.JointGrid(data=df, x=&quot;X&quot;, y=&quot;Y&quot;)
g.plot(sns.scatterplot, sns.histplot)
g.refline(x=df.X.mean(), y=df.Y.mean())

plt.show()
</code></pre>
<p><img src=""https://i.stack.imgur.com/XVKnD.png"" alt=""seaborn plot"" /></p>
<p>When I do something similar on plotly using <code>add_line</code> and <code>add_hline</code> I get the following:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame(np.random.randint(0,100,size=(100, 2)), columns=list('XY'))
fig = px.scatter(df, x='X', y='Y',
                marginal_y=&quot;histogram&quot;, marginal_x=&quot;histogram&quot;,
                width=800, height=600)
fig.add_hline(y=df['X'].mean(), line_dash='dash', annotation_text= f&quot;{df['X'].mean():.0f}&quot;)
fig.add_vline(x=df['Y'].mean(), line_dash='dash', annotation_text=f&quot;{df['Y'].mean():.0f}&quot;)

fig.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/KvCCn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KvCCn.png"" alt=""plotly plot"" /></a></p>
<p>My issue is that the horizontal line is plotted also plotted on the marginal x plot and the vertical line is also plotted on the marginal y plot. Is there a way to prevent the horizontal line to be plotted on the marginal x plot and the vertical line to be plotted on the marginal y plot?</p>
",14048976.0,4819376.0,2022-03-25 19:42:16,2022-03-25 19:59:45,Can I add a vline and hline to a plotly plot ignoring marginal plots?,<python><plotly><data-science><data-visualization>,1,2,N/A,CC BY-SA 4.0
71626366,1,-1.0,2022-03-26 08:15:02,0,1261,"<p>Recently I was working on a Data cleaning assignment, where I used age_of_marriage dataset. I started to clean data, but in the dataset there is a &quot;height&quot; column which is of Object type. It is in the format of feet and inch.
<img src=""https://i.stack.imgur.com/Yjg31.png"" alt=""Dataset Image"" /></p>
<p>I want to extract 'foot' and 'inch' from the data and convert it into 'cm' using the formula. I have the formula ready for the conversion but I am not able to extract it.
Also I want to convert it into Int datatype before applying the formula. I am stuck on this mode.</p>
<p>-------- 2   height     2449 non-null   object --------</p>
<p>I am trying to extract it using String manipulation, but not able to do it. Can anybody help.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>height</th>
</tr>
</thead>
<tbody>
<tr>
<td>5'3&quot;</td>
</tr>
<tr>
<td>5'4&quot;</td>
</tr>
</tbody>
</table>
</div>
<p>I have attached a github link to access the dataset.
<a href=""https://github.com/atharva07/Age-of-marriage"" rel=""nofollow noreferrer"">text</a></p>
<pre><code>import numpy as np
import pandas as pd
from collections import Counter

agemrg = pd.read_csv('age_of_marriage_data.csv')

for height in range(len(height_list)):
    BrideGroomHeight = height_list[height].rstrip(height_list[height][-1])
    foot_int = int(BrideGroomHeight[0])
    inch_int = int(BrideGroomHeight[2:4])
    print(foot_int)
    print(inch_int)
    
    if height in ['nan']:
        continue

output - 
5
4
5
7
5
7
5
0
5
5
5
5
5
2
5
5
5
5
5
1
5
3
5
9
5
10
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_12772/2525694838.py in &lt;module&gt;
      1 for height in range(len(height_list)):
----&gt; 2     BrideGroomHeight = height_list[height].rstrip(height_list[height][-1])
      3     foot_int = int(BrideGroomHeight[0])
      4     inch_int = int(BrideGroomHeight[2:4])
      5     print(foot_int)

AttributeError: 'float' object has no attribute 'rstrip'
</code></pre>
<p>There are some nan values, due to which I am not able to perform this operation.</p>
",9149925.0,-1.0,N/A,2022-03-26 08:49:51,How to extract numbers from a DataFrame column in python?,<python><pandas><numpy><data-science><data-cleaning>,2,1,N/A,CC BY-SA 4.0
74339180,1,74339402.0,2022-11-06 19:43:11,0,47,"<p>I'm trying to sort a small df by values in two columns, but I need different sorting orders (ascending/descending) for column A (price) depending on the values in column B (action).</p>
<p>So the df looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>dfindex</th>
<th>price</th>
<th>action</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>0.9</td>
<td>'sell'</td>
</tr>
<tr>
<td>7</td>
<td>0.7</td>
<td>'buy'</td>
</tr>
<tr>
<td>2</td>
<td>0.9</td>
<td>'buy'</td>
</tr>
<tr>
<td>3</td>
<td>0.4</td>
<td>'sell'</td>
</tr>
<tr>
<td>6</td>
<td>0.6</td>
<td>'sell'</td>
</tr>
<tr>
<td>5</td>
<td>0.8</td>
<td>'buy'</td>
</tr>
<tr>
<td>1</td>
<td>0.7</td>
<td>'buy'</td>
</tr>
<tr>
<td>8</td>
<td>0.9</td>
<td>'buy'</td>
</tr>
</tbody>
</table>
</div>
<p>My current sorting is via pandas:</p>
<pre><code>tx_hist = tx_hist.sort_values(by=['dfindex', 'price'], ascending=[True, False], ignore_index=True)

</code></pre>
<p>I've also tried a separate sorting function, which takes the 'buy' prices separately, sorts them and inserts them back in the df, but I can't make it work as intended either.</p>
<pre><code>def sorttxhist(tx_hist):
    for i in range(len(tx_hist)):
        w = i
        buytxs = []
        selltx = []
        if tx_hist['action'].iloc[w] == 'buy':
            while tx_hist['action'].iloc[w] == 'buy':
                print(buytxs)
                buytxs.append(tx_hist['price'].iloc[w])
                w = w + 1
            buytxs = buytxs.sort(reverse=True)
            tx_hist['price'].iloc[i:w] = buytxs
            #buytxs.clear()

        elif tx_hist['action'].iloc[w] == 'sell':
            while tx_hist['action'].iloc[w] == 'sell':
                print(selltx)
                selltx.append(tx_hist['price'].iloc[w])
                w = w + 1
            selltx = selltx.sort(reverse=False)
            tx_hist['price'].iloc[i:w] = selltx
            print(type(selltx))
    print(tx_hist)
    return tx_hist

</code></pre>
<p>But it doesn't sort it how I need it, which is a first sort by 'dfindex', and a secondary sort in descending order (price-wise) for 'buy' (in the 'action' col) and in ascending order (price-wise) for 'sell' (in the 'action' col).</p>
<p>So the outcome should look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>dfindex</th>
<th>price</th>
<th>action</th>
<th>Sorting order (not in the df)</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>0.9</td>
<td>'buy'</td>
<td>desc</td>
</tr>
<tr>
<td>1</td>
<td>0.7</td>
<td>'buy'</td>
<td>desc</td>
</tr>
<tr>
<td>3</td>
<td>0.4</td>
<td>'sell'</td>
<td>asc</td>
</tr>
<tr>
<td>4</td>
<td>0.9</td>
<td>'sell'</td>
<td>asc</td>
</tr>
<tr>
<td>5</td>
<td>0.8</td>
<td>'buy'</td>
<td>- / desc</td>
</tr>
<tr>
<td>6</td>
<td>0.6</td>
<td>'sell'</td>
<td>- / asc</td>
</tr>
<tr>
<td>8</td>
<td>0.9</td>
<td>'buy'</td>
<td>desc</td>
</tr>
<tr>
<td>7</td>
<td>0.7</td>
<td>'buy'</td>
<td>desc</td>
</tr>
</tbody>
</table>
</div>
<p>Any help is much appreciated!
Cheers</p>
",14954694.0,-1.0,N/A,2022-11-06 20:18:20,Have different sorting orders in single column in pandas,<python><pandas><sorting><data-science>,1,0,N/A,CC BY-SA 4.0
71628260,1,-1.0,2022-03-26 13:00:06,1,25,"<p>pd.set_option(&quot;precision&quot;, 2)</p>
<p>pd.options.display.float_format = '{:.2f}'.format</p>
<p>Im not able to figure out what these code line do</p>
",17189668.0,-1.0,N/A,2022-03-26 13:20:26,"can anyone explain me these pandas code, Im looking at a EDA project on a Banking Data",<python><pandas><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
71628304,1,-1.0,2022-03-26 13:05:49,1,67,"<p>How does pandas' DataFrame.interpolation() work in relation to the amount of rows it considers:</p>
<ol>
<li>is it just the row before the NaNs and the row right after?</li>
<li>Or is it the whole DataFrame (how does that work at 1 million rows?)</li>
<li>Or another way (please explain)</li>
</ol>
<p>Edit:
(with method=='polynomial' ideally)</p>
",15347019.0,15347019.0,2022-03-26 15:33:53,2022-03-26 15:37:59,how many rows does interpolation consider?,<python><pandas><data-science><interpolation><data-cleaning>,1,0,N/A,CC BY-SA 4.0
71627658,1,-1.0,2022-03-26 11:30:04,0,83,"<p>I have two pandas dataframes and some of the values overlap and I'd like to append to the original dataframe if the time_date value and the origin values are the same.</p>
<p>Here is my original dataframe called flightsDF which is very long, it has the format:</p>
<pre><code>year    month   origin  dep_time   dep_delay    arr_time    time_hour
2001    01       EWR       15:00       15         17:00     2013-01-01T06:00:00Z
</code></pre>
<p>I have another dataframe weatherDF (much shorter than flightsDF) with some extra infomation for some of the values in the original dataframe</p>
<pre><code>origin  temp    dewp    humid   wind_dir    wind_speed  precip  visib   time_hour
0   EWR     39.02   26.06   59.37   270.0   10.35702    0.0     10.0    2013-01-01T06:00:00Z
1   EWR     39.02   26.96   61.63   250.0   8.05546     0.0     10.0    2013-01-01T07:00:00Z
2   LGH     39.02   28.04   64.43   240.0   11.50780    0.0     10.0    2013-01-01T08:00:00Z
</code></pre>
<p>I'd like to append the extra information (temp, dewp, humid,...) from weatherDF to the original data frame if <strong>both</strong> the time_hour and origin match with the original dataframe flightsDF</p>
<p>I have tried</p>
<pre><code>for x in weatherDF:
    if x['time_hour'] == flightsDF['time_hour'] &amp; flightsDF['origin']=='EWR':
        flights_df.append(x)
</code></pre>
<p>and some other similar ways but I can't seem to get it working, can anyone help?</p>
<p>I am planning to append all the corresponding values and then dropping any from the combined dataframe that don't have those values.</p>
",18587835.0,-1.0,N/A,2022-03-26 11:49:27,Appending from one dataframe to another dataframe (with different sizes) when two values match,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
74348608,1,74349128.0,2022-11-07 14:58:21,-1,79,"<p>I have been working on a code to help automate one of the processes in the lab I work at. To summarize the entire purpose of the code, I am creating a way to take experiment data, create a file of data, and send it to the website host that retains all of this information. I'm having some trouble figuring out how to create a dictionary that will fill in sample ID's from a certain, start and end date. Here is the section of the code I'm having issues with:</p>
<pre><code>def get_vcf(run_name, start_date, end_date, auth_token):
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'Authorization': auth_token,
    }

    params = {
        'format': 'json',
        'name': run_name,
        'start_date': start_date,
        'end_date': end_date,
    }

    response = requests.get('https://ionreporter.thermofisher.com/api/v1/getvcf', params=params, headers=headers,
                            verify=False)
    # todo this will return everything in the date range, we will want a dictionary for each of the the sampleIDs
    # todo paired with the VCFs it goes with that we will send to XXX, or some csv like list of the samples-
    vcfs = response.json()
    # todo dictionary construction
    vcf_dict = {0: vcfs[0]}
    return vcfs, vcf_dict
</code></pre>
<p>A sample record from the result of the web query looks like this:</p>
<pre><code>[
    {
        &quot;data_links&quot;: &quot;ionreporter.thermofisher.com/api/v1/download? filePath=/data/IR/data/IR_Org/ion.reporter@lifetech.com/JohnSmithSample /Sample_20160429014705727/Sample_c150_2016-04-29-14-16-534.zip&quot;, 
        &quot;name&quot;: &quot;Sample_c150_2016-04-29-14-16-534&quot;,
        &quot;id&quot;: &quot;ff808181545d90790154613336be0008&quot;
    }
]
</code></pre>
<p>At the bottom, I began creating the dictionary. I understand that this would only return the first response from vcfs. I'd like for it to fill with whatever the amount of sample information we have for the date range, since it will vary. Thanks in advance!</p>
",20160057.0,7631480.0,2022-11-07 15:27:19,2022-11-07 16:01:14,How to create a dictionary that will automatically be filled in from sample ID's I need?,<python><dictionary><data-science><export-to-csv><biopython>,1,2,N/A,CC BY-SA 4.0
71634343,1,-1.0,2022-03-27 07:11:50,0,70,"<p>I have the dataframe:</p>
<pre><code>df = b_150 h_200 b_250 h_300 b_350 h_400  c1  c2 q4
       1.    2.    3.     4    5.    6.   3.  4.  4
</code></pre>
<p>I want to add rows with possible shuffles between values of b_150, b_250, b_350 and h_200, h_300, h_400</p>
<p>So for example</p>
<pre><code>df = add_shuffles(df, cols=[b_150, b_250, b350], n=1)
df = add_shuffles(df, cols=[h_200, h_300, h_400], n=1)
</code></pre>
<p>I will add 2 combinations (1 for l1 and one for l2) to get:</p>
<pre><code>df = b_150 h_200 b_250 h_300 b_350 h_400   c1  c2 q4
       1.    2.    3.     4    5.    6.    3.  4.  4
       3.    2.    5.     4    1.    6.    3.  4.  4 
       1.    2.    3.     6    5.    4.    3.  4.  4
</code></pre>
<p>What is the most efficient way to do it?</p>
",6057371.0,6057371.0,2022-03-27 10:14:10,2022-03-27 14:48:42,pandas dataframe add rows that are shuffle of values of specific columns,<python><pandas><dataframe><data-science><data-munging>,1,2,N/A,CC BY-SA 4.0
71636236,1,71906427.0,2022-03-27 12:16:05,1,647,"<p>I have a data frame and I wanted to generate a new column for colour codes which stars from red for the least value of <strong>Opportunity</strong> and moves toward green for highest value of <strong>Opportunity</strong></p>
<p>My Data Frame -</p>
<pre><code>State       Brand       DYA  Opportunity    

Jharkhand   Ariel     0.15   0.00853    
Jharkhand   Fusion    0.02   0.00002
Jharkhand   Gillett   0.04   -0.0002
</code></pre>
",16647912.0,-1.0,N/A,2022-04-18 00:13:48,Add a new column for color code from red to green based on the increasing value of Opportunity in data frame,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
71639825,1,71639875.0,2022-03-27 19:55:57,1,51,"<p>I am working on converting a dataset into <a href=""https://github.com/activeloopai/Hub"" rel=""nofollow noreferrer"">Activeloop Hub format</a>. The dataset I am working with has <code>NaN</code> values however I am not sure how to handle these values with the Hub dataset format.</p>
<p>The <code>NaN</code> values are appearing in the labels of the dataset.</p>
<p>I know that <code>NaN</code> value represents the absence of that value in the database. Also, from some reading, I know that sklearn implemented algorithms can’t perform on datasets that have such values. I was thinking of erasing the rows that have the <code>NaN</code> values however I don't want to lose any information in the dataset.</p>
<p>Is there a best practice way to input <code>NaN</code> values in Activeloop Hub format?</p>
<p>I am using Hub version 2.3.1.</p>
",18513914.0,18513914.0,2022-03-28 14:05:54,2022-03-28 14:05:54,How to handle NaN values in Activeloop Hub datasets?,<python><machine-learning><dataset><data-science><hub>,1,0,2022-03-28 06:36:56,CC BY-SA 4.0
74355028,1,-1.0,2022-11-08 02:19:15,0,60,"<p>I have multiple data sources of financial data that I want to parse into a common data model.</p>
<ul>
<li>API retrieval. Single format from single source (currently)</li>
<li>CSV files – multiple formats from multiple sources
Once cleaned and validated, the data is stored in a database (this is a Django project, but I don’t think that’s important for this discussion).</li>
</ul>
<p>I have opted to use Pydantic for the data cleaning and validation, but am open to other options.</p>
<p>Where I’m struggling is with the preprocessing of the data, especially with the CSVs.</p>
<p>Each CSV has a different set of headers and data structure. Some CSVs contain all information over a single row, while others present in multiple rows. As your can tell, there are very specific rules for each data source based on its origin. I have a dict that maps all the header variations to the model fields. I filter this by source.</p>
<p>Currently, I’m loading the CSV into a Pandas data frame using the group by function break the data up into blocks. I can then loop through the groups, modify the data based on it’s origin, and then assign the data to the appropriate columns to pass into a Pydantic BaseModel. After I did this, it seemed a bit pointless to be using Pydantic, as all the work was being done beforehand.</p>
<p>To make things more reusable, I thought of moving all the logic into the Pydantic BaseModel, passing the raw grouped data into a property, and processing into the appropriate data elements. But, this just seems wrong.</p>
<p>As with most problems, I’m sure this has been solved before. <strong>I’m looking for some guidance on appropriate patterns for this style of processing.</strong> All of the examples I’ve found to date are based on a single input format.</p>
",5555611.0,-1.0,N/A,2022-11-08 02:19:15,Patterns for processing multi source CSVs in Python,<python><csv><data-science><etl><pydantic>,0,3,N/A,CC BY-SA 4.0
71628546,1,71629448.0,2022-03-26 13:42:07,1,499,"<p>I've got a large .csv file (5GB) from UK land registry. I need to find all real estate that has been bought/sold two or more times.</p>
<p>Each row of the table looks like this:</p>
<pre><code>{F887F88E-7D15-4415-804E-52EAC2F10958},&quot;70000&quot;,&quot;1995-07-07 00:00&quot;,&quot;MK15 9HP&quot;,&quot;D&quot;,&quot;N&quot;,&quot;F&quot;,&quot;31&quot;,&quot;&quot;,&quot;ALDRICH DRIVE&quot;,&quot;WILLEN&quot;,&quot;MILTON KEYNES&quot;,&quot;MILTON KEYNES&quot;,&quot;MILTON KEYNES&quot;,&quot;A&quot;,&quot;A&quot;
</code></pre>
<p>I've never used pandas or any data science library. So far I've come up with this plan:</p>
<ol>
<li><p>Load the .csv file and add headers and column names</p>
</li>
<li><p>Drop unnecessary columns</p>
</li>
<li><p>Create hashmap of edited df and find duplicates</p>
</li>
<li><p>Export duplicates to a new .csv file</p>
</li>
<li><p>From my research I found that pandas are bad with very big files so I used dask</p>
</li>
</ol>
<pre><code>df = dd.read_csv('pp-complete.csv', header=None, dtype={7: 'object', 8: 'object'}).astype(str)
df.columns = ['ID', 'Price', 'Date', 'ZIP', 'PropType', 'Old/new', 'Duration', 'Padress', 'Sadress', 'Str', 'Locality', 'Town', 'District', 'County', 'PPDType', 'Rec_Stat']
df.head()
</code></pre>
<ol start=""2"">
<li>After I tried to delete unnecessary columns</li>
</ol>
<pre><code>df.drop('ID', axis=1).head()
</code></pre>
<p>also tried</p>
<pre><code>indexes_to_remove = [0, 1, 2, 3, 4, 5, 6, 7, 14, 15, 16]
for index in indexes_to_remove:
    df.drop(df.index[index], axis=1)
</code></pre>
<p>Nothing worked.</p>
<p>The task is to show the property that has been bought/sold two or more times. I decided to use only address columns because every other column's data isn't consistent (ID - is unique code of transaction, Date, type of offer etc.)</p>
<p>I need to do this task with minimum memory and CPU usage that's why I went with hashmap.</p>
<p>I don't know if there's another method to do this easier or more efficient.</p>
",17406036.0,10693596.0,2022-08-06 04:30:06,2022-08-06 04:30:06,Operating large .csv file with pandas/dask Python,<python><pandas><data-science><dask><dask-dataframe>,1,1,N/A,CC BY-SA 4.0
71632140,1,-1.0,2022-03-26 21:59:55,0,32,"<p>I have the following dataframe:</p>
<pre><code>    Age    Sex Votes Rating
1  &lt; 18   MALE     7    6.6
2  &lt; 18 FEMALE     1    5.0
3 18-29   MALE  2661    8.7
4 18-29 FEMALE   324    8.6
5 30-44   MALE 11480    8.7
6 30-44 FEMALE  1148    8.6
7   &gt;45   MALE  5543    8.6
8   &gt;45 FEMALE   612    8.5
</code></pre>
<p>Basically, those are ratings of a movie. I am thinking about what might be the best way to explain all of the information it contains (both the sex distribution and the age one). I think an appropriate way could be plotting some pie-charts, but I would like to hear others opinions.
Thank you a lot in advance!</p>
<h5>UPDATE</h5>
<p>I also have data about the nationality of the voters:</p>
<pre><code>  Location Votes Rating
1       US  1847    8.6
2   NON-US 15145    8.6
</code></pre>
<p>But I think these should be plotted separately, right?</p>
",18571016.0,-1.0,N/A,2022-03-26 22:41:21,Most appropriate way to graph this data,<r><plot><data-science><pie-chart>,1,0,N/A,CC BY-SA 4.0
71635814,1,-1.0,2022-03-27 11:12:59,-1,104,"<pre><code>df1[&quot;state&quot;] = df1[&quot;place_with_parent_names&quot;].str.split(&quot;|&quot;,expand=True)[2]
</code></pre>
<p>what [2] actually indicate of a string split method.</p>
",13824261.0,13824261.0,2022-03-27 11:15:12,2022-03-27 11:25:44,"Can anyone explain me what actually the value inside third brackets / [2] after str.split(""|"", expand=True) means?",<python><pandas><data-science><data-cleaning><data-wrangling>,2,2,N/A,CC BY-SA 4.0
71641204,1,71641260.0,2022-03-28 00:01:59,0,155,"<pre><code>df &lt;- data.frame(hour=c(5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23),
                 total=c(15507,132129,156909,81306,44413,51448,55308,63542,57564,54031,70319,53345,35137,15509,20134,5183,2554,20,203))


plot(df$hour, df$total)

fit1 &lt;- lm(total~hour, data = df)
fit2 &lt;- lm(total~poly(hour,2, raw = TRUE), data = df)
fit3 &lt;- lm(total~poly(hour,3, raw = TRUE), data = df)
fit4 &lt;- lm(total~poly(hour,4, raw = TRUE), data = df)
fit5 &lt;- lm(total~poly(hour,5, raw = TRUE), data = df)

summary(fit1)$adj.r.squared
summary(fit2)$adj.r.squared
summary(fit3)$adj.r.squared
summary(fit4)$adj.r.squared
summary(fit5)$adj.r.squared
</code></pre>
<p>How do I determine the best fit for regression for my data</p>
<p>How can I calculate for critical points, global maxima and local maxima if any.</p>
<p>Tried using the adjusted r squares as the basis for selection of the best curve but my critical point do not correlate with the curve.</p>
",18477482.0,6574038.0,2022-03-28 04:26:23,2022-03-28 15:22:28,Non-linear fit regression,<r><data-science><non-linear-regression>,1,2,N/A,CC BY-SA 4.0
74350952,1,74350990.0,2022-11-07 18:00:36,0,46,"<p>From my understanding, dropna for pandas in python drops rows with an empty entry.</p>
<pre><code>import pandas as pd
the_data = pd.read_csv(&quot;EnzymeTrainingData.csv&quot;)
the_data.dropna(inplace = True)
seqs = the_data[&quot;protein_sequence&quot;]
amino_numeros = seqs.apply(len)
hands = amino_numeros.apply(lambda n : float(n*(n + 1))/2)
the_pH = the_data[&quot;pH&quot;]


additive = 14
x_train = []
for i in range(len(the_pH)):
    print(i)
    print(the_pH[i])
</code></pre>
<p>But for my code, it says at index 69:
KeyError: 69</p>
<p>Why is this? Is it because I am doing something wrong with the dropna? Or is it something else?</p>
",18924580.0,-1.0,N/A,2022-11-07 18:04:51,Why does dropna not drop na?,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
71635617,1,71635874.0,2022-03-27 10:43:23,1,46,"<p>I have this dataframe with which i am trying to create a new column <strong>rank</strong> on basis of increasing values of column <strong>Opportunity</strong> with pandas</p>
<pre><code>State   Brand       DYA     Opportunity
Delhi   Pampers     -8.58   -1.24139
Delhi   Ariel       0.53    0.04800
Delhi   Fusion      0.68    0.00492
Delhi   Gillette    1.56    0.02073
</code></pre>
<p>required output --</p>
<pre><code>State   Brand       DYA     Opportunity Rank
Delhi   Pampers     -8.58   -1.24139     1
Delhi   Ariel       0.53    0.04800      4      
Delhi   Fusion      0.68    0.00492      2
Delhi   Gillette    1.56    0.02073      3
</code></pre>
",16647912.0,16647912.0,2022-03-27 11:19:35,2022-03-27 11:22:01,How to add a new column rank on based on increasing value of other column in Pandas,<python><pandas><dataframe><data-science>,1,2,N/A,CC BY-SA 4.0
74352490,1,74352610.0,2022-11-07 20:19:41,0,62,"<p>I have the following code that generates the two columns.</p>
<pre><code>import pandas as pd
  
data = {'Group': ['1', '1', '1', '1', '1', '1',
                  '2', '2', '2', '2', '2', '2',
                  '3', '3', '3', '3', '3', '3',
                  '4', '4', '4', '4', '4', '4',],
        'Test1': ['ABC', 'CDE', 'EFG', 'GHI', 'IJK', 'KLM',
                  'MNO', 'OPQ', 'QRS', 'STU', 'UVW', 'WXYZ',
                  'ABC', 'CDE', 'EFG', 'GHI', 'IJK', 'KLM',
                  'MNO', 'OPQ', 'QRS', 'STU', 'UVW', 'WXYZ',],
        'Test2': ['1234','4567', '8910', '1112', '1314', '1415',
                  '1516', '1718', '1920', '2122', '2324', '2526',
                  '2728', '2930', '3132', '3334', '3536', '3738',
                  '2940', '4142', '4344', '4546', '4748', '4950'],
        'Value': [True, True, False, False, False, True,
                  True, True, True, True, True, True,
                  True, True, True, True, True, False,
                  True, True, True, False, True, True,],
        }
  
df = pd.DataFrame(data)

print(df)
</code></pre>
<p>So, by checking the last 2, 3, or 4 rows in each group if they return False, I want to return False. And if all the values are True then, I want to return true for all rows. From the above code, the expected outcome is this. If we check for the last 3 rows in each group</p>
<pre><code>Group | Value
----- | -----  
  1   |   False 
  1   |   False
  1   |   False
  2   |   True
  2   |   True
  2   |   True
  3   |   False
  3   |   False
  3   |   False
  4   |   False
  4   |   False
  4   |   False
</code></pre>
",16875907.0,16875907.0,2022-11-08 02:53:41,2022-11-08 03:56:49,How to check different rows values of a column within the same group and return a specific value?,<python><pandas><dataframe><group-by><data-science-experience>,1,0,N/A,CC BY-SA 4.0
71642254,1,71642635.0,2022-03-28 03:56:58,-1,78,"<p>I am trying to have i in range 0 to 20. I have tried a for loop, but then I deleted it due to run time. How would one do it for List comprehension?</p>
<pre><code>    finallinearsystem = [
        [np.transpose(i), np.transpose(pts_3d[i]) , np.dot(-y[i],np.transpose(pts_3d[i]))],
        [np.transpose(pts_3d[i]), np.transpose(i) , np.dot(-x[i],np.transpose(pts_3d[i]))],]
</code></pre>
",14640458.0,14640458.0,2022-03-28 04:10:07,2022-03-28 05:06:59,How do I use list comprehension for slicing in a 2d matrix with multiple entries?,<python><data-science>,2,1,N/A,CC BY-SA 4.0
74357991,1,-1.0,2022-11-08 08:55:06,-2,706,"<p><code>filterYear = data['Year'] == 1970</code></p>
<p>I am getting an error of</p>
<p>1 filterYear = data['Year'] == 1970</p>
<p>TypeError: list indices must be integers or slices, not str</p>
<p>I tried to see the datatype of the series and its numeric.</p>
<p>I am at a loss</p>
",17538232.0,-1.0,N/A,2022-11-08 09:07:28,"TypeError: list indices must be integers or slices, not str in Global terrorism dataset",<python><data-science><exploratory-data-analysis>,3,0,2023-11-13 07:43:34,CC BY-SA 4.0
71646907,1,-1.0,2022-03-28 11:43:05,0,55,"<p>I have a dataframe and I want to create and populate the column with values inside the function <code>process()</code>.</p>
<pre><code>import random
import pandas as pd

df = pd.DataFrame()

def process():
    global df
    df['z'] = random.randint(0, 100)
    
for i in range(5):
    process()

print(df)
</code></pre>
<p>The expected output:</p>
<pre><code>    z
0   21
1   83
2   29
3   10
4   43
</code></pre>
<p>Currently I get an empty dataframe with column <code>z</code> printed.</p>
<p>Update:</p>
<p>The following line will create and populate the column values.</p>
<pre><code>df.loc[len(df), 'z'] = random.randint(0, 100)
</code></pre>
",18276331.0,18276331.0,2022-03-28 15:13:57,2022-03-28 15:25:18,Populating pandas column inisde a function,<python><pandas><dataframe><data-science>,2,7,N/A,CC BY-SA 4.0
74363948,1,-1.0,2022-11-08 16:19:17,-1,82,"<p>I have like a list of coin id's available from Coingecko.com in sheet1 and want the output of the api address (<a href=""https://api.coingecko.com/api/v3/coins/bitcoin/market_chart/range?vs_currency=eur&amp;from=1392577232&amp;to=1422577232"" rel=""nofollow noreferrer"">https://api.coingecko.com/api/v3/coins/bitcoin/market_chart/range?vs_currency=eur&amp;from=1392577232&amp;to=1422577232</a>) in a sheet 2.</p>
<p>But as I have a lot of coin id's in the sheet1 and the list will change after time, I would like to have like a loop in the power query settings to output the coins prices in sheet2 automatically.</p>
<p>Until now I need to configure each api address through changing the /bitcoin/ term in the api address and then going through the settings in power query for each coin.</p>
<p>Does anyone know an approach?</p>
<p><a href=""https://i.stack.imgur.com/Ldz2e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ldz2e.png"" alt=""sheet1"" /></a>
<a href=""https://i.stack.imgur.com/HB1zl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HB1zl.png"" alt=""sheet2"" /></a></p>
<p>I tried to manipulate the code by myself but as I am not really familiar with Power Query M, there is always an error. I also tried to create the sheet1 as a list in power query, but not 100% confident with it neither.</p>
",20120795.0,4294399.0,2022-11-08 16:22:44,2022-11-11 12:25:28,Excel Power Query loop for list of API's (coingecko.com),<excel><vba><data-science><powerquery>,1,0,N/A,CC BY-SA 4.0
74367461,1,-1.0,2022-11-08 21:41:28,0,37,"<p>I have a very simple Pandas dataframe:</p>
<pre><code>Revenue     City
27          &quot;New York&quot;
59          &quot;New York&quot;
52          &quot;New York&quot;
34          &quot;London&quot;
14          &quot;London&quot;
24          &quot;London&quot;
45          &quot;Tokyo&quot;
54          &quot;Los Angeles&quot;
24          &quot;Los Angeles&quot;
</code></pre>
<p>I would like to remove all duplicates in 'City' column, <strong>except</strong> if the city name <strong>is not</strong> in the following list : `[&quot;Los Angeles&quot;, &quot;New York&quot;]. The expected output is :</p>
<pre><code>Revenue     City
27          &quot;New York&quot;
34          &quot;London&quot;
14          &quot;London&quot;
24          &quot;London&quot;
45          &quot;Tokyo&quot;
54          &quot;Los Angeles&quot;
</code></pre>
<p>So far I found this, but I don't think it solves the same problem as mine and it doesn't work anyway :</p>
<pre><code>mask = [&quot;Los Angeles&quot;, &quot;New York&quot;]
df = df.loc[(df['City'].duplicated(keep=False) == False) | (~df['City'].isin(mask)]
</code></pre>
",16661695.0,-1.0,N/A,2022-11-08 22:03:00,How to drop duplicates based on value in dataframe column?,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
71640092,1,-1.0,2022-03-27 20:35:48,0,68,"<p>How do i create a new column in data frame that will say &quot;Cheap&quot; if the price is below 50000, &quot;Fair&quot; is the price is between 50000 and 100000 and &quot;Expensive&quot; if the price is over 100000<a href=""https://i.stack.imgur.com/KdE9R.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",18600745.0,-1.0,N/A,2022-03-27 21:30:05,Create a new column in a dataframe pandas,<python-3.x><pandas><dataframe><data-science>,4,0,N/A,CC BY-SA 4.0
71643384,1,71643707.0,2022-03-28 06:46:36,0,86,"<p>So, the Indian Meteorological Sub-Divisions are different from the state boundaries. One large state may have multiple Sub-Divisions and a few small states together can for one single Sub-Division. For example the state of Maharashtra has 4 Sub-Divisions where as the 4 states of Nagaland, Manipur, Mizoram and Tripura together form 1 Sub-Division. The problem is that i could not find the list of districts that fall into each Sub-Division. If i do have the district data then it would be easy as in we categorically group districts that are part of the same Sub-Division. I dont know how to go about this.</p>
",18604895.0,-1.0,N/A,2022-03-28 07:21:24,How to plot Indian Meteorological Sub-Divisions on the map?,<pandas><data-science><data-visualization><geopandas><folium>,1,0,N/A,CC BY-SA 4.0
71648626,1,-1.0,2022-03-28 13:52:22,-3,267,"<pre class=""lang-py prettyprint-override""><code>
def compute_statistics(age_and_salary_data):  
    histograms(age_and_salary_data)
    age = age_and_salary_data.column(&quot;Age&quot;)
    salary = age_and_salary_data.column(&quot;Salary&quot;)
    return make_array(np.mean(age), np.mean(salary))
    

full_stats = compute_statistics(full_data)
full_stats

</code></pre>
<p>This code is to:
Create a function called compute_statistics that takes a Table containing ages and salaries and:</p>
<p>Draws a histogram of ages
Draws a histogram of salaries
Return a two-element list containing the average age and average salary</p>
",18271583.0,18271583.0,2022-03-28 13:56:53,2022-03-28 13:59:03,Output 'DataFrame' object has no attribute 'column' (Python in Jupyter notebook),<python><dataframe><jupyter-notebook><data-science>,2,1,N/A,CC BY-SA 4.0
74368342,1,74368790.0,2022-11-08 23:32:10,0,427,"<p>I am new to R,<br />
I have 3 columns named A1, A2, ChangeInA that looks like this in a dataset</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>A1</th>
<th>A2</th>
<th>ChangeInA</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>20</td>
<td>10</td>
</tr>
<tr>
<td>24</td>
<td>30</td>
<td>24</td>
</tr>
<tr>
<td>22</td>
<td>35</td>
<td>35</td>
</tr>
<tr>
<td>54</td>
<td>65</td>
<td>65</td>
</tr>
<tr>
<td>15</td>
<td>29</td>
<td>15</td>
</tr>
</tbody>
</table>
</div>
<p>The column 'ChangeInA' is either (A1 or A2)</p>
<p>I want to determine the number of times the 3rd column ('ChangeInA') changes.<br />
Is there any function in R to do that?</p>
<p>Let me explain:
From the table, we can see that the 'ChangeInA' column switched twice,
first at row 3 and it switched again at row 5 (note that 'ChangeInA' can only have values of A1 or A2) so I want an R function to print how many times the switch happened. I can see the change on the dataset but I need to prove it on R</p>
<p>Below is a code I tried from previous answers</p>
<pre><code>change&lt;- rleid(rawData$ChangeInA == rawData$A1)
</code></pre>
<ul>
<li>This showed me all the ChangeInA</li>
</ul>
<pre><code>change&lt;- max(rleid(rawData$ChangeInA == rawData$A1))
</code></pre>
<ul>
<li>This showed me the maximum number in ChangeInA</li>
</ul>
",6686671.0,6686671.0,2022-11-09 12:23:09,2022-11-09 16:50:54,R function to count number of times when values changes,<r><dataframe><data-science>,1,4,N/A,CC BY-SA 4.0
74368519,1,-1.0,2022-11-09 00:00:40,0,57,"<p>I'm trying to read the <em>Train File</em> directly into a pandas dataframe from the link address instead of downloading to my local computer then reading.</p>
<p>The website is:</p>
<p><a href=""https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/download/#ProblemStatement"" rel=""nofollow noreferrer"">https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/download/#ProblemStatement</a></p>
<p>The link address when you right click the <em>Train File</em> at the bottom of the page is:</p>
<p><a href=""https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/download/train-file"" rel=""nofollow noreferrer"">https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/download/train-file</a></p>
<p>I tried:</p>
<pre><code>import pandas as pd

url = 'https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/download/train-file'
df = pd.read_csv(url)
</code></pre>
<p>The error is:</p>
<pre><code>HTTPError: HTTP Error 403: Forbidden
</code></pre>
<p>I also tried using <code>requests</code> to download the CSV then reading it from my local computer, but I couldn't get that to work either.</p>
",19746738.0,19746738.0,2022-11-09 00:06:42,2022-11-09 22:53:18,How do I read a CSV directly into a pandas dataframe from a download link button?,<python><pandas><csv><data-science>,1,0,N/A,CC BY-SA 4.0
74364407,1,74364864.0,2022-11-08 16:53:55,1,89,"<p>I am trying to remove some NA's in one variable depending on the values gotten in a previous variable but I haven't managed to succeed...</p>
<p>Thanks in advance!</p>
<p>There are two variables: <code>info_geo</code> and <code>info_geo_actzd</code>. If the answer in the former one is &quot;No&quot;, I want the answers in <code>info_geo_actzd</code> to change from &quot;NA&quot; to &quot;No&quot;. The rest of the answers I want to leave them the same as now</p>
<p>I have tried various ways but I can't do it properly</p>
<pre><code>db &lt;- db %&gt;%
    mutate(info_geo_actzd = case_when(
      info_geo == 'No' ~ &quot;No&quot;,
      TRUE ~ .))
</code></pre>
<pre><code>db &lt;- db %&gt;% 
  mutate(info_geo_actzd=ifelse('info_geo' == 'No' &amp; 'info_geo_actzd' == 'NA', No))
</code></pre>
<p>I've also tried this but no result</p>
<pre><code>db &lt;- db  %&gt;%
  mutate(info_geo_actzd = case_when(
    any(info_geo == 'No') ~ 'NA',
    TRUE ~ .))

</code></pre>
<p>Thanks in advance</p>
",19642403.0,15293191.0,2022-11-08 17:21:16,2022-11-08 18:07:30,Trying to change values in one value depending on values on another variable,<r><dplyr><data-science><data-wrangling>,1,0,N/A,CC BY-SA 4.0
74368090,1,-1.0,2022-11-08 22:57:53,0,19,"<p>Equations:</p>
<p>$ E_1 = \sum_{j=0}^{J-1} a_j e^{-i(j\Delta+w_o )t}$</p>
<p>$E_2 = e^{-iw_ot} \sum_{m=J}^{J+M} e^{-im\Delta t}(a_m+b_m e^{-iw_ot})$</p>
<p>code:</p>
<pre><code>%matplotlib inline


import random
import numpy as np
import matplotlib.pyplot as plt
from numpy.fft import ifft, fftshift

tstart = -10e-9
tstop = 10e-9
delta = 31.6e6 #rep rate
wo = 5e6    #offset frequency
i = 1j


aj = np.array([1 for i in range(500)]) # no of comblines
t = np.linspace (tstart,tstop, 1000)
E1 = np.linspace(0,0,1000).astype(&quot;complex&quot;) # PSD

for s in range(len(t)):
    for k in range(len(aj)):
        E1[s]+= aj[k]**2*(np.exp(-i*(k*delta+wo)*t[s]))
        
###########################################################

am = np.array([1 for i in range(500)])
bm = np.array([1 for i in range (500)])# no of comblines
t2 = np.linspace (tstart,tstop, 1000)
E2 = np.linspace(0,0,1000, dtype = &quot;complex_&quot;) # PSD
m = np.array([i for i in range (500,1000,1)])

for s2 in range(len(t)):
    for k2 in range(len(m)):
        E2[s2]+= (np.exp(-i*wo*t[s2]))*[np.exp(-i*(m[k2]*delta)*t[s2])*(am[k2]+bm[k2]*np.exp(-i*(wo)*t[s2]))]


Then I want to add E1 and E2
</code></pre>
<p>I am try to simulate two equations.
superposition (addition) of these two equations should give me interference beat signal.</p>
",20454208.0,-1.0,N/A,2022-11-09 08:36:34,How to I simulate these equations. So far I have written code but this gives give me errors,<python><pandas><matplotlib><data-science>,1,0,N/A,CC BY-SA 4.0
71656799,1,-1.0,2022-03-29 05:15:45,0,74,"<p>A dataset with information around messages sent between users in a P2P messaging application. Below is the dataset's schema:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column Name</th>
<th>Data Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>date</td>
<td>string</td>
<td>date of the message sent/received,format is 'YYYY-mm-dd'</td>
</tr>
<tr>
<td>timestamp</td>
<td>integer</td>
<td>timestamp of the message sent/received, epoch seconds</td>
</tr>
<tr>
<td>sender_id</td>
<td>integer</td>
<td>id of the message sender</td>
</tr>
<tr>
<td>receiver_id</td>
<td>integer</td>
<td>id of the message receiver</td>
</tr>
</tbody>
</table>
</div>
<p>My requirement is to find the fraction of messages that are sent between the same sender and receiver within five minutes (e.g. the fraction of messages that receive a response within 5 minutes).</p>
<p>I have tried</p>
<p><code>`</code>
<code>df = df.sort_values(&quot;timestamp&quot;, ascending=True)</code></p>
<p><code>df['timegap'] = df['timestamp'].diff()</code></p>
<p><code>df['timegap'] = df['timegap'].fillna(0)</code></p>
<p><code>df['gapinminutes'] = df['timegap']/60</code></p>
<p><code>df1 = df[df.gapinminutes &lt; 5]</code></p>
<p><code>df1.reset_index(drop=True,inplace=True)</code></p>
<p><code>conversation = 0</code></p>
<p><code>for i in range(0,len(df1)-1):</code></p>
<p><code>if df1['sender_id'][i] ==  df1['receiver_id'][i+1] and df1['sender_id'][i+1] ==  df1['receiver_id'][i] :</code></p>
<p><code>conversation = conversation+1</code></p>
<p><code>fraction = conversation/len(df)</code></p>
<p>Being a novice developer - I need to know if my solution is correct for the requirement and is there a better/simple way to do it</p>
",18616013.0,18616013.0,2022-03-29 05:21:35,2022-03-29 05:58:56,Find the fraction of messages that are sent between the same sender and receiver within five minutes,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
74338808,1,-1.0,2022-11-06 18:53:20,0,474,"<p>I have a time-series dataset which is having 10000 samples and 50 features, I want to use ConvLSTM for prediction for that I am using TensorFlow for implementation but I am getting the below error</p>
<pre><code>Input 0 of layer &quot;conv_lstm1d_10&quot; is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 1, 10000)
</code></pre>
<p>The model that I have made is below</p>
<pre><code>model = tf.keras.models.Sequential(
    [
        tf.keras.layers.ConvLSTM1D(filters=64, kernel_size=3, activation='relu', input_shape=(1,len(df[features]))),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.MaxPool1D(),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(1, activation='linear'),
    ]
)
</code></pre>
<p>I am confused now with the input size so let me know what should be the input size.</p>
",13378134.0,1740577.0,2022-11-07 20:35:22,2022-11-07 20:35:22,What should be the input shape for ConvLSTM1D,<python><tensorflow><data-science>,0,5,N/A,CC BY-SA 4.0
71621791,1,76064214.0,2022-03-25 18:49:40,1,402,"<p>So I got this chart which displays a time series in X.Axis using yearmonth('date'):T. The thing is, it duplicates the labels to fill the spaces, but I don't want that behaviour. I'd like a single label each time. How can I do this?</p>
<p><img src=""https://i.stack.imgur.com/QhmyR.png"" alt=""The chart"" /></p>
<pre><code>example df:
ID      date        total
3425    2022-01-04  30
15161   2022-01-12  1730
18192   2022-01-13  1526
21014   2022-01-21  99
24692   2022-02-07  28
30163   2022-02-21  175
31531   2022-02-22  446
34467   2022-03-22  57
34880   2022-03-23  500
35144   2022-03-30  41
</code></pre>
",17563557.0,17563557.0,2022-03-25 19:26:34,2023-04-27 20:57:31,Avoid X axis labels repetition in Altair chart,<python><data-science><data-visualization><altair>,1,3,N/A,CC BY-SA 4.0
71621984,1,-1.0,2022-03-25 19:06:54,0,53,"<pre><code>accuracy=accuracy_score(y_test,prediction)
print(&quot;accuracy: %.2f%%&quot; % (accuracy*100.0))
</code></pre>
<p>I tried with this code but it gives me type error</p>
",15602240.0,1367454.0,2022-03-25 19:11:33,2022-03-25 19:14:36,What is type error while finding the accuracy_score?,<python><jupyter-notebook><data-science>,0,10,N/A,CC BY-SA 4.0
71629260,1,71629296.0,2022-03-26 15:12:43,0,44,"<p>I have a dataFrame(just a column) which has vehicle brands and its models like, <code>Toyota Rav4</code>, <code>Kia Soul</code>, (brand and models at the same column), I want to show all of Volvo's models.
Output should be like that,</p>
<pre><code>Volvo xc90
Volvo xc60
Volvo V90
.
.
.
</code></pre>
<p>What is the best coding?</p>
",15926654.0,15926654.0,2022-03-26 15:16:02,2022-03-26 15:19:18,Data Frame with unique pandas,<python><pandas><numpy><data-science>,2,2,N/A,CC BY-SA 4.0
74347701,1,74347724.0,2022-11-07 13:53:42,1,923,"<p>I have the dataframe:</p>
<pre><code>df = batch Code
      a     100
      a     120
      a     130
      a     120 
      b     140
      b     150
      c     100
</code></pre>
<p>I want to add a column 'add_code' that will be the value of the column 'Code' from the next row, per batch.
So the output will be:</p>
<pre><code>df = batch Code next_code
      a     100    120
      a     120    130
      a     130    120
      a     120    END
      b     140    150
      b     150    END
      c     100    END
</code></pre>
<p>What is the best way to do it?</p>
",6057371.0,-1.0,N/A,2022-11-07 15:19:40,Pandas add a column of value of next row in another column (per group),<pandas><dataframe><data-science><data-munging>,2,0,N/A,CC BY-SA 4.0
74355361,1,-1.0,2022-11-08 03:22:45,1,269,"<p>I have executed &gt;1000 drug molecules in a Vina docking study. Now I have &gt;1000 log files that contain the top ten binding scores for every compound in a log file for each. I want to extract only the lowest binding energy from each log file and get it as an output.txt file which must contain drug ID (file Name) in first column and lowest binding energy in second column. Can anybody show me the python script for this kind of workflow?</p>
<p>Output file is just look like this, I want to extract the lowest energy value that is the first negaitve value (Ex: -3.7 kcal/mol) in the present example. There are &gt;1000 this kind log files in a folder. I want to extract the lowest energy value from all the log files and make it asx: a vector for further analysis.</p>
<pre><code>Ex:
#################################################################
# If you used AutoDock Vina in your work, please cite:          #
#                                                               #
# O. Trott, A. J. Olson,                                        #
# AutoDock Vina: improving the speed and accuracy of docking    #
# with a new scoring function, efficient optimization and       #
# multithreading, Journal of Computational Chemistry 31 (2010)  #
# 455-461                                                       #
#                                                               #
# DOI 10.1002/jcc.21334                                         #
#                                                               #
# Please see http://vina.scripps.edu for more information.      #
#################################################################

WARNING: The search space volume &gt; 27000 Angstrom^3 (See FAQ)
Output will be 1_out.pdbqt
Detected 4 CPUs
Reading input ... done.
Setting up the scoring function ... done.
Analyzing the binding site ... done.
Using random seed: -2146285232
Performing search ... done.
Refining results ... done.

mode |   affinity | dist from best mode
     | (kcal/mol) | rmsd l.b.| rmsd u.b.
-----+------------+----------+----------
   1         -3.7      0.000      0.000
   2         -3.6     18.479     19.190
   3         -3.6      3.437      4.976
   4         -3.6      1.802      2.411
   5         -3.6      2.650      3.645
   6         -3.5      2.071      3.134
   7         -3.5      2.069      3.501
   8         -3.4      1.638      2.141
   9         -3.4      1.617      2.511
  10         -3.3     18.549     19.217
Writing output ... done.
</code></pre>
<p>I tried the python os modul following other people's scripts, but couldn't get what I need.</p>
<p>This was the script I followed so far,</p>
<pre><code>import os
import os.path
import glob
import itertools
import collections
import pprint
import sys
import fnmatch


#get path of current dir

mypath = os.getcwd()

print (&quot;\nDirectory path detected \n&quot;)


#get compound name

comp_name = sys.argv[1]


#read all filenames in the dir

file_list = os.listdir(mypath)


#collecting the total number of log files in the directory.

num_files = len(glob.glob1(mypath,&quot;*log*.txt&quot;))
print('There are',num_files, 'log files in the current directory\n\n')


#looking for Binding affinity

for file_name in file_list:


    if fnmatch.fnmatch(file_name, '*'+comp_name+'*.txt'):
        with open(os.path.join(mypath, file_name), &quot;r&quot;) as src_file:
                        
            for line in src_file:
                try:
                    if '-+' in line:                                                    #looking for binding affinity table
                        nextline = next(src_file)
                        value = nextline[nextline.find(&quot;-&quot;)+0:].split()[0]                  #split at '-' and print binding affinity including '-'
                        
                except IndexError:
                    continue

print(&quot;The Binding Affinity of &quot;+comp_name+&quot; is : &quot;+value+&quot;\n&quot;)
</code></pre>
",20445662.0,1426065.0,2022-11-08 03:44:22,2022-11-08 03:44:22,How to extract lowest binding energy from thousand Vina dock log files and put in a txt file using python?,<python><database><dataframe><machine-learning><data-science>,0,4,N/A,CC BY-SA 4.0
74373884,1,74384186.0,2022-11-09 11:07:05,0,112,"<p>I'm quite new to python and pandas so I hope I can get some help.</p>
<p>I have a <code>train_df</code> that looks like this:</p>
<pre><code>  x        y1         y2        y3         y4
0   -20.0 -0.702864  10.392012  1.013891 -8794.9050
1   -19.9 -0.591605   9.450884  1.231116 -8667.2340
2   -19.8 -0.983952  10.240055  0.675153 -8541.5720

</code></pre>
<p>And an <code>ideal_df</code> that looks like this:</p>
<pre><code>   x        y1        y2  ...       y48       y49       y50
0   -20.0 -0.912945  0.408082  ... -0.186278  0.912945  0.396850
1   -19.9 -0.867644  0.497186  ... -0.215690  0.867644  0.476954
2   -19.8 -0.813674  0.581322  ... -0.236503  0.813674  0.549129
</code></pre>
<p>Both have 400 rows.</p>
<p>I want to to sum up the squared deviation (distance) between y-values of <code>train_df</code> and <code>ideal_df</code> at each given x-value, e.g.:</p>
<p>For the 1st value of x, <code>y1</code> from <code>train_df</code> and <code>y1</code> from <code>ideal_df</code>, then <code>y1</code> from <code>train_df</code> and <code>y2</code> from <code>ideal_df</code>, etc.</p>
<p>Then repeat the same for every one of the 400 rows of <code>y1</code> from <code>train_df</code>.</p>
<p>After that, repeat it for <code>y2</code>, <code>y3</code>, and <code>y4</code> of train_df, but that is the easy part.</p>
<hr />
<p>I wrote this</p>
<pre class=""lang-py prettyprint-override""><code>squared_deviations_y1_train = (((train_df.y1)-(ideal_df.loc[:,&quot;y1&quot;:&quot;y50&quot;])) ** 2).sum()
</code></pre>
<p>But I have no idea what I'm doing to be honest.</p>
",20458338.0,20458338.0,2022-11-09 15:32:01,2022-11-10 04:33:19,Repeating same operation for multiple columns of another df,<python><pandas><dataframe><data-science><data-analysis>,1,5,N/A,CC BY-SA 4.0
74375757,1,74376135.0,2022-11-09 13:37:33,1,156,"<p>I have a list of lists of strings (Essentially it's a corpus) and I'd like to convert it to a matrix where a row is a document in the corpus and the columns are the corpus' vocabulary.</p>
<p>I can do this with <code>CountVectorizer</code> but it would require quite a lot of memory as I would need to convert each list into a string that in turn <code>CountVectorizer</code> would tokenize.</p>
<p>I think it's possible to do it with Pandas only but I'm not sure how.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>corpus = [['a', 'b', 'c'],['a', 'a'],['b', 'c', 'c']]
</code></pre>
<p>expected result:</p>
<pre><code>| a | b | c |
|---|---|---|
| 1 | 1 | 1 |
| 2 | 0 | 0 |
| 0 | 1 | 2 |
</code></pre>
",19239577.0,-1.0,N/A,2022-11-09 14:11:09,How to transform a list of lists of strings to a frequency DataFrame?,<python><pandas><scikit-learn><nlp><data-science>,3,1,N/A,CC BY-SA 4.0
74379996,1,-1.0,2022-11-09 18:53:55,1,669,"<p>I am creating simple dash app using plotly dash frame work when user dynamically inputs the excel file it takes the the input as excelfile and do some caliculation and cleaning,using pandas.actually iam new to plotly dash. here iam facing trouble by creating call backs and &quot;app.layout&quot; my app is worked fine while uploading the excel file and returns the processessed file as data table. instead of html table output i need the download button for downloading the processed file and also line graph for
x=dh3[&quot;Time_stamp&quot;],y=[&quot;Analog input2&quot;] here is my data which i uploaded dynamically (<a href=""https://docs.google.com/spreadsheets/d/e/2PACX-1vT6bVLJX6gkXuASbU4T1X0zv0dPLmuxTDvQ1MRG9251ncPXKUlbleBdFuhew1sKkluil8NP4P_xT1QA/pub?output=xlsx"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/e/2PACX-1vT6bVLJX6gkXuASbU4T1X0zv0dPLmuxTDvQ1MRG9251ncPXKUlbleBdFuhew1sKkluil8NP4P_xT1QA/pub?output=xlsx</a>) i have bunch of files this type.
i used this code</p>
<pre><code>import datetime
import io
from dash import dcc
import plotly.express as px
import plotly.graph_objs as go

import dash
from dash.dependencies import Input, Output, State
from dash import dcc, html, dash_table

import pandas as pd

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']
colors = {&quot;graphBackground&quot;: &quot;#F5F5F5&quot;, &quot;background&quot;: &quot;#ffffff&quot;, &quot;text&quot;: &quot;#000000&quot;}

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
app.layout = html.Div([
    dcc.Upload(
        id='upload-data',
        children=html.Div([
            'Drag and Drop or ',
            html.A('Select Files'),


        ]),
        style={
            'width': '100%',
            'height': '60px',
            'lineHeight': '60px',
            'borderWidth': '1px',
            'borderStyle': 'dashed',
            'borderRadius': '5px',
            'textAlign': 'center',
            'margin': '10px'
        },
        # Allow multiple files to be uploaded
        multiple=True
    ),
    html.Div(id='output-data-upload'),

])

def parse_contents(contents, filename, date):
    content_type, content_string = contents.split(',')

    decoded = base64.b64decode(content_string)
    try:
        if 'csv' in filename:
            # Assume that the user uploaded a CSV file
            df = pd.read_csv(
                io.StringIO(decoded.decode('utf-8')))
        elif 'xls' in filename:
            # Assume that the user uploaded an excel file
            df = pd.read_excel(io.BytesIO(decoded),skiprows=10,nrows=16, usecols=[12],header=None)
            df2 = pd.read_excel(io.BytesIO(decoded),skiprows=8,usecols=[0,1,2,3,4,5,6,7,8,9,11])

            dfn =df.applymap(lambda x: x.replace(':::5548###[', ' ') if isinstance(x, str) else x)
            dfm = dfn.applymap(lambda x: x.replace(' ]]######', '') if isinstance(x, str) else x)
            dfl = dfm.applymap(lambda x: x.replace('###', '') if isinstance(x, str) else x)
            dfk = dfl.applymap(lambda x: x.replace(',', ' ') if isinstance(x, str) else x)
            df1 = df[12].str.extract(
                r&quot;\[Priority=(\d*)\] \[GPS element=\[X=(-?\d*)\] \[Y=(-?\d*)\] \[Speed=(-?\d*)\] \[Angle=(-?\d*)\] \[Altitude=(-?\d*)\] \[Satellites=(-?\d*)\]]&quot;)
            df1.columns = [&quot;Priority&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Speed&quot;, &quot;Angle&quot;, &quot;Altitude&quot;, &quot;Satellites&quot;]

    except Exception as e:
      print(e)
      return html.Div([
            'There was an error processing this file.'
        ])


    return html.Div([
        html.H5(filename),
        html.H6(datetime.datetime.fromtimestamp(date)),

        dash_table.DataTable(
            df1.to_dict('records'),
            export_format=&quot;csv&quot;
        ),


        html.Hr(),  # horizontal line

        # For debugging, display the raw contents provided by the web browser
        html.Div('Raw Content'),
        html.Pre(contents[0:200] + '...', style={
            'whiteSpace': 'pre-wrap',
            'wordBreak': 'break-all'
        })
    ])
</code></pre>
<p><a href=""https://i.stack.imgur.com/8ncyb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8ncyb.png"" alt=""this out put i got begore adding figure function and callback"" /></a></p>
<p>after adding the figure call back and download button on click getting call back error how to get ride of this and i dont need html table as output the excel file has to be downloaded on button click and graph Time_stamp vs Analog input any suggestions thanks in advance</p>
",19419337.0,-1.0,N/A,2023-06-23 00:08:43,"Plotly Dash, dynamically importing excel file and perform operation on dataframe and plotting the graph and download excel file and graph button click",<python><html><pandas><data-science><plotly-dash>,0,0,N/A,CC BY-SA 4.0
71665961,1,-1.0,2022-03-29 16:50:25,0,80,"<p>I'm trying to do get a tall table (with just 3 columns indicating variable, timestamp and value) into a wide format where timestamp is the index, the columns are the variable names, and the values are the values of the new table.</p>
<p>In python/pandas this would be something along the lines of</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
df = pd.read_csv(&quot;./mydata.csv&quot;) # assume timestamp, varname &amp; value columns
df.pivot(index=&quot;timestamp&quot;, columns=&quot;varname&quot;, values=&quot;value&quot;)
</code></pre>
<p>for PostgreSQL there exists <code>crosstab</code>, so far I have:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM crosstab(
  $$
  SELECT
    &quot;timestamp&quot;,
    &quot;varname&quot;,
    &quot;value&quot;
  FROM mydata
  ORDER BY &quot;timestamp&quot; ASC, &quot;varname&quot; ASC
  $$
) AS ct(
  &quot;timestamp&quot; timestamp,
  &quot;varname1&quot; numeric,
  ...
  &quot;varnameN&quot; numeric
);
</code></pre>
<p>The problem is that I can potentially have dozens to hundreds of variable names. The types are always numeric, number of variable names is not stable (we could need more variables or realize that others are not necessary).</p>
<p>Is there a way to automate the &quot;ct&quot; part so that some other query (e.g. <code>select distinct &quot;varname&quot; from mydata</code>) produces it instead of me having to type in every single variable name present?</p>
<p>PS: PSQL version is 12.9 at home, 14.0 in production. Number of rows in the original table is around 2 million, however I'm going to filter by timestamp and varname, so potentially only a few hundreds of thousands rows. After filtering I got ~50 unique varnames, but that will increase in a few weeks.</p>
",6560267.0,6560267.0,2022-03-30 07:09:32,2022-03-30 07:09:32,"In a PostgreSQL crosstab, can I automate the tuple part?",<postgresql><dataframe><data-science><crosstab>,0,3,N/A,CC BY-SA 4.0
74382678,1,-1.0,2022-11-09 23:52:24,1,23,"<p>I have two data sets:</p>
<p><strong>Ist (AKA &quot;OLD&quot;) [smaller - just a sample]:</strong></p>
<pre><code>Origin   | Alg.Result   | Score
Star123  | Star123      | 100
Star234  | Star200      | 90
Star421  | Star420      | 98
Star578  | Star570      | 95
...      | ...          | ...
</code></pre>
<p><strong>IInd (AKA &quot;NEW&quot;) [bigger - used all real data]:</strong></p>
<pre><code>Origin   | Alg.Result | Score
Star123  | Star120    | 90
Star234  | Star234    | 100
Star421  | Star423    | 98
Star578  | Star570    | 95
...      | ...        | ...
</code></pre>
<p>Those DFs are the results of two different algorithms. Let's call them &quot;OLD&quot; and &quot;NEW&quot;.
The logic of those algorithms is following:</p>
<ul>
<li>it takes value from some table (represented in the column: 'Origin'), and tries to match this value from some different table (outcome represented as a column: Alg. Result). Plus it calculates a score of the match based on some internal logic (column: Score).</li>
</ul>
<p>Additionally important information:</p>
<ol>
<li>I DF (old) is a smaller sample</li>
<li>II DF (new) is a bigger sample</li>
<li>Values in ORIGIN are the same for both datasets, excluding the fact that the old dataset has fewer of them compared to the NEW set.</li>
<li>Values in Alg. Result can:</li>
</ol>
<ul>
<li>be exactly the same as in Origin</li>
<li>can be similar</li>
<li>can be completely something else</li>
</ul>
<ol start=""5"">
<li>In a solution where those algorithms are used, the threshold is used based on SCORE. For OLD it's a Score &gt; 90. For the new, it's the same.</li>
</ol>
<p>What I want to achieve is to:</p>
<ol>
<li>How accurate is the new algorithm?</li>
<li>Validate how accurate is the new approach (&quot;NEW&quot;) in matching values with Origin values.</li>
<li>What are the discrepancies between the OLD and NEW sets:</li>
</ol>
<ul>
<li>which cases the OLD has that the NEW doesn't have</li>
<li>which cases the NEW has, which the OLD doesn't have</li>
</ul>
<p>What kind of comparison would you do to achieve those goals?</p>
<p>I thought about checking:</p>
<ol>
<li>True positive =&gt; by taking NEW dataset with condition NEW.Origin == NEW.Alg.Result and NEW.Score == 100</li>
<li>False positive =&gt; by taking NEW dataset with condition NEW.Origin != NEW.Alg.Result and NEW.Score == 100</li>
<li>False-negative =&gt; by taking NEW dataset with condition NEW.Origin == NEW.Al.Result and NEW.Score != 100</li>
<li>I don't see a sense to count True negatives if the algorithm always generates some match. I'm not sure what this could look like.</li>
</ol>
<p>What else you'd suggest? What to do to compare OLD and NEW values? Do you have some ideas?</p>
",8916474.0,-1.0,N/A,2022-11-09 23:52:24,How to validate two data sets coming from an algorithm to check its effectiveness,<dataset><data-science>,0,0,N/A,CC BY-SA 4.0
74367617,1,-1.0,2022-11-08 21:58:19,0,19,"<p>Create a row that sums the rows that do not have a data in all the columns.</p>
<p>I'm working on a project that keeps throwing dataframes like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">1</th>
<th style=""text-align: center;"">2</th>
<th style=""text-align: center;"">3</th>
<th style=""text-align: center;"">4</th>
<th style=""text-align: center;"">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">108.864</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">INTERCAMBIADORES DE</td>
<td style=""text-align: center;"">1123.60      210.08     166.71     1333.68</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">CALOR 8419500300</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">147.420       5.000</td>
<td style=""text-align: center;"">PZ</td>
<td style=""text-align: center;"">1A0181810000</td>
<td style=""text-align: center;"">81039.25       15149.52    19237.754880        96188.77</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">147.420</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">INTERCAMBIADORES DE</td>
<td style=""text-align: center;"">3882.25      725.75     921.60     4608.00</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">CALOR 8419500300</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">566.093      12.000</td>
<td style=""text-align: center;"">PZ</td>
<td style=""text-align: center;"">1A0183660000</td>
<td style=""text-align: center;"">66187.40       12374.29     6546.806709        78561.68</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">566.093</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">INTERCAMBIADORES DE</td>
<td style=""text-align: center;"">3170.76      592.80     313.63     3763.56</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">CALOR 8419500300</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">3.645       1.000</td>
<td style=""text-align: center;"">PZ</td>
<td style=""text-align: center;"">1A0185890000</td>
<td style=""text-align: center;"">836.64          159.69      996.330339          996.33</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">3.645</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">INTERCAMBIADORES DE</td>
<td style=""text-align: center;"">40.08        7.65      47.73       47.73</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">CALOR 8419500300</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">131.998       3.000</td>
<td style=""text-align: center;"">PZ</td>
<td style=""text-align: center;"">1A0190390000</td>
<td style=""text-align: center;"">32819.41        6135.17    12984.858315        38954.57</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">131.998</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">INTERCAMBIADORES DE</td>
<td style=""text-align: center;"">1572.24      293.91     622.05     1866.15</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">CALOR 8419500300</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">123.833       3.000</td>
<td style=""text-align: center;"">PZ</td>
<td style=""text-align: center;"">1A0190790000</td>
<td style=""text-align: center;"">54769.36       10238.84    21669.402087        65008.21</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">123.833</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">INTERCAMBIADORES DE</td>
<td style=""text-align: center;"">2623.77      490.50    1038.09     3114.27</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">CALOR 8419500300</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">115.214       2.000</td>
<td style=""text-align: center;"">PZ</td>
<td style=""text-align: center;"">1A0195920000</td>
<td style=""text-align: center;"">54642.66       10215.05    32428.851279        64857.70</td>
</tr>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">115.214</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">INTERCAMBIADORES DE</td>
<td style=""text-align: center;"">2617.70      489.36    1553.53     3107.06</td>
</tr>
</tbody>
</table>
</div>
<p>This is going to insert a sql database, I don't know how to add the empty rows with the row that has all the information.
<strong>NOTE:</strong> Spacing Empty cells is variable</p>
",20453265.0,20453265.0,2022-11-09 05:28:39,2022-11-09 05:32:41,Create a row that sums the rows that do not have a data in all the columns pandas,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
71653739,1,-1.0,2022-03-28 21:07:19,1,51,"<p>I have a Pandas data frame with relevant columns A, B, C. For context, there are several C's for every combination of A, B. I would like to compute the number of elements of C that each combination (a_i, b_j) of A, B has in common with the combination (a_0, b_j), where a_0 is fixed.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>C</th>
</tr>
</thead>
<tbody>
<tr>
<td>a_0</td>
<td>b_0</td>
<td>c_0</td>
</tr>
<tr>
<td>a_0</td>
<td>b_0</td>
<td>...</td>
</tr>
<tr>
<td>a_0</td>
<td>b_0</td>
<td>c_m</td>
</tr>
<tr>
<td>a_0</td>
<td>b_1</td>
<td>c_m+1</td>
</tr>
<tr>
<td>a_0</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>a_0</td>
<td>b_n</td>
<td>c_p</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>a_q</td>
<td>b_n</td>
<td>c_q</td>
</tr>
</tbody>
</table>
</div>
<p>I have tried both using for-loops and pandas.Dataframe.apply, but both were taking too long due to the size of the data frame.</p>
",15572520.0,15572520.0,2022-03-29 08:55:50,2022-03-29 08:55:50,How to create a Pandas column to store the number of common elements between two filters of a data frame,<python><pandas><dataframe><numpy><data-science>,0,5,N/A,CC BY-SA 4.0
74375400,1,74375563.0,2022-11-09 13:10:12,-1,1665,"<p>I am new to python and jupyter notebook and I am using windows. Recently I installed Anaconda Navigator 2.3.1 and the python verson 3.9.13 on my computer. After entering the command <code>jupyter notebook</code> on the command-line, my browser doesn't open jupyter notebook, instead that showing me this error message:</p>
<pre><code>(base) C:\Users\USER&gt;jupyter notebook
Traceback (most recent call last):
  File &quot;D:\Anaconda3\anaconda3\Scripts\jupyter-notebook-script.py&quot;, line 6, in &lt;module&gt;
    from notebook.notebookapp import main
  File &quot;D:\Anaconda3\anaconda3\lib\site-packages\notebook\notebookapp.py&quot;, line 79, in &lt;module&gt;
    from .services.contents.manager import ContentsManager
  File &quot;D:\Anaconda3\anaconda3\lib\site-packages\notebook\services\contents\manager.py&quot;, line 17, in &lt;module&gt;
    from nbformat import sign, validate as validate_nb, ValidationError
  File &quot;C:\Users\USER\AppData\Roaming\Python\Python39\site-packages\nbformat\__init__.py&quot;, line 14, in &lt;module&gt;
    from . import v1
  File &quot;C:\Users\USER\AppData\Roaming\Python\Python39\site-packages\nbformat\v1\__init__.py&quot;, line 19, in &lt;module&gt;
    from .nbjson import reads as reads_json, writes as writes_json
  File &quot;C:\Users\USER\AppData\Roaming\Python\Python39\site-packages\nbformat\v1\nbjson.py&quot;, line 19, in &lt;module&gt;
    from base64 import encodestring
ImportError: cannot import name 'encodestring' from 'base64' (D:\Anaconda3\anaconda3\lib\base64.py)
</code></pre>
<p>I don't understand what should do now? And how to solve this issue! Please help..........</p>
",12904389.0,12904389.0,2022-11-09 13:16:02,2022-11-14 09:04:20,"jupyter notebook showing this message, ImportError: cannot import name 'encodestring' from 'base64'",<python><windows><jupyter-notebook><anaconda><data-science>,2,2,N/A,CC BY-SA 4.0
74394252,1,74397358.0,2022-11-10 19:16:19,0,154,"<p>Scikit learn has had _ufuncs errors for the past week and I can't figure out why. I'm using python on anaconda and a Dell computer, while using Jupyter notebooks. It was previously fine until last week when I got these issues</p>
<p><a href=""https://i.stack.imgur.com/OeaQO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OeaQO.png"" alt=""ImportError: DLL load failed while importing _ufuncs: The specified procedure could not be found."" /></a></p>
<p>I've looked on other stack overflow questions and tried them. I've tried uninstalling and reinstalling scipy using pip and conda. Uninstalling and reinstalling anaconda navigator doesn't work. Uninstalling and reinstalling icc_rt didn't work at all either. I got scikit learn to work after uninstalling and reinstalling mkl, but after that it stopped working. Any help would be much appreciated because the only idea I can come up with is to factory reset my computer and see if that works.</p>
",20471739.0,-1.0,N/A,2022-11-11 02:17:32,Scikit Learn _ufuncs error in Jupyter notebooks,<python><scikit-learn><jupyter-notebook><anaconda><data-science>,1,0,N/A,CC BY-SA 4.0
74376946,1,74411727.0,2022-11-09 14:58:35,1,218,"<p>I have some functions that are being executed in ordinal form, that is, one is executed and then the next one and so on.
Due to computational time, I am interested in parallelizing such functions in R. Performing a search here in the forum I had access to some information contained in the following topic:
<a href=""https://stackoverflow.com/questions/50593531/run-several-r-functions-in-parallel"">Run several R functions in parallel</a>.</p>
<p>However, I confess that I am a beginner in this area of data science and I have never done parallel programming. My goal is fully nested to the fact that I'm joining some databases via the <code>left_join()</code> function and for each of those joins I'm applying six different functions. After that, I'm creating some variables and storing all this in a data.frame, see the following computational routine:</p>
<pre><code>Datajoin &lt;- function(Data1, 
                     Data2, 
                     Data3){
fnctions &lt;- Fun1(Data1, Data3) %&gt;% 
left_join(Fun2(Data1)) %&gt;% 
left_join(Fun3(Data1, Data2, Data3)) %&gt;% 
left_join(Fun4(Data1)) %&gt;% 
left_join(Fun5(Data1, Data3), by = c('St_ab_Data1' = 'St_ab')) %&gt;% 
left_join(Fun6(Data1, Data3), by = c('St_ab_Data1' = 'St_ab')) %&gt;% 
cbind(Data2 %&gt;% 
select(St_ab, Var2) %&gt;% 
mutate(Var2 = factor(Var2, levels = unique(Data3$Var2))) %&gt;% 
{model.matrix(~ -1 + .$Var2) %&gt;% 
as.data.frame}) %&gt;% 
left_join(Data2 %&gt;% 
as.data.frame %&gt;% 
                select(St_ab, Var3,
                contains(&quot;VarA&quot;), 
                contains(&quot;VarB&quot;), 
                contains(&quot;VarC&quot;),
                contains(&quot;VarD&quot;)) %&gt;% 
                mutate(Var4 = year(Var3)), by = c('St_ab_Data1' = 'St_ab'))
   names(fnctions)[names(fnctions) == &quot;St_ab_Data1&quot;] &lt;- 'St_id'
   return(fnctions)
}
</code></pre>
<p>As I already mentioned, my big problem is associated with computational time, given that the databases are gigantic, that is, there is a computational inefficiency associated with this process. How could I perform such a procedure by paralleling the work of these functions, given the above computational structure?</p>
<p>I tried to do it as follows, based on the forum post highlighted at the beginning of this post:</p>
<pre><code>Datajoin &lt;- function(Data1, 
                     Data2, 
                     Data3){
fnctions &lt;- Fun1(Data1, Data3) %&gt;% 
left_join(Fun2(Data1)) %&gt;% 
left_join(Fun3(Data1, Data2, Data3)) %&gt;% 
left_join(Fun4(Data1)) %&gt;% 
left_join(Fun5(Data1, Data3), by = c('St_ab_Data1' = 'St_ab')) %&gt;% 
left_join(Fun6(Data1, Data3), by = c('St_ab_Data1' = 'St_ab')) %&gt;% 
cbind(Data2 %&gt;% 
select(St_ab, Var2) %&gt;% 
mutate(Var2 = factor(Var2, levels = unique(Data3$Var2))) %&gt;% 
{model.matrix(~ -1 + .$Var2) %&gt;% 
as.data.frame}) %&gt;% 
left_join(Data2 %&gt;% 
as.data.frame %&gt;% 
                select(St_ab, Var3,
                contains(&quot;VarA&quot;), 
                contains(&quot;VarB&quot;), 
                contains(&quot;VarC&quot;),
                contains(&quot;VarD&quot;)) %&gt;% 
                mutate(Var4 = year(Var3)), by = c('St_ab_Data1' = 'St_ab'))
   names(fnctions)[names(fnctions) == &quot;St_ab_Data1&quot;] &lt;- 'St_id'
   return(fnctions)
}

tasks1 = list(wrk1 = function(x) Datajoin(x))
library(paralell)
clus = makeCluster(6)
clusterExport(clus, c('Datajoin', 
                    'Data1', 'Data2', 'Data3'))

outPUT = clusterApply( 
  clus,
  tasks1
)
stopCluster(clus)
</code></pre>
<p>However, in the same way that an error does not appear, it does not release the desired one.</p>
<p><strong>Note:</strong> my Fun1 - Fun6 functions are their own functions, but I can't share them in this topic. I tried to adapt the post to be reproducible by introducing RBase functions but I came across a series of errors, I apologize in advance.</p>
",10478454.0,10478454.0,2022-11-09 15:32:37,2022-11-12 09:20:25,Parallelizing functions from a function in R,<r><database><dataframe><parallel-processing><data-science>,1,5,N/A,CC BY-SA 4.0
71668827,1,-1.0,2022-03-29 21:12:08,0,46,"<p>Given data with rows and columns, how do i access rows that have a common name, for example in a cars sales data, i want the output to be dataframe of cars with &quot;honda&quot; in their name mind you honda has different models<a href=""https://i.stack.imgur.com/N6Mzy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N6Mzy.png"" alt=""car sells"" /></a></p>
",18600745.0,-1.0,N/A,2022-03-29 21:17:05,access rows with a common name from DataFrame,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
74384561,1,-1.0,2022-11-10 05:35:09,0,33,"<p>I want to retrieve values from a data set that matches a certain value. The &quot;.loc&quot; method is working fine if I give one value at a time. But when trying to get the value from a list nothing is happening.</p>
<p>The below script work fine.</p>
<pre><code>df.loc[df.domains==&quot;IN&quot;] 
</code></pre>
<p>The below script is not. I want to use each item from the list to match and get the desired data frame from the data set</p>
<pre><code>list=[&quot;&quot;AE&quot;,&quot;AU&quot;,&quot;BE&quot;,&quot;BR&quot;,&quot;CN&quot;,&quot;DE&quot;,&quot;EG&quot;,&quot;ES&quot;,&quot;FR&quot;,&quot;IN&quot;,&quot;IT&quot;,&quot;JP&quot;,&quot;MX&quot;,&quot;NL&quot;,&quot;PL&quot;,&quot;SE&quot;,&quot;SG&quot;,&quot;UK&quot;]

for i in list:
      a=f'&quot;{i}&quot;'  
      print(a)  
      df.loc[df.domains==a] 
</code></pre>
",6378979.0,7117003.0,2022-11-10 05:55:27,2022-11-10 05:55:27,How to use each element in the list to check and locate the matching value from a data set?,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
74397280,1,-1.0,2022-11-11 01:57:52,0,24,"<p>I have an existing data frame with 3 columns: <code>location </code>,<code>contaminants </code>and <code>Concentration</code>.</p>
<p>It looks something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Location</th>
<th>Contaminants</th>
<th>Concentration</th>
</tr>
</thead>
<tbody>
<tr>
<td>NYC</td>
<td>Chlorine</td>
<td>10</td>
</tr>
<tr>
<td>Los Angeles</td>
<td>Lead</td>
<td>5</td>
</tr>
<tr>
<td>Los Angeles</td>
<td>Chlorine</td>
<td>2</td>
</tr>
<tr>
<td>Miami</td>
<td>Sulfur</td>
<td>5</td>
</tr>
<tr>
<td>Miami</td>
<td>Lead</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<p>I need to sort it so that there is only one row per <code>location</code> which contains the concentration for each contaminant in order to make it ideal for machine learning.</p>
<p>I need it to look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Location</th>
<th>Chlorine Concentration</th>
<th>Lead Concentration</th>
<th>Sulfur Concentration</th>
</tr>
</thead>
<tbody>
<tr>
<td>NYC</td>
<td>10</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Los Angeles</td>
<td>2</td>
<td>5</td>
<td>0</td>
</tr>
<tr>
<td>Miami</td>
<td>0</td>
<td>4</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
<p>I'm trying to do this in pandas-- thank you so much for the help!</p>
<p>Right now, its not in a compatible form. There needs to be only one row per location, but I can't seem to figure it out</p>
",19488318.0,19488318.0,2022-11-12 01:33:31,2022-11-12 01:54:52,How to reorganize a dataframe in order to increase dimensionality?,<python><pandas><dataframe><machine-learning><data-science>,1,1,N/A,CC BY-SA 4.0
74405080,1,74408913.0,2022-11-11 15:56:32,0,696,"<p>I have the following dataframe and a vector of names.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>age</th>
</tr>
</thead>
<tbody>
<tr>
<td>panda</td>
<td>5</td>
</tr>
<tr>
<td>polarbear</td>
<td>7</td>
</tr>
<tr>
<td>seahorse</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>I would like to select rows by the names in the vector and calculate the average age of selected rows. I have the following code:</p>
<pre class=""lang-rust prettyprint-override""><code>let names = vec![&quot;panda&quot;, &quot;seahorse&quot;];
let avg = df.lazy()
    .select([col(&quot;name&quot;).filter(|c| names.contains(c))])
    .agg([col(&quot;age&quot;).mean()]);
</code></pre>
<p>Intuition says, pass a function to the filter (like I have done), however this is wrong. Apparently there is some sort of Expr API in play. How does it work? I find the docs a bit puzzling.</p>
",14495288.0,-1.0,N/A,2022-11-11 22:55:03,Selecting rows by id and calculating the mean value in Polars with Rust,<rust><data-science><rust-polars>,2,3,N/A,CC BY-SA 4.0
74370440,1,-1.0,2022-11-09 05:51:24,-2,24,"<p>They both end up doing the same but which one is more efficient?<a href=""https://i.stack.imgur.com/sfh1w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sfh1w.png"" alt=""enter image description here"" /></a></p>
<p>I want to know the meaning behind them</p>
",19229519.0,-1.0,N/A,2022-11-09 06:02:48,What is the difference between pd.value_counts(df['length']) and df['length'].value_counts()?? which one would be better?,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
71658431,1,-1.0,2022-03-29 08:01:32,0,60,"<p>My code is:</p>
<pre><code>last_steps = valid[['Close']].tail(60)
last_values = np.reshape(last_steps, ( -1, last_steps.shape[1], last_steps.shape[2]))
</code></pre>
<p>The second line rises this error:</p>
<pre><code>IndexError                                Traceback (most recent call last)
C:\Users\UTKARS~1\AppData\Local\Temp/ipykernel_8696/2291656061.py in &lt;module&gt;
      1 last_steps = valid[['Close']].tail(60)
----&gt; 2 last_values = np.reshape(last_steps, ( -1, last_steps.shape[1], last_steps.shape[2]))

IndexError: tuple index out of range
</code></pre>
",16330706.0,11728488.0,2022-03-29 18:02:40,2022-03-29 20:08:35,Getting an error during reshaping the dataframe,<python><pandas><dataframe><data-science><reshape>,2,1,N/A,CC BY-SA 4.0
74374836,1,74375498.0,2022-11-09 12:23:06,0,87,"<p>What is the best way to read data from txt/csv file, separate values based on columns to arrays (no matter how many columns there are) and how skip for example first row if file looks like this:</p>
<p><a href=""https://i.stack.imgur.com/fzx6z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fzx6z.png"" alt=""image of the table"" /></a></p>
<p>Considering existing libraries in python.</p>
<p>So far, I've done it this way:</p>
<pre><code>pareto_front_file = open(&quot;Pareto Front.txt&quot;)
data_pareto_front = pareto_front_file.readlines()
for pareto_front_row in data_pareto_front:
    x_pareto.append(float(pareto_front_row.split('  ')[0]))
    y_pareto.append(float(pareto_front_row.split('  ')[1]))
</code></pre>
<p>but creating more complicated things I see that this way is not very effective</p>
",-1.0,10878534.0,2022-11-12 06:46:17,2022-11-12 06:46:17,Read tabular data (rows and named columns) in Python - best practices,<python><data-science>,1,3,N/A,CC BY-SA 4.0
74412374,1,74412813.0,2022-11-12 11:12:06,7,1217,"<p>I have the following code to find the mean of the ages in the dataframe.</p>
<pre class=""lang-rust prettyprint-override""><code>let df = df! [
    &quot;name&quot; =&gt; [&quot;panda&quot;, &quot;polarbear&quot;, &quot;seahorse&quot;],
    &quot;age&quot; =&gt; [5, 7, 1],
].unwrap();

let mean = df
    .lazy()
    .select([col(&quot;age&quot;).mean()])
    .collect().unwrap();

println!(&quot;{:?}&quot;, mean);
</code></pre>
<p>After finding the mean, I want to extract the value as an <code>f64</code>.</p>
<pre><code>┌──────────┐
│ age      │
│ ---      │
│ f64      │   -----&gt; how to transform into a single f64 of value 4.333333?
╞══════════╡
│ 4.333333 │
└──────────┘
</code></pre>
<p>Normally, I would do something like <code>df[0,0]</code> to extract the only value. However, as Polars is not a big proponent of indexing, how would one do it using Rust Polars?</p>
",14495288.0,-1.0,N/A,2023-02-06 05:30:44,Extracting a Rust Polars dataframe value as a scalar value,<rust><data-science><rust-polars>,2,0,N/A,CC BY-SA 4.0
74413145,1,74413243.0,2022-11-12 13:00:42,-4,23,"<p>I am going to share a question that I was asked in a data science interview. I failed the interview as I couldn't answer the question and still now I don't have any idea how to solve it.</p>
<p>Below a data table is given. It's about number of users of a mobile game.
Users Assigned = Newly added users on a day.
Active users = Total users active on a day.
<a href=""https://i.stack.imgur.com/2gh1t.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2gh1t.jpg"" alt=""enter image description here"" /></a></p>
<p>The question is- What % of users are still playing the game daily?
Options are</p>
<ol>
<li>Almost 5%</li>
<li>Almost 10% to 15%.
And I was ask to show the step by step processes.</li>
</ol>
<p>It was my first-ever interview in life, so I was nervous and couldn't solve within given time. I hope someone will answer me here and will explain it clearly.</p>
",16076505.0,3773011.0,2022-11-24 21:36:41,2022-11-24 21:36:41,What % of users are still playing the game daily?,<data-science><data-analysis><exploratory-data-analysis>,1,1,2022-11-12 13:39:58,CC BY-SA 4.0
74398053,1,74402339.0,2022-11-11 04:34:39,0,363,"<p>I want to get the indices for a list of filters in polars and get a sparse matrix from it, how can I parallel the process? This is what I have right now, a pretty naive and brute force way for achieving what I need, but this is having some serious performance issue</p>
<pre class=""lang-py prettyprint-override""><code>def get_sparse_matrix(exprs: list[pl.Expr]) -&gt; scipy.sparse.csc_matrix:
    df = df.with_row_count('_index')
    rows: list[int] = []
    cols: list[int] = []
    for col, expr in enumerate(exprs):
        r = self.df.filter(expr)['_index']
        rows.extend(r)
        cols.extend([col] * len(r))

    X = csc_matrix((np.ones(len(rows)), (rows, cols)), shape= 
   (len(self.df), len(rules)))

    return X
</code></pre>
<p>Example Input:</p>
<pre class=""lang-py prettyprint-override""><code># df is a polars dataframe with size 8 * 3
df = pl.DataFrame(
[[1,2,3,4,5,6,7,8], 
[3,4,5,6,7,8,9,10], 
[5,6,7,8,9,10,11,12],
[5,6,41,8,21,10,51,12],
])

# three polars expressions
exprs = [pl.col('column_0') &gt; 3, pl.col('column_1') &lt; 6, pl.col('column_4') &gt; 11]
</code></pre>
<p>Example output:
X is a sparse matrix of size <code>8 (number of records) X 3 (number of expressions)</code>, where the element at <code>i,j</code> equals to 1 if <code>i</code>th record matches the <code>j</code>th expression</p>
",5257450.0,5257450.0,2022-11-11 09:47:14,2022-11-11 17:17:57,Parallel querying indices for a list of filter expressions in polars dataframe,<python><pandas><scipy><data-science><python-polars>,2,0,N/A,CC BY-SA 4.0
74398311,1,74411380.0,2022-11-11 05:21:37,0,515,"<p>I have a lot of categorical columns  and want  to convert values in those columns to numerical values so that I will be able to apply ML model.</p>
<p>Now by data looks something like below.</p>
<p>Column 1- Good/bad/poor/not reported
column 2- Red/amber/green
column 3- 1/2/3
column 4- Yes/No</p>
<p>Now I have already assigned numerical values of 1,2,3,4 to good, bad, poor, not reported in column 1 .</p>
<p>So, now can I give the same numerical values like 1,2,3 to red,green, amber etc in column 2 and in a similar fashion to other columns or will doing that confuse model when I implement it</p>
",19476450.0,-1.0,N/A,2022-11-12 08:18:09,Convert Categorical features to Numerical,<python><machine-learning><encoding><data-science><categorical>,2,1,N/A,CC BY-SA 4.0
74413899,1,-1.0,2022-11-12 14:47:14,0,486,"<p>I really do not understand the difference between the output of the two function that I've given below.
<a href=""https://i.stack.imgur.com/E26Hf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E26Hf.png"" alt=""enter image description here"" /></a></p>
<p>What is the use of <strong>lambda x: ast.literal_eval(x) if isinstance(x,str) else np.nan</strong> here ?</p>
",18142963.0,-1.0,N/A,2022-11-12 15:13:51,Use of ast.literal_eval,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
74425994,1,-1.0,2022-11-14 01:00:50,0,37,"<p>I want to convert a data frame that I built in R (Web scraping) to csv format and export to excel/ download it onto my desktop. I have it as dataframe &quot;allNYTSearch&quot;. I have run the is.data.frame function to confirm the object is indeed a data frame, however and it comes back as &quot;[1] TRUE&quot;, however, I still get the following error.</p>
<p>I tried a few different google search answers however none have been successful as of yet.</p>
<pre><code>Error in utils::write.table(allNYTSearch, &quot;\\Users\\khalidali\\desktop\\allNYTSearch.csv&quot;,  : 
  unimplemented type 'list' in 'EncodeElement'
</code></pre>
",20437052.0,5221626.0,2022-11-14 01:35:27,2022-11-14 01:35:27,How can I convert a data frame in R to to a csv file without getting the following error? unimplemented type 'list' in 'EncodeElement',<r><dataframe><statistics><data-science>,0,2,N/A,CC BY-SA 4.0
74426736,1,-1.0,2022-11-14 03:50:08,-4,34,"<p>I am practicing pandas dataframes and I'm confused about one thing since I'm still a newbie at Python coming from a strong Java, C family background.</p>
<pre><code>for i in dataframe1.columns:
    dataframe1[i] = np.where(dataframe1[i] == 0, np.nan, dataframe1[i])
</code></pre>
<p>I am confused with what I am iterating over? As I take it, dataframe1.columns would return the column names or columns objects (however it's done in Python) of the dataframe.
So when using the where() function for the condition, dataframe1[i] == 0 wouldn't that just check if the column name (whether in string format or object format) == 0 or not? Or does python implicitly iterate through the values within each column even though it's not explicitly specified so in code?</p>
<p>Am I missing something? Please advise.</p>
",5525159.0,-1.0,N/A,2022-11-14 13:39:10,What am I iterating over?,<python><pandas><dataframe><numpy><data-science>,1,9,N/A,CC BY-SA 4.0
74395200,1,74395309.0,2022-11-10 20:55:34,1,161,"<p>I have a list in the following form:
['C_k', 'c_f', 'm_1', 'T_1', 'T_m']</p>
<p>I wanna creat a sympy symbol for every &quot;variable&quot; in this list, which is possible with the following function:</p>
<p>a,k,m_n=symbols('a k m_n')</p>
<p>How would this work?
The end goal is to do (Guassian-)error propagation, which requires me to evaluate a derivative of an expression at a given point.</p>
<p>What my code currently only does is convert a latex (math) string to a sympy expression and then extract a list of symbols that I need to differentiate by</p>
<pre><code>import matplotlib.pyplot as plt
from latex2sympy2 import latex2sympy
import numpy as np
from sympy import *
import re

# task: perform error propagation for a given formula and a given dataset regarding some associated uncertainty 

Formula=r'(m_1\cdot c_f+C_k)\cdot (T_1-T_m)' # Formula is given as latex code
Formula=latex2sympy(Formula) # Formula converted to sympy

print(re.findall(r'[A-Za-z_]+\d*',str(Formula))) # this creates a list of symbols in the expression using regex. task is to differentiate the function by these variables

</code></pre>
<p>output: ['C_k', 'c_f', 'm_1', 'T_1', 'T_m']</p>
",17451566.0,17451566.0,2022-11-10 21:01:45,2022-11-10 21:07:56,How do I create a number of sympy symbols from a list?,<python><latex><data-science><sympy>,1,0,N/A,CC BY-SA 4.0
74419049,1,-1.0,2022-11-13 06:54:41,1,1417,"<p>If I have string column namely, 'Cabin' in my dataframe, containing values as shown below:</p>
<pre><code>Series: 'Cabin' [str]
[
    &quot;B/0/P&quot;
    &quot;F/0/S&quot;
    &quot;A/0/S&quot;
    &quot;A/0/S&quot;
    &quot;F/1/S&quot;
]
</code></pre>
<p>I want to know the process of splitting the 'Cabin' column into multiple columns as shown below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">A</th>
<th style=""text-align: center;"">B</th>
<th style=""text-align: center;"">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">str</td>
<td style=""text-align: center;"">i8</td>
<td style=""text-align: center;"">str</td>
</tr>
<tr>
<td style=""text-align: center;"">&quot;B&quot;</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">&quot;P&quot;</td>
</tr>
<tr>
<td style=""text-align: center;"">&quot;F&quot;</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">&quot;S&quot;</td>
</tr>
<tr>
<td style=""text-align: center;"">&quot;A&quot;</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">&quot;S&quot;</td>
</tr>
<tr>
<td style=""text-align: center;"">&quot;C&quot;</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">&quot;S&quot;</td>
</tr>
</tbody>
</table>
</div>
<p>I did the initial splitting operation on the column by <code>train.select(pl.col(&quot;Cabin&quot;).str.split(by=&quot;/&quot;)).to_series()</code> to get</p>
<pre><code>Series: 'Cabin' [list]
[
    [&quot;B&quot;, &quot;0&quot;, &quot;P&quot;]
    [&quot;F&quot;, &quot;0&quot;, &quot;S&quot;]
    [&quot;A&quot;, &quot;0&quot;, &quot;S&quot;]
    [&quot;A&quot;, &quot;0&quot;, &quot;S&quot;]
    [&quot;F&quot;, &quot;1&quot;, &quot;S&quot;]
]
</code></pre>
<p>So I want to know the next steps to get my desired output as shown above.</p>
",20474952.0,-1.0,N/A,2022-11-13 17:31:24,How to split the contents of a column into multiple columns inside a polars dataframe,<python><dataframe><data-science><python-polars><data-preprocessing>,1,0,N/A,CC BY-SA 4.0
71686482,1,-1.0,2022-03-31 03:14:00,2,1936,"<p>I want to save the data set as a parquet file, called power.parquet, and I use df.to_parquet(&lt;filename&gt;). But it gives me this errer &quot;ValueError: Error converting column &quot;Global_reactive_power&quot; to bytes using encoding UTF8. Original error: bad argument type for built-in operation&quot; And I installed the fastparquet package.</p>
<p><code>from fastparquet import write, ParquetFile </code></p>
<p><code>dat.to_parquet(&quot;power.parquet&quot;) </code></p>
<p><code>df_parquet = ParquetFile(&quot;power.parquet&quot;).to_pandas() </code></p>
<p><code>df_parquet.head() # Test your final value </code></p>
<pre><code>`*Traceback (most recent call last):

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/fastparquet/writer.py&quot;, line 259, in convert
    out = array_encode_utf8(data)

  File &quot;fastparquet/speedups.pyx&quot;, line 50, in fastparquet.speedups.array_encode_utf8

TypeError: bad argument type for built-in operation


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File &quot;/var/folders/4f/bm2th1p56tz4rq_zffc8g3940000gn/T/ipykernel_85477/3080656655.py&quot;, line 1, in &lt;module&gt;
    dat.to_parquet(&quot;power.parquet&quot;, compression=&quot;GZIP&quot;)

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/dataframe/core.py&quot;, line 4560, in to_parquet
    return to_parquet(self, path, *args, **kwargs)

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py&quot;, line 732, in to_parquet
    return compute_as_if_collection(

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/base.py&quot;, line 315, in compute_as_if_collection
    return schedule(dsk2, keys, **kwargs)

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/threaded.py&quot;, line 79, in get
    results = get_async(

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/local.py&quot;, line 507, in get_async
    raise_exception(exc, tb)

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/local.py&quot;, line 315, in reraise
    raise exc

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/local.py&quot;, line 220, in execute_task
    result = _execute_task(task, data)

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/core.py&quot;, line 119, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/utils.py&quot;, line 35, in apply
    return func(*args, **kwargs)

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/dask/dataframe/io/parquet/fastparquet.py&quot;, line 1167, in write_partition
    rg = make_part_file(

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/fastparquet/writer.py&quot;, line 716, in make_part_file
    rg = make_row_group(f, data, schema, compression=compression,

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/fastparquet/writer.py&quot;, line 701, in make_row_group
    chunk = write_column(f, coldata, column,

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/fastparquet/writer.py&quot;, line 554, in write_column
    repetition_data, definition_data, encode[encoding](data, selement), 8 * b'\x00'

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/fastparquet/writer.py&quot;, line 354, in encode_plain
    out = convert(data, se)

  File &quot;/opt/anaconda3/lib/python3.9/site-packages/fastparquet/writer.py&quot;, line 284, in convert
    raise ValueError('Error converting column &quot;%s&quot; to bytes using '

ValueError: Error converting column &quot;Global_reactive_power&quot; to bytes using encoding UTF8. Original error: bad argument type for built-in operation

*
</code></pre>
<p>I tried by adding object_coding = &quot;bytes&quot;.I want to solve this problem.</p>
",16495140.0,16495140.0,2022-04-01 22:29:59,2022-04-01 22:29:59,"Error shows up when using df.to_parquet(""filename"")",<python><dataframe><data-science><parquet><writefile>,0,6,N/A,CC BY-SA 4.0
74425266,1,-1.0,2022-11-13 22:27:13,0,93,"<p>I am working on an assignment and I run into this error. I am using python to perform an KNN on a data set. I pretty sure I defined the variable but it says otherwise. This code is written below.</p>
<p>`</p>
<pre><code>import pandas as PD
import numpy as np
import matplotlib.pyplot as mtp

data_set= PD.read_csv('hw6.data.csv.gz')

x= data_set.iloc[:,[2,3]].valuesS
y= data_set.iloc[:, 4].values

from sklearn.model_selection import train_test_split
x_train, x_train, y_train, y_train= train_test_split(x,y, test_size=.25, random_state=0)

from sklearn.preprocessing import StandardScaler
st_x= StandardScaler()
x_train= st_x.fit_transform(x_train)
x_test= st_x.transform(x_test)
</code></pre>
<p>`</p>
<p>`</p>
<pre><code>import pandas as PD
import numpy as np
import matplotlib.pyplot as mtp

data_set= PD.read_csv('hw6.data.csv.gz')

x= data_set.iloc[:,[2,3]].valuesS
y= data_set.iloc[:, 4].values

from sklearn.model_selection import train_test_split
x_train, x_train, y_train, y_train= train_test_split(x,y, test_size=.25, random_state=0)

from sklearn.preprocessing import StandardScaler
st_x= StandardScaler()
x_train= st_x.fit_transform(x_train)
x_test= st_x.transform(x_test)
</code></pre>
<p>`</p>
<p>The error says &quot;x_test&quot; is not defined Pylance (reportUndefinedVarible)</p>
",14295557.0,14295557.0,2022-11-14 02:18:47,2023-04-21 11:56:22,KNN: why is my variable is not defined in python?,<python><data-science><knn>,0,2,N/A,CC BY-SA 4.0
74433774,1,-1.0,2022-11-14 15:06:44,0,69,"<p>I'm trying to create a relationship between Client and Client Doctor, so here is what i've done:
I tried this first then got &quot;no changes, no records&quot; notice</p>
<pre><code>LOAD CSV WITH HEADERS FROM &quot;file:///Consultation.csv&quot; AS row
MATCH (c:Client), (cd:ClientDoctor)  
WHERE c.Client = row.ClientNum AND cd.CDNum = row.CDNum
CREATE (c)-[Consultation:consults]-(cd)
SET Consultation = row
</code></pre>
<p>then I tried this way but still receive the same notice:</p>
<pre><code>LOAD CSV WITH HEADERS FROM &quot;file:///Consultation.csv&quot; AS row  
MATCH (c:Consultation{ClientNum: row.ClientNum})
MATCH (cd:Consultation {CDNum: row.CDNum})
MERGE (c)-[:is_consulted_by]-&gt;(cd);
Set Consultation = row
</code></pre>
<p>Here is the csv file:
<a href=""https://i.stack.imgur.com/etpos.png"" rel=""nofollow noreferrer"">CSV FIle</a>
Does anyone know how to fix this problem? please help
Thank you!</p>
",20501809.0,-1.0,N/A,2022-12-20 15:01:31,"Receive "" No changes, no records "" when creating relationship between 2 nodes",<database><neo4j><data-science>,1,3,N/A,CC BY-SA 4.0
74441842,1,-1.0,2022-11-15 07:15:54,0,44,"<p>I have a data in CSV format like.</p>
<pre><code>Patient_ID,Analyte_line
KYN059AQP,&quot;[['Urea', 3.0, '3', ''], ['Creatinine', 3.0, '3', ''], ['Uric Acid', 3.0, '3', '']]&quot;
    KQT767JLU,&quot;[['Total Protein', '', '6', ''], ['Albumin', '', '6', ''], ['Globulin', '', '4', ''], ['Total Bilirubin', '', '6', ''], ['Direct Bilirubin', '', '4', ''], ['Indirect Bilirubin', '', '4', ''], ['Alkaline Phosphatase', '', '4', ''], ['SGPT', '', '5', ''], ['SGOT', '', '5', ''], ['Gamma GT', '', '5', ''], ['AG Ratio', '', '4', '']]&quot;
PWV009AGQ,&quot;[['HGB', '', '18', ''], ['RBC', '', '1', ''], ['HCT', '', '2', ''], ['MCV', '', '3', ''], ['MCH', '', '3', ''], ['MCHC', '', '3', ''], ['RDWcv', '', '2', ''], ['RDWsd', '', '3', ''], ['WBC', '', '4', ''], ['NEU', '', '5', ''], ['LYM', '', '6', ''], ['MON', '', '', ''], ['BAS', '', '', ''], ['EO', '', '', ''], ['NEU%', '', '', ''], ['LYM%', '', '', ''], ['MON%', '', '', ''], ['EO%', '', '', ''], ['BAS%', '', '', ''], ['PLT', '', '170', ''], ['PCT', '', '3', ''], ['MPV', '', '', ''], ['PDWsd', '', '', ''], ['PDWcv', '', '', ''], ['ESR', '', '5', ''], ['GRA#', '', '', '']]&quot;
PWV009AGQ,&quot;[['Total Protein', '', '23', ''], ['Albumin', '', '2', ''], ['Globulin', '', '2', ''], ['Total Bilirubin', '', '2', ''], ['Direct Bilirubin', 2.0, '', ''], ['Indirect Bilirubin', 2.0, '', ''], ['Alkaline Phosphatase', '', '3', ''], ['SGPT', 1.0, '', ''], ['SGOT', '', '4', ''], ['Gamma GT', 33.0, '31', ''], ['AG Ratio', '', '2', '']]&quot;
WUY523UZO,&quot;[['HGB', '', '11', ''], ['RBC', '', '2', ''], ['HCT', '', '4', ''], ['MCV', '', '5', ''], ['MCH', '', '6', ''], ['MCHC', '', '6', ''], ['RDWcv', '', '7', ''], ['RDWsd', '', '8', ''], ['WBC', '', '9', ''], ['NEU', '', '9', ''], ['LYM', '', '1', ''], ['MON', '', '', ''], ['BAS', '', '', ''], ['EO', '', '', ''], ['NEU%', '', '', ''], ['LYM%', '', '', ''], ['MON%', '', '', ''], ['EO%', '', '', ''], ['BAS%', '', '', ''], ['PLT', '', '', ''], ['PCT', '', '', ''], ['MPV', '', '', ''], ['PDWsd', '', '', ''], ['PDWcv', '', '', ''], ['ESR', '', '', ''], ['GRA#', '', '', '']]&quot;
ZMO679WDS,&quot;[['Dengue Ig-G', '', '8', '']]&quot;
TVZ695TUB,&quot;[['Rapid Malaria', '', 'Negative', '']]&quot;
</code></pre>
<p>I want to set in proper format like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Patient_ID</th>
<th>Urea</th>
<th>Rapid Malaria</th>
<th>SGPT</th>
<th>HGB</th>
</tr>
</thead>
<tbody>
<tr>
<td>KYN059AQP</td>
<td>3.0,3</td>
<td>-</td>
<td>3</td>
<td>-</td>
</tr>
<tr>
<td>KQT767JLU</td>
<td>-</td>
<td>Negative</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PWV009AGQ</td>
<td>4.0</td>
<td>-</td>
<td>-</td>
<td>9</td>
</tr>
<tr>
<td>TVZ695TUB</td>
<td>-</td>
<td>Negative</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>",20439082.0,7117003.0,2022-11-15 07:26:39,2022-11-15 09:37:08,How to set the data in proper form,<python><pandas><dataframe><csv><data-science>,1,4,N/A,CC BY-SA 4.0
74415739,1,74416211.0,2022-11-12 19:01:05,0,30,"<p>I'm Newbie to Pandas.</p>
<p>My df is a CSV with accounting information, I have variables with a list of accounts inside them, and I want to add them to another variable first and then use df.loc</p>
<pre><code>
df = pd.read_csv('2021BALANCE.csv')

menor = (df['account_1'] &lt; 10000)  # just filtering 


#LIST OF ACCOUNTS inside variables 
  
assets = ['account_1', 'account_2', 'account_3']
liabilities = ['account_6' , 'account_4']
profits = ['account_20' , 'account_21']

#THIS VARIABLE already have accounts, and I want to add the accounts above to it.

all_accounts = ['account_9345', 'account_234623' ,assets, liabilities, profits]


df.loc[menor, all_accounts]


TypeError: unhashable type: 'list'
</code></pre>
<p>There are tons of variables with accounts inside them, I just put 3 here in order to simplify, and I want all of them in just one variable <strong>all_accounts</strong>  so I can use <strong>df.loc[menor, all_accounts]</strong> .</p>
<p>Thank you so much for your help!!</p>
",19302534.0,-1.0,N/A,2022-11-12 20:09:54,How can I add list to another list? PANDAS df.loc,<python><pandas><dataframe><numpy><data-science>,1,2,N/A,CC BY-SA 4.0
74424083,1,-1.0,2022-11-13 19:28:02,0,105,"<p>I would like to add different weights to clients in Federated learning, so in the aggregation stage, each client has a different impact on the global model.</p>
<p>For example:</p>
<pre><code>Client_1 has 2X impact
Client_2 has X impact
</code></pre>
<p>I am looking for suggestions to implement this approach.</p>
",4409800.0,4685471.0,2022-11-13 19:48:32,2023-06-01 01:20:19,Federated learning weighted aggregation,<python><machine-learning><data-science><federated-learning>,1,0,N/A,CC BY-SA 4.0
74428514,1,74428540.0,2022-11-14 08:02:09,0,48,"<p>I have a dataframe</p>
<pre><code>df = 
Col Val
 a.  8
 a.  9
 c.  4
 c.  0
 d.  3
 d.  9
</code></pre>
<p>I want to sort by Val of the smallest value within group and then foreach row get the index of the groupby Col
So the new df will df</p>
<pre><code>df_new = 
Col Val Idx
 c.  4.  0
 c.  0.  0
 d.  3.  1
 d.  9.  1
 a.  8.  2
 a.  9.  2
</code></pre>
<p>What is the best way to do so?</p>
",6057371.0,6057371.0,2022-11-14 08:47:18,2022-11-14 08:56:06,pandas how to get sorted value in groupby object,<python><pandas><group-by><data-science><data-munging>,1,0,N/A,CC BY-SA 4.0
74445086,1,-1.0,2022-11-15 11:44:56,0,24,"<p>I am trying to compare native column with each model column like -&gt; if native is less than model1 then highlight it , same  if native is less than model 2 then highlight it . and count how many columns are less than native if it is less than 5 then make a new column and put done else put repeat for each row . we are having more than 20k row.</p>
<p><a href=""https://i.stack.imgur.com/sbWY4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sbWY4.png"" alt="""" /></a></p>
<pre><code>df.style.apply(lambda x: ['background: lightgreen' if x.MODEL 1 &lt;= x.NATIVE    == 1 else '' for i in x], 
               axis=1)
</code></pre>
<p>THis is just refrence i dont know how do i do it.</p>
",14011179.0,15415267.0,2022-11-15 11:57:48,2022-11-15 11:57:48,How to highlight columns by comparing all models form native in pandas and count,<python><pandas><dataframe><group-by><data-science>,0,3,N/A,CC BY-SA 4.0
74445743,1,-1.0,2022-11-15 12:36:06,0,34,"<p>I wanted to create a new column (Group ID) on the basis of following conditions:
If the DOB and first three letters of Name are same, then it must fall is same Group ID.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>DOB</th>
<th>Group ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>Anny</td>
<td>18-01-1922</td>
<td>0</td>
</tr>
<tr>
<td>Anny Scott</td>
<td>01-01-1950</td>
<td>1</td>
</tr>
<tr>
<td>Annie</td>
<td>01-01-1950</td>
<td>1</td>
</tr>
<tr>
<td>David</td>
<td>14-02-1950</td>
<td>2</td>
</tr>
<tr>
<td>David Kern</td>
<td>15-02-1951</td>
<td>3</td>
</tr>
<tr>
<td>William Perry</td>
<td>15-02-1953</td>
<td>4</td>
</tr>
<tr>
<td>Kenneth Field</td>
<td>15-02-1953</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
<p>This how I want to create the groups</p>
<p>I have used the following code, to create the group ID for name (If first three letters are matched)
df['Group ID Name']=df.groupby(df['name'].str[:3]).ngroup()</p>
<p>The following code is used to create the group ID for DOB (If two records have the same DOB)
df['Group ID DOB']=df.groupby('Date of Birth').ngroup()</p>
<p>I want to use both the condition to create the Group ID, please help me out for the same.</p>
",20507143.0,-1.0,N/A,2022-11-15 12:40:46,Create a new column for group based on condition,<pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
74430924,1,-1.0,2022-11-14 11:28:59,0,152,"<p>Is there an algorithm such as Reservoir Sampling (algorithm that randomly chooses an item from a stream such that each item is equally likely to be selected), but once an item is selected it is yielded (and therefore cannot be overridden)?
i.e, not only can't I hold the stream in memory, I can't hold the sample in memory either.
I need a Python solution, but an algorithm name is enough.</p>
<p>The code I currently have (taken from <a href=""https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c"" rel=""nofollow noreferrer"">https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c</a>):</p>
<pre class=""lang-py prettyprint-override""><code>k=5
reservoir = []
for i, element in enumerate(stream):
    if i+1&lt;= k:
        reservoir.append(element)
    else:
        probability = k/(i+1)
        if random.random() &lt; probability:
            # Select item in stream and remove one of the k items already selected
             reservoir[random.choice(range(0,k))] = element # This is why I can't currently do it - elements in the list are overridden
</code></pre>
",16698040.0,-1.0,N/A,2022-11-14 11:28:59,"Reservoir Sampling, except I can't hold the sample in memory",<python><algorithm><data-science><sampling><reservoir-sampling>,0,4,N/A,CC BY-SA 4.0
74447277,1,74447594.0,2022-11-15 14:28:06,0,230,"<p>Good morning.
I would need a script that reads information in one file and reports it in an order I indicated in another text file.
With simple input/output statements I managed to do it but the problem is that I need this to be done for all the files in the directory and that each output produced has the same name as the file from which it comes.
Could someone help me?
I am new to Python but I have to learn quickly for my master’s thesis work.</p>
<p>I describe the problem in more detail.
I have a directory with about 7000 files with the .xyz extension that consist of a variable number of rows. I have to discard the second line and copy the remaining ones into a new file with different extension by adding text and converting all the letters ‘H’ of the lines to ‘1’.
I would also like each new .txt file to have the same name as the .xyz file from which it was generated and to be saved in a different directory.</p>
<p>This is the following working code but it is for only one file at a time:</p>
<pre><code>c_input=open('C:/.../test_input.txt', 'w')
c_input.write('MOLECULE')
c_input.write('\n')
c_input.write('1')
c_input.write('\n')

file_to_read=open('C:/.../Water_1.xyz', 'r')
a=file_to_read.readline()
c_input.write(a)
list_file_to_read=file_to_read.readlines()
for i in range(len(list_file_to_read)):
    if i&gt;0:
        c_input.write((list_file_to_read[i]))
file_to_read.close()
c_input.close()

c_input=open('C:/.../test_input.txt', 'r')
file_source=c_input.read()
c_input.close()
c_input=open('C:/.../test_input.txt', 'w')
replace_string=file_source.replace('H', '1')
c_input.write(replace_string)
c_input.close()

</code></pre>
",20511097.0,-1.0,N/A,2022-11-17 17:56:10,Python - Create output files from an input file that has the same name as it for all file in a directory,<python><io><iteration><data-science><spyder>,1,1,N/A,CC BY-SA 4.0
71695495,1,71695820.0,2022-03-31 15:42:22,0,173,"<p>I have surveyed people about which fruit they like to eat (see data below) and I want to see whether there are clusters in the data. Do people who like bananas frequently also like loganberries, say.  There's 23 different types of fruit and 400 respondents.</p>
<p>I would like to conduct the analysis in Python with Pandas, because that's what I know best. If this is a sane option, is there a common approach to this type of problem (there seems to be a lot of conflicting advice)? Does anyone have a recommended approach?</p>
<pre><code>Participant | Bananas |  Apples | Kumquats | Loganberries
------------|-------------------------------------------
1           |  Yes   |   No    |   Yes    |    Yes
2           |  Yes   |   Yes   |   No     |    Yes
3           |  Yes   |   No    |   Yes    |    No
4           |  No    |   No    |   No     |    Yes
5           |  Yes   |   No    |   Yes    |    Yes
6           |  Yes   |   Yes   |   No     |    No

</code></pre>
",4896331.0,-1.0,N/A,2022-03-31 16:07:24,Finding clusters in string data,<python><pandas><data-science><cluster-analysis>,1,0,N/A,CC BY-SA 4.0
71686545,1,-1.0,2022-03-31 03:25:54,0,250,"<p>I am currently doing an experiment with perlin noise, but generating this noise uses a series of plotted markers. Like, a bunch of them, and i need to render a lot of graphs, rendering just 20 takes around 15 minutes. Is there any way to speed up my code?</p>
<pre><code>import matplotlib.pyplot as plt
import math, random, noise, time
import numpy as np

def GeneratePlot(x:int,y:int) -&gt; list:
  pnoiseValues = []
  fig, ax = plt.subplots()
  for X in range(x**2):
    plt.gcf().canvas.flush_events()
    marker = str(random.choice((&quot;o&quot;, &quot;s&quot;)))
    YPOS = y+noise.pnoise2(X*x/x**2/50, y/x**2/50)
    ax.scatter(X, YPOS, marker=marker)
    pnoiseValues.append(YPOS)
  plt.show()
  return np.array(pnoiseValues)

def GetResults(amount:int,  useDelay=True) -&gt; list:
  results = []
  for i in range(amount):
    print(f&quot;Generating Images &amp; Arrays.. (This may take a while depending on the # of points)&quot;)
    time.sleep(.100 if useDelay else 0)
    results.append(GeneratePlot(i**2,i//2**2))
  print(results)
  return results;
GetResults(16)
</code></pre>
<p>So I haven’t tried anything yet, since i am new to matplotlib</p>
",18092124.0,-1.0,N/A,2022-12-20 10:21:31,How can I generate matplotlib graphs faster,<python><numpy><matplotlib><data-science><perlin-noise>,1,2,N/A,CC BY-SA 4.0
74448186,1,-1.0,2022-11-15 15:26:23,0,31,"<p>im new to python and to working with datasets, i'm using a data set that has certain stocks and stuff about them since the 1980's till the late 2010's, i dont want to use any of the stocks in the data set when i use the knn prediction, what can i do?</p>
<pre><code>for i in df[&quot;Date&quot;]:
  if(i.startswith(&quot;19&quot;)):
    f=df.drop(['Adj Close','Volume','High','Date','Low','Open', 'Close'], axis=1)
</code></pre>
<p>then i just get a copy of an empy dataset</p>
<pre><code>print(f)
</code></pre>
<p>Empty DataFrame</p>
",13859338.0,4727702.0,2022-11-15 15:28:42,2022-11-15 15:48:36,how can i clean part of a data set that contains data from before 2000?,<python><data-science><google-colaboratory>,4,0,N/A,CC BY-SA 4.0
74464863,1,-1.0,2022-11-16 17:37:40,0,15,"<p>I have a <strong>copy dataset</strong> using <code>df.dropna()</code> and I have compiled the mean of those data using <code>df.groupby</code> based on different groups with the converted code below assigned in:</p>
<pre><code># Suppose this is a result from df.groupby script

impute_data = pd.DataFrame({'PClass': [1, 1, 2, 2, 3, 3], 'Sex': ['male', 'female', 'male', 'female', 'male', 'female',], 'Mean': [34, 29, 24, 40, 18, 25]})
</code></pre>
<p>Suppose I have this <strong>real dataset</strong> and I want to impute the missing values based on the means from <strong>copy dataset</strong>, how can it be achieved?</p>
<pre><code>d = {'PClass': [1, 3, 2, 3, 2, 1, 2, 1, 3, 2, 3, 1], 
     'Sex': ['male', 'male', 'female', 'male', 'female', 'female', 'male', 'male', 'female', 'male', 'female', 'female'], 
     'Age': [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]}
df = pd.DataFrame(data=d)
</code></pre>
<p>My intial solution for this is an if else statement where for example if <code>Pclass=1 and Sex='male'</code> impute <code>34</code> and so on, but I am not certain on how I can implement it.</p>
",18193889.0,-1.0,N/A,2022-11-16 17:42:46,Impute null values based on a group statistic,<python><pandas><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
74465477,1,74465757.0,2022-11-16 18:28:45,1,60,"<p>I am new to this data science world and trying to understand some basic pandas examples.
I have a pandas data frame that I would like to create a new column and add some conditional values as below:
It will include <code>yes</code> at every 2 seconds. Otherwise include <code>no</code>. Here is an example:
This is my original data frame.</p>
<pre><code>    id  name    time
0   1   name1   260.123
1   2   name2   261.323
2   3   name3   261.342
3   4   name4   261.567
4   5   name5   262.123
...
</code></pre>
<p>The new data frame will be like this:</p>
<pre><code>    id  name    time     time_delta
0   1   name1   260.123  yes
1   2   name2   261.323  no
2   3   name3   261.342  no
3   4   name4   261.567  no
4   5   name5   262.123  yes
5   6   name6   262.345  yes
6   7   name7   264.876  yes
7   8   name8   265.234  no
8   9   name9   266.234  yes
9   10  name10  267.234  no
...
</code></pre>
<p>The code that I was using is:
<code>df['time_delta'] = df['time'].apply(apply_test)</code>
And the actual code of the function:</p>
<pre><code>def apply_test(num):
    prev = num
    if round(num) != prev + 2:
        prev = prev
        return &quot;no&quot;
    else:
        prev = num
        return &quot;yes&quot;
</code></pre>
<p>Please note that the time column has decimals and no patterns.</p>
<p>The result came as all <code>no</code> since the prev is assigned to the next number at each iteration. This was the way I thought it would be. Not sure if there are any other better ways. I would appreciate any help.</p>
<p>UPDATE:</p>
<ul>
<li>Please note that the time column has decimals and the decimal values have no value in this case. For instance, time=234.xxx will be considered as 234 seconds. Therefore, the next 2 second point is 236.</li>
<li>The data frame has multiple second value if we round it down. In this case, all of them have to be marked as <code>yes</code>. Please refer to the updates result data frame as an example.</li>
</ul>
",3450163.0,3450163.0,2022-11-16 19:33:53,2022-11-16 19:33:53,Using df.apply() to a time column that indicates times at every 2 seconds in pandas,<python><pandas><data-science>,2,3,N/A,CC BY-SA 4.0
71695974,1,-1.0,2022-03-31 16:19:18,0,3919,"<p>I am working on this ML project; here is a look of the training dataset
<a href=""https://i.stack.imgur.com/7wGwG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7wGwG.png"" alt=""enter image description here"" /></a></p>
<p>Now since the training dataset is really large I am trying to get 1% of the random data from training using  the following code:</p>
<pre><code>from numpy import float32
dtypes={'id': float32,
        'store_nbr':float32,
        'item_nbr':float32,
        'unit_sales':float32,
        'onpromotion': bool
}

def skip_row(row_idx):
  if row_idx==0:
    return False
  return random.random() &gt; sample_fraction\
  # random.random randomly retuns numbers that lie between 0 and 1
  # So for 1% of the rows it returns false, meaning that it asks to keep the row and for the rest 99% of the data it returns True meaning that it it has to frop the value
random.seed(42)
# by setting the seed to a number it ensures that we get the same random outputs everytime we run this notebook

df= pd.read_csv(data_dir + &quot;/train.csv&quot;,
                usecols=selected_cols,
                parse_dates=['date'], 
                dtype=dtypes,
                skiprows=skip_row)
</code></pre>
<p>However when I run this I am hit by the following error;
<a href=""https://i.stack.imgur.com/4l9Oz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4l9Oz.png"" alt=""enter image description here"" /></a></p>
",16355422.0,-1.0,N/A,2022-03-31 16:49:48,ValueError: Missing column provided to 'parse_dates': 'date',<python><pandas><datetime><data-science><valueerror>,1,0,N/A,CC BY-SA 4.0
74464301,1,-1.0,2022-11-16 16:52:04,0,33,"<pre><code>    import numpy as np

    m1 = np.arange(1,10).reshape(3,3)

    diagonal = np.diag(m1)

    antdiagonal =[]

    for j in range(0,3):

       x = m1[j][3-1-j]
       antdiagonal.append(x)

    def common_data(list1, list2):
        result = False
         for x in list1:
           for y in list2:
              if x == y:
                result = True
                return result 

    if(common_data(list(diagonal), list(antdiagonal))):

       print(&quot;hitter&quot;)
  
    else:

       print(&quot;Non-hitter&quot;)
</code></pre>
<p>In the above code snippet , the Matrix (m1) will be considered as “hitter” if any integer is repeating in both the principal diagonal and the anti-diagonal of m1. Otherwise should print “non hitter”. The principal diagonal of the above matrix(m1) is {1,5,9} and the principle antidiagonal will be {3,5,7}. and For the given matrix(m1) the output will be “non hitter”.
Please modify the above code to get the result.</p>
<p>i have tried with above code snippet but missing the logic for displaying &quot;hitter&quot; or &quot;non-hitter&quot;</p>
",20522246.0,-1.0,N/A,2022-11-17 17:09:30,How to manipulate a python list based on the following restrictions?,<python><data-science>,1,5,N/A,CC BY-SA 4.0
74468471,1,74468539.0,2022-11-16 23:28:45,1,69,"<p>I need help with processing an unsorted dataset.  Sry, if I am a complete noob. I never did anything like that before. So as you can see, each conversation is identified by a dialogueID which consists of multiple rows of &quot;from&quot; &amp; &quot;to&quot;, as well as text messages.
I would like to concatenate the text messages from the same sender of a dialogueID to one column and from the receiver to another column. This way, I could have a new csv-file with just [dialogueID, sender, receiver].</p>
<p><a href=""https://i.stack.imgur.com/L5JDq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L5JDq.png"" alt=""dataset"" /></a>
the new dataset should look like this
<a href=""https://i.stack.imgur.com/3wuNB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3wuNB.png"" alt=""new dataset"" /></a></p>
<p>I watched multiple tutorials and really struggle to figure out how to do it. I read in this <a href=""https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas"">9-year-old post</a> that iterating through data frames are not a good idea. Could someone help me out with a code snippet or give me a hint on how to properly do it without overcomplicating things? I thought something like this pseudo code below, but the performance with 1 million rows is not great, right?</p>
<pre><code>while !endOfFile
  for dialogueID in range (0, 1038324)
    if dialogueID+1 == dialogueID and toValue.isnull()
      concatenate textFromPrevRow + &quot; &quot; + textFromCurrentRow
      add new string to table column sender
    else
      add text to column receiver
</code></pre>
",8789802.0,8789802.0,2022-11-16 23:44:47,2022-11-17 08:44:21,NLP: pre-processing dataset into a new dataset,<python><dataframe><nlp><data-science><data-preprocessing>,2,0,N/A,CC BY-SA 4.0
74462791,1,74463406.0,2022-11-16 15:04:43,0,113,"<p>I am new in jupyter notebook and python. Recently I'm working in this code but I can't find out the problem. I want to rename <code>&quot;Tesla Quarterly Revenue(Millions of US $)&quot; and &quot;Tesla Quarterly Revenue(Millions of US $).1&quot;</code>  into <code>&quot;Data&quot; and &quot;Revenue&quot;</code> but it not changed. Here is my code:</p>
<pre><code>!pip install pandas
!pip install requests
!pip install bs4
!pip install -U yfinance pandas 
!pip install plotly
!pip install html5lib
!pip install lxml
</code></pre>
<pre><code>import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import plotly.graph_objects as go
from plotly.subplots import make_subplots
</code></pre>
<pre><code>url = &quot;https://www.macrotrends.net/stocks/charts/TSLA/tesla/revenue?utm_medium=Exinfluencer&amp;utm_source=Exinfluencer&amp;utm_content=000026UJ&amp;utm_term=10006555&amp;utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2022-01-01&quot;
html_data  = requests.get(url).text
</code></pre>
<pre><code>soup = BeautifulSoup(html_data, 'html5lib')
tesla_revenue = pd.read_html(url, match = &quot;Tesla Quarterly Revenue&quot;)[0]
tesla_revenue = tesla_revenue.rename(columns={&quot;Tesla Quarterly Revenue(Millions of US $)&quot;:&quot;Date&quot;,&quot;Tesla Quarterly Revenue(Millions of US $).1&quot;:&quot;Revenue&quot;})
tesla_revenue.head()
</code></pre>
<p>Here is the Output:</p>
<p><a href=""https://i.stack.imgur.com/cDhIm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cDhIm.png"" alt=""enter image description here"" /></a></p>
",12904389.0,11138259.0,2022-11-16 17:25:41,2022-11-16 17:25:41,Can not rename the table's column name using pandas dataframe,<python><jupyter-notebook><data-science><jupyter>,2,0,N/A,CC BY-SA 4.0
74480433,1,-1.0,2022-11-17 18:30:51,0,16,"<p>I have 500 GB of PDF files that are structured even less constant than web pages. Now I need to make an algorithm to retrieve some points from their heading (author, issuing organization, etc).</p>
<p>I need a tool to label parts of the text and train a model to detect this stuff. Can you propose me a tool to do this? I am definitely not the first one who has this task, so there must be a solution out there.</p>
<p>If you direct me to someone, who asked this previously and got an answer - it will be even better.</p>
<p>FAQ:
RegEx doesn't work.
No, I can not get the list of all organizations for easy matching.
Many names are not European, so it will not work.</p>
",10238292.0,-1.0,N/A,2022-11-17 18:30:51,How to tag words in text by hand? (making custom dataset),<dataset><data-science>,0,2,N/A,CC BY-SA 4.0
74484066,1,-1.0,2022-11-18 01:41:23,0,21,"<p>I have two dataframes with a column called &quot;US Postal State Code&quot; and I am trying to merge them together on that column into a new dataframe. The problem is that the column has an object dtype in the first dataframe and a int64 dtype in the second dataframe.</p>
<p>I tried to change the column with the object dtype to int64 using</p>
<pre><code>    Enterprise3['US Postal State Code']=Enterprise3['US Postal State Code'].astype(int)
</code></pre>
<p>however I got an error that states</p>
<pre><code>ValueError: invalid literal for int() with base 10: 'AL'
</code></pre>
<p>Is there another way to change the dtypes so that they match and can be merged?</p>
",20535301.0,-1.0,N/A,2022-11-18 02:06:39,How can you merge two data frames on a column that both data frames have if the column d types are not the same?,<python><data-science><computer-science>,2,0,N/A,CC BY-SA 4.0
71706108,1,-1.0,2022-04-01 11:20:59,0,115,"<p>I want to produce a graph with Seaborn or Matplotlib of the evolution of population for different regions , during different years.</p>
<p>I want all the columns to be in X axis, and the population count in Y axis, and every line will be representative to a specific region, so different colours.</p>
<p>Does anyone know how to display this ?</p>
<p>Here is my data frame in the image below.
<a href=""https://i.stack.imgur.com/4scrQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4scrQ.png"" alt=""data frame regions, years from 1995 to 2050 and number of population"" /></a>
<a href=""https://i.stack.imgur.com/WC2JP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WC2JP.png"" alt=""The data before the group by I added "" /></a></p>
<p>Thanks in advance.</p>
",9670093.0,9670093.0,2022-04-01 13:46:22,2022-04-01 13:46:22,"graphs for evolution with Seaborn or Matplotlib ( many columns as X axis , very column holds a number )",<python><matplotlib><graphics><seaborn><data-science>,0,3,N/A,CC BY-SA 4.0
74465808,1,74466638.0,2022-11-16 18:58:04,-1,272,"<p>I want a heat map plot as can be seen in the attached image
<a href=""https://i.stack.imgur.com/eivKk.png"" rel=""nofollow noreferrer"">day in x axis, time in y axis and a heatmap plot</a></p>
<p>data- <a href=""https://1drv.ms/x/s!Av8bxRzsdiR7tEYmXDBWSUKriCSJ?e=m2objJ"" rel=""nofollow noreferrer"">https://1drv.ms/x/s!Av8bxRzsdiR7tEYmXDBWSUKriCSJ?e=m2objJ</a>
I tried plotting the data, but its leading to daily plots of the values</p>
",20520363.0,-1.0,N/A,2022-11-16 20:12:08,"How to plot day in x axis, time in y axis and a heatmap plot for the values in python as shown in the figure?",<python><data-science><data-analysis><heatmap><timeserieschart>,1,0,N/A,CC BY-SA 4.0
74478602,1,74478902.0,2022-11-17 16:05:51,0,371,"<p>I have a pandas dataframe with columns names as ['INV01_M1_I', 'INV01_M1_V', 'INV01_M2_I', 'INV01_M2_V', 'INV02_M1_I', 'INV02_M1_V', 'INV02_M2_I', 'INV02_M2_V'....] AND SO ON. I want to sum those columns which have same 'INV_no_here' and the last character i.e. I or V. That is sum INV01_M1_I+INVO1_M2_I in one column and INV02_M1_I+INV02_M2_I in one column(if i can name them there it will be nice to do so). I have almost 100+ columns where number changes from 01 to the end for INV. I have gone through different answers where regex, filter(like=), and other different solutions are provided. But I need to match first 5 characters and last character and then also sum those columns.</p>
<pre><code>import numpy as np
import pandas pd
data = [[20, 10, 13, 16, 18, 20, 9, 6], [7, 15, 11, 16, 27, 7, 19, 10]]
df = pd.DataFrame(data, columns=['INV01_M1_I', 'INV01_M1_V','INV01_M2_I','INV01_M2_V',
'INV02_M1_I','INV02_M1_V','INV02_M2_I','INV02_M2_V'])
print(df)
</code></pre>
",19486982.0,-1.0,N/A,2022-11-17 16:55:28,Selecting columns based on characters in column names,<python><pandas><dataframe><data-science><character>,1,0,N/A,CC BY-SA 4.0
74490987,1,-1.0,2022-11-18 14:15:15,-1,175,"<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

x = np.linspace(-5.0, 5.0, 100)
y = np.sqrt(10**2 - x**2)
y=np.hstack([y,-y])
x=np.hstack([x,-x])

x1 = np.linspace(-5.0, 5.0, 100)
y1 = np.sqrt(5**2 - x1**2)
y1=np.hstack([y1,-y1])
x1=np.hstack([x1,-x1])


plt.scatter(y,x)
plt.scatter(y1,x1)
# print(plt.show())

import pandas as pd
df1 =pd.DataFrame(np.vstack([y,x]).T,columns=['X1','X2'])
df1['Y']=0

df2 =pd.DataFrame(np.vstack([y1,x1]).T,columns=['X1','X2'])
df2['Y']=1


df1.merge(df2)

# We need to find components for the Polynomical Kernel
#X1,X2,X1_square,X2_square,X1*X2
df1['X1_Square']= df1['X1']**2
df1['X2_Square']= df1['X2']**2
df1['X1*X2'] = (df1['X1'] *df1['X2'])

# print(df1.head())

### Independent and Dependent features
X = df1[['X1','X2','X1_Square','X2_Square','X1*X2']]
y = df1['Y']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size = 0.25, 
                                                    random_state = 0)


from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
classifier = SVC(kernel=&quot;linear&quot;)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
accuracy_score(y_test, y_pred)

</code></pre>
<p>ValueError: The number of classes has to be greater than one; got 1 class</p>
<p>I Don't know How to resolve this one error.May be there is error in merge of two data frames or I to append df1 and df2 but i tried it that doesn't work.</p>
",19026830.0,-1.0,N/A,2022-11-24 15:13:23,The number of classes has to be greater than one; got 1 class in SVM,<machine-learning><data-science><artificial-intelligence><libsvm>,1,0,N/A,CC BY-SA 4.0
74491145,1,74491395.0,2022-11-18 14:26:53,0,33,"<p>Although it might be easy but i am not able to get a hang of it..</p>
<p>I want to assign the result to a new variable every time the for loop iteration occurs. I don't wish to do with initializing the list or dict and then adding the result. Because that way still it won't assign to a new variable each time.</p>
<p>Basically i want to automate this set of code.</p>
<pre><code>data_Absen_1 = pd.read_excel('Absenteesim jan22.xlsx')
data_Absen_2 = pd.read_excel('Absenteesim feb22.xlsx')
data_Absen_3 = pd.read_excel('Absenteesim mar22.xlsx')
data_Absen_4 = pd.read_excel('Absenteesim apr22.xlsx')
data_Absen_5 = pd.read_excel('Absenteesim may22.xlsx')
data_Absen_6 = pd.read_excel('Absenteesim jun22.xlsx')
</code></pre>
<p>or you can consider automating this</p>
<pre><code>data_Absen_1 = data[0]
data_Absen_2 = data[1]
data_Absen_3 = data[2]
data_Absen_4 = data[3]
</code></pre>
",20540274.0,20540274.0,2022-11-18 14:28:16,2022-11-18 15:07:13,How can we assign new variables after each for loop iteration in python?,<python><data-science-experience>,1,2,N/A,CC BY-SA 4.0
71699098,1,71793504.0,2022-03-31 21:02:07,5,2541,"<p>I am getting an error on my modeling of lightgbm searching for optimal auc. Any help would be appreciated.</p>
<pre><code>import optuna  
from sklearn.model_selection import StratifiedKFold
from optuna.integration import LightGBMPruningCallback
def objective(trial, X, y):
    param = {
        &quot;objective&quot;: &quot;binary&quot;,
        &quot;metric&quot;: &quot;auc&quot;,
        &quot;verbosity&quot;: -1,
        &quot;boosting_type&quot;: &quot;gbdt&quot;,
        &quot;lambda_l1&quot;: trial.suggest_loguniform(&quot;lambda_l1&quot;, 1e-8, 10.0),
        &quot;lambda_l2&quot;: trial.suggest_loguniform(&quot;lambda_l2&quot;, 1e-8, 10.0),
        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),
        &quot;feature_fraction&quot;: trial.suggest_uniform(&quot;feature_fraction&quot;, 0.4, 1.0),
        &quot;bagging_fraction&quot;: trial.suggest_uniform(&quot;bagging_fraction&quot;, 0.4, 1.0),
        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),
        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),
    }


    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)

    cv_scores = np.empty(5)
    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, &quot;auc&quot;)
        
        model = lgb.LGBMClassifier(**param)
        
        model.fit(
            X_train,
            y_train,
            eval_set=[(X_test, y_test)],
            early_stopping_rounds=100,
            callbacks=[pruning_callback])
        
        preds = model.predict_proba(X_test)
        cv_scores[idx] = log_loss(y_test, preds)
        auc_scores[idx] = roc_auc_score(y_test, preds)
        
    return np.mean(cv_scores), np.mean(auc_scores)
    


study = optuna.create_study(direction=&quot;minimize&quot;, study_name=&quot;LGBM Classifier&quot;)
func = lambda trial: objective(trial, sample_df[cols_to_keep], sample_df[target])

study.optimize(func, n_trials=1)
</code></pre>
<blockquote>
<p>Trial 0 failed because of the following error: ValueError('The
intermediate values are inconsistent with the objective values in
terms of study directions. Please specify a metric to be minimized for
LightGBMPruningCallback.',)*</p>
</blockquote>
",2812625.0,-1.0,N/A,2022-04-08 07:52:26,Optuna LightGBM LightGBMPruningCallback,<python><python-3.x><data-science><lightgbm><optuna>,1,0,N/A,CC BY-SA 4.0
74468763,1,74469588.0,2022-11-17 00:19:14,0,22,"<p>I have a csv file with 50 comma seperated values. for example, a row:</p>
<pre><code>3290,171,12,134,23,1824,228,245,147,2999,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
</code></pre>
<p>I want to specify headers to the 11 first columns in this csv file. I tried some ways but the data seems is corrupted.<br />
What I did:</p>
<pre><code>df = pd.read_csv(&quot;info/info.data&quot;, sep=',', header=None)
df_11 = df.iloc[:, 1:11]
df_11.columns = ['A', 'B', 'C', 'D', 'E', 'F' 'G', 'H', 'I', 'J', 'K']
</code></pre>
<p>What I'm doing wrong?</p>
",16356219.0,-1.0,N/A,2022-11-17 02:50:52,How to specify header to a specific number of columns in csv and panda dataframe,<python><python-3.x><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
74494041,1,-1.0,2022-11-18 18:38:29,0,148,"<p>I'm developing a script in Python 3.9 that works perfectly on my personal PC. When I tried moving and running it on a server, it gives me the following error:</p>
<p><a href=""https://i.stack.imgur.com/oqGp1.png"" rel=""nofollow noreferrer"">there the error</a></p>
<p>Does anyone know how to fix this error?</p>
<p>Laptop version: 3.10.7
server version: 3.9.0</p>
<p>I tried to install dotenv via pip manually, but it doesn't work.</p>
<p>also tried shebang but doesnt work.</p>
",20514449.0,-1.0,N/A,2023-09-08 15:34:55,OSError: Starting path not found (dotenv-0.21.0-py3.9.egg) (python),<python-3.x><data-science><dotenv><python-dotenv>,1,1,N/A,CC BY-SA 4.0
74492647,1,74492776.0,2022-11-18 16:27:52,1,34,"<p>Here is the code we're working with; basically just takes data from multiple scrapped datasets and then concatenates them.</p>
<pre><code>import pandas as pd
import numpy as np # for numeric python functions
from pylab import * # for easy matplotlib plotting
from bs4 import BeautifulSoup
import requests
url1='http://openinsider.com/screener?s=&amp;o=&amp;pl=&amp;ph=&amp;ll=&amp;lh=&amp;fd=30&amp;fdr=&amp;td=0&amp;tdr=&amp;fdlyl=&amp;fdlyh=&amp;daysago=&amp;xp=1&amp;vl=&amp;vh=&amp;ocl=&amp;och=&amp;sic1=-1&amp;sicl=100&amp;sich=9999&amp;grp=0&amp;nfl=&amp;nfh=&amp;nil=&amp;nih=&amp;nol=&amp;noh=&amp;v2l=&amp;v2h=&amp;oc2l=&amp;oc2h=&amp;sortcol=0&amp;cnt=100&amp;page=1'
df1 = pd.read_html(url1)
table=df1[11]
#the table works - now lets make it look at change owned to find the largest value
#sorting
n = np.quantile(table['Qty'], [0.50])
print(&quot;99th percentile: &quot;,n)
q=table.sort_values('Qty', ascending = False)
page = requests.get(url1)
name=q['Ticker'].str.replace('\d+', '')
name1 = (table['Ticker'])
n = name1.count()
#Buyers for the company
All = []
url = 'http://openinsider.com/'
for entry in name1:
  table2 = pd.read_html(url+entry)
  dfn=table2[11]
  All.append(dfn)
All = pd.concat(All)
print(All.columns)#&lt;- my sanity check
print(All['Insider Name'])#&lt;- where the problem lies
</code></pre>
<p>Now if you look at the concatenated dataset, you'll see the &quot;Insider Name&quot; column. I want to isolate this column, but when I do, python says:</p>
<pre><code>
KeyError: 'Insider Name'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3361                 return self._engine.get_loc(casted_key)
   3362             except KeyError as err:
-&gt; 3363                 raise KeyError(key) from err
   3364 
   3365         if is_scalar(key) and isna(key) and not self.hasnans:

KeyError: 'Insider Name'
</code></pre>
<p>So the column exists, but it also doesn't? Any tips would be greatly appreciated!
Thanks in advance!</p>
",19362012.0,-1.0,N/A,2022-11-18 16:43:45,Column Does not Show Up in Pandas?,<python><pandas><web-scraping><data-science>,1,1,N/A,CC BY-SA 4.0
74493144,1,74493203.0,2022-11-18 17:08:51,1,449,"<pre><code>import umap as UMAP

import umap


retarget = {df_train['target'].value_counts().reset_index()['index'][i]: i for i in range(len(df_train['target'].value_counts()))}
retarget2 = {i: k for k, i in retarget.items()}
df_train['target'] = df_train['target'].map(retarget)

umap = umap(n_components = 2, n_neighbors = 10, min_dist = 0.99).fit_transform(df_train.drop('target', axis = 1).sample(15000, random_state = 228), df_train['target'].sample(15000, random_state = 228))

</code></pre>
<p>I am trying to use UMAP for visualization but it is keep on giving me error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-15-bb51a04f463b&gt; in &lt;module&gt;
      8 df_train['target'] = df_train['target'].map(retarget)
      9 
---&gt; 10 umap = umap(n_components = 2, n_neighbors = 10, min_dist = 0.99).fit_transform(df_train.drop('target', axis = 1).sample(15000, random_state = 228), df_train['target'].sample(15000, random_state = 228))
     11 
     12 plt.figure(figsize=(15, 12))

TypeError: 'module' object is not callable
</code></pre>
<p>I have umap installed in my system:</p>
<pre><code>(base) C:\Users\bakumari\Anaconda3\Lib\site-packages&gt;pip install umap
Collecting umap
  Using cached umap-0.1.1-py3-none-any.whl
Installing collected packages: umap
Successfully installed umap-0.1.1
</code></pre>
<p>I am trying to use umap for visualization purpose.</p>
",15944204.0,-1.0,N/A,2023-10-03 09:34:58,TypeError: 'module' object is not callable - while using UMAP,<python><data-science><runumap>,1,1,N/A,CC BY-SA 4.0
74497166,1,74506560.0,2022-11-19 02:36:17,1,7143,"<p>I am confusing about my fine-tune model implemented by Huggingface model. I am able to train my model, but while I want to predict it, I always get this error. The most similar problem is <a href=""https://discuss.huggingface.co/t/runtimeerror-expected-all-tensors-to-be-on-the-same-device-but-found-at-least-two-devices-cpu-and-cuda-0-when-checking-arugment-for-argument-index-in-method-wrapper-index-select/9255"" rel=""nofollow noreferrer"">this</a>. My transformers version is 4.24.0, but it didn't seem to help me. I also try <a href=""https://stackoverflow.com/questions/70102323/runtimeerror-expected-all-tensors-to-be-on-the-same-device-but-found-at-least"">this</a>. Below is my code snippet.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer
from transformers import DataCollatorForSeq2Seq
from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from transformers import pipeline
from tqdm import tqdm
from datasets import Dataset

import pandas as pd
import numpy as np
import pyarrow as pa
import gc
import torch as t
import pickle

PATH = './datas/Batch_answers - train_data (no-blank).csv'
EPOCH = 1
LEARNING_RATE = 2e-5
TRAIN_BATCH_SIZE = 16
EVAL_BATCH_SIZE = 16
DEVICE = 'cuda' if t.cuda.is_available() else 'cpu'

df = pd.read_csv(PATH)
df = df.drop(labels='s', axis=1)
df = df.iloc[:, 1:5]
df = df.to_numpy()
qData = []

for i in tqdm(range(len(df))):
    argument = df[i][0][1:-1]
    response = df[i][1][1:-1]
    qprime = df[i][2][1:-1]
    
    qData.append({'statement':argument+'\n'+response, 'argument_sentence_summary':qprime})
    
qtable = pa.Table.from_pylist(qData)
qDataset = Dataset(qtable)
qDataset = qDataset.train_test_split(train_size=0.8)

qModel = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-small&quot;)
qTokenizer = AutoTokenizer.from_pretrained(&quot;t5-small&quot;)
qData_collator = DataCollatorForSeq2Seq(tokenizer=qTokenizer, model=qModel)

def Qpreprocessing(data):
    model_input = qTokenizer(data['statement'], max_length=250, truncation=True)
    labels = qTokenizer(text_target=data['argument_sentence_summary'], max_length=75, truncation=True)

    model_input['labels'] = labels['input_ids']
    
    return model_input

qToken = qDataset.map(Qpreprocessing, batched=True)

qTraining_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./result&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=EVAL_BATCH_SIZE,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=EPOCH,
    fp16=True,
)

qTrainer = Seq2SeqTrainer(
    model=qModel,
    args=qTraining_args,
    train_dataset=qToken['train'],
    eval_dataset=qToken['test'],
    tokenizer=qTokenizer,
    data_collator=qData_collator
)

old_collator = qTrainer.data_collator
qTrainer.data_collator = lambda data: dict(old_collator(data))
qTrainer.train()

qp = pipeline('summarization', model=qModel, tokenizer=qTokenizer)
qp(qDataset['test'][0]['statement']) #break in this line
</code></pre>
<p>The full traceback:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
Cell In [20], line 3
      1 qp = pipeline('summarization', model=qModel, tokenizer=qTokenizer)
      2 # temp = t.tensor(qDataset['test'][0]['statement']).to(DEVICE)
----&gt; 3 qp(qDataset['train'][0]['statement'])

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:250, in SummarizationPipeline.__call__(self, *args, **kwargs)
    226 def __call__(self, *args, **kwargs):
    227     r&quot;&quot;&quot;
    228     Summarize the text(s) given as inputs.
    229 
   (...)
    248           ids of the summary.
    249     &quot;&quot;&quot;
--&gt; 250     return super().__call__(*args, **kwargs)

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:150, in Text2TextGenerationPipeline.__call__(self, *args, **kwargs)
    121 def __call__(self, *args, **kwargs):
    122     r&quot;&quot;&quot;
    123     Generate the output text(s) using text(s) given as inputs.
    124 
   (...)
    147           ids of the generated text.
    148     &quot;&quot;&quot;
--&gt; 150     result = super().__call__(*args, **kwargs)
    151     if (
    152         isinstance(args[0], list)
    153         and all(isinstance(el, str) for el in args[0])
    154         and all(len(res) == 1 for res in result)
    155     ):
    156         return [res[0] for res in result]

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:1074, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1072     return self.iterate(inputs, preprocess_params, forward_params, postprocess_params)
   1073 else:
-&gt; 1074     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:1081, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1079 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1080     model_inputs = self.preprocess(inputs, **preprocess_params)
-&gt; 1081     model_outputs = self.forward(model_inputs, **forward_params)
   1082     outputs = self.postprocess(model_outputs, **postprocess_params)
   1083     return outputs

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\base.py:990, in Pipeline.forward(self, model_inputs, **forward_params)
    988     with inference_context():
    989         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
--&gt; 990         model_outputs = self._forward(model_inputs, **forward_params)
    991         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(&quot;cpu&quot;))
    992 else:

File ~\anaconda3\envs\ame\lib\site-packages\transformers\pipelines\text2text_generation.py:172, in Text2TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)
    170 generate_kwargs[&quot;max_length&quot;] = generate_kwargs.get(&quot;max_length&quot;, self.model.config.max_length)
    171 self.check_inputs(input_length, generate_kwargs[&quot;min_length&quot;], generate_kwargs[&quot;max_length&quot;])
--&gt; 172 output_ids = self.model.generate(**model_inputs, **generate_kwargs)
    173 out_b = output_ids.shape[0]
    174 if self.framework == &quot;pt&quot;:

File ~\anaconda3\envs\ame\lib\site-packages\torch\autograd\grad_mode.py:27, in _DecoratorContextManager.__call__.&lt;locals&gt;.decorate_context(*args, **kwargs)
     24 @functools.wraps(func)
     25 def decorate_context(*args, **kwargs):
     26     with self.clone():
---&gt; 27         return func(*args, **kwargs)

File ~\anaconda3\envs\ame\lib\site-packages\transformers\generation_utils.py:1339, in GenerationMixin.generate(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)
   1331         logger.warning(
   1332             &quot;A decoder-only architecture is being used, but right-padding was detected! For correct &quot;
   1333             &quot;generation results, please set `padding_side='left'` when initializing the tokenizer.&quot;
   1334         )
   1336 if self.config.is_encoder_decoder and &quot;encoder_outputs&quot; not in model_kwargs:
   1337     # if model is encoder decoder encoder_outputs are created
   1338     # and added to `model_kwargs`
-&gt; 1339     model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
   1340         inputs_tensor, model_kwargs, model_input_name
   1341     )
   1343 # 4. Prepare `input_ids` which will be used for auto-regressive generation
   1344 if self.config.is_encoder_decoder:

File ~\anaconda3\envs\ame\lib\site-packages\transformers\generation_utils.py:583, in GenerationMixin._prepare_encoder_decoder_kwargs_for_generation(self, inputs_tensor, model_kwargs, model_input_name)
    581 encoder_kwargs[&quot;return_dict&quot;] = True
    582 encoder_kwargs[model_input_name] = inputs_tensor
--&gt; 583 model_kwargs[&quot;encoder_outputs&quot;]: ModelOutput = encoder(**encoder_kwargs)
    585 return model_kwargs

File ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\module.py:1130, in Module._call_impl(self, *input, **kwargs)
   1126 # If we don't have any hooks, we want to skip the rest of the logic in
   1127 # this function, and just call forward.
   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1130     return forward_call(*input, **kwargs)
   1131 # Do not call functions when jit is used
   1132 full_backward_hooks, non_full_backward_hooks = [], []

File ~\anaconda3\envs\ame\lib\site-packages\transformers\models\t5\modeling_t5.py:941, in T5Stack.forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    939 if inputs_embeds is None:
    940     assert self.embed_tokens is not None, &quot;You have to initialize the model with valid token embeddings&quot;
--&gt; 941     inputs_embeds = self.embed_tokens(input_ids)
    943 batch_size, seq_length = input_shape
    945 # required mask seq length can be calculated via length of past

File ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\module.py:1130, in Module._call_impl(self, *input, **kwargs)
   1126 # If we don't have any hooks, we want to skip the rest of the logic in
   1127 # this function, and just call forward.
   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1130     return forward_call(*input, **kwargs)
   1131 # Do not call functions when jit is used
   1132 full_backward_hooks, non_full_backward_hooks = [], []

File ~\anaconda3\envs\ame\lib\site-packages\torch\nn\modules\sparse.py:158, in Embedding.forward(self, input)
    157 def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 158     return F.embedding(
    159         input, self.weight, self.padding_idx, self.max_norm,
    160         self.norm_type, self.scale_grad_by_freq, self.sparse)

File ~\anaconda3\envs\ame\lib\site-packages\torch\nn\functional.py:2199, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2193     # Note [embedding_renorm set_grad_enabled]
   2194     # XXX: equivalent to
   2195     # with torch.no_grad():
   2196     #   torch.embedding_renorm_
   2197     # remove once script supports set_grad_enabled
   2198     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2199 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)
</code></pre>
<p>Should that mean I need another way to predict my test dataset instead of using pipeline? Big thanks for help.</p>
",20482542.0,20482542.0,2022-11-19 06:03:13,2022-11-20 08:12:37,"Huggingface: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu",<python><pytorch><data-science><huggingface-transformers>,1,5,N/A,CC BY-SA 4.0
74497195,1,-1.0,2022-11-19 02:46:01,0,515,"<p>I have created a database <strong>main_data</strong> with a table <strong>users</strong>. When I try to add a new record to the table, I get the following error:</p>
<pre><code>sqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (1364, &quot;Field 'id' doesn't have a default value&quot;)
</code></pre>
<p>There are no normal guides and solutions to the problem on the Internet.
Source code:</p>
<pre><code>from sqlalchemy import create_engine
from sqlalchemy_utils import database_exists
from sqlalchemy import Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_NAME = &quot;main_data&quot;
engine = create_engine(f&quot;mysql+pymysql://root:toor/{DATABASE_NAME}&quot;)
Base = declarative_base()


class Users(Base):
    __tablename__ = &quot;users&quot;

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer)
    username = Column(String)
    status = Column(String)
    name = Column(String)
    location_city = Column(String)
    location_address = Column(String)
    contacts = Column(String)
    announcements = Column(String)
    balance = Column(Integer)


Session = sessionmaker(bind=engine)
session = Session()


def add_user(user_id, username, status, name, location_city, location_address, contacts, announcements, balance):
    user = Users(user_id=user_id, username=username, status=status, name=name, location_city=location_city,
                 location_address=location_address, contacts=contacts, announcements=announcements, balance=balance)
    session.add(user)
    session.commit()

</code></pre>
",13471945.0,-1.0,N/A,2022-11-19 02:46:01,SQLAlchemy error: Field 'id' doesn't have a default value,<python><mysql><database><sqlalchemy><data-science>,0,5,N/A,CC BY-SA 4.0
71710186,1,72578385.0,2022-04-01 16:34:09,0,2062,"<p>I have used bert base pretrained model with 512 dimensions to generate contextual features. Feeding those vectors to random forest classifier is providing 83 percent accuracy but in various researches i have seen that bert minimal gives 90 percent.
I have some other features too like word2vec, lexicon, TFIDF and punctuation features.
Even when i merged all the features i got 83 percent accuracy. The research paper which i am using as base paper mentioned an accuracy score of 92 percent but they have used an ensemble based approach in which they classified through bert and trained random forest on weights.
But i was willing to do some innovation thus didn't followed that approach.
My dataset is biased to positive reviews so according to me the accuracy is less as model is also biased for positive labels but still I am looking for an expert advise</p>
<p>Code implementation of bert</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/Bert_Features.ipynb</code></p>
<p>Random forest on all features independently</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/RandomForestClassifier.ipynb</code></p>
<p>Random forest on all features jointly</p>
<p><code>https://github.com/Awais-mohammad/Sentiment-Analysis/blob/main/Merging_Feature.ipynb</code></p>
",13368640.0,-1.0,N/A,2022-06-10 18:28:14,Creating word embedings from bert and feeding them to random forest for classification,<machine-learning><nlp><data-science><classification><bert-language-model>,1,0,N/A,CC BY-SA 4.0
71715619,1,-1.0,2022-04-02 07:23:34,0,543,"<p>Hello I'm trying to display the figure of this code in google colab it's figuring the plot but when I try to click each point it's not working, it should be turning a green circle, I think the problem is on_click function but i couldn't resolute that problem can anyone help me? I think f.show makes my figure static but i need dynamic figure each touch should call my function</p>
<pre><code>        import plotly.graph_objects as go

import numpy as np
np.random.seed(1)

x = np.random.rand(100)
y = np.random.rand(100)

f = go.FigureWidget([go.Scatter(x=x, y=y, mode='markers')])

scatter = f.data[0]
colors = ['#a3a7e4'] * 100
scatter.marker.color = colors
scatter.marker.size = [10] * 100
f.layout.hovermode = 'closest'


# create our callback function
def update_point(trace, points, selector):
    c = list(scatter.marker.color)
    s = list(scatter.marker.size)
    for i in points.point_inds:
        c[i] = '#bae2be'
        s[i] = 20
        with f.batch_update():
            scatter.marker.color = c
            scatter.marker.size = s


scatter.on_click(update_point)

f
</code></pre>
<p>f.show() makes figure static but i need interactive figure in here</p>
",18677259.0,18677259.0,2022-04-03 11:04:07,2022-04-03 11:04:07,Plotly on_click Function not working on Google Colab,<python><numpy><plotly><google-colaboratory><graph-data-science>,0,2,N/A,CC BY-SA 4.0
71696782,1,-1.0,2022-03-31 17:22:27,0,814,"<p>Given a table below</p>
<pre><code>X   Y   pr
0   1   0.30
0   2   0.25
1   1   0.15
1   2   0.30
</code></pre>
<p>I want to create a custom function that calculates the covariance and variance between X and Y</p>
<p>I need to find the mean of both x and y, then subtract each value from the means obtained earlier. Then multiply the previous results</p>
<p>Here is the not so good code.</p>
<pre><code>def cor(data_frame):
    data_frame[['X']].mean()
    data_frame[['Y']].mean()

    cov = pd.merge(distr_table.groupby('X', as_index=False)['pr'].mean(), distr_table.groupby('Y', as_index=False)['pr'].mean(), how='cross')
</code></pre>
<p>I need to find a way to iterate and loop through. Thanks</p>
",17097058.0,-1.0,2022-03-31 17:28:04,2022-04-01 01:09:22,How to calculate correlation and covariance of X and Y in Python,<python><pandas><numpy><data-science><correlation>,2,4,N/A,CC BY-SA 4.0
74508246,1,-1.0,2022-11-20 12:53:04,0,25,"<p>I was trying to return a dataframe of all the results who's 'high' value of the present day breaches after given number of days. and also return after how many days did it breach its 'high' value. but i get this error now.</p>
<pre class=""lang-py prettyprint-override""><code>instruments = df['instrumnet'].unique()
buy_df = pd.DataFrame()
wait_days = [1,2]
for i in instruments:
    stock_data = new_df.loc[df['instrumnet'] == i]
    stock_data.reset_index()
    for j in range(1,len(stock_data.axes[0])):         
        if ((stock_data['emb_flag'].iloc[j] == 1) &amp; (stock_data['emb_flag'].iloc[j-1] == 0)):
            for k in wait_days:
                if ((stock_data['high'].iloc[j])&lt;(stock_data['high'].iloc[j+k])):
                    buy_df = buy_df.append(stock_data.iloc[j])
                    buy_df['wait_time'] = k
            
</code></pre>
",20554669.0,20554669.0,2022-11-20 14:51:36,2022-11-20 14:51:36,"Error while comparing two cells from a dataframe. ""IndexError: single positional indexer is out-of-bounds"" is the error i get",<python><pandas><dataframe><data-science>,0,2,N/A,CC BY-SA 4.0
74508877,1,-1.0,2022-11-20 14:19:35,1,188,"<p>I am trying to go over all transactions data from every block on the bitcoin blockchain from the previous 4 years. With almost 2k transaction per block, it will take a lot of queries per block.
I have a full node running locally and I tried two ways:</p>
<p>Python with RPC: This is very slow and keeps losing connection after some time (httpx.ReadTimeout)</p>
<p>Python with os.popen commands: Doesn't have the connection problem, but still very slow.</p>
<p>Would there be any other way? Any recommendation on how to analyze bulk data from the blockchain? The methods listed above are unfeasible given the time it would take.</p>
<p>EDIT: The problem isn't memory, but the time the bitcoin node takes to answer the queries.</p>
",6132153.0,6132153.0,2022-11-20 14:44:47,2022-11-28 16:17:08,Analyse huge amount of blockchain data,<data-science><blockchain><data-analysis><bitcoin><cryptocurrency>,1,4,N/A,CC BY-SA 4.0
71716723,1,-1.0,2022-04-02 10:26:03,0,95,"<p>Basically what the question above asks.
KM survival function considers censored data in its calculations untill it is censored.</p>
<p>But, how will the change in each point of time would be affected if we assume from the start that there is no censoring at all in the data?</p>
<p>Thanks in advance!</p>
",18679394.0,-1.0,N/A,2022-04-02 13:25:07,How is the change in Kaplan Meier survival function is effect if there is no censoring?,<data-science><survival-analysis>,1,0,N/A,CC BY-SA 4.0
71717732,1,-1.0,2022-04-02 12:56:49,0,66,"<p>I have such data shown on the histogram below:</p>
<p><a href=""https://i.stack.imgur.com/Tiyl4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tiyl4.png"" alt=""enter image description here"" /></a></p>
<p>I would like to cut using median, but the outcome I received was filtering on values, not their count (horizontal axis is value, vertical is quantity of such values). I want it to be something like rectangle on the plot. How can I make it? My present code is shown below:</p>
<pre class=""lang-py prettyprint-override""><code>def cutting(arr):

    df = pd.DataFrame(xy)
    df = df[df&lt;df.median()]
    
    return df.to_numpy()

arr = cutting(arr)

plt.hist(arr, bins=100);
plt.show()
</code></pre>
<p>Edit: generated example of the data
<a href=""https://drive.google.com/file/d/12Kt11m7F15kUsStRSj3LIYCkszzon_X9/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/12Kt11m7F15kUsStRSj3LIYCkszzon_X9/view?usp=sharing</a></p>
<p>Edit_2 (better explanation of what I'd like to have): When you look at the plot above, there are some spikes that tell us, there are much more of some values than the other. I don't want it. I want to keep it as it is BUT, when some of the data is bigger than median, of any other threshold value it is changed (lower) to the threshold. After such filtering, data shown on the plot would be smooth, without spikes.</p>
",16431938.0,16431938.0,2022-04-02 14:20:58,2022-04-02 14:39:02,How to cut data to be uniform?,<python><pandas><filter><data-science>,0,4,N/A,CC BY-SA 4.0
74494117,1,-1.0,2022-11-18 18:45:56,3,89,"<p>I'm a fantasy basketball player and there is a recurring problem that I have not been able to figure out, and I can't find a similar enough example online that I can adapt for my own usage. For those who don't know, streaming (in fantasy basketball) is when you have an open spot on your individual team that you cycle available / free agent players through, in an attempt to maximize points, based on when they play during the week.</p>
<p>For example, Player 1 has games on Monday, Tuesday and Wednesday, and P2 plays on Thursday, Friday and Sunday. You keep P1 rostered through Wednesday, then drop them and pick up P2 for the rest of the week. Both players' points are added to your total; this allows you to essentially benefit from having two players' scoring for the week while only using one roster spot.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>M</th>
<th>T</th>
<th>W</th>
<th>Th</th>
<th>F</th>
<th>S</th>
<th>S</th>
<th>Avg Pts / Game</th>
</tr>
</thead>
<tbody>
<tr>
<td>P1</td>
<td>x</td>
<td>x</td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>20</td>
</tr>
<tr>
<td>P2</td>
<td></td>
<td></td>
<td></td>
<td>x</td>
<td>x</td>
<td>x</td>
<td></td>
<td>25</td>
</tr>
</tbody>
</table>
</div>
<p>The optimal strategy yields 135 points, vs 60 or 75 from just playing one player for the whole week.</p>
<p>The complexity comes from three places.</p>
<ol>
<li><p>Players you want to play sometimes overlap on the same day, and their schedules often overlap in a way such that it might be worth it to start a worse player early in the week if it means you get access to a slate of their games later in the week. A player may have a lower points per game than another player, but by virtue of playing more games it’s worth it to start the player with a lower average.</p>
</li>
<li><p>Players can’t be re-added for the rest of the week once dropped, due to the nature of the fantasy sports platform. So you can’t toggle back and forth between two good players if they have complementary schedules. Related to this, there are a set number of “adds” you can use per week. Without this you could just add 7 different players, one each day, and figure out who was going to be the best for each day of the week. For the purposes of this league, the number of adds is 4 per week.</p>
</li>
<li><p>There are lots of available players. Figuring this out manually sometimes means starting players who are ranked far outside the top 130 (the total number of players across all of the leagues rosters) in Points Per Game, but due to their scheduling quirks, offer the best value for the remainder of the week. Trying each combination of players against each other involves a huge number of potential options.</p>
</li>
</ol>
<p>I am a CS hobbyist, but the things I build do not require these sorts of optimizations, and I haven’t been able to find an example that would solve all parts of this problem.</p>
<p>Take the below schedule, for example. An “x” indicates the player is playing that day:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>M</th>
<th>T</th>
<th>W</th>
<th>Th</th>
<th>F</th>
<th>S</th>
<th>S</th>
<th>Avg Pts / Game</th>
</tr>
</thead>
<tbody>
<tr>
<td>P1</td>
<td>x</td>
<td>x</td>
<td></td>
<td></td>
<td>x</td>
<td>x</td>
<td></td>
<td>20</td>
</tr>
<tr>
<td>P2</td>
<td></td>
<td>x</td>
<td>x</td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td>21</td>
</tr>
<tr>
<td>P3</td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>x</td>
<td>x</td>
<td>22</td>
</tr>
</tbody>
</table>
</div>
<p>The points maximizing solution is to start Player 1 Monday, drop them and switch to Player 2 for the next three days, start no one on Thursday (as P1 would not be available after dropping them), and then switch to Player 3 for the last two days.</p>
<p>Start P1: 20 + 63 + 44 = 127</p>
<p>Constraints:</p>
<ul>
<li>There are a set number of available days (7)</li>
<li>Only one player can be played per day</li>
<li>Each player can only be added one time (but can play consecutive games after being added with no penalty).</li>
<li>You can only add new players a set number of times: “n”</li>
<li>There are “p” possible players, each of whom has days of the week they can be played and an average points per game (same across all days)</li>
<li>The objective is to return the schedule of players in the proper combination that optimizes the total points in a given week.</li>
</ul>
<p>My (somewhat brief) look through the existing algorithms led me to considering the knapsack problem, optimal scheduling, and greedy. Greedy doesn’t seem to work because it would immediately decide to start Player 3, which would not ultimately lead to the optimal solution, ending up with 125 total points</p>
<p>The scheduling algorithm would seem to require considering the start and end of a “task” as the first and last day a player plays, which creates problems here; Player 1’s “block” or whole week between first and last day would show that Player 1 offers the best total value at 80 points, and that the optimal strategy would be to just play them, but it doesn’t consider prematurely ending the task if something else was available.</p>
<p>Likewise I can’t figure out how to introduce the “switching” element into solutions to the knapsack problem.</p>
<p>There is a related question here but it doesn't include the scheduling aspect: <a href=""https://stackoverflow.com/questions/13154995/algorithm-to-select-player-with-max-points-but-with-a-given-cost"">Algorithm to select Player with max points but with a given cost</a></p>
<p>I feel like there’s a method that I’m not quite grasping, or a simplification that would allow me to use one of the aforementioned solutions. Any help in solving this completely inconsequential problem is greatly appreciated!</p>
",20535203.0,-1.0,N/A,2022-11-18 18:45:56,Which algorithm combines knapsack optimization with the efficiency of job scheduling (to create a points optimizing fantasy basketball lineup),<algorithm><data-science><computer-science><mathematical-optimization><combinatorics>,0,6,N/A,CC BY-SA 4.0
74494441,1,-1.0,2022-11-18 19:23:24,0,1001,"<p>I want to calculate the percentage change for the following data frame.</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C'],
                   'points': [12, 0, 19, 22, 0, 25, 0, 30],
                   'score': [12, 0, 19, 22, 0, 25, 0, 30] 
                   
                   })
print(df)
</code></pre>
<p>When I applied this step, it returns inf which is obvious because we are dividing by zero.</p>
<pre><code>df['score'] = df.groupby('team', sort=False)['score'].apply(
     lambda x: x.pct_change()).to_numpy()
</code></pre>
<p>But if we see in each column the change from 0 to 19 the change is 100%, from 0 to 25 the change is 100%, and from 0 to 30 the change is 100%. So, I was wondering how can I calculate those values.</p>
<p>current result
<a href=""https://i.stack.imgur.com/NIws7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NIws7.png"" alt=""enter image description here"" /></a></p>
<p>Expected result is
<a href=""https://i.stack.imgur.com/8PmH6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8PmH6.png"" alt=""enter image description here"" /></a></p>
",16875907.0,16875907.0,2022-11-18 20:34:10,2023-05-02 11:41:16,How to calculate percentage change with zero in pandas?,<python><pandas><dataframe><group-by><data-science-experience>,4,5,N/A,CC BY-SA 4.0
74510597,1,-1.0,2022-11-20 18:01:44,1,31,"<p>I am currently simulating some protein diffusion with python. This is my procedure</p>
<ol>
<li>Have some random walkers across a canvas of 32x32 pixels</li>
<li>Apply a Gaussian convolution filter to smooth out the random walkers pixels</li>
<li>Apply some random noise</li>
</ol>
<p>I obtain something like this</p>
<p><a href=""https://i.stack.imgur.com/svbzSm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/svbzSm.png"" alt=""enter image description here"" /></a></p>
<p>Where each pixel has an associated intensity value.</p>
<p>I am looking for ways to make this look more realistic. Particularly, maybe some ways to make the blobs more irregular? Maybe some spots of continuous brightness, I don't know. Any suggestions? I wish to have something that looks like this</p>
<p><a href=""https://i.stack.imgur.com/Cq3Pz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cq3Pz.png"" alt=""enter image description here"" /></a></p>
",12985747.0,-1.0,N/A,2022-11-20 18:01:44,Tips to make simulated images look more realistic?,<python><data-science><simulation>,0,0,N/A,CC BY-SA 4.0
74511256,1,-1.0,2022-11-20 19:28:08,0,66,"<p>I'm running a quick Monte Carlo error analysis. One of the parameters I'm looking at has a non-Gaussian error distribution; let's say Log10[param] = 9.33 +0.21 -0.01. In NumPy, how would I draw trials from a distribution centered at the parameter representing these asymmetric errors?</p>
",5012734.0,-1.0,N/A,2022-11-20 19:28:08,Drawing trials from asymmetric error distribution (Numpy),<python><numpy><random><data-science><montecarlo>,0,4,N/A,CC BY-SA 4.0
71716021,1,-1.0,2022-04-02 08:37:59,1,486,"<p>EDIT: Let me clarify more the point I am struggling to achieve as it seems not to be clear. I already built a table with TextFields as you see in my code, and I tried the excel package and I tried several other packages, however nothing allows me to COPY the input from an external spreadsheet to my web app directly. The user will eventually have to manually type all the digital sample values one by one or they have to upload an external excel sheet to the site. These samples can be as big as several thousand samples in some cases, the only option is to copy them from their original spreadsheet or text source to the web app.</p>
<p>In my flutter web app, I want to get input from the users: they input time samples of any digital signal into a series of <code>TextField</code>s then the site will plot these samples and do some processing on them etc..
The problem is that digital signals are usually long and users will want to copy them from other text files or spreadsheets rather than manually typing them one by one in the browser.
Below is part of the code I used so far</p>
<pre><code>class InputData extends StatefulWidget {

  @override
  State&lt;InputData&gt; createState() =&gt; _InputDataState();
}

class _InputDataState extends State&lt;InputData&gt; {
  @override
  Widget build(BuildContext context) {
    return Material(
      child:Scaffold(
        body: Column(
          children: &lt;Widget&gt;[
            TitleBanner(),
            Expanded(
              child: Row(
                children: [
                  Container(
                      height: 40,
                      width: 100,
                      child: Text('Sample Interval')),
                  Container(
                      height: 40,
                      width: 100,
                      child: TextField())
                ],
              ),
            ),
            SizedBox(height: 100,),
            Expanded(
              child: Padding(
                padding: const EdgeInsets.fromLTRB(8.0,0,0,0),
                child: Row(
                  children: &lt;Widget&gt;[
                    Column(
                        children: &lt;Widget&gt;[
                          Container(
                              decoration: BoxDecoration(border: Border.all() ),
                              height: 20,
                              width: 100,
                              child: Text('Samples')),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),

                        ]

                    ),
                    Column(
                        children: &lt;Widget&gt;[
                          Container(
                              decoration: BoxDecoration(border: Border.all() ),
                              height: 20,
                              width: 100,
                              child: Text('Amplitudes')),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                          buildTextField(),
                        ]
                    )


                  ],
                ),
              ),
            )

          ],
        ),
      )

    );
  }

  Container buildTextField() {
    return Container(
                          decoration: BoxDecoration(border: Border.all() ),
                          height: 20,
                          width: 100,
                          child: TextField(keyboardType: TextInputType.number,
                            decoration: kTextFieldDecoration
                          ),);
  }
}
</code></pre>
<p>This way the user can input the samples and amplitudes manually but they cannot copy them from another source. I tried using the <code>excel</code> package in flutter, but it seems to allow us to use a full excel spreadsheet and export it rather than integrating the cells inside the browser.
I tried also some other packages like <code>sticky_headers</code> but they seem to do good formatting only, without allowing a copy from external spreadsheet.</p>
<p>Is there a way to directly paste the values using this implementation?</p>
",2765900.0,2765900.0,2022-04-03 09:54:55,2022-04-05 19:14:29,Copy values from a spreadsheet to multiple textFields in flutter,<excel><flutter><data-science><signal-processing><flutter-web>,3,0,N/A,CC BY-SA 4.0
74524671,1,-1.0,2022-11-21 20:41:06,0,125,"<p>I am plotting buy and sell signals on a graph with plotly. I have successfully added the Buy and Sell signals. But how do I add a line chart for the price?</p>
<p>Here's my code I have taken random values:</p>
<pre><code>import plotly.graph_objects as go
fig = go.Figure()

x1=[0, 1, 2, 3, 4]
y1=[0, 1, 4, 9, 16]

x2=[9,3,6,1,6]
y2=[7,2,56,25,22]

# Add traces
fig.add_trace(go.Scatter(x=x1, y=y1,
                    mode='markers',
                    name='Buy'))

fig.add_trace(go.Scatter(x=x2, y=y2,
                    mode='markers',
                    name='Sell'))

fig.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/qieSL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qieSL.png"" alt=""enter image description here"" /></a></p>
<p>But how do I add a line chart for the price?</p>
",19689239.0,16733101.0,2022-11-21 20:59:13,2022-11-21 20:59:13,Add Line Chart in Plotly,<python><matplotlib><plotly><data-science><visualization>,0,2,N/A,CC BY-SA 4.0
74495142,1,74495345.0,2022-11-18 20:44:26,2,148,"<p>i have the following code data...</p>
<pre><code>import pandas as pd, numpy as np
from datetime import datetime
end_dt = datetime.today()
st_dt = (end_dt + pd.DateOffset(-10)).date()
df_index = pd.date_range(st_dt, end_dt)
df = pd.DataFrame(index=df_index, columns=['in_range'])

data = [pd.to_datetime(['2022-11-08','2022-11-10']), pd.to_datetime(['2022-11-13','2022-11-15'])]
dt_ranges = pd.DataFrame(data,columns={'st_dt':'datetimens[64]', 'end_dt': 'datetimens[64]'})
</code></pre>
<p>this produces the following two dataframes:<br />
df:</p>
<pre><code>            in_range
2022-11-08  NaN
2022-11-09  NaN
2022-11-10  NaN
2022-11-11  NaN
2022-11-12  NaN
2022-11-13  NaN
2022-11-14  NaN
2022-11-15  NaN
2022-11-16  NaN
2022-11-17  NaN
2022-11-18  NaN
</code></pre>
<p>and date_ranges:</p>
<pre><code>    st_dt       end_dt
0   2022-11-08  2022-11-10
1   2022-11-13  2022-11-15
</code></pre>
<p>I would like to update the 'in_range' column to indicate if the index falls within any of the pairs of start and end dates of the 2nd dataframe. so i should end up with this:</p>
<pre><code>            in_range
2022-11-08  True
2022-11-09  True
2022-11-10  True
2022-11-11  NaN
2022-11-12  NaN
2022-11-13  True
2022-11-14  True
2022-11-15  True
2022-11-16  NaN
2022-11-17  NaN
2022-11-18  NaN
</code></pre>
<p>I've gone down the path of trying to do this with using lambda and iteration.  but to me that seems in efficient.</p>
<pre><code>    def in_range(index_date, date_ranges):
        for r in date_ranges.values:
            if (r[0] &gt;= index_date) &amp; (r[1] &lt;= index_date):
                return True
        return False

     df['in_range'] = df.reset_index().apply(lambda x: in_range(x.date, dt_ranges), axis=1)
</code></pre>
<p>the above sets in_range to NaNs always, despite the code returning the correct values. i suspect it's because i am resetting the index and so it can not align.  Also, as mentioned - this solution probably is pretty inefficient</p>
<p>is there a more pythonic/pandemic way of doing this?</p>
",480118.0,480118.0,2022-11-18 20:58:14,2022-11-19 05:20:51,determine if datetime index is within a list of date ranges,<python><pandas><numpy><data-science>,2,0,N/A,CC BY-SA 4.0
74499347,1,74499446.0,2022-11-19 10:48:17,0,49,"<p>I have a list with strings and super script characters(as power).
I need to concatenate scripted values with strings.
But according to my coding part It repeating same value twice.
I have no idea to remove unnecessary values from the list.</p>
<p>my original list --&gt;</p>
<pre><code>separate_units = ['N', 'm', '⁻²⁴', 'kJ', 's', '⁻¹', 'km', '⁻²¹', 'kJ', '⁻²', 'm', '⁻²']
</code></pre>
<p>result according to my coding part --&gt;</p>
<pre><code>result = ['N', 'm', 'm⁻²⁴', 'kJ', 's', 's⁻¹', 'km', 'km⁻²¹', 'kJ', 'kJ⁻²', 'm', 'kJ⁻²']
</code></pre>
<p>I expected result --&gt;</p>
<pre><code> result = ['N', 'm⁻²⁴', 'kJ', 's⁻¹',  'km⁻²¹',  'kJ⁻²', 'm', 'kJ⁻²']
</code></pre>
<p>I tried to delete last index when concatenate two items.But It gave me an error.</p>
<p>--&gt;  del (separating_units[(separate_units.index(su) - 1)])  # grab the last (index of unit) value to delete from the list.</p>
<p>Error coding part --&gt;</p>
<pre><code>def power_set_to_unit():
    result = [su if su.isalpha() else del (separating_units[(separate_units.index(su) - 1)]) (separate_units[(separate_units.index(su) - 1)] + su) for su in separate_units]
    print(result)
</code></pre>
<p><em>I prefer to do this in single line coding part.</em>
Please help me to do this...</p>
",18161524.0,-1.0,N/A,2022-11-19 11:33:46,How to delete list items from list in python,<python><arraylist><data-science>,2,1,N/A,CC BY-SA 4.0
71723880,1,-1.0,2022-04-03 07:33:14,0,107,"<pre><code>df
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Unnamed: 0</th>
<th>Name</th>
<th>Age</th>
<th>Gender</th>
<th>Height</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>Asish</td>
<td>20</td>
<td>m</td>
<td>5.11</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>Meghali</td>
<td>23</td>
<td>f</td>
<td>5.9</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>Parimita</td>
<td>49</td>
<td>f</td>
<td>5.6</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>SatyaNarayan</td>
<td>60</td>
<td>m</td>
<td>5.1</td>
</tr>
</tbody>
</table>
</div>
<pre><code>df.reset_index(drop=True,inplace=True)

df
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Unnamed: 0</th>
<th>Name</th>
<th>Age</th>
<th>Gender</th>
<th>Height</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>Asish</td>
<td>20</td>
<td>m</td>
<td>5.11</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>Meghali</td>
<td>23</td>
<td>f</td>
<td>5.9</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>Parimita</td>
<td>49</td>
<td>f</td>
<td>5.6</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>SatyaNarayan</td>
<td>60</td>
<td>m</td>
<td>5.1</td>
</tr>
</tbody>
</table>
</div>
<pre><code>df=df.reset_index(drop=True)

df
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Unnamed: 0</th>
<th>Name</th>
<th>Age</th>
<th>Gender</th>
<th>Height</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>Asish</td>
<td>20</td>
<td>m</td>
<td>5.11</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>Meghali</td>
<td>23</td>
<td>f</td>
<td>5.9</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>Parimita</td>
<td>49</td>
<td>f</td>
<td>5.6</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>SatyaNarayan</td>
<td>60</td>
<td>m</td>
<td>5.1</td>
</tr>
</tbody>
</table>
</div>
<p>I have tried these above mentioned steps. However, they doesn't seem to resolve. I want something like below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Name</th>
<th>Age</th>
<th>Gender</th>
<th>Height</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Asish</td>
<td>20</td>
<td>m</td>
<td>5.11</td>
</tr>
<tr>
<td>1</td>
<td>Meghali</td>
<td>23</td>
<td>f</td>
<td>5.9</td>
</tr>
<tr>
<td>2</td>
<td>Parimita</td>
<td>49</td>
<td>f</td>
<td>5.6</td>
</tr>
<tr>
<td>3</td>
<td>SatyaNarayan</td>
<td>60</td>
<td>m</td>
<td>5.1</td>
</tr>
</tbody>
</table>
</div>",17053719.0,7212686.0,2022-04-03 07:42:55,2022-04-03 08:16:22,Indexing not working properly in DataFrame,<python><pandas><dataframe><data-science>,2,2,N/A,CC BY-SA 4.0
74532115,1,-1.0,2022-11-22 11:45:40,1,181,"<p>I want to fit some data to obtain the minimum (both x and y), but when I use UnivariateSpline it will generate an error: 'ValueError: x must be increasing if s &gt; 0'</p>
<p>This is my code:</p>
<pre><code>import matplotlib.pyplot as plt
from scipy.interpolate import UnivariateSpline
import numpy as np
import pandas as pd



list_dist=pd.read_csv('C:/.../dist.txt')
dist=list_dist['Distanze']
ar_dist=np.array(dist)



list_energy=pd.read_csv('C:/.../energy.txt')
energy=list_energy['Energie']
ar_energy=np.array(energy)

x = ar_dist
y = ar_energy
print(x)
print(y)

s = UnivariateSpline(x, y)

xs = np.linspace(0, 10, 10000)

ys = s(xs)

ys_min=np.min(ys)
ys_min_pos=np.argmin(ys)
xs_min=xs[ys_min_pos]
print('UnivariateSpline: ', '\n', 'dist_min: ',xs_min , '\t', 'en_min: ', '\t',ys_min)

plt.plot(x, y, 'o')
plt.plot(xs, ys)
plt.show()
</code></pre>
",20511097.0,-1.0,N/A,2022-11-22 11:45:40,Using UnivariateSpline generates an error. ValueError: x must be increasing if s > 0,<data-science><curve-fitting><spline><minimum><data-fitting>,0,0,N/A,CC BY-SA 4.0
74532943,1,-1.0,2022-11-22 12:53:40,0,131,"<p>I want to run a roberta model, But it has a connection error...
Here is the error: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.</p>
<pre><code>from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax
MODEL = f&quot;cardiffnlp/twitter-roberta-base-sentiment&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
</code></pre>
",16949013.0,16949013.0,2022-11-22 13:27:45,2022-12-03 17:43:07,What is this connection error for roberta model?,<python><data-science>,1,0,N/A,CC BY-SA 4.0
74536456,1,-1.0,2022-11-22 17:02:11,0,41,"<p>I want to plot a grid of plots, with the faceted row and columns determined by different variables. I would like to use a single FacetChart for this, but because the rows contain different variables on the x axis, the order of magnitude of those variables is different, and I want the x-axis scale shared separately in each row, I have no choice but to create two separate FacetCharts and to VConcat them together. When I do this, the spacing of the grids is different so plots do not align, and the result is visually unpleasant. What settings can I use to ensure the grids align?</p>
<p>I have two data frames
<img src=""https://i.stack.imgur.com/bOjAy.png"" alt=""Dataframes used"" />.</p>
<p>I plot the data with this code</p>
<pre><code>(
    alt.Chart(vert_heat_flux_data_1hr).mark_line().encode(
        alt.X('value:Q', sort='y', title=&quot;w'tc'&quot;),
        alt.Y('height:Q', axis=alt.Axis(grid=False)),
        alt.Color('day:N')
    ).properties(
        width=73,
        height = 100
    ).facet(
        column='hour:O',
        spacing=7
    ) &amp; 
    alt.Chart(vert_moisture_flux_data_1hr).mark_line().encode(
        alt.X('value:Q', sort='y', title=&quot;w'q'&quot;),
        alt.Y('height:Q', axis=alt.Axis(grid=False)),
        alt.Color('day:N')
    ).properties(
        width=73,
        height = 100
    ).facet(
        column='hour:O',
        spacing=7
    )
)
</code></pre>
<p>which results in this plot.
<img src=""https://i.stack.imgur.com/H7947.png"" alt=""The resulting plot"" /></p>
<p>Why do the grids not align? It seems that after specifying plot width and height and facet spacing that the grids would align.</p>
",8290368.0,2847330.0,2022-11-23 09:27:52,2022-11-23 09:27:52,Vconcat of separate FacetCharts do not have aligned grids - how to get them to align?,<python><plot><data-science><altair><vega>,0,2,N/A,CC BY-SA 4.0
74545006,1,-1.0,2022-11-23 10:06:40,0,87,"<p><a href=""https://i.stack.imgur.com/sUNeY.png"" rel=""nofollow noreferrer"">enter image description here</a>  green is ground truth and the red box is another ground truth An image-1 has one ground truth annotation file and I have drawn a new annotation for it. I need to know whether the new annotations contain all the previous annotations.</p>
<p>I am expecting I want to compare these two ground truths and come up with false positive, false negative, and true positive and sum it up and find out how many, FP, FN and TP in each image</p>
<p>I want results like this image anybody has any idea, I would be very much grateful, thank you in advance</p>
",20386588.0,20386588.0,2022-11-24 04:25:17,2022-11-24 04:25:17,"need to find false positive, false negative, true positive from comparing two different ground truth for same image",<python><data-science><pycocotools>,0,2,N/A,CC BY-SA 4.0
74537840,1,74538044.0,2022-11-22 19:07:57,0,80,"<p>I have a dataframe:</p>
<pre><code>df = Batch_ID           DateTime            Code A1 A2
      ABC.      '2019-01-02 17:03:41.000'   230  2. 4 
      ABC.      '2019-01-02 17:03:41.000'   230  1. 5 
      ABC.      '2019-01-02 17:03:42.000'   231  1. 4 
      ABC.      '2019-01-02 17:03:48.000'   232  2. 7 
      ABC.      '2019-01-02 17:04:41.000'   230  2. 9 
      ABB.      '2019-01-02 17:04:41.000'   235  5. 4 
      ABB.      '2019-01-02 17:04:45.000'   236  2. 0 
</code></pre>
<p>I need to generate an plot of an histogram of &quot;number of different codes per &lt;Batch_ID, minute&gt;.
Notice that 'Code' may have multiple occurrences but should be taken after unique.</p>
<p>So in this case some entries will be:</p>
<pre><code>&lt;ABC, 2019-01-02 17:03&gt; : 3
&lt;ABC, 2019-01-02 17:04&gt; : 1
&lt;ABB, 2019-01-02 17:04&gt; : 2
</code></pre>
<p>How can it be done?</p>
",6057371.0,17562044.0,2022-11-23 12:32:05,2022-11-23 12:32:05,Pandas histogram of number of occurences of other columns after groupby,<pandas><dataframe><group-by><data-science><data-munging>,1,0,N/A,CC BY-SA 4.0
74541698,1,74541979.0,2022-11-23 04:09:28,0,357,"<p>I have been trying this. Model can't give me confusion matrix</p>
<pre><code>    fit &lt;- rpart(taste ~ ., data = train,method=&quot;class&quot;,control = rpart.control(cp = 0.01)) 
    summary(fit)



    knn_prediction &lt;- predict(fit, test)
    confusionMatrix(knn_prediction, test$taste)
</code></pre>
<p>#when i tried the confusion matrix it gives me error : matrix must have equal dimensions</p>
",20212441.0,20212441.0,2022-11-23 05:47:51,2022-11-23 05:47:51,error : Matrix must have equal dimensions,<r><machine-learning><data-science><knn>,1,0,N/A,CC BY-SA 4.0
74545933,1,74546157.0,2022-11-23 11:17:14,0,67,"<pre><code>my_list = ['A', 'B', 'C', 'D', 'E', 'B', 'F', 'D', 'C', 'B']

idx = my_list.index('B')
print(&quot;index :&quot;, idx)
</code></pre>
<p>In here I used the '.index()' function.</p>
<pre><code>for i in my_list:
    print(f&quot;index no. {my_list.index(i)}&quot;)
</code></pre>
<p>I tried to find each index number of the items of the (my_list) list.
But it gave same result for same values. But they located in difference places of the list.</p>
<pre><code>if 'B' == my_list[(len(my_list) - 1)]:
    print(&quot;True&quot;)

if 'B' == my_list[(len(my_list) - 4)]:
    print(&quot;True&quot;)
</code></pre>
<p>I need to mention particular values by the index number of their (to do something).
Imagine; I need to set values to nesting with the values of the list.
i.e :</p>
<pre><code>my_list_2 = ['A', 'B', '2', 'C', '3', 'D', '4', 'E', 'B', '2', 'F', '6', 'D', 'C', '3', 'B']
              -    ------    ------    ------    -    ------    ------    -    ------    -
</code></pre>
<p>If I want to nesting values with their Consecutive (number type) items and
the other values need to nest with '*' mark (as default).Because they have no any Consecutive (numeric) values.</p>
<p>so then how I mention each (string) values and (numeric) values in a coding part to nesting them.
In this case as my example I expected result:</p>
<p>--&gt; my_list_2 = [['A', '<em>'], ['B', '2'], ['C', '3'], ['D', '4'], ['E', '</em>'], ['B', '2'], ['F', '6'], ['D', '<em>'], ['C', '3'], ['B', '</em>']]</p>
<p>This is the coding part which I tried to do this :</p>
<pre><code>def_setter = [
    [my_list_2[i], '*'] if my_list_2[i].isalpha() and my_list_2[i + 1].isalpha() else [my_list_2[i], my_list_2[i + 1]]
    for i in range(0, len(my_list_2) - 1)]
</code></pre>
<p>print(&quot;Result : &quot;, def_setter)</p>
<p>But it not gave me the expected result.</p>
<p>Could you please help me to do this !</p>
",15162490.0,-1.0,N/A,2022-11-23 13:11:09,How to get particular index number of list items,<python><arraylist><data-science>,1,0,N/A,CC BY-SA 4.0
74545960,1,74546951.0,2022-11-23 11:19:23,0,54,"<p>Hello I am trying to drop rows that have in a specific column string that is not a year.
For example <a href=""https://i.stack.imgur.com/SdQEQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SdQEQ.png"" alt=""here"" /></a> I have the in last rows year formats that have decimal points or '-'.</p>
<p>I have tried to convert the year column into a string and then drop them using the code below but it only removes the row with 2011-21, the ones with decimal points stay.</p>
<pre><code>df.level_1=df.level_1.astype(str)

df.loc[
    (~df.level_1.str.contains(&quot;.&quot;))
    |~(df.level_1.str.contains(&quot;-&quot;)),
    :]
</code></pre>
<p>is there a way to fix this issue ??</p>
",20521674.0,-1.0,N/A,2022-11-23 12:47:25,"merging dataframes by country and year while the countries are not named the same (for example US,United states )",<python><pandas><dataframe><data-science><fuzzy-logic>,2,0,N/A,CC BY-SA 4.0
74534021,1,74534089.0,2022-11-22 14:12:01,1,27,"<p>I have a list with few items and I need to compare Consecutive items of them.</p>
<p>e.g:</p>
<pre><code>     'item_1' 'item_2'
     'item_2' 'item_3'
     'item_3', 'item_4'
     'item_4', 'item_5'
     'item_5', 'item_6'
     'item_6', 'item_7'
</code></pre>
<p>I tried with some coding part like this. But it unsuccessfully !</p>
<p><code>my_list = ['item_1', 'item_2', 'item_3', 'item_4', 'item_5', 'item_6', 'item_7']</code></p>
<pre><code>for (a, b) in zip(my_list [0::2], my_list [1::2]):
    print(f&quot;{a} {b}&quot;)
</code></pre>
<p>Please help me to get the result accurately.</p>
",15162490.0,6016064.0,2022-11-22 14:14:48,2022-11-22 14:23:07,How to compare list items,<python-3.x><arraylist><data-science>,1,0,N/A,CC BY-SA 4.0
74538587,1,74538606.0,2022-11-22 20:27:38,2,30,"<p>I have a csv file which has 3 columns. The first one is session_id, second one is item_id and last one is date. I am trying to delete the rows which does NOT have 2020 as date. Because I want to work on only 2020 dated data.</p>
<p>I did some research but I couldn't find any clear information.</p>
<p>This is my code so far;</p>
<pre><code>import numpy as np

dataset = pd.read_csv('purchases.csv')
dataset.info() #no null value, no missing data, nothing to drop
</code></pre>
<p>Can you help me ?</p>
",10964038.0,-1.0,N/A,2022-11-22 20:29:47,How to delete the elements with the date except 2020 from csv file with Python?,<python-3.x><pandas><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
71725996,1,-1.0,2022-04-03 12:41:21,0,152,"<p>I have a data with over 700 observations but below is a sample. Using geom_curve I want to make a plot where the line size(total_trips) corresponds to a color say 3 different colors.  For instance between 0-100 (total_trips) can have a color of red</p>
<pre><code>df &lt;- data.frame(
 origin_x = c(659627.8,642136.2,648774.7,659627.8,659627.8,658455.7,659627.8,659620.6,661641.8,656246.4),
 origin_y = c(6473200,6473200,6462166,6473200,6473200,6467413,6473200,6467163,6479577,6487039),
 dest_x = c(642136.2,659627.8,659627.8,648774.7,659620.6,659627.8,658455.7,659627.8,659627.8,659627.8),
 dest_y = c(6456563,6473200,6473200,6462166,6467163,6473200,6467413,6473200,6473200,6473200
),
 total_trips = c(4002,49878,2011,500,100,3000,2500,654,900,600))

</code></pre>
<p>I tried</p>
<pre><code>ggplot() + geom_sf(data=shapefile, colour='grey', fill='grey93', size = 0.25) + 
    geom_curve(
        data = df), 
        aes(
            x = origin_x,
            xend = dest_x,
            y = origin_y,
            yend = dest_y,
             size = n,
            colour= as.factor(c('red','blue'))),
        curvature = 0.3
    )  + scale_alpha_continuous(range = c(0.09,1)) +
    theme(
        axis.title = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        plot.title = element_text(hjust = 0.5, size = 6),
        plot.caption = element_text(hjust = 1),
        plot.caption.position = 'plot',
        axis.ticks = element_blank(),
        panel.background = element_rect(fill = 'white'),
        panel.grid = element_blank(),
        plot.background = element_rect(color = NA, size = 0.5, fill=NA),
        panel.border = element_rect(color = 'black', fill = NA, size=0.2) ,
        legend.position = c(0.89,0.15),
        legend.key.size = unit(0.4, 'cm'), 
        legend.text = element_text(size=7)  
    ) +
    annotation_scale(location = 'br', style = 'ticks') + coord_sf(crs=3301) +
    annotation_north_arrow(location = 'tr', width = unit(0.20, 'cm'),height = unit(0.5,'cm')) 

</code></pre>
",18477482.0,18477482.0,2022-04-03 13:13:46,2022-04-03 15:54:40,How to draw color line with size in R,<r><ggplot2><data-science><geospatial><geom-curve>,1,4,N/A,CC BY-SA 4.0
74546695,1,-1.0,2022-11-23 12:20:01,0,120,"<pre><code>    Amount
0   250000
1   ₹40,000,000
2   ₹65,000,000
3   2000000
4   —
... ...
521 225000000
522 —
523 7500
524 ₹35,000,000
525 35000000
526 rows × 1 columns
</code></pre>
<p>how can we split the Amount column in separate of Currency Symbol and amount in separate column</p>
",19783978.0,16343464.0,2022-11-23 12:26:44,2022-11-23 12:26:44,How to split the prefix of currency symbol in separate column in Pandas Data Frame,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
74523322,1,-1.0,2022-11-21 18:22:46,0,11,"<p>I'am trying to use Hue function from seaborn to auto select colors from a cmap, but when it comes to labelisation, it seems like seaborn doesn't found every value that should pass to hue= 'Target'</p>
<p>here is the code</p>
<pre class=""lang-py prettyprint-override""><code>import seaborn as sns

sns.relplot(data=data,x='Trial',y='Reaction_time',hue='Target')
plt.show()
</code></pre>
<p>For now, it only display 5 out of the 18 Target categories and I have no clue why.</p>
<p><img src=""https://i.stack.imgur.com/LOI84.jpg"" alt=""enter image description here"" /></p>
",19343282.0,12046409.0,2022-11-21 20:09:41,2022-11-21 20:09:41,Seaborn hue not showing every values,<dataframe><matplotlib><plot><seaborn><data-science>,0,3,2022-11-21 20:14:05,CC BY-SA 4.0
74526777,1,-1.0,2022-11-22 02:09:03,0,59,"<p>Im trying to deal with some timeseries data that looks like this. As you can see the data is monthly, but some dates are at EOM, some at BOM and some simply a month name:</p>
<p><a href=""https://i.stack.imgur.com/qE8Nj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qE8Nj.png"" alt=""enter image description here"" /></a></p>
<p>The solution i thought of was: assuming this is monthly data and that i know the start and end dates, i would like to create a date range from that start and end date and re-assign to the dataframe as the index.   however, when i run this code:</p>
<pre><code>pd.date_range('11/30/2020','03/01/2021', freq='MS')
</code></pre>
<p>I end up with this:</p>
<pre><code>DatetimeIndex(['2020-12-01', '2021-01-01', '2021-02-01', '2021-03-01'], dtype='datetime64[ns]', freq='MS')
</code></pre>
<p>This is starting at December instead of november, so one less row than i expect.
Why is this happening and what is a good solution here?</p>
<hr />
<p><strong>UPDATE</strong></p>
<p>doing something like the following solves the problem for me</p>
<pre><code>pd.date_range(pd.to_datetime('11/30/2020').to_period('M').to_timestamp(),'03/01/2021', freq='MS')
</code></pre>
<p>Im fine with that - but are there better ways to solve these kind of date issues?</p>
",480118.0,480118.0,2022-11-22 02:27:30,2022-11-22 02:39:57,"Pandas: resampling data with mixed, missing or difficult to 'normalize' dates",<python><pandas><numpy><data-science>,2,2,N/A,CC BY-SA 4.0
74527084,1,-1.0,2022-11-22 03:08:15,-1,1603,"<p>I tried to install pyplot using 'pip install pyplot' in command prompt while it was installing by mistake i closed command prompt then again i am trying to install pyplot using the same command but it was not installing.Can anyone guide me how to install pyplot<a href=""https://i.stack.imgur.com/gs9RZ.png"" rel=""nofollow noreferrer"">Kindly find the error in this image</a></p>
<p>error rectification in installing pyplot</p>
",20281849.0,20281849.0,2022-11-22 03:11:25,2022-11-22 04:52:02,How can I install pyplot?,<python><pandas><matplotlib><data-science><graph-data-science>,2,2,N/A,CC BY-SA 4.0
71729537,1,-1.0,2022-04-03 20:17:18,0,298,"<p>How can I get the best parameters?
<code>wrapped = KerasClassifier(build_fn=createmodel_batch, epochs=100, batch_size=5, verbose=0)</code>
<code>folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=15)</code>
<code>results = cross_val_score(wrapped, X, Y, cv=folds)</code></p>
",16125676.0,-1.0,N/A,2022-04-04 04:39:50,How to Get the Best Parameters of Cross_val_score?,<python><data-science><cross-validation><grid-search>,1,0,N/A,CC BY-SA 4.0
71729780,1,-1.0,2022-04-03 20:52:57,0,853,"<p>I have a dataset containing various columns of created_at, text, author.id, i am trying to do a time analysis:</p>
<pre><code>  df['date'] = pd.to_datetime(df['author.created_at'])
    df = df.set_index('date') 
    timestamp = (df_time!=0).resample('AS').sum()
</code></pre>
<p>I am getting error as:</p>
<p><a href=""https://i.stack.imgur.com/CAutW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CAutW.png"" alt=""enter image description here"" /></a>TypeError: Only valid with DatetimeIndex,
TimedeltaIndex or PeriodIndex, but got an instance of 'Index'</p>
<p>Can anyone help me with this?Thank you</p>
",15563833.0,-1.0,N/A,2022-04-03 20:52:57,"how can i fix error: ""Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'""?",<python><datetime><jupyter-notebook><data-science><tweets>,0,2,N/A,CC BY-SA 4.0
74564978,1,74566545.0,2022-11-24 18:32:21,1,114,"<p>I have a data frame that contains the names of 3 friends and 10 restaurants that are ranked 1-10 (where Rank 1 indicates most likely to be interested, while rank 10 means least likely to be interested) as InterestRank for each friend. The Data frame contains attributes of restaurants too like Cost, Cuisine and Alcohol served or not. The Data frame looks like following:</p>
<pre><code>FriendName,Restaurant,InterestRank,Cuisine,Cost,Alcohol
Amy,R2,1,French,$$,No
Ben,R2,3,French,$$,No
Cathy,R2,8,French,$$,No
Amy,R1,2,French,$$$,Yes
Ben,R1,9,French,$$$,Yes
Cathy,R1,5,French,$$$,Yes
Amy,R4,3,French,$$$,Yes
Ben,R4,5,French,$$$,Yes
Cathy,R4,10,French,$$$,Yes
Amy,R3,4,French,$$,Yes
Ben,R3,10,French,$$,Yes
Cathy,R3,6,French,$$,Yes
Amy,R10,5,Mexican,$$$,Yes
Ben,R10,6,Mexican,$$$,Yes
Cathy,R10,7,Mexican,$$$,Yes
Amy,R7,6,Japanese,$$,Yes
Ben,R7,1,Japanese,$$,Yes
Cathy,R7,9,Japanese,$$,Yes
Amy,R6,7,Japanese,$,No
Ben,R6,8,Japanese,$,No
Cathy,R6,3,Japanese,$,No
Amy,R8,8,Mexican,$$,No
Ben,R8,4,Mexican,$$,No
Cathy,R8,2,Mexican,$$,No
Amy,R5,9,Japanese,$$,No
Ben,R5,2,Japanese,$$,No
Cathy,R5,1,Japanese,$$,No
Amy,R9,10,Mexican,$$,No
Ben,R9,7,Mexican,$$,No
Cathy,R9,4,Mexican,$$,No
</code></pre>
<p>I want to recommend the top 4 restaurants to each friend according to their InterestRank as well as a condition that no more than 2 restaurants with the same cuisine type will be recommended to each of them. How to achieve this in a Pythonic way?</p>
<p><strong>Edit: Expected output data frame</strong></p>
<p>I want the final output to be something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>FriendName</th>
<th>Restaurant</th>
<th>RecommendationRank</th>
</tr>
</thead>
<tbody>
<tr>
<td>Amy</td>
<td>R2</td>
<td>1</td>
</tr>
<tr>
<td>Amy</td>
<td>R1</td>
<td>2</td>
</tr>
<tr>
<td>Amy</td>
<td>R10</td>
<td>3</td>
</tr>
<tr>
<td>Amy</td>
<td>R7</td>
<td>4</td>
</tr>
<tr>
<td>Ben</td>
<td>R7</td>
<td>1</td>
</tr>
<tr>
<td>Ben</td>
<td>R2</td>
<td>2</td>
</tr>
<tr>
<td>Ben</td>
<td>R5</td>
<td>3</td>
</tr>
<tr>
<td>Ben</td>
<td>R8</td>
<td>4</td>
</tr>
<tr>
<td>Cathy</td>
<td>R5</td>
<td>1</td>
</tr>
<tr>
<td>Cathy</td>
<td>R8</td>
<td>2</td>
</tr>
<tr>
<td>Cathy</td>
<td>R6</td>
<td>3</td>
</tr>
<tr>
<td>Cathy</td>
<td>R9</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>",5974639.0,5974639.0,2022-11-24 19:24:33,2022-11-25 02:29:49,Rule-based recommendation system,<python-3.x><pandas><data-science><recommendation-engine>,1,0,N/A,CC BY-SA 4.0
74577402,1,-1.0,2022-11-25 19:57:50,0,161,"<p>My goal is to find the best predictor variable for winning a match. I have a slight knowledge of basic statistics, so I decided to use logistic regression because result of match is a binary variable.</p>
<pre><code>logit_model=sm.Logit(y,X)
result=logit_model.fit()
result.summary()
</code></pre>
<p>This comes out with following result:</p>
<pre><code>========================================================================
Model:                 Logit              Pseudo R-squared:   0.515     
Dependent Variable:    win                AIC:                92784.8133
Date:                  2022-11-25 20:30   BIC:                92932.3349
No. Observations:      137967             Log-Likelihood:     -46377.   
Df Model:              14                 LL-Null:            -95631.   
Df Residuals:          137952             LLR p-value:        0.0000    
Converged:             1.0000             Scale:              1.0000    
No. Iterations:        7.0000                                           
------------------------------------------------------------------------
                        Coef.  Std.Err.     z     P&gt;|z|   [0.025  0.975]
------------------------------------------------------------------------
kills                   0.2116   0.0050   42.7261 0.0000  0.2019  0.2213
assists                 0.2439   0.0025   98.3537 0.0000  0.2391  0.2488
deaths                 -0.4083   0.0039 -103.6498 0.0000 -0.4160 -0.4005
baronKills              0.7598   0.0338   22.4612 0.0000  0.6935  0.8261
dragonKills             0.3566   0.0157   22.6557 0.0000  0.3257  0.3874
timeCCingOthers        -0.0096   0.0006  -17.2654 0.0000 -0.0107 -0.0085
wardsPlaced             0.0051   0.0012    4.1346 0.0000  0.0027  0.0076
goldEarned             -0.0003   0.0000  -45.5422 0.0000 -0.0003 -0.0003
inhibitorTakedowns      2.1111   0.0212   99.5492 0.0000  2.0696  2.1527
largestKillingSpree    -0.0504   0.0070   -7.2300 0.0000 -0.0641 -0.0367
largestMultiKill        0.4043   0.0159   25.5014 0.0000  0.3732  0.4354
totalMinionsKilled      0.0043   0.0002   21.6630 0.0000  0.0039  0.0047
consumablesPurchased   -0.0395   0.0032  -12.3773 0.0000 -0.0458 -0.0333
damageDealtToBuildings  0.0002   0.0000   25.9200 0.0000  0.0001  0.0002
turretKills             0.3140   0.0131   23.8875 0.0000  0.2882  0.3397
========================================================================
</code></pre>
<p>What would be the best predictor for match win given these results? My initial thinking was I can't use the coefficient, because all variables come from different distributions. Is it valid thinking to use the z-score, since it standardizes values to the same distribution?
Can variable assists and inhibitorTakedowns considered to be the best predictor for winning a match, since it has the highest z-score, or is this thinking flawed?</p>
",7190595.0,-1.0,N/A,2022-11-25 21:46:28,Which of these variables is the best predictor for winning a match given this logit model,<python><machine-learning><statistics><data-science><logistic-regression>,1,0,N/A,CC BY-SA 4.0
74549641,1,-1.0,2022-11-23 15:53:35,2,244,"<p>Scikit-Learn RandomForestClassifier throws an error for a multilabel classification problem.</p>
<ol>
<li>This code creates a RandomForestClassifier multilabel object, given predictors <code>C</code> and multi-labels <code>out</code> with no error.</li>
</ol>
<pre><code>C = np.array([[2,4,6],[4,2,1],[8,3,1]])
out = np.array([[0,1],[0,1],[1,0]])
rf = RandomForestClassifier(n_estimators=100, oob_score=True)
rf.fit(C,out) 
</code></pre>
<ol start=""2"">
<li>If I modify the multilabels, so that all the elements at a certain index are the same, say (where all the first components of the multilabels equals zero)</li>
</ol>
<pre><code>out = np.array([[0,1],[0,1],[0,0]])
</code></pre>
<p>I get an error and traceback:</p>
<pre><code>VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a 
list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. 
If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  y_pred = np.array(y_pred, copy=False)

raise ValueError(
    507             &quot;The type of target cannot be used to compute OOB &quot;
    508             f&quot;estimates. Got {y_type} while only the following are &quot;
    509             &quot;supported: continuous, continuous-multioutput, binary, &quot;
    510             &quot;multiclass, multilabel-indicator.&quot;
    511         )
ValueError: could not broadcast input array from shape (2,1) into shape (2,)
</code></pre>
<ol start=""3"">
<li>Not requesting OOB predictions does not result in an error:</li>
</ol>
<pre><code>rf_err = RandomForestClassifier(n_estimators=100, oob_score=False)
</code></pre>
<p>I cannot figure out why keeping the OOB predictions would trigger such an error, when all the n-component of a multilabel are equal.</p>
",4913660.0,2347649.0,2022-12-12 11:09:00,2022-12-17 04:43:24,Scikit-Learn issues error for RandomForestClassifier for multilabel classification - Jagged arrays,<machine-learning><scikit-learn><data-science><random-forest><multilabel-classification>,1,0,N/A,CC BY-SA 4.0
74579456,1,-1.0,2022-11-26 03:11:42,1,86,"<p>I tried to make graphs for my csv dataset in Jupyter Notebook, using this line of code:</p>
<pre><code>bank['marital'].value_counts().plot(kind='pie',autopct='%.2f') 
plt.show()
</code></pre>
<p>However, the system return, &quot;string indices must be integers&quot;.</p>
<p>I have tried to use many different methods like changing the string to a number,... but nothing really worked</p>
",20603841.0,20174226.0,2022-11-28 12:29:43,2022-12-09 19:44:37,Is there any way I can fix the error string indices must be integers?,<string><graph><jupyter-notebook><dataset><data-science>,1,2,N/A,CC BY-SA 4.0
71739872,1,-1.0,2022-04-04 15:30:34,1,65,"<p>I have a file called im.db in my current working directory, as shown here:
<img src=""https://i.stack.imgur.com/mO0rQ.png"" alt=""enter image description here"" /></p>
<p>I also am able to query this database directly from sqlite3 at the command line:</p>
<pre><code>% sqlite3
SQLite version 3.28.0 2019-04-15 14:49:49
Enter &quot;.help&quot; for usage hints.
Connected to a transient in-memory database.
Use &quot;.open FILENAME&quot; to reopen on a persistent database.
sqlite\&gt; .open im.db
sqlite\&gt; select count(\*) from writers;
255873
</code></pre>
<p>However, when running the same query inside of my notebook:</p>
<pre><code>import sqlite3 as sql
con = sql.connect(&quot;im.db&quot;)
writers_dataframe = pd.read_sql_query(&quot;SELECT COUNT(\*) from writers&quot;, con)
</code></pre>
<p>I get an error message which states <code>no such table: writers</code>
Any help would be much appreciated. Thanks!</p>
",18703854.0,1357494.0,2022-04-04 15:37:03,2022-04-04 15:37:03,I am having difficulty accessing a SQLite3 table in my Jupyter notebook (I am using Pandas),<pandas><dataframe><sqlite><jupyter-notebook><data-science>,0,0,N/A,CC BY-SA 4.0
74579920,1,74580128.0,2022-11-26 05:20:04,-1,57,"<p>I was going through the book &quot;Elements of Statistical Learning&quot;, reading chapter 4 - Linear Methods for Classification... and got stuck on this equation... It is written that this is a popular model for posterior probabilities. But I searched everywhere and not got the exact proof of this equation I want to know how this formula is derived.</p>
<p><a href=""https://i.stack.imgur.com/7Ra2f.jpg"" rel=""nofollow noreferrer"">this is the image of the formula taken from the book..pg
102</a></p>
<p>formula 👇</p>
<p><strong>Pr(G = 1|X = x) =exp(β0 + βT x) / 1 + exp(β0 + βT x)</strong></p>
<p><strong>Pr(G = 2|X = x) = 1 / 1 + exp(β0 + βT x)</strong></p>
<p>I know the basic formula of posterior probability, i.e., p(x | y) = p(y | x)p(x) /p(y) but this formula is new to me. How we can relate expectation to the posterior probability formula?</p>
<p>Please stick to the basics while answering this question as I am a beginner.</p>
",20604227.0,20604227.0,2022-11-26 05:27:06,2022-11-26 06:14:18,How this formula of posterior probability is derived?,<math><statistics><data-science><linear-algebra><probability>,1,0,2022-11-26 18:39:49,CC BY-SA 4.0
74567982,1,74570803.0,2022-11-25 03:11:42,1,504,"<p>I am new to ibm db2 and phpmyadmin. In this code I was trying to update a specific column value twice. This code was working well in phpmyadmin but not work in db2. Here is my code (ibm db2):</p>
<pre><code>    CREATE TABLE PETSALE (
    ID INTEGER NOT NULL,
    PET CHAR(20),
    SALEPRICE DECIMAL(6,2),
    PROFIT DECIMAL(6,2),
    SALEDATE DATE
    );
    
    INSERT INTO PETSALE VALUES
    (1,'Cat',450.09,100.47,'2018-05-29'),
    (2,'Dog',666.66,150.76,'2018-06-01'),
    (3,'Parrot',50.00,8.9,'2018-06-04'),
    (4,'Hamster',60.60,12,'2018-06-11'),
    (5,'Goldfish',48.48,3.5,'2018-06-14');
    
        ALTER TABLE PETSALE
        ADD COLUMN QUANTITY INTEGER;
        
        SELECT * FROM PETSALE;
    
        UPDATE PETSALE SET QUANTITY = 9 WHERE ID = 1;
        UPDATE PETSALE SET QUANTITY = 3 WHERE ID = 2;
        UPDATE PETSALE SET QUANTITY = 2 WHERE ID = 3;
        UPDATE PETSALE SET QUANTITY = 6 WHERE ID = 4;
        UPDATE PETSALE SET QUANTITY = 24 WHERE ID = 5;
        
        SELECT * FROM PETSALE;
</code></pre>
<p>then I open the PETSALE table to update last value of QUANTITY again.</p>
<pre><code>UPDATE &quot;NDH77177&quot;.&quot;PETSALE&quot;
  SET &quot;QUANTITY&quot; = 22 
WHERE &quot;ID&quot; = 5;
</code></pre>
<p>but it's not working on ibm db2. It's showing an error message-</p>
<pre><code>Operation not allowed for reason code &quot;7&quot; on table &quot;NDH77177.PETSALE&quot;.. SQLCODE=-668, SQLSTATE=57007, DRIVER=4.31.10
</code></pre>
<p>Here is the image also:
<a href=""https://i.stack.imgur.com/CWXuo.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>how to fix this issue :(</p>
",12904389.0,12904389.0,2022-11-25 09:18:27,2022-11-25 09:41:16,UPDATE statement not working in ibm db2 but it working in phpmyadmin,<sql><phpmyadmin><db2><data-science><ibm-cloud>,1,2,N/A,CC BY-SA 4.0
71740221,1,-1.0,2022-04-04 15:56:05,1,1205,"<p>I am working with the most famous <a href=""https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"" rel=""nofollow noreferrer"">Credit Card Fraud Detection</a> dataset which includes 28 PCA transformed columns. I'm dealing with the most skewed feature of all which after running the following snippet of code turns out to be <code>V28</code>:</p>
<pre class=""lang-py prettyprint-override""><code>abs_skew_values = pca.skew().abs().sort_values(ascending=False)
selected_feature = abs_skew_values.index[0]  # index[0]: most skewed feature
selected_feature  # 'V28'
</code></pre>
<p><code>pca</code> is the Pandas DataFrame containing the entire dataset with the PCA columns (V1, V2, V3, etc.).</p>
<p>Now, I wanted to test two things:</p>
<ol>
<li>How much does the original distribution resemble a <strong>normal distribution</strong>?</li>
<li>How much <strong>skeweness</strong> (left or right) is there in the original distribution?</li>
</ol>
<p>The first thing I have done is plot the histogram of the feature <code>V28</code>:</p>
<p><a href=""https://i.stack.imgur.com/xVO9r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xVO9r.png"" alt=""histogram"" /></a></p>
<p>There are a lot of data points far from 0, these are right skewing the distribution with a score of <code>11.192</code>. Also, tons of outliers outside of the boxplot fences.</p>
<p>I fixed this by applying a log transformation <code>sign(x) * log(|x|)</code> rather than plain <code>log(x)</code> because there are negative values in the distribution.</p>
<p><a href=""https://i.stack.imgur.com/fCzvh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fCzvh.png"" alt=""transformed_histogram"" /></a></p>
<p>It significantly reduced the skew score to <code>0.184</code> and you can see less outliers in the distribution.</p>
<p>Running some normality tests also give an insight into how this is clearly not coming from a normal distribution.</p>
<pre><code>Anderson-Darling test
---------------------
15.000: 0.576, data does not look normal (reject H0)
10.000: 0.656, data does not look normal (reject H0)
5.000: 0.787, data does not look normal (reject H0)
2.500: 0.918, data does not look normal (reject H0)
1.000: 1.092, data does not look normal (reject H0)

D'Agostino K^2 test
-------------------
statistic=96189.836, pvalue=0.000
</code></pre>
<p>It turns out that, after the log transformation, there are only 26 outliers that may (or may not) be outliers in other features, therefore I don't think I can outright remove them from the original dataset.</p>
<p>So, my question is, am I right in assuming that the transformation I applied is enough to correct the skewness that originally came from the given distribution?</p>
<p>Bonus points: Why is the <code>pvalue</code> in D'Agostino's test exactly 0, shouldn't it be a small number?</p>
",14563929.0,-1.0,N/A,2022-04-04 16:04:25,How do I remove skewness from a distribution?,<python><pandas><data-science><normal-distribution><skew>,0,0,N/A,CC BY-SA 4.0
74555843,1,-1.0,2022-11-24 05:03:29,0,40,"<p>I am trying to get the the shortest leaf node with a non-zero gini. For that, I am trying to get the depth of a decision tree at each corresponding leaf nodes. I have looked into the documentation but haven't been able to come across anything. Any suggestions on this ?</p>
",15184837.0,-1.0,N/A,2022-11-24 05:03:29,Get the shortest leaf node with a non-zero gini in Decision Tree,<python><scikit-learn><data-science><binary-search-tree><decision-tree>,0,2,N/A,CC BY-SA 4.0
74561497,1,-1.0,2022-11-24 13:24:18,1,98,"<p>How do I select a range from given values when drawing <code>degree_centrality</code> graph.</p>
<pre><code>B1:  0.64
E2: 0.61
C3: 0.60
B2: 0.58
M1: 0.50
C1: 0.328
R1: 0.228
</code></pre>
<pre><code>def draw(G, pos, measures, measure_name):
    
    nodes = nx.draw_networkx_nodes(G, pos, node_size=250, cmap=plt.cm.plasma, 
                                   node_color=list(measures.values()),
                                   nodelist=measures.keys())
    nodes.set_norm(mcolors.SymLogNorm(linthresh=0.01, linscale=1, base=10))
    # labels = nx.draw_networkx_labels(G, pos)
    edges = nx.draw_networkx_edges(G, pos)

    plt.title(measure_name)
    plt.colorbar(nodes)
    plt.axis('off')
    plt.show()
</code></pre>
<pre><code>pos = nx.spring_layout(G, seed=675)
</code></pre>
<pre><code>draw(G, pos, nx.degree_centrality(G), 'Degree Centrality')
</code></pre>
<p>I am trying to use Network <a href=""https://aksakalli.github.io/2017/07/17/network-centrality-measures-and-their-visualization.html"" rel=""nofollow noreferrer"">centrality measure visualisation</a>
to draw visualise centrality measure but i am only interested in visualising nodes within a range of values.</p>
<p>I only wany to visualise range 0.64 to 0.60 from the given range above.</p>
<pre><code>B1:  0.64
E2: 0.61
C3: 0.60
</code></pre>
",20580952.0,-1.0,N/A,2022-11-24 14:02:59,Python Networkx centrality measure range of nodes,<python><python-3.x><dataframe><data-science><networkx>,1,1,N/A,CC BY-SA 4.0
74582320,1,-1.0,2022-11-26 12:52:04,0,541,"<p>I have two tables (using mysql) : <em><strong>tactic_themes(code_tac,code_th)</strong></em> and <em><strong>tactics(code_tac,tactic_data)</strong></em> which tactic_themes contains 13 million rows and tactics 3 million rows but running query that makes &quot;code_tac&quot; column a foreign key takes too long, i mean more than 2 hours!</p>
<p><strong>The query SQL</strong> :</p>
<pre><code>ALTER TABLE tactic_themes
ADD CONSTRAINT fk_foreign_key_name
FOREIGN KEY (code_tac)
REFERENCES tactics(code_tac);
</code></pre>
<p><strong>My mysql server variables</strong> :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Variable_name</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>aria_pagecache_buffer_size</td>
<td>134217728</td>
</tr>
<tr>
<td>aria_sort_buffer_size</td>
<td>268434432</td>
</tr>
<tr>
<td>bulk_insert_buffer_size</td>
<td>8388608</td>
</tr>
<tr>
<td>innodb_buffer_pool_chunk_size</td>
<td>16777216</td>
</tr>
<tr>
<td>innodb_buffer_pool_dump_at_shutdown</td>
<td>ON</td>
</tr>
<tr>
<td>innodb_buffer_pool_dump_now</td>
<td>OFF</td>
</tr>
<tr>
<td>innodb_buffer_pool_dump_pct</td>
<td>25</td>
</tr>
<tr>
<td>innodb_buffer_pool_filename</td>
<td>ib_buffer_pool</td>
</tr>
<tr>
<td>innodb_buffer_pool_instances</td>
<td>1</td>
</tr>
<tr>
<td>innodb_buffer_pool_load_abort</td>
<td>OFF</td>
</tr>
<tr>
<td>innodb_buffer_pool_load_at_startup</td>
<td>ON</td>
</tr>
<tr>
<td>innodb_buffer_pool_load_now</td>
<td>OFF</td>
</tr>
<tr>
<td>innodb_buffer_pool_size</td>
<td>16777216</td>
</tr>
<tr>
<td>innodb_change_buffer_max_size</td>
<td>25</td>
</tr>
<tr>
<td>innodb_change_buffering</td>
<td>all</td>
</tr>
<tr>
<td>innodb_log_buffer_size</td>
<td>8388608</td>
</tr>
<tr>
<td>innodb_sort_buffer_size</td>
<td>1048576</td>
</tr>
<tr>
<td>join_buffer_size</td>
<td>262144</td>
</tr>
<tr>
<td>join_buffer_space_limit</td>
<td>2097152</td>
</tr>
<tr>
<td>key_buffer_size</td>
<td>16777216</td>
</tr>
<tr>
<td>mrr_buffer_size</td>
<td>262144</td>
</tr>
<tr>
<td>myisam_sort_buffer_size</td>
<td>8388608</td>
</tr>
<tr>
<td>net_buffer_length</td>
<td>8192</td>
</tr>
<tr>
<td>preload_buffer_size</td>
<td>10485760</td>
</tr>
<tr>
<td>read_buffer_size</td>
<td>262144</td>
</tr>
<tr>
<td>read_rnd_buffer_size</td>
<td>524288</td>
</tr>
<tr>
<td>sort_buffer_size</td>
<td>524288</td>
</tr>
<tr>
<td>sql_buffer_result</td>
<td>OFF</td>
</tr>
</tbody>
</table>
</div>
<p>Can someone help?</p>
",14490827.0,2347649.0,2022-12-09 22:00:11,2022-12-09 23:32:42,Adding foreign Key in table is taking too long,<mysql><sql><database><data-science>,1,5,N/A,CC BY-SA 4.0
74596102,1,74596471.0,2022-11-28 04:48:12,0,167,"<p>I'm working on this dataset.</p>
<p><img src=""https://i.stack.imgur.com/pM5mY.png"" alt=""dataset"" /></p>
<p>My question is how do I group this dataset based on the same timestamp and merge these strings into one with <strong>unique tokens</strong>, so, for example, I could have:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>string</th>
</tr>
</thead>
<tbody>
<tr>
<td>2011-02-01 15:00:00</td>
<td>Richmond Service Index S&amp;P/CS HPI Composite - 20 s.a. n.s.a Texas Services Sector Outlook TIC Net Long-Term Transactions including Swaps</td>
</tr>
</tbody>
</table>
</div>
<p>I don't have any idea on what method should I use to solve this problem. Does anyone know how to solve it?</p>
",19212237.0,2847330.0,2022-11-28 11:45:12,2022-11-28 11:45:12,Pandas append string tokens into list with corresponding column where those column in those string rows having same value,<pandas><machine-learning><nlp><data-science><nltk>,1,0,N/A,CC BY-SA 4.0
74596793,1,-1.0,2022-11-28 06:31:43,1,2359,"<p>Trying to implement the reaserch paper:
<a href=""https://ieeexplore.ieee.org/document/9479786/"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/document/9479786/</a>
Training a Monotone Network with architechture:</p>
<pre class=""lang-py prettyprint-override""><code>class Model(nn.Module):
  def __init__(self, q, s):
    self.layer_s_list = [nn.Linear(5, s) for _ in range(q)]
    self.inv_w, self.inv_b = self.get_layer_weights()
      
  def forward(self, x):
    # print(inv_w[0].shape, inv_b[0].shape)
    output_lst = []
    for layer in self.layer_s_list:
      v, id = torch.max(layer(x), 1)
      output_lst.append(v.detach().numpy())
    output_lst = np.array(output_lst)
    output_lst = torch.from_numpy(output_lst)
    out, _ = torch.min(output_lst, 0)
    allo_out = F.softmax(out)
    pay_out = nn.ReLU(inplace = True)(out)
    inv_out_lst = []
    
    for q_idx in range(len(self.inv_w)):
      # print(inv_w[q_idx].shape, pay_out.shape, inv_b[q_idx].shape)
      y, _ = torch.min(torch.linalg.pinv(self.inv_w[q_idx]) * (pay_out - self.inv_b[q_idx]), 0)
      inv_out_lst.append(y.detach().numpy())
    final_out = np.array(inv_out_lst)
    final_out = torch.from_numpy(final_out)
    final_out, _ = torch.max(final_out, 1)
    return final_out, allo_out

  
  def get_layer_weights(self):
    weights_lst = []
    bias_lst = []
    for layer in self.layer_s_list:
      weights_lst.append(layer.state_dict()['weight'])
      bias_lst.append(layer.state_dict()['bias'])
    return weights_lst, bias_lst
</code></pre>
<p>When I initialise the network and run for random inputs:</p>
<pre class=""lang-py prettyprint-override""><code>q = 5
s = 10
x = torch.rand((10, 5), requires_grad = True)
net = Model(q, s)
y, z = net(x)`
</code></pre>
<p>It gives the following error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-3-aac6d239df1f&gt; in &lt;module&gt;
      1 x = torch.rand((10, 5), requires_grad = True)
      2 net = Model(5, 10)
----&gt; 3 y = net(x)

1 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in __getattr__(self, name)
   1206                 return modules[name]
   1207         raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(
-&gt; 1208             type(self).__name__, name))
   1209 
   1210     def __setattr__(self, name: str, value: Union[Tensor, 'Module']) -&gt; None:

AttributeError: 'Model' object has no attribute '_backward_hooks'
</code></pre>
<p>Please help me understand what this error is and how to fix it.</p>
",12249140.0,2847330.0,2022-11-28 11:14:16,2022-11-28 11:14:16,AttributeError: 'Model' object has no attribute '_backward_hooks',<machine-learning><deep-learning><neural-network><data-science><federated-learning>,1,0,N/A,CC BY-SA 4.0
74596952,1,-1.0,2022-11-28 06:49:35,1,189,"<p>I have four lists of longitude and latitude</p>
<p>'''
shop_long = [-123.223, -127.223, -123.223, -123.048]</p>
<pre><code>shop_lat = [49.1534, 55.1303, 49.1534, 53.2563]

cus_long = [-126.07325247944962, -126.07255765553835, -126.07485428820583, 
            -126.0733578858899, -126.07270416708549]

cus_lat = [51.29548801984406, 51.29486187466757, 51.29566033167437, 
           51.295612714656855]
distance = []
shop =  (shop_long, shop_lat)
customer = (cus_long, cus_lat)
print(geodesic(shop, customer).miles)'''
</code></pre>
<p>I want to calculate the distance between customer and shop using their latitude and longitude and append it into distance list. Please help me.</p>
",20212441.0,-1.0,N/A,2022-11-28 07:09:19,i want to calculate the distance using geodesic from geopy library,<python><r><machine-learning><data-science><geopy>,1,0,N/A,CC BY-SA 4.0
74601457,1,74601499.0,2022-11-28 13:39:25,1,23,"<p>Ok so this is more of a question about how to properly use the groupby method since I am kinda struggling to use the DataFrameGroupBy object itself. Basically I have a big DataFrame with the following structure:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>DATE</th>
<th>PRODUCT</th>
<th>PRICE</th>
<th>CAPACITY</th>
</tr>
</thead>
<tbody>
<tr>
<td>01.07.2022</td>
<td>NEG_00_04</td>
<td>3,7</td>
<td>7</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>NEG_00_04</td>
<td>1,7</td>
<td>3</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>NEG_00_04</td>
<td>2,4</td>
<td>5</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>NEG_00_04</td>
<td>2,2</td>
<td>7</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>POS_00_04</td>
<td>3,7</td>
<td>2</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>POS_00_04</td>
<td>3,2</td>
<td>5</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>POS_00_04</td>
<td>1,5</td>
<td>2</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>POS_00_04</td>
<td>2,4</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>My goal is to groupby the 'DATE' and 'PRODUCT' columns and get a cumulative capacity based on an ascending price. So basically the order of operation is to groupby the two columns then sort each group by the 'PRICE' column and calculate the cumulative capacity. the end result based on the sample table should look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>DATE</th>
<th>PRODUCT</th>
<th>PRICE</th>
<th>CAPACITY</th>
<th>CUMULATIVE</th>
</tr>
</thead>
<tbody>
<tr>
<td>01.07.2022</td>
<td>NEG_00_04</td>
<td>1,7</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>NEG_00_04</td>
<td>2,2</td>
<td>7</td>
<td>10</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>NEG_00_04</td>
<td>2,4</td>
<td>5</td>
<td>15</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>NEG_00_04</td>
<td>3,7</td>
<td>7</td>
<td>22</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>POS_00_04</td>
<td>1,5</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>POS_00_04</td>
<td>2,4</td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>POS_00_04</td>
<td>3,2</td>
<td>5</td>
<td>10</td>
</tr>
<tr>
<td>01.07.2022</td>
<td>POS_00_04</td>
<td>3,7</td>
<td>2</td>
<td>12</td>
</tr>
</tbody>
</table>
</div>
<p>I already have a solution that does work but I was wondering if there isn't a better way to work with DataFrameGroupBy objects since I always just iterate through them with a for loop and it just doesn't seem right. This is how I did it:</p>
<pre><code>df_result = pd.DataFrame()
for i, group in df.groupby(by=['DATE', 'PRODUCT']):
    group.sort_values('PRICE', inplace=True)
    group['CUMULATIVE'] = group['CAPACITY'].cumsum()
    df_result = pd.concat([df_result, group], ignore_index=True)
</code></pre>
<p>I would appreciate any suggestions for improvement :)</p>
",17575465.0,-1.0,N/A,2022-11-28 13:43:15,Better way to use pandas DataFrameGroupBy objects,<python><pandas><dataframe><group-by><data-science>,1,0,N/A,CC BY-SA 4.0
74601759,1,-1.0,2022-11-28 14:03:26,0,181,"<p>I am currentry working with some kind of data analysis by R studio.
My project code is something like this:</p>
<pre><code>library(Momocs)
library(geomorph)

CharredCoords &lt;- list.files(&quot;C:\\Users\\grain_coords_mod&quot;, full.names = TRUE)
CharredFrame &lt;- read.csv(&quot;C:\\Users\\data_matrix_new_mod.csv&quot;, header = TRUE)
CharredTxt &lt;- import_txt(CharredCoords, fileEncoding=&quot;UTF-8-BOM&quot;)
CharredOut &lt;- Out(CharredTxt, fac=CharredFrame)
CharredOut1 &lt;-coo_scale (CharredOut)
CharredOut.l &lt;- filter(CharredOut1, View == &quot;l&quot;)
CharredOut.d &lt;- filter(CharredOut1, View == &quot;d&quot;)
CharredOut.l.efour &lt;- efourier(CharredOut.l, nb.h=8, norm = FALSE, start = TRUE)
CharredOut.d.efour &lt;- efourier(CharredOut.d, nb.h=8, norm = FALSE, start = TRUE)
</code></pre>
<p>Till its okay... Then after when I execute the following line, error occurs</p>
<pre><code>CharredOut.l %&gt;% chop(~View) %&gt;% lapply(efourier,nb.h=8, norm = FALSE, start = FALSE) %&gt;% combine %&gt;% LDA ('G_Variety', scale=FALSE, center= TRUE)
</code></pre>
<p>Output: Error in svd(X, nu = 0L) : infinite or missing values in 'x'
In addition: Warning message:
In lda.default(x, grouping, ...) : variables are collinear</p>
<hr />
<p>Looking for some suggestion like what I suppose to do or check to solve that isuue.
Thanks in advance.`<a href=""https://i.stack.imgur.com/u2TMa.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Code should be render for further ploting procedure.</p>
",19704788.0,-1.0,N/A,2022-11-28 14:03:26,"Error in svd(X, nu = 0L) : infinite or missing values in 'x'",<r><dataframe><validation><data-science><linear-programming>,0,3,N/A,CC BY-SA 4.0
74581517,1,-1.0,2022-11-26 10:49:30,-1,35,"<p>I have 2 variables from polynomial regression:</p>
<pre><code>y_test = [1.57325397 0.72686416]
y_pred= [1.57325397 0.72686416]
</code></pre>
<p><code>y_test</code> is the y axis of the test i did, while <code>y_pred</code> is are the values i got from <code>regressor.predict</code> (<code>regressor</code> is the object of <code>LinearRegression</code> class).</p>
<p>I tried to use <code>np.concatenate((y_test),(y_predict))</code> but it did not work and it said only integer scalar arrays can be converted to a scalar index. So what should I do here? it OK to round of the values to integers or should I do something else?</p>
",16677630.0,1235698.0,2022-11-26 10:53:09,2022-11-26 10:59:52,I am unable to use np.concatenate,<python><data-science><linear-regression><np>,1,1,2022-11-26 11:02:35,CC BY-SA 4.0
71743902,1,71746144.0,2022-04-04 21:27:09,0,112,"<p>I am tasked with a supervised learning problem on a dataset and want to create a full Pipeline from complete beginning to end.
Starting with the train-test splitting. I wrote a custom class to implement sklearns train_test_split into the sklearn pipeline. Its fit_transform returns the training set. Later i still want to accsess the test set, so i made it an instance variable in the custom transformer class like this:</p>
<pre><code>self.test_set = test_set
</code></pre>
<pre><code>from sklearn.model_selection import train_test_split

class train_test_splitter([...])
[... 
...]
    def transform(self, X):
        train_set, test_set = train_test_split(X, test_size=0.2)
        self.test_set = test_set
        return train_set

split_pipeline = Pipeline([
    ('splitter', train_test_splitter() ),    
])
df_train = split_pipeline.fit_transform(df)

</code></pre>
<p>Now i want to get the test set like this:</p>
<pre><code>df_test = splitter.test_set
</code></pre>
<p>Its not working. How do I get the variables of the instance &quot;splitter&quot;. Where does it get stored?</p>
",17678102.0,-1.0,N/A,2022-04-05 03:56:13,Get instance variable of costum transformer in sklearn pipeline,<python><machine-learning><scikit-learn><data-science><pipeline>,1,0,N/A,CC BY-SA 4.0
74603450,1,-1.0,2022-11-28 16:11:39,-3,32,"<p>I am working on an application to predict a disease from it's symptoms, I have some trouble making a dataset.
If someone has a dataset on this, please link it to drive and share it here.
Also I have a question on a good model for this(sklearn only). I am currently using decision tree classifier as my model for the project. Give suggestions if you have any.
Thank you for reading.
EDIT: Got the solution</p>
",20625247.0,20625247.0,2022-11-29 11:28:58,2022-11-29 11:28:58,Dataset for a python application,<python><machine-learning><scikit-learn><dataset><data-science>,1,0,N/A,CC BY-SA 4.0
74607953,1,-1.0,2022-11-29 00:38:24,0,36,"<p>Looking at a project and dont understand why both classes have a recall score when recall only involves the positive class? (converted is the positive class)</p>
<p>Code for the confusion matrix:</p>
<pre><code>def metrics_score(actual, predicted):
    print(classification_report(actual, predicted))
    
    cm = confusion_matrix(actual, predicted)
    
    plt.figure(figsize = (8, 5))
    
    sns.heatmap(cm, annot = True,  fmt = '.2f', xticklabels = ['Not Converted', 'Converted'], yticklabels = ['Not Converted', 'Converted'])
    
    plt.ylabel('Actual')
    
    plt.xlabel('Predicted')
    
    plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/RJKbt.png"" rel=""nofollow noreferrer"">Confusion Matrix</a></p>
<p>Which recall score is the correct measure for my model? I believe it is 86% but then the precision score would be 93%? But that doesnt make sense as the model is doing better on recall considering there are fewer false negatives than false positives 57 vs 224 (thats accounting for the ratio difference of having more not converted cases vs converted cases). What am I getting wrong here?</p>
",17110597.0,17110597.0,2022-11-29 00:41:25,2022-11-29 00:41:25,Confusion Matrix - What is my models recall score?,<machine-learning><data-science><decision-tree><confusion-matrix>,0,3,N/A,CC BY-SA 4.0
70791403,1,-1.0,2022-01-20 18:15:48,0,2058,"<pre><code>from sklearn.feature_selection import SequentialFeatureSelector
</code></pre>
<p>When I import SequentialFeatureSelector from sklearn.feature_selection I get an import error</p>
<pre><code> ImportError: cannot import name 'SequentialFeatureSelector'


sklearn.__version__
    -&gt; 0.22.2.post1

    !pip3 install --upgrade
    -&gt; Requirement already up-to-date: sklearn in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.0)
-&gt; Requirement already satisfied, skipping upgrade: scikit-learn in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sklearn) (0.22.2.post1)
</code></pre>
<p>I seem to have the most up to date version of skikit learn. I think the problem has to do with the environment I'm coding in, I'm using Azure Machine Learning Notebooks. When I import the same class on google colab, it works perfectly.</p>
",13419063.0,-1.0,N/A,2022-01-20 18:35:12,ImportError: cannot import name 'SequentialFeatureSelector',<python><azure><machine-learning><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
70795029,1,70795205.0,2022-01-21 00:32:07,4,80,"<p>I have a multi index df, with column &quot;Turtle&quot;</p>
<pre><code>|           |        | Turtle | Net Pos|
|-----------|--------|--------|--------|
|2004-11-04 |09:24   |6       |        |
|           |09:34   |2       |        |
|           |09:44   |0       |        |
|           |09:54   |4       |        |
|           |09:04   |1       |        |
|           |09:14   |2       |        |
|           |09:24   |9       |        |

turtle_factor = 3
base_quantity = 2
</code></pre>
<p>what I need is the &quot;Net Pos&quot;. I couldn't figure out a way to do it elegantly.
what I need is the col Net position using Numpy or Pandas
The data set is huge, and need to use recursion and avoid crash.</p>
<h2>Calculation</h2>
<p>6 will be spilt in 6 times 1</p>
<p>1 1 1 1 1 1</p>
<p>first 1 will be multiplied base_quantity, so 1*2</p>
<p>second 1 will be multiplied with the result from first 1 with turtle factor 2*3</p>
<p>third 1 will be multiplied with the result from second 1 calculated multiplied with turtle factor above 6*3</p>
<p>forth one will be multiplied with the result from third 1 calculated multiplied with turtle factor above 18*3 and so on
Summing them at the end to get the result as below for 1st row as 728</p>
<pre><code>|           |        | Turtle | Net Pos|
|-----------|--------|--------|--------|
|2004-11-04 |09:24   |6       |    728 |
|           |09:34   |2       |      8 |
|           |09:44   |0       |      0 |
|           |09:54   |4       |     80 |
|           |09:04   |1       |      2 |
|           |09:14   |2       |      8 |
|           |09:24   |9       |  19682 |


</code></pre>
",17975869.0,8075540.0,2022-01-21 01:04:02,2022-01-21 01:04:02,is there any effective or efficient way to find net position of numbers from a data frame in python,<python><recursion><data-science>,1,1,N/A,CC BY-SA 4.0
70803628,1,70803883.0,2022-01-21 15:22:18,0,62,"<p>I have dataframe with bus stop arrival forecast:</p>
<pre><code>path_id | forecast | forecast_made_at | bus_id
 int    | datetime |  datetime        | int
</code></pre>
<p>We make predictions every 5 minutes, so database entries can be duplicated. for example</p>
<pre><code>In 11:50 we predict bus #11544 will arrive at 11:59
In 11:50 we predict bus #95447 will arrive at 11:55
--......--
In 11:55 we predict bus #11544 will arrive at 12:02
</code></pre>
<p>I want to get newest prediction with biggest forecast_made_at parameter:</p>
<pre><code>res = pd.DataFrame()
for k, row in t_data.iterrows():
  prediction = dict(**row)
  forecasts = t_data[t_data[&quot;bus_id&quot;] == prediction[&quot;bus_id&quot;]] # Forecasts with the same bus_id
  prediction[&quot;best&quot;] = (prediction[&quot;forecast_made_at&quot;] == max(forecasts[&quot;forecast_made_at&quot;]))
  res = res.append(prediction, ignore_index=True)

res = res[res[&quot;best&quot;] == True]
</code></pre>
<p>In this code, we are working with dictionaries and not with pandas objects, so this one is very slow. How can I do this using pandas tools</p>
",13716256.0,-1.0,N/A,2022-01-21 15:47:05,How to minimize parameter in row pandas dataframe,<python><pandas><dataframe><data-science>,3,2,N/A,CC BY-SA 4.0
71740225,1,-1.0,2022-04-04 15:56:13,0,114,"<p>I have a data of transaction and there's refunds in it and i want to select it and delete it, you can say my data look like this :</p>
<pre class=""lang-py prettyprint-override""><code>product key     mtligne_ttc      partner_id     date
1001            14.50            10024          20-12-2018
1001            14.50            10024          20-12-2018 
1002            21.30            22444          10-10-2018
1003            11.10            516            05-10-2018
1002           -21.30            22444          23-10-2018
1005            5.50             1800           01-09-2018
1006            8.30             4221           01-09-2018
1003           -11.10            516            06-10-2018
1003            11.10            516            09-10-2018
</code></pre>
<p>i found this code online but it doesn't work properly because it check if the value has a negative value and he added to the new one</p>
<pre class=""lang-py prettyprint-override""><code>def refunded(data):
    return data[data[&quot;mtligne_ttc&quot;].isin(-data[&quot;mtligne_ttc&quot;])]
dataset1 = dataset.groupby(&quot;partner_id&quot;).apply(refunded).reset_index(0, drop=True)
</code></pre>
<p>so the output would be like :</p>
<pre class=""lang-py prettyprint-override""><code>product key     mtligne_ttc      partner_id     date
1002            21.30            22444          10-10-2018
1003            11.10            516            05-10-2018
1002           -21.30            22444          23-10-2018
1003           -11.10            516            06-10-2018
1003            11.10            516            09-10-2018
</code></pre>
<p>there's like a 11.10 added which is wrong.</p>
",18704001.0,-1.0,N/A,2022-04-04 19:31:37,i want to find the negative and positive of the same value in column and delete it,<python><data-science><data-cleaning>,1,4,N/A,CC BY-SA 4.0
74615686,1,-1.0,2022-11-29 14:34:46,0,24,"<p>Here is a code to find IQR in a dataset</p>
<blockquote>
<p>cols = ['M01AB', 'M01AE', 'N02BA', 'N02BE', 'N05B', 'N05C', 'R03', 'R06']
Q1 = df[cols].quantile(0.25)
Q3 = df[cols].quantile(0.75)
IQR = Q3 - Q1</p>
<p>df[<strong>~</strong>((df[cols] &lt; (Q1 - 1.5 * IQR)) |(df[cols] &gt; (Q3 + 1.5 * IQR))).any(axis=1)]</p>
</blockquote>
<p>I am unable to understand the use of the Tilda operator in the last line of this code</p>
<p>Not able to understand the use case of the tilda operator in this code</p>
",19367814.0,-1.0,N/A,2022-11-29 14:34:46,What is the purpose of the Tilda operator in this code?,<data-science>,0,2,N/A,CC BY-SA 4.0
70804076,1,-1.0,2022-01-21 15:54:45,0,163,"<p><strong>Naive problem here:</strong> let's say I have a dataframe, that is divided into <code>df1</code> and <code>df2</code>.
Now, <code>df1</code> is composed of the following variables:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>categ_var_1</th>
<th>categ_var_2</th>
<th>binary_target</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>C</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>B</td>
<td>5</td>
<td>0</td>
</tr>
<tr>
<td>B</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Goal:</strong> I want to use <code>df1</code> to fill in the missing column in <code>df2</code>, which has the same categorical variables (with different data), but <em>the binary_target is completely missing</em>:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>categ_var_1</th>
<th>categ_var_2</th>
</tr>
</thead>
<tbody>
<tr>
<td>F</td>
<td>3</td>
</tr>
<tr>
<td>B</td>
<td>2</td>
</tr>
<tr>
<td>A</td>
<td>5</td>
</tr>
<tr>
<td>A</td>
<td>5</td>
</tr>
<tr>
<td>B</td>
<td>1</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p>Which would be the best approach to resolve this in a simple manner?
My first guess was to use a Machine Learning model (using categorical variables as predictors), but I wouldn't be able to contrast the results, since <code>df2</code> has no target variable.
My second guess was to merge both sets and do a column-imputation, though the final result wouldn't be accurate.
What do you think? Any help would be highly apppreciated! (The only restriction is to use Python!)</p>
",12065665.0,-1.0,N/A,2022-01-21 21:14:26,How to use a dataset to fill in a missing column in another set,<python><pandas><dataframe><data-science><analytics>,1,2,N/A,CC BY-SA 4.0
70810857,1,70811799.0,2022-01-22 07:31:03,7,189,"<p>I am trying to achieve a calculation involving geometric progression (split). Is there any effective/efficient way of doing it. The data set has millions of rows.
I need the column &quot;Traded_quantity&quot;</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th></th>
<th>Marker</th>
<th>Action</th>
<th>Traded_quantity</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019-11-05</td>
<td>09:25</td>
<td>0</td>
<td></td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>09:35</td>
<td>2</td>
<td>BUY</td>
<td>3</td>
</tr>
<tr>
<td></td>
<td>09:45</td>
<td>0</td>
<td></td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>09:55</td>
<td>1</td>
<td>BUY</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>10:05</td>
<td>0</td>
<td></td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>10:15</td>
<td>3</td>
<td>BUY</td>
<td>56</td>
</tr>
<tr>
<td></td>
<td>10:24</td>
<td>6</td>
<td>BUY</td>
<td>8128</td>
</tr>
</tbody>
</table>
</div>
<p>turtle = 2
(User defined)</p>
<p>base_quantity = 1
(User defined)</p>
<pre><code>    def turtle_split(row):
        if row['Action'] == 'BUY':
            return base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)
        else:
            return 0
    df['Traded_quantity'] = df.apply(turtle_split, axis=1).round(0).astype(int)
</code></pre>
<h2>Calculation</h2>
<p>For 0th Row, Traded_quantity should be zero (because the Marker is zero)</p>
<p>For 1st Row, Traded_quantity should be (1x1) + (1x2) = 3 (Marker 2 will be split into 1 and 1, First 1 will be multiplied with the base_quantity&gt;&gt;1x1, Second 1 will be multiplied with the result from first 1 times turtle&gt;&gt;1x2), then we make a sum of these two numbers)</p>
<p>For 2nd Row, Traded_quantity should be zero (because the Marker is zero)</p>
<p>For 3rd Row, Traded_quantity should be (2x2) = 4(Marker 1 will be multiplied with the last split from row 1 time turtle i.e 2x2)</p>
<p>For 4th Row, Traded_quantity should be zero(because the Marker is zero)</p>
<p>For 5th Row, Traded_quantity should be (4x2)+(4x2x2)+(4x2x2x2) = 56(Marker 3 will be split into 1,1 and 1, First 1 will be multiplied with the last split from row3 times turtle &gt;&gt;4x2, Second 1 will be multiplied with the result from first 1 with turtle&gt;&gt;8x2), third 1 will be multiplied with the result from second 1 with turtle&gt;&gt;16x2) then we make a sum of these three numbers)</p>
<p>For 6th Row, Traded_quantity should be (32x2)+(32x2x2)+(32x2x2x2)+(32x2x2x2x2)+(32x2x2x2x2x2) = 8128</p>
<p>Whenever there will be a BUY, the traded quantity will be calculated using the last batch from Traded_quantity times turtle.</p>
<p>Turns out the code is generating correct Traded_quantity when there is no zero in Marker. Once there is a gap with a couple of zeros geometric progression will not help, I would require the previous fig(from Cache) to recalculate Traded_q. tried with lru_cache for recursion, didn't work.</p>
",10476993.0,10476993.0,2022-01-22 08:48:37,2022-01-22 10:09:51,split geometric progression efficiently in Python (Pythonic way),<python><math><data-science>,1,3,N/A,CC BY-SA 4.0
70818371,1,-1.0,2022-01-23 00:55:45,1,159,"<p>I am attempting to use GDS 1.8.2 as part of a system running an embedded Neo4j 4.4.3 server. The embedded server has been an operational component for several years, and several versions of Neo4j, so that component on its own has been time-tested and functions great. This is the first attempt to add support for graph algorithms into that component.</p>
<p>My first test is simply to submit a CQL query:</p>
<p><code>CALL gds.graph.create(&quot;someNamedGraph&quot;, [&quot;SomeNodeLabel&quot;], [&quot;SomeRelationshipType&quot;])</code></p>
<p>In the course of getting this to work, I found I had to register the <i>org.neo4j.gds.catalog.GraphCreateProc</i> class in the <i>GlobalProcedures</i> registry of the graph database. This seems to have been successful because, while I  was initially encountering a CQL exception saying the <code>gds.graph.create</code> procedure is unknown, now it appears to execute without exception. However, I am now seeing that the transaction doesn't produce the named graph (verified by checking the graph database using out-of-the-box Neo4j Community Edition server mode).  It only runs for perhaps 0.1 seconds (vs several seconds when done through the Neo4j Community Edition server mode where it works just fine).</p>
<p>What I now see is that the Query Execution Type (as indicated in the Result object coming back from the execution) is marked as READ_ONLY. There are no exceptions, notifications etc.  I have verified that a subsequent write transaction in the same test code, which creates a simple node (as a test), succeeds in writing a node and the Result object provides all the verifying information for that transaction.</p>
<p>Can anyone suggest why the <i>gds.graph.create</i> procedure would seem to execute with no exceptions yet somehow is getting marked as a READ_ONLY transaction? Is this even the reason why the named graph isn't getting created?</p>
<p>Thank you for suggestions or tips! I'm happy to provide more details if anyone has exploratory questions that might help unearth the root cause for this.</p>
",18005799.0,-1.0,N/A,2022-02-12 15:22:30,Neo4j embedded mode use of GDS,<java><neo4j><graph-data-science>,1,0,N/A,CC BY-SA 4.0
70810493,1,70814125.0,2022-01-22 06:25:08,-2,88,"<p>I'm trying to train the model on a MagnaTagAtune dataset. Is the model properly trained? What is the problem, does anyone know? Will waiting solve the problem?</p>
<p>The results are shown in the image.
<a href=""https://i.stack.imgur.com/ktF0q.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<hr />
<p>Thank you pseudo_random_here for your answer. Your tips were helpful, but the problem was still there.</p>
<p>Unfortunately, changing the learning rate did not work. Now, after your advice, I will use the SGD optimizer with a learning rate of 0.1. I even used another model that was for this but the problem was not solved.</p>
<pre class=""lang-py prettyprint-override""><code>from keras.optimizers import SGD
opt = SGD(lr=0.1)
model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = opt)
</code></pre>
",13368907.0,13368907.0,2022-01-26 08:37:52,2022-01-26 08:37:52,"After a few epochs, the difference between Valid loss and Loss increases",<conv-neural-network><data-science><training-data>,1,0,N/A,CC BY-SA 4.0
70815451,1,-1.0,2022-01-22 17:39:42,1,183,"<p>Is there a way to use a Microsoft Access database in a machine learning project? or a way to migrate to get access to another database more suitable for a machine learning project?</p>
",18003934.0,-1.0,N/A,2022-01-23 03:22:29,how to use access database in a AI project or switch it?,<python-3.x><machine-learning><data-science>,1,1,N/A,CC BY-SA 4.0
70816368,1,-1.0,2022-01-22 19:26:24,1,234,"<p>The system I'm working on uses a mobile phone app to take images. The white speckles in the images are reflective particles that need to be activated by the flash on the mobile phone for cataloging in an image processing pipeline. The downside is that we get unwanted specular reflection from the plastic in which the reflective particles are embedded. So the idea is that by taking multiple images and somehow 'stitching' them together the speckles could be preserved and the unwanted specular reflection removed to create one final 'clean' image.</p>
<p>I haven't been able to find any existing imaging processing techniques in the literature that use this approach but it seems like it might work. Any pointers on this approach would be much appreciated be it papers, pseudo-code or open source projects.</p>
<p><a href=""https://i.stack.imgur.com/43yQR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/43yQR.jpg"" alt=""Image 1 with specular reflection"" /></a></p>
<p><a href=""https://i.stack.imgur.com/n8NM5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n8NM5.jpg"" alt=""Image 2 with specular reflection"" /></a></p>
<p><a href=""https://i.stack.imgur.com/rvCpT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rvCpT.jpg"" alt=""Image 3 with specular reflection"" /></a></p>
<p><a href=""https://i.stack.imgur.com/uraqB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uraqB.jpg"" alt=""Image 4 with specular reflection"" /></a></p>
",8274752.0,8274752.0,2022-01-23 11:26:35,2022-01-23 11:41:36,Removing specular reflection from multiple images by 'merging' images,<image-processing><computer-vision><data-science>,1,3,N/A,CC BY-SA 4.0
70821846,1,-1.0,2022-01-23 12:04:56,0,380,"<p>I have two datasets in this form:</p>
<p>First Dataset:</p>
<pre><code>Booking_id   Booking amount   
6678           576545
6429           654556
6452           755849
6056           358749
6084           NaN
6542           768087
</code></pre>
<p>Second Dataset:</p>
<pre><code>ID_Universal   Price
4857           576545
6084           465488
4886           755849
3868           358749
8669           765889
4684           768087
</code></pre>
<p>In First dataset there are few NaN values of Booking_amount... But they are present in dataset 2. For example, if you will see id=6084 has Nan value in dataset 1 but for the same id, the amount value is present in dataset 2. Simillarly there are multiple Nan values for Booking_amount in dataset 1 that are present in dataset 2. So, I want to copy the values from dataset 2 to dataset 1 where dataset 1 has Nan values but for the same id, it's value is present in dataset 2.</p>
",18009048.0,18009048.0,2022-01-23 12:55:09,2022-01-23 12:55:09,How to copy a column in Pandas from one dataset to another based on condition,<python><pandas><data-science>,2,0,N/A,CC BY-SA 4.0
71750496,1,-1.0,2022-04-05 10:51:28,2,798,"<p>I did embeddings with fasttext and I have clusters thanks to KMeans.</p>
<p>I would like to calculate similarities inside each cluster to check if the sentences inside are well clustered. I want to keep sentences with good similarities in each clusters. If the similarity is not good, I want to exit sentence that not belong to a cluster, and next group similar sentences not belonging to clusters.</p>
<p>How can I do it in a good manner ? I thought using cosine similarity but don't know how to compare all sentences inside a cluster</p>
",17541416.0,17541416.0,2022-04-06 13:24:32,2022-04-12 01:02:37,How calculate clusters coherence/quality?,<machine-learning><data-science><cluster-analysis><k-means><cosine-similarity>,1,0,N/A,CC BY-SA 4.0
70909861,1,71109907.0,2022-01-29 21:25:53,0,1387,"<p>I'm trying to extract information of the Tesla stock but, I always get the error while coding it. Here's the code so far:</p>
<pre><code>import plotly.graph_objects as go
import yfinance as yf
import pandas as pd
import requests

from bs4 import BeautifulSoup
from plotly.subplots import make_subplots

def make_graph(stock_data, revenue_data, stock):

    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=(&quot;Historical Share Price&quot;, &quot;Historical Revenue&quot;), vertical_spacing = .3)
    stock_data_specific = stock_data[stock_data.Date &lt;= '2021--06-14']
    revenue_data_specific = revenue_data[revenue_data.Date &lt;= '2021-04-30']
    fig.add_trace(go.Scatter(x=pd.to_datetime(stock_data_specific.Date, infer_datetime_format=True), y=stock_data_specific.Close.astype(&quot;float&quot;), name=&quot;Share Price&quot;), row=1, col=1)
    fig.add_trace(go.Scatter(x=pd.to_datetime(revenue_data_specific.Date, infer_datetime_format=True), y=revenue_data_specific.Revenue.astype(&quot;float&quot;), name=&quot;Revenue&quot;), row=2, col=1)
    fig.update_xaxes(title_text=&quot;Date&quot;, row=1, col=1)
    fig.update_xaxes(title_text=&quot;Date&quot;, row=2, col=1)
    fig.update_yaxes(title_text=&quot;Price ($US)&quot;, row=1, col=1)
    fig.update_yaxes(title_text=&quot;Revenue ($US Millions)&quot;, row=2, col=1)
    fig.update_layout(showlegend=False,
    height= 900,
    title= stock,
    xaxis_rangeslider_visible=True)
    fig.show()

Tesla= yf.Ticker('TSLA')
tesla_database= Tesla.history(period= &quot;max&quot;)
</code></pre>
<p>In the tesla_database part is where the following error comes:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;pyshell#23&gt;&quot;, line 1, in &lt;module&gt;
    tesla_database= Tesla.history(period= &quot;max&quot;)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/yfinance/base.py&quot;, line 295, in history
    df.index = df.index.tz_localize(&quot;UTC&quot;).tz_convert(
AttributeError: 'Index' object has no attribute 'tz_localize'
</code></pre>
<p>How can I fix this? According to the IBM Watson Course in Coursera, this is the correct answer but I get this error all the time. Can someone correct it, please?</p>
",18068326.0,14856241.0,2022-01-30 05:19:06,2023-11-15 17:06:07,"""AttributeError: 'Index' object has no attribute 'tz_localize'"" while using the function history to extract stock information",<python><data-science><ibm-cloud><watson-studio>,1,5,N/A,CC BY-SA 4.0
70911236,1,-1.0,2022-01-30 01:57:55,0,282,"<p>I have 12 csv files with total size 8.45 GB.
I would like to read all csv files into pasdas dataframe with read_csv.</p>
<p>I tried using this code</p>
<pre><code># Example of 3 files

list = ['file-01.csv',
       'file-02.csv',
       'file-03.csv']


li = []

for filename in list:
    df = pd.read_csv(filename, index_col=None, header=0)
    li.append(df)

concat_df = pd.concat(li, axis=0, ignore_index=True)

</code></pre>
<p>Then it showed</p>
<p>MemoryError: Unable to allocate 784. MiB for an array with shape (1, 102804250) and data type int64</p>
<p>How can I solve this issue?</p>
<p>Thanks,</p>
",10477236.0,-1.0,N/A,2022-01-30 02:17:54,How can I concat the huge pandas dataframes into one?,<python><pandas><dataframe><memory-leaks><data-science>,2,5,N/A,CC BY-SA 4.0
70837432,1,-1.0,2022-01-24 16:48:56,1,121,"<p>I want to write a general function that can take in different machine learning algorithms and it's parameters and initialise a model.</p>
<p>I wrote:</p>
<pre><code>def create_model(model_name,kernel='rbf',C=1.0):
      clf = model_name()
print(create_model('SVC'))
</code></pre>
<p>and the error is:</p>
<blockquote>
<p>clf = model_name()
TypeError: 'str' object is not callable</p>
</blockquote>
<p>I understand the error is saying you can't have a string ('SVC') as a model object. So then I tried adding <code>evaluate(model_name)()</code> to turn the string into an object but got <code>name 'evaluate' is not defined</code>.</p>
<p>Could someone show me the correct way to do this? I want to do this so then I can add a set of optional parameters to the function, and then e.g. replace SVC with logistic regression in the same function.</p>
",8407951.0,12370687.0,2022-08-27 15:59:03,2022-08-28 01:54:16,How to convert a string into the literal meaning?,<python><string><machine-learning><scikit-learn><data-science>,2,1,N/A,CC BY-SA 4.0
70843948,1,-1.0,2022-01-25 05:47:48,0,314,"<p>I have many audio files in one folder and I want links of that files and want to paste into the data frame, for one file it's possible to copy the link, but there are 10000 files, so is there any code to automate this in python ?</p>
<p><a href=""https://i.stack.imgur.com/apaZ3.png"" rel=""nofollow noreferrer"">Click for the image</a></p>
",18024429.0,-1.0,N/A,2022-01-25 06:18:22,Get Drive File Link Using Python,<python><google-drive-api><data-science><drive>,2,3,2022-01-28 15:48:49,CC BY-SA 4.0
70867036,1,-1.0,2022-01-26 16:39:25,0,45,"<p>I am generating all two letter (digram) combinations of the standard alphabet</p>
<pre><code>import string
import itertools
UPPERCASE_ALPHABET = string.ascii_uppercase
TWO_LETTER_PERMS = itertools.permutations(UPPERCASE_ALPHABET, 2)
</code></pre>
<p>By using the 650 generated digrams in a search application of drug names,I am finding that some of them always retrieve no results probably being not pronounceable.</p>
<p>For example &quot;ZQ&quot; or &quot;KM&quot; (just as an example). While I do understand that each language might carry some differences, at least for English is there a way to judge if a generated digram is pronounceable?</p>
",7800760.0,-1.0,N/A,2022-01-26 16:39:25,Generating pronounceable digrams in Python,<python><nlp><data-science><linguistics>,0,2,N/A,CC BY-SA 4.0
70915709,1,70917309.0,2022-01-30 14:42:40,0,206,"<p>I have a Pandas DataFrame with a start column of dtype of datetime64[ns, UTC] and the DataFrame is sorted in ascending order based on the start column. From this DataFrame I used the following to create a new (updated) DataFrame indicating the day of the week for the start column</p>
<pre><code>format_datetime_df['day_of_week'] = format_datetime_df['start'].dt.dayofweek
</code></pre>
<p>I want to pass the DataFrame into a function. The function needs to loop through the days of the week, so from 0 to 6, and keep a running total of the distance (kept in column 'distance') covered. If the distance covered is greater than 15, then a counter is incremented. It needs to do this for all rows of the DataFrame. The return of the function will be the total number of weeks over 15.</p>
<p>I am getting stuck on how to implement this as my 'day_of_week' column starts as follows</p>
<pre><code>3
3
5
1
5
</code></pre>
<p>So, week 1 would be comprised of 3, 3, 5 and week 2 would be comprised of 1, 5, ...</p>
<p>I want to do something like</p>
<pre><code>number_of_weeks_over_10km = format_datetime_df.groupby().apply(weeks_over_10km)
</code></pre>
<p>but am not really sure what should go in the groupby() function. I also feel like I am overcomplicating this.</p>
",3727648.0,-1.0,N/A,2022-01-30 18:46:13,Loop Through Days of the Week in Pandas Dataframe,<python><pandas><dataframe><data-science>,2,2,N/A,CC BY-SA 4.0
70920394,1,-1.0,2022-01-31 01:19:51,0,173,"<p>I have a Pandas DataFrame that looks something like this:</p>
<pre><code>                                                       activity_id                            start                              end type  ...                                        site  heart_rate  grp_idx        date
user_id                                                                                                                                    ...
72xyy89c74cc57178e02f103187ad579  dcb12345678b5c8e84cf2931b1a553cb 2015-09-12 00:40:33.171000+00:00 2015-09-12 00:53:33.171000+00:00  run  ...  data\dcb12345678b5c8e84cf2931b1a553cb.json         NaN  2015-37  2015-09-12

</code></pre>
<p>The DataFrame has already been filtered such that all user_id is the same.</p>
<p>The dtypes are:</p>
<pre><code>activity_id                 object
start          datetime64[ns, UTC]
end            datetime64[ns, UTC]
type                        object
distance                   float64
steps                      float64
speed                      float64
pace                       float64
calories                   float64
ascent                     float64
descent                    float64
site                        object
heart_rate                 float64
grp_idx                     object
date                        object
</code></pre>
<p>I need to determine if there are x (e.g. 4) consecutive days in a row and find the number of times that has occurred. For example:</p>
<pre><code>2015-09-12
2015-09-13
2015-09-14
2015-09-15
2015-09-16
2015-09-17
2015-09-18
2015-09-19
</code></pre>
<p>Would count as two.</p>
<p>I have tried using groupby, e.g.:</p>
<pre><code>s = repeat_runner_df.groupby('user_id').start.diff().dt.days.fillna(1).cumsum()
repeat_runner_df.groupby(['user_id', s]).filter(lambda x: len(x) &lt; 3)
print(repeat_runner_df)
</code></pre>
<p>but that has gotten me nowhere and neither has my Google skills. Any help would be greatly appreciated.</p>
<pre><code>c7e962db02da12345f02fe3d8a86c99d          2018-04-08 03:16:06+00:00        2018-04-08 03:41:30+00:00  run  ...           NaN  2018-14  2018-04-08
c7e962db02da12345f02fe3d8a86c99d          2018-04-09 17:21:37+00:00        2018-04-09 18:27:17+00:00  run  ...           NaN  2018-14  2018-04-09
c7e962db02da12345f02fe3d8a86c99d          2018-04-10 19:05:39+00:00        2018-04-10 19:38:32+00:00  run  ...           NaN  2018-15  2018-04-10
</code></pre>
",3727648.0,3727648.0,2022-01-31 02:15:39,2022-01-31 02:15:39,Determining Consecutive Days Using Pandas,<python><pandas><time-series><data-science>,2,0,N/A,CC BY-SA 4.0
70831764,1,-1.0,2022-01-24 09:41:44,-1,1531,"<p>As a part of my Master's project I collect data with an RTK receiver and for matching some x,y,z,time data with scientific measurements I need to convert some GPS time ( GPS Week &amp; Milliseconds since the beginning of the GPS week) to UTC (hh:mm:ss:ms).
Can someone instruct me?</p>
<p>For example:
<a href=""https://i.stack.imgur.com/lznPn.png"" rel=""nofollow noreferrer"">data example to convert</a>
Cheers</p>
",18016086.0,-1.0,N/A,2022-01-27 02:30:45,Converting GPS time to UTC,<python><timestamp><gps><data-science><utc>,1,2,N/A,CC BY-SA 4.0
70859115,1,-1.0,2022-01-26 05:51:22,0,33,"<pre><code>df.Transmission.unique()

array(['Manual', 'DL9C', 'Automatic', 'DL5C', 'HR26', 'DL10', 'HR87',
   'HR29', 'UP16', 'DL6C', 'DL1C', 'DL2C', 'KA04', 'KA01', 'KA05',
   'KA02', 'KA41', 'KA51', 'KA53', 'KA50', 'KA03', 'KA22', 'KA09',
   'MH05', 'MH46', 'MH48', 'MH43', 'MH01', 'MH47', 'MH03', 'MH04',
   'MH02'], dtype=object)

df['InsuranceType'].unique()

array(['Third Party insurance', 'Manual', 'Not Available', 'Automatic',
   'Third Party'], dtype=object)
</code></pre>
<p>I want to swap these two column values at particular location. I m confused to how to do this</p>
<p>I tried to replace the values using</p>
<pre><code>if df[(df['Transmission'] != 'Manual') | (df['Transmission'] != 'Automatic')]:
    df['Transmission'] = 'Manual'
</code></pre>
<p>Manual being the mode of the column.</p>
<p>But No Luck it didnt work</p>
",17767556.0,17767556.0,2022-01-26 06:02:06,2022-01-31 12:41:18,How to swap the multiple values of dataframe in particular column with other columns from particular location,<dataframe><numpy><data-science>,1,1,N/A,CC BY-SA 4.0
70865699,1,70866502.0,2022-01-26 15:10:49,6,2673,"<p>I developed a custom dataset by using the PyTorch dataset class. The code is like that:</p>
<pre><code>class CustomDataset(torch.utils.data.Dataset):

    def __init__(self, root_path, transform=None):
        self.path = root_path
        self.mean = mean
        self.std = std
        self.transform = transform
        self.images = []
        self.masks = []

        for add in os.listdir(self.path):
            # Some script to load file from directory and appending address to relative array
            ...

        self.masks.sort()
        self.images.sort()

    def __len__(self):
        return len(self.images)

    def __getitem__(self, item):
        image_address = self.images[item]
        mask_address = self.masks[item]



        if self.transform is not None:
            augment = self.transform(image=np.asarray(Image.open(image_address, 'r', None)),
                                     mask=np.asarray(Image.open(mask_address, 'r', None)))
            image = Image.fromarray(augment['image'])
            mask = augment['mask']

        if self.transform is None:
            image = np.asarray(Image.open(image_address, 'r', None))
            mask = np.asarray(Image.open(mask_address, 'r', None))

        # Handle Augmentation here

        return image, mask
</code></pre>
<p>Then I created an object from this class and passed it to torch.utils.data.DataLoader. Although this works well with DataLoader but with torch.utils.data.DataLoader2 I got a problem. The error is this:</p>
<blockquote>
<p><code>dataloader = torch.utils.data.DataLoader2(dataset=dataset, batch_size=2, pin_memory=True, num_workers=4)</code></p>
<blockquote>
<p>Exception: thread parallelism mode is not supported for old DataSets</p>
</blockquote>
</blockquote>
<p>My question is why DataLoader2 module was added to PyTorch what is different with DataLoader and what are its benefits?</p>
<p>PyTorch Version: <code>1.10.1</code></p>
",13503187.0,-1.0,N/A,2022-01-26 16:05:19,What is different between DataLoader and DataLoader2 in PyTorch?,<python><deep-learning><pytorch><data-science>,1,0,N/A,CC BY-SA 4.0
70925275,1,70926461.0,2022-01-31 11:34:23,0,58,"<p>I am working on some data in <code>R</code>. For reproducibility, the data is as follows:</p>
<pre><code>month,source,amount1,amount2,total
jan,central,200,400,600
jan,tax,100,200,300
jan,fines,100,200,300
jan,east,150,50,200
jan,tax,100,25,125
jan,fine,0,75,75
jan,levies,0,0,0
Jan,tithe,0,0,0
</code></pre>
<p>Note that the amount for <code>central</code> is the sum of <code>tax</code> and <code>fines</code>. I want to rearrange the data by adding a column that will hold the term central, as follows.</p>
<pre><code>month,source,amount1,amount2,total,new_column
jan,tax,100,200,300,central
jan,fines,100,200,300,central
jan,tax,100,25,125,east
jan,fine,0,75,75,east
jan,levies,0,0,0,east
Jan,tithe,0,0,0,east
</code></pre>
<p>I appreciate any help. Note that the rows are not uniform.</p>
",13216931.0,13216931.0,2022-01-31 13:00:34,2022-01-31 13:05:39,Using R to Rearrange Data,<r><tidyverse><data-science><tidyr>,2,3,N/A,CC BY-SA 4.0
70835527,1,70835637.0,2022-01-24 14:36:03,0,31,"<p>I have a problem regarding the following issue:</p>
<p>I need all columnames aport from V1 till the end V200 in a list, but I don't know how and where to insert the index into these lines of code.
Like this, it gives me a list of all columnames but I don't need the first two. Please help, thank you!</p>
<p><a href=""https://i.stack.imgur.com/5GMIQ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",17885291.0,-1.0,N/A,2022-01-24 17:34:50,How to index when using colnames to create a list with all colnames in R(data frames),<r><dataframe><indexing><data-science>,1,0,N/A,CC BY-SA 4.0
70838589,1,-1.0,2022-01-24 18:19:07,0,25,"<p>I am using below simple neural network for classification</p>
<pre><code>model2 = keras.Sequential([
    keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),
    keras.layers.Dense(64, activation ='relu'),
       
    keras.layers.Dense(32, activation ='relu'),
    
    keras.layers.Dense(16, activation ='relu'),
       
    keras.layers.Dense(5, activation ='softmax')
])
</code></pre>
<p>I would like to check how the values for different classes change after going through each layer using the Euclidean distance to see which layer is the most useful and which is the least.
Does it make sense? If so, how can I do it using functional API? I encountered the first problems at the very beginning - they are related to the shape of the data - originally it is (1991, 13, 1292).</p>
",13075436.0,4685471.0,2022-01-24 21:52:25,2022-01-24 21:52:25,getting values between layers of a neural network using functional API,<machine-learning><keras><neural-network><data-science>,0,3,N/A,CC BY-SA 4.0
70873010,1,70873068.0,2022-01-27 03:34:06,0,116,"<br />
I need to replace the values in a column of my dataset with values that refer to the values with the same name in dictionary. <br />
So my data looks like this (don't pay attention on column names): 
<pre><code>data = data[['A?',
       'B',...,]]
</code></pre>
<p>And the dictionary looks like ths:</p>
<pre><code>felt_index = {
  &quot;First opt&quot;: 1,
  &quot;Second opt&quot;: 1,
  &quot;Third opt&quot;: 0.72
}

</code></pre>
<p>I want that instead of my column looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>A?</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>First opt</td>
<td>...</td>
</tr>
<tr>
<td>Second opt</td>
<td>...</td>
</tr>
<tr>
<td>Third opt</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p>It would have look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>A?</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>...</td>
</tr>
<tr>
<td>1</td>
<td>...</td>
</tr>
<tr>
<td>0.72</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p>I've tried some solutions and it didn't work out.
The last thing I tried is:</p>
<pre><code>for val in data[['A?']]:        
        data[['A']][val] = felt_index.get(data[['A?']][val])
</code></pre>
<p>And I got this error:</p>
<pre><code>TypeError: unhashable type: 'Series'
</code></pre>
<p>I can't figure t out how to solve it:( Please, help.</p>
",18043492.0,-1.0,N/A,2022-01-27 05:45:05,How do I replace values in data column according to dictionary?,<python><pandas><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
70937066,1,-1.0,2022-02-01 07:48:30,1,897,"<p>I have several columns (eg, Column, y1, y2, y3..) that I need to relate to column &quot;X&quot; on a scatter plot in Altair. I have included a dropdown combo box to make the selection between the &quot;y&quot; columns however the plots fail to change according to the selection. How can I make the y-axis selection responsive? Here is the code</p>
<pre><code># CHART 1
input_dropdown =  alt.binding_select(options = \
                                     np.array(df.drop([&quot;Student IDs&quot;, &quot;Average Marks&quot;], 
                                             axis = 1).columns),
                                    name = &quot;Module&quot;)

selection = alt.selection_single(bind = input_dropdown)


# plot the first chart
chart1 = alt.Chart(df).mark_point().encode(
    x = &quot;Average Marks&quot;,
    y = &quot;CSE103&quot;
).add_selection(
    selection)

chart1
</code></pre>
<p><a href=""https://i.stack.imgur.com/Wo70b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wo70b.png"" alt=""scatter plot is here"" /></a></p>
",12360445.0,13107804.0,2022-02-01 08:45:40,2023-03-26 19:14:02,Make dropdown selection responsive for y axis Altair python,<python><data-science><data-visualization><altair>,1,2,N/A,CC BY-SA 4.0
70923994,1,-1.0,2022-01-31 09:52:40,-3,28,"<p>I have a list that has been extracted from a dictionary and the first element of the new list looks like this:
<a href=""https://i.stack.imgur.com/A9VZZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A9VZZ.png"" alt=""enter image description here"" /></a></p>
<p>The total # of elements in my list are 4200; I was wondering how would I extract the <code>'uri'</code> of each individual element and place it into a new list.</p>
",15831486.0,13959139.0,2022-02-01 07:16:28,2022-02-01 07:16:28,Extract info from a dict turned list,<python><list><dictionary><data-science>,1,1,N/A,CC BY-SA 4.0
70924010,1,-1.0,2022-01-31 09:54:17,1,2301,"<p>So I am just starting a data science/stats class and I am trying to setup a R notebook within Dataspell I am able to create a Jupyter notbook but it only wants a python interpreter and I can't seem to change the interpreter to R</p>
<p><a href=""https://i.stack.imgur.com/WySV7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WySV7.png"" alt=""dataspell"" /></a></p>
<p><a href=""https://i.stack.imgur.com/9wo1I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9wo1I.png"" alt=""python"" /></a></p>
<p><a href=""https://i.stack.imgur.com/W2gSV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W2gSV.png"" alt=""enter image description here"" /></a></p>
<p>I only allows me to set a python interpreter. I am able to run R files just fine but I am trying to do it in a notebook. (Whether that be Jupyter or some other notebook I couldn't care less)</p>
<p>I would like to stick to Jetbrains IDE's either Dataspell or Pycharm. I tried our Datalore and got an R notebook working but its really slow for me.</p>
",7846634.0,-1.0,N/A,2022-05-04 20:34:28,Getting an R Notebook to work in Jetbrains Dataspell,<python><r><jupyter-notebook><data-science><dataspell>,2,0,N/A,CC BY-SA 4.0
70938625,1,-1.0,2022-02-01 10:01:47,0,297,"<p>I have a Pandas DataFrame with the following format.</p>
<ol>
<li>2 Columns</li>
<li>Each Column has a list in each row</li>
<li>First Column has a list in each row of size 768 which is Embedding and input to a model</li>
<li>Second Column has a list in each row of size 669, each of which is a output vector with 1 at the appropriate label and 0 at others.</li>
</ol>
<p>So the DataFrame looks something like this.</p>
<pre><code>    embeddings                                              output_vector
0   [-0.051661342, 0.03851345, 0.039971624, -0.032...   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
1   [-0.049478885, 0.051000055, 0.02100463, 0.0374...   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
2   [0.012464094, -0.009561883, -0.015517956, -0.0...   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
3   [0.028822085, 0.010243478, -0.027818449, 0.007...   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
4   [0.011971775, -0.0076607964, 0.041782353, 0.01...   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
</code></pre>
<p>Now, I have a total of 4659904 rows. So the shape of dataframe is (4659904, 2).</p>
<p>I want to convert this into the following format.</p>
<pre><code>       1           2                3     .... 768        769  770  771 ....  1437
0  0.012464094 -0.009561883 -0.015517956.... 0.34545       0    0    1          0
</code></pre>
<p>So I hope you got what I want to do.</p>
<p>Now I have tried 2 approaches.</p>
<p>1st Approach was</p>
<pre><code>data = self.df[&quot;embeddings&quot;].apply(pd.Series)
output = self.df[&quot;output_vector&quot;].apply(pd.Series)

df = pd.concat([data, output], axis=1)  # first 0-767 are embeddings, next 669 are output
</code></pre>
<p>Now this was giving memory error which was</p>
<pre><code>MemoryError: Unable to allocate 23.2 GiB for an array with shape (4659904, 669) and data type float64
</code></pre>
<p>And my second approach was</p>
<pre><code>data = self.df['embeddings'].to_list()
output = self.df['output_vector'].to_list()
self.train = pd.DataFrame(data).astype(np.float32)
self.target = pd.DataFrame(output).astype(np.int8)
</code></pre>
<p>But this was crashing the kernel(jupyter notebook again and again. Can someone help me with an efficient solution of this.</p>
<pre><code>Unnamed: 0,embeddings,output_vector
0,&quot;[-0.05166134  0.03851345  0.03997162 -0.03290318  0.01512185 -0.0819034
  0.0354522  -0.02535481 -0.02994072  0.00252245 -0.05784905 -0.03258096
  0.02258191  0.00183583  0.01800507 -0.06517535 -0.04593017  0.06183863
 -0.00041441  0.03079461 -0.03463263  0.03325753 -0.04068028  0.00273206
  0.00164782  0.01204575  0.02352095  0.02062964 -0.00591917 -0.02536847
  0.04765983  0.04625681 -0.03899917 -0.02615303  0.00705981 -0.03074866
  0.05641373 -0.03783266 -0.01139919  0.03948573 -0.00663179 -0.00589749
 -0.01257528 -0.03447495  0.0518124  -0.06747232  0.01029693 -0.00407829
 -0.06775934  0.04813158  0.00797615  0.07733657  0.00468081 -0.02471192
  0.01499321 -0.00821735 -0.01809384 -0.06386615  0.021714    0.03442325
  0.05850095 -0.0393686   0.05336163  0.03389135 -0.02499526  0.01388281
 -0.02674991  0.01138506  0.01022744 -0.03536162  0.02412011 -0.02426333
 -0.0206612  -0.07695793 -0.03040081  0.08638129 -0.06665266  0.03451493
 -0.01657975 -0.00086051  0.08815629 -0.12328484  0.06338329 -0.05311166
  0.00779183  0.03247199 -0.01067247 -0.04316234  0.00292004  0.00668126
  0.06147375 -0.02413378 -0.07050701 -0.0273391   0.00202257 -0.06629693
 -0.05837518  0.01078095 -0.02034491  0.04327701 -0.04553655 -0.01302541
 -0.01778577  0.05996176  0.05047935  0.00962515  0.01945804 -0.01035258
  0.03964177 -0.05716408 -0.03015794  0.07490353  0.00869034 -0.03777959
 -0.09420589  0.04103042  0.00352532  0.00483662  0.05727965 -0.03090096
  0.01135611 -0.03559055  0.04751372  0.01635346 -0.04467079  0.01799376
 -0.04339822 -0.03085382  0.01088353  0.07680215  0.00916065 -0.0214838
  0.00708571  0.0242582  -0.01336136 -0.05095749  0.03820392  0.00641685
  0.01867483  0.02065186 -0.07288514 -0.01070622  0.01591502 -0.03233102
 -0.07069992 -0.02052268 -0.05775747  0.01626526 -0.01104282  0.03581671
  0.04181774  0.00955072  0.03185135 -0.03269299  0.02027919  0.05170641
 -0.0187701   0.01201797  0.00777477  0.02877075 -0.02029566 -0.01786695
 -0.02663675 -0.03032351  0.03577211 -0.02235947 -0.07418466 -0.0126325
  0.10378824 -0.06620239 -0.04234699 -0.01125341  0.04993306 -0.03689389
 -0.05988461 -0.01296578  0.01776955 -0.00946942 -0.00703304  0.05030575
  0.00698223 -0.03722965 -0.01775958 -0.04928617 -0.03738888  0.04596115
 -0.02617255 -0.02787984 -0.02640495  0.01742701 -0.01677947 -0.0683789
  0.02945788  0.00630781 -0.00723888 -0.06381033 -0.00552338 -0.03850625
  0.02462378  0.05535    -0.02705379 -0.05988522  0.00022431 -0.02738986
  0.0093813   0.08072495  0.02500053 -0.03987784 -0.02250899 -0.03223332
  0.06255687 -0.02878852 -0.03344632  0.00139858  0.00359832 -0.03169511
 -0.00646856  0.10101542  0.00459585  0.03286723 -0.06437351  0.01282636
  0.00808889 -0.06423829 -0.04668027 -0.01475877 -0.00268935 -0.01117125
  0.0431009   0.01491173  0.05566576 -0.02226559  0.04101704 -0.04917342
 -0.0476597   0.01837071  0.01589093 -0.02911637  0.05973773 -0.01171169
  0.03905578  0.00807729 -0.0050068   0.06247048  0.02635357  0.05955895
 -0.00128761 -0.01398617  0.00569759  0.03363681 -0.02126136  0.00090667
  0.09445269  0.00604673  0.05231062 -0.00340813  0.0390805   0.00941924
 -0.03902109  0.06293902 -0.0670393   0.05001613  0.04281409  0.02461288
 -0.00030589 -0.02006223  0.00669593  0.04665446 -0.03402932 -0.00506028
 -0.01015078 -0.05799649 -0.00326196  0.02541533 -0.03319807  0.05974461
  0.01731517  0.02651091  0.07834585 -0.03986605 -0.01025968  0.08683844
 -0.02862466 -0.01438637  0.0153399   0.00875968 -0.03147566  0.02061091
 -0.08586337  0.0597867   0.05241873 -0.0708408   0.05001192 -0.0004473
  0.05756317  0.06532589  0.08924173  0.00346854 -0.00928313 -0.00892056
  0.01053016  0.0343186   0.00204539  0.01702691 -0.04748971 -0.05348249
 -0.0029539   0.00260539  0.0098225  -0.01978774 -0.05967304 -0.0061838
  0.01658366 -0.04486167 -0.05831086  0.03481287 -0.00621805 -0.07058373
  0.01721285  0.03547975 -0.06746124  0.02165006 -0.0325329  -0.03091994
 -0.05494453 -0.00630422 -0.00797902 -0.00281971 -0.02259897  0.02063943
  0.005752    0.06251206  0.02921022  0.06215975  0.0558803  -0.00424553
 -0.01338048  0.00175178  0.00870867  0.03154808 -0.01568847 -0.00888845
  0.00431952  0.05251616 -0.03060358 -0.04899014  0.04396039 -0.02023075
  0.026639   -0.01332658 -0.00505289 -0.11766905  0.03330288 -0.06424968
 -0.03973993  0.03034413 -0.00426532  0.02609361 -0.00482141 -0.04717631
 -0.00707188  0.03314037 -0.01248469  0.06797072  0.01562233 -0.0105147
  0.00354261 -0.00955186  0.01875797  0.03931813  0.0239697   0.00619115
  0.0040127   0.0075978  -0.06994274  0.02481538 -0.00961018 -0.02109162
  0.04067701 -0.0362495  -0.01002722  0.00390972 -0.07194793  0.03292149
 -0.03827338  0.01660496  0.06368925  0.00781327  0.05230084 -0.03625728
  0.02170477  0.00077182 -0.01657453  0.00719836  0.02092477  0.01984384
 -0.00859195 -0.05363973 -0.03805922 -0.03077629  0.02392779 -0.05323811
 -0.02951413 -0.03964449  0.03160063  0.01864669 -0.00454986  0.00783485
 -0.04530802 -0.01766081 -0.03344866  0.08781784 -0.02142147  0.03471855
 -0.01810967  0.06000137  0.00132435 -0.07387639 -0.02370103 -0.04454115
 -0.00343656  0.08652093  0.02587918  0.00052981 -0.00308254  0.00107671
 -0.00158151  0.01070594  0.01756192  0.03237168  0.01560741 -0.00077113
  0.01926707 -0.00077644  0.02262588 -0.04123246  0.03067257  0.0016649
 -0.0147792   0.0135532  -0.05892413 -0.02719664  0.01523045 -0.00762503
 -0.01342774  0.02971043 -0.02252426  0.00893138  0.02293     0.03111324
  0.01178017  0.00409572 -0.01850494  0.02166766 -0.01495638 -0.04588847
  0.04611809 -0.04341035  0.03234496 -0.06332202 -0.0603806  -0.03889009
 -0.06374518  0.06303922  0.00397166 -0.0103854  -0.07204134  0.0079536
 -0.01958643 -0.00570502 -0.00287138 -0.00522236  0.02472756  0.02283627
 -0.03949913 -0.05794566  0.03683843 -0.03112188  0.00712023 -0.02576118
 -0.00179857 -0.03020315 -0.07193667 -0.01003695  0.01468337 -0.02517877
  0.07590153 -0.01844895  0.04198132  0.00439076  0.04841577  0.03099028
 -0.02027995  0.04000163 -0.00691791 -0.03516426 -0.01683368 -0.05104186
  0.02026835 -0.06700286 -0.05765624 -0.04418447  0.08306263  0.00191237
  0.08406755  0.02541559  0.0241987  -0.02964545 -0.02499546 -0.00687738
  0.03740811 -0.01796946]&quot;,&quot;[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]&quot;
1,&quot;[-0.04947888  0.05100005  0.02100463  0.03748612  0.07402965  0.02355377
  0.00783554 -0.00441764  0.02037947 -0.01101317 -0.07272038 -0.01317635
  0.01208586  0.03214251  0.04312667 -0.05143728  0.03066461  0.0466751
  0.03715968 -0.00359702 -0.03551948  0.00267533 -0.00440571  0.03804535
 -0.02729472  0.01140058 -0.02763288  0.04737644 -0.00771125  0.005436
 -0.01340484  0.03636687  0.0186327  -0.00719269 -0.02292635 -0.02180105
  0.02612758 -0.04786601  0.00830678 -0.00820715  0.01277367  0.00289445
  0.05075581 -0.01381035  0.10703952 -0.01868866 -0.00276579 -0.02056527
  0.00550196  0.00948418 -0.01380098  0.01052737  0.0310314  -0.07236905
  0.00497095 -0.00481839  0.03363784 -0.04349018  0.01994482 -0.04397196
 -0.00484212  0.02875007  0.09342907 -0.00678766  0.01629852 -0.02953294
 -0.00327071 -0.00457183  0.0472579   0.01426319  0.01121215 -0.02678555
  0.0221851  -0.04992006 -0.03069272  0.02178229 -0.03155816  0.05580636
 -0.0033723  -0.01350003  0.06767849 -0.08352942  0.04405996 -0.01334711
  0.0120817  -0.00189286 -0.05611677  0.03474858  0.00700947  0.00623281
  0.07472326  0.04464442 -0.01372399 -0.01544881 -0.05644816 -0.02245107
 -0.00894202 -0.02940686 -0.03199704 -0.04313582 -0.04734996 -0.00951774
 -0.01484249  0.0441848   0.02249759 -0.01967675  0.0609702  -0.04995132
  0.06243914  0.0170641  -0.08805049  0.08661282 -0.0300777   0.01642903
  0.00658593  0.02000937  0.0255114  -0.04933698  0.07683811 -0.00782323
 -0.00899224  0.03329384  0.0220267  -0.03019185  0.02728132  0.00642568
 -0.0289499   0.02845548  0.00438163  0.01891153  0.00699697 -0.0294739
 -0.0021636   0.00888361  0.03458149 -0.01702423  0.0025404  -0.05348604
  0.03069593  0.00315084 -0.04713039 -0.03305872 -0.03173357  0.04649892
 -0.04547594 -0.02613497 -0.08691893 -0.01103899  0.00898269 -0.05313889
 -0.01163842 -0.05988954  0.03866214 -0.036987   -0.01109766  0.01573566
  0.04886296 -0.05685489  0.11012544 -0.04491482  0.07410064 -0.04501069
  0.02934901 -0.07100621  0.01520117 -0.01938212 -0.05686614 -0.05348065
  0.02143558  0.00394072 -0.08290003 -0.05853761 -0.01117312  0.00471243
 -0.00375203  0.04955852  0.01673758  0.05197054  0.0216698   0.05463026
 -0.00984788 -0.02647099  0.00252488  0.03244945 -0.00879634  0.03651949
  0.00740927  0.05746639 -0.08177374  0.04510127  0.0081488  -0.02886484
  0.04043742  0.01306033 -0.00463971 -0.05626136  0.01834616 -0.11957496
  0.03398205  0.02999268  0.00874698 -0.02382371  0.00983801 -0.04475022
  0.03303507 -0.00638326 -0.01239157 -0.04301523  0.06119924 -0.07059982
  0.05701525  0.01266128  0.01218241 -0.08232405  0.02706719 -0.04054825
 -0.05780607  0.17704122  0.05719048  0.03980062 -0.03590212  0.00614331
  0.02345392 -0.00790866 -0.02183374  0.01864438  0.01037393 -0.03245463
  0.0170967  -0.01328714 -0.05193226  0.00059576 -0.06048357 -0.02022821
 -0.00310479 -0.01914704 -0.05023568  0.02727042  0.00878176 -0.02216586
  0.00938931 -0.00390131 -0.04837844  0.03407615  0.0228362   0.08055416
  0.04116102 -0.02927751 -0.00195664  0.04617704  0.00212777  0.00241917
  0.03892139  0.01910733 -0.0418299   0.02023345 -0.00865303  0.01240681
  0.00640258  0.00800394 -0.00889974 -0.03450283 -0.01890989  0.01912797
 -0.02197257 -0.07663889  0.02719502  0.01061596 -0.00026483  0.03370981
  0.03650144  0.01263927  0.01336175 -0.04579114  0.0328994   0.0248688
 -0.04855581 -0.01974464  0.03756651 -0.00639009  0.03779132  0.02220182
  0.00616892  0.01141659 -0.01113698  0.04302325 -0.03216238  0.03149698
  0.02046196  0.01607756  0.00692186 -0.0374178  -0.02151899 -0.0098618
  0.02830259  0.0264569   0.01805272  0.04347522  0.01352492  0.03655376
 -0.01035947  0.02406344 -0.01063821  0.01583334 -0.05044172 -0.02398267
 -0.05738604 -0.01207048  0.0108248  -0.00301657 -0.00202845  0.09184012
  0.03679291 -0.0312708   0.02452561  0.08745933  0.05773052  0.01184117
  0.07111108 -0.04748241 -0.01939094 -0.09380461 -0.00566202 -0.03906861
 -0.02450904 -0.02724767  0.01328717  0.01607102 -0.05773607 -0.04570262
 -0.00164463  0.05305042 -0.02471338  0.111563   -0.01652718 -0.02199577
  0.02703861  0.04213524  0.04073446  0.03265747  0.04197201 -0.01796196
  0.04649751  0.01913586 -0.04377626 -0.00598586  0.0146745  -0.00752803
 -0.01065366 -0.00598944 -0.03583477 -0.08123961 -0.02519088 -0.04696031
 -0.05836546 -0.00623604 -0.1190364   0.06981929  0.01100622 -0.00460153
  0.05733385  0.06145079  0.03309435 -0.06192654  0.01429266  0.06567662
 -0.04621225  0.05313936  0.00336892  0.0003691  -0.04774792 -0.03295233
  0.01684359  0.02263164  0.01227753  0.09997382  0.01808568 -0.0239109
  0.01934133 -0.03824954  0.02999054 -0.01835003  0.01665515  0.06795614
 -0.07662991  0.00332418  0.07094113 -0.03043634  0.01041338 -0.05544253
  0.04332612  0.02299813  0.03224237  0.01786769  0.08371793  0.03054925
 -0.04367657  0.01642182  0.00532826 -0.00530441  0.05108052 -0.02680718
 -0.00417385 -0.04220665  0.0257977  -0.05653239 -0.04292049  0.04279727
  0.04139696 -0.00987987 -0.06639605 -0.00620872 -0.03221336  0.00646489
  0.04656329  0.11048093  0.0409079  -0.05480802  0.03511234 -0.00826276
  0.06744502  0.01281725  0.01174337 -0.00049231 -0.07301029 -0.03269533
  0.06007022  0.04813014 -0.03067294  0.09440122  0.05669014  0.02734192
  0.0341919   0.01266754 -0.02646296 -0.02491366  0.01102427 -0.03399333
  0.02538971 -0.07020739  0.02144649  0.07457864  0.03454475 -0.01533873
  0.04102416  0.02037716  0.03714816 -0.05033535 -0.06547187  0.00366059
 -0.00640169  0.04219402 -0.00574962  0.00542243  0.00290861  0.03108776
  0.01167466 -0.00421096  0.03817317 -0.06365499 -0.02736501  0.00309962
 -0.04416456  0.05299811  0.02675757  0.00842932  0.00477113  0.03170447
 -0.02259091 -0.0268518  -0.03465753 -0.02036886  0.02564894  0.03522039
 -0.03034502 -0.016796    0.06861627 -0.0352925   0.0303243  -0.01676265
 -0.00409323 -0.04394965 -0.03225574 -0.01515265  0.03908413 -0.01042351
 -0.03201464 -0.08628961  0.04659296 -0.00790099  0.00291772  0.0178647
  0.02020123  0.03364336  0.007617   -0.03353637 -0.01597506  0.00604231
 -0.01836335 -0.10303532 -0.0270133  -0.04139721 -0.01958671  0.02472844
  0.04268465 -0.02371885  0.02706097 -0.0339911   0.0309197   0.00829664
  0.03402448 -0.03474662]&quot;,&quot;[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]&quot;
</code></pre>
<p>I have added the csv of head(2) of dataframe. You can check it. It might have the list in string format, as I did df.to_csv()</p>
",10342778.0,10342778.0,2022-02-01 10:12:48,2022-02-01 10:12:50,Efficent way to convert list into columns Pandas,<python><arrays><pandas><dataframe><data-science>,1,9,N/A,CC BY-SA 4.0
70943719,1,70946382.0,2022-02-01 16:08:31,-1,78,"<p>The result would be something similar to:</p>
<ul>
<li>Customer xxx will churn within n months/weeks.</li>
</ul>
<p>My data:</p>
<ol>
<li>Business data about each client [company type, country, contract type, etc.]</li>
<li>Activity data [when they did some activities related to the service...].</li>
<li>A column that shows if they are churned or if they are still active.</li>
</ol>
<p>My questions are:</p>
<p>a) How can I calculate a good duration for churn? E.g., is a week before they churn is good, or should it be longer? Is there a formula to calculate this?</p>
<p>b) How do I prepare my data for the chosen duration? E.g., if we want a month of churn [probability of the customer xxx churn within the next month], should I use the activity data up until last month? In other words, should I exclude last month's activity data in my model?</p>
<p>I'm going to use a random forest model for this task.</p>
",9137379.0,-1.0,N/A,2022-02-01 19:25:51,"How to choose the best duration to churn? E.g., this customer will churn within a month",<machine-learning><statistics><data-science><analytics><random-forest>,1,0,N/A,CC BY-SA 4.0
70953489,1,71118574.0,2022-02-02 09:52:27,1,929,"<p>I want to train an arima model and want some forecast.</p>
<p>So I am doing this:</p>
<pre><code>arima&lt;-function(train, val, column)
{
model = auto.arima(train[[column]], trace=TRUE, stepwise=FALSE, approximation = FALSE)
acc=accuracy(model)
pred=forecast(model, length(val[[column]]))
acc=accuracy(pred, val[[column]])
return(acc)
}

</code></pre>
<p>Then I am getting this error:</p>
<blockquote>
<p>Error in accuracy.default(pred, val[[column]]):
First argument should be a forecast object or a time series.</p>
</blockquote>
<p>According to me pred should be a forecast object.</p>
<p>pred right now :</p>
<pre><code>$pred
Time Series:
Start=220
End=220
Frequency=1
[1] 2.72
$se
Time Series:
Start=220
End=220
Frequency=1
[1] 0.13
</code></pre>
<p>Please give me some advice I am stuck here...</p>
<p>Thanks in advance</p>
",12924273.0,16647496.0,2022-02-14 15:43:33,2022-02-14 21:36:13,Error in forecast and accuracy function in R,<r><time-series><data-science><predict>,1,1,N/A,CC BY-SA 4.0
70925367,1,-1.0,2022-01-31 11:41:41,1,51,"<p>I dont quite get what kind of Machine Learning Problem this is.</p>
<p>I do have Data consisting of time and a specific count.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>_time</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>7:15</td>
<td>190</td>
</tr>
<tr>
<td>7:20</td>
<td>240</td>
</tr>
</tbody>
</table>
</div>
<p>and so on.</p>
<p>With this Data I would like to create a model and &quot;predict&quot; the count value of specific times. The following Data looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>_time</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>7:30</td>
<td></td>
</tr>
<tr>
<td>7:35</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>For this Data i use the trained model and get a valid count out of it. Now I am wondering if it is supervised (because in the model we know the true counts and apply it to another time with unknown count) or if it is unsupervised.</p>
",18080507.0,12370687.0,2022-08-27 15:24:53,2022-08-28 01:55:11,Unsupervised or Supervised Machine Learning,<machine-learning><model><data-science><prediction>,2,1,N/A,CC BY-SA 4.0
70931709,1,-1.0,2022-01-31 19:23:49,0,40,"<p>I'm building an application in React that lets users upload pictures to the S3 bucket. After the upload is finished, I want to spin up some workers to process the pictures through a neural network that will analyze the image and tag it based on its content.</p>
<p>I don't want to run this action on the server itself, rather delegate it to a separate set of instances that will handle the processing. What would be the best solution to handle such a problem? It'll need to scale nicely depending on the amount of data to process.</p>
<p>It'd be great if this can be easily integrated using Node.js or Python if possible.</p>
",1662988.0,-1.0,N/A,2022-01-31 19:23:49,Server-side solution for handling asynchronous tasks,<javascript><reactjs><deep-learning><data-science><worker>,0,2,N/A,CC BY-SA 4.0
70945594,1,-1.0,2022-02-01 18:20:56,1,172,"<p>How can I append more than 2 filters in the <em><strong>search_filter</strong></em> of <strong>FuelSDK - Python</strong> which I am using to access Salesforce Marketing Cloud?</p>
<p>Lets say there are 3 filters:</p>
<pre><code>name_filter = {'Property' : 'Name','SimpleOperator' : 'equals','Value' : name}
age_filter = {'Property' : 'Age','SimpleOperator' : 'equals','Value' : age}
country_filter = {'Property' : 'Country','SimpleOperator' : 'equals','Value' : country} 
</code></pre>
<p><em><strong>Now here in the search_filter how to include all the above three filters since there are only two options i.e LeftOperand and RightOperand? If using FuelSdk it's not possible then what is the other way I could do the same?</strong></em></p>
<pre><code>row = ET_Client.ET_DataExtension_Row()
row.auth_stub = stubObj
row.CustomerKey = customerKey
row.search_filter = {'LeftOperand': name_filter, 'LogicalOperator':'AND', 'RightOperand':age_filter}
</code></pre>
<p><strong>PS:</strong>
<strong>I also tried nesting these filters but that also didn't worked.</strong> <em>For Example:</em></p>
<pre><code>name_filter = {'Property' : 'Name','SimpleOperator' : 'equals','Value' : name}
age_filter = {'Property' : 'Age','SimpleOperator' : 'equals','Value' : age}
country_filter = {'Property' : 'Country','SimpleOperator' : 'equals','Value' : country} 

complex_filter1={'LeftOperand': name_filter, 'LogicalOperator':'AND', 'RightOperand':age_filter}

complex_filter2={'LeftOperand': complex_filter1, 'LogicalOperator':'AND', 'RightOperand':country_filter}
</code></pre>
",9245146.0,-1.0,N/A,2022-02-01 18:20:56,How to add more than 2 filters in search_filter of Salesforce Marketing Cloud Fuel SDK - Python,<python><pyspark><soap><data-science><salesforce-marketing-cloud>,0,2,N/A,CC BY-SA 4.0
70974147,1,70974294.0,2022-02-03 15:39:08,0,309,"<p>I am having issues understanding Flask API's and what is needed to return the required information. I can print the data such as:</p>
<pre><code>crunchbaseNY = crunchbase[crunchbase['city'] == 'New York']
print(crunchbaseNY)
</code></pre>
<p>but when put into Flask it is returned as 'None'</p>
<pre><code>from flask import Flask, render_template
app = Flask(&quot;MyApp&quot;)

@app.route('/')
def hello_world():
    output = print(crunchbaseNY)
    return render_template('index.html', result=output)
    #return output
    
app.run(host='localhost', port=5001)
</code></pre>
",16714955.0,-1.0,N/A,2022-02-03 15:49:52,Why is my Flask API returning none in the html,<python><data-science>,1,2,2022-02-03 19:09:07,CC BY-SA 4.0
70955855,1,70955869.0,2022-02-02 12:43:56,1,1174,"<p>Tried to skip reading a few lines from csv file using the on_bad_lines argument set to .
TypeError: read_csv() got an unexpected keyword argument 'on_bad_lines'. The documentation has this keyword but is throwing up this error.</p>
",5810845.0,-1.0,N/A,2022-02-02 12:45:13,Unable to skip reading few lines in pandas read_csv() function,<python><pandas><csv><jupyter-notebook><data-science>,1,1,N/A,CC BY-SA 4.0
70972821,1,70973091.0,2022-02-03 14:15:47,0,1408,"<p>I have two dataframe as shown below</p>
<pre><code>|a|b|c|
|1|2| |
|4|5| |
|7|8| |
</code></pre>
<p>2nd dataframe is</p>
<pre><code>|a_1|b_1|
|1  |5  |
|4  |2  |
|7  |8  | 
</code></pre>
<p>I want to compare two common columns (a,a_1) and update the 3rd column in my 1st dataframe. Expected output is shown below</p>
<pre><code>    |a|b|c|
    |1|2|5|
    |4|5|2|
    |7|8|8|
</code></pre>
<p>Is there a code to match common column and populate 3rd column?</p>
",18110623.0,4420967.0,2022-02-03 15:39:27,2022-02-03 15:41:22,how to compare two columns in dataframe and update a column based on matching fields,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
70982112,1,-1.0,2022-02-04 06:10:08,0,349,"<p>Still a beginner in coding.
I have created a  multiple bar graphs  on a single plot and now wanted scores to print on the two different bargraphs.
Since both the bar graphs are taken from different dataframes i am able to print only one bar  graph i.e. scores considered from data (mentioned in the code). However printing is happening because of the for loop, plt.text but not on the correct bar graph as in the position of printing is incorrect.</p>
<p>And please suggest how to print on the first/second bar graph  for which i need to select the scores from data and Data1</p>
<pre><code>X = ['A','B','C']
data = customer_survey_TSS['Advocacy4']/4
Data1 = Emp_Survey_TSS['Advocacy']

  
X_axis = np.arange(len(X))

plt.rcParams[&quot;figure.figsize&quot;] = (10, 5)

  
plt.bar(X_axis - 0.2, data, 0.4, label = 'Customer scores')
plt.bar(X_axis + 0.2, Data1, 0.4, label = 'Employee scores')
  

plt.xticks(X_axis, X)
plt.xlabel(&quot;Quarters-FY21&quot;)
plt.ylabel(&quot;Aggregate score of customer &amp; Employee&quot;)
plt.title(&quot;Customer-Employee&quot;)
plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')

    for index,data in enumerate(VOC):
        plt.text(x=index , y =data+0.0065 , s=f&quot;{data}&quot; , fontdict=dict(fontsize=13))
        

plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/GxyXt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GxyXt.png"" alt=""enter image description here"" /></a></p>
",18116476.0,4420967.0,2022-02-04 07:20:34,2022-02-04 08:24:30,Trying to display different scores on the multiple bar graphs plotted in python,<python><matplotlib><data-science>,0,2,2022-02-04 08:24:12,CC BY-SA 4.0
70986710,1,70997494.0,2022-02-04 12:53:53,0,40,"<p>[[0.12673968 0.15562803 0.03175346 0.6858788 ]]
This is how my predict function is giving its output, I want to fetch the index of the highest value.
Tried this:
<code>pred= pred.tolist() print(max(pred)) index_l=pred.index(max(pred)) print(index_l)</code></p>
<p>But it seems to output only 0.</p>
<p>Printing max(pred) is giving the output:
[0.12673968076705933, 0.1556280255317688, 0.031753458082675934, 0.6858788132667542]</p>
<p>The network uses sequential with hidden layers (embedding, BiLSTM, BiLSTM, Dense, Dense)</p>
",18119594.0,-1.0,N/A,2022-02-05 11:17:07,How to capture index of highest valued array from predict function?,<python-3.x><data-science><recurrent-neural-network>,1,0,N/A,CC BY-SA 4.0
71015182,1,-1.0,2022-02-07 08:03:43,0,167,"<p>I'm trying to model raw data by an asymptotic function with the equation $$f(x) = a + (b-a)(1-\exp(-c x))$$ using R. To do so I used the following code:</p>
<pre><code>rawData &lt;- import(&quot;path/StackTestData.tsv&quot;)

# executing regression
X &lt;- rawData$x
Y &lt;- rawData$y
model &lt;- drm(Y ~ X, fct = DRC.asymReg())

# creating the regression function
f_0_ &lt;- model$coefficients[1] #value for y if x=0
steepness &lt;- model$coefficients[2]
plateau &lt;- model$coefficients[3] 
eq &lt;- function(x){f_0_+(plateau-f_0_)*(1-exp(-steepness*x))}

# plotting the regression function together with the raw data
ggplot(rawData,aes(x=x,y=y)) +
  geom_line(col=&quot;red&quot;) +
  stat_function(fun=eq,col=&quot;blue&quot;) +
  ylim(10,12.5)
</code></pre>
<p>In some cases, I got a proper regression function. However, with the attached <a href=""http://sendanywhe.re/PW99ESG6"" rel=""nofollow noreferrer"">data</a> I don't get one. The regression function is not showing any correlation with the raw data whatsoever, as shown in the figure below. Can you perhaps offer a better solution for performing the asymptotic regression or do you know where the error lies?</p>
<p><a href=""https://i.stack.imgur.com/IulG2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IulG2.png"" alt=""enter image description here"" /></a></p>
<p>Best Max</p>
<p><em>R4.1.2 was used using R Studio 1.4.1106. For ggplot the package ggpubr, for DRC.asymReg() the packages aomisc and drc were load.</em></p>
",12788729.0,3789665.0,2022-02-27 09:26:06,2022-02-27 09:26:06,Asymptotic regression function not correlating with raw data,<r><regression><data-science><non-linear-regression>,0,6,N/A,CC BY-SA 4.0
71007066,1,-1.0,2022-02-06 12:18:44,-1,727,"<p>I'm working on a project that aims to find conflicting <strong>Semantic Sentences</strong> (<strong>NLP - Semantic Search</strong> )
For example</p>
<p>Our text is: &quot;<strong>I ate today. The lunch was very tasty. I was an honest guest.</strong>&quot;</p>
<p>Query: &quot;<strong>I had lunch with my friend</strong>&quot;</p>
<p>Do we want to give the query model and find the meaning of the sentences with a certain point in terms of synonyms and antonyms?</p>
<p>The solution that came to my mind was to first find the <strong>synonymous sentences</strong> and extract the key words from the synonymous sentences and then get the <strong>semantic opposite</strong> words and then find the <strong>semantic synonymous</strong> sentences based on these opposite words.
Do you think this idea is possible? If you have a solution or experience in this area, please reply</p>
<p>Thanks</p>
",16829292.0,-1.0,N/A,2022-02-06 13:19:06,Finding contradictory semantic sentences through natural language processing,<machine-learning><nlp><data-science><semantic-analysis>,1,1,N/A,CC BY-SA 4.0
71008276,1,71008378.0,2022-02-06 14:47:36,1,39,"<p>I have the pandas dataframe as below</p>
<pre><code>A        B     C
Apple   20     A1
Apple   30     A2
Apple   40     A3
Kiwi    20     K1
Kiwi    30     K2
Kiwi    10     K3
</code></pre>
<p>I want the output as</p>
<pre><code>A        B     C
Apple   20     A1
        30     A2
        40     A3
Kiwi    20     K1
        30     K2
        10     K3
</code></pre>
",18124427.0,-1.0,N/A,2022-02-06 15:02:43,Combine similar column values in pandas,<python-3.x><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
71026019,1,-1.0,2022-02-07 22:05:42,0,61,"<p>I would like to find in dataframe of prices all all-time highs in that were in history.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>timestamp</th>
<th>close</th>
<th>ath</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>1234</td>
<td>0</td>
</tr>
<tr>
<td>x</td>
<td>2000</td>
<td>1</td>
</tr>
<tr>
<td>x</td>
<td>1956</td>
<td>0</td>
</tr>
<tr>
<td>x</td>
<td>1884</td>
<td>0</td>
</tr>
<tr>
<td>x</td>
<td>2234</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>So for specific row I would like to find a maximum for all previous rows and check if value in current row is greater than this maximum. I cannot do it without indexing for all rows and I know that it is inefficient.</p>
",8511957.0,-1.0,2022-02-07 22:06:12,2022-02-07 22:14:03,Do rolling on all dataframe rows,<python><pandas><dataframe><data-science>,3,2,N/A,CC BY-SA 4.0
71052648,1,-1.0,2022-02-09 15:48:11,0,700,"<p>Here is a snippet of the code I have right now:</p>
<pre><code>  '0_parcel_mapicon': None,
  '0_parcel_mapiconselected': None,
  '0_parcel_sections_0_0_0_buildingId': 123456,
  '0_parcel_sections_0_0_0_id': 123,
  '0_parcel_sections_0_0_0_LandUseCode': 'R00',
  '0_parcel_sections_0_0_0_Neighborhood': 'S110',
  '0_parcel_sections_0_0_0_landUseCodeDescription': 'SINGLE',
  '0_parcel_sections_0_0_0_LandSize': '1 LOT',
  '0_parcel_sections_0_0_0_Exemption': None,
  '0_parcel_sections_0_0_0_Tax_MunDist': 'NYC',
  '0_parcel_sections_0_0_0_SaleDate': '05/10/2021',
...
  '1_parcel_mapicon': None,
  '1_parcel_mapiconselected': None,
  '1_parcel_sections_0_0_0_buildingId': 56789,
  '1_parcel_sections_0_0_0_id': 321,
  '1_parcel_sections_0_0_0_LandUseCode': 'Z00',
  '1_parcel_sections_0_0_0_Neighborhood': 'D110',
  '1_parcel_sections_0_0_0_landUseCodeDescription': 'SINGLE',
  '1_parcel_sections_0_0_0_LandSize': '2 LOT',
  '1_parcel_sections_0_0_0_Exemption': None,
  '1_parcel_sections_0_0_0_Tax_MunDist': 'NYC',
  '1_parcel_sections_0_0_0_SaleDate': '09/10/2021',
...etc
</code></pre>
<p>This is in JSON format but has been converted to be a &quot;flatter&quot; file, incrementing each section by 1 as it goes down the JSON. I now need to be able to take this file and import into Excel in order to perform more advanced filtering. Unfortunately, Excel does not easily import this and treats each section as if its a new individual column (delimited by a colon).</p>
<p>I have tried to remove the incremeter from the Python code but by doing so it just exported one entity. Below is the code I used to flatten the data:</p>
<pre><code>import json
import requests

def flatten_json(y):
    out = {}

    def flatten(x, name=''):
        # If the Nested key-value
        # pair is of dict type
        if type(x) is dict:
            for a in x:
                flatten(x[a], name + a + '_')
                # If the Nested key-value
        # pair is of list type
        elif type(x) is list:
            i = 0
            for a in x:
                flatten(a, name + str(i) + '_')
                i += 1
        else:
            out[name[:-1]] = x

    flatten(y)
    return out
</code></pre>
<p>Is there any way to get this to be more acceptable to Excel?</p>
",12915431.0,-1.0,N/A,2022-02-09 16:03:08,How to Convert JSON to be readable by Excel (Already Flattened)?,<python><json><excel><data-science>,1,1,N/A,CC BY-SA 4.0
70915045,1,-1.0,2022-01-30 13:23:25,0,104,"<p>Why is the compiler giving me a <strong>ValueError</strong> on the last line?</p>
<pre><code> # Get coefficiants
    theta = np.array([0,0], dtype = float)
    
    theta[1] = get_covariance(x, x_mean, y, y_mean) / get_variance(x, x_mean)
    theta[0] = y - theta[1] * x
</code></pre>
",12987297.0,-1.0,N/A,2022-01-30 13:23:25,ValueError: setting an array element with a sequence when ascribing a value to an array index,<python><numpy><machine-learning><data-science>,0,6,N/A,CC BY-SA 4.0
70919818,1,70919872.0,2022-01-30 23:18:56,-1,579,"<p>I have a CSV file which contains the columns as follows:</p>
<p><code>&quot;Date&quot;,&quot;Time&quot;,&quot;TimeZone&quot;,&quot;Name&quot;,&quot;Type&quot;,&quot;Status&quot;,&quot;Currency&quot;,&quot;Gross&quot;,&quot;Fee&quot;,&quot;Net&quot;,&quot;From Email Address&quot;,&quot;To Email Address&quot;,&quot;Transaction ID&quot;,&quot;Shipping Address&quot;,&quot;Address Status&quot;,&quot;Item Title&quot;,&quot;Item ID&quot;,&quot;Shipping and Handling Amount&quot;,&quot;Insurance Amount&quot;,&quot;Sales Tax&quot;,&quot;Option 1 Name&quot;,&quot;Option 1 Value&quot;,&quot;Option 2 Name&quot;,&quot;Option 2 Value&quot;,&quot;Reference Txn ID&quot;,&quot;Invoice Number&quot;,&quot;Custom Number&quot;,&quot;Quantity&quot;,&quot;Receipt ID&quot;,&quot;Balance&quot;,&quot;Address Line 1&quot;,&quot;Address Line 2/District/Neighborhood&quot;,&quot;Town/City&quot;,&quot;State/Province/Region/County/Territory/Prefecture/Republic&quot;,&quot;Zip/Postal Code&quot;,&quot;Country&quot;,&quot;Contact Phone Number&quot;,&quot;Subject&quot;,&quot;Note&quot;,&quot;Country Code&quot;,&quot;Balance Impact&quot;</code></p>
<p>I am trying to just grab the rows of data that contain the string <strong>Chain × Jewelry × Necklace</strong> in the <strong>Item Title</strong> column.</p>
<p>The name under each item title is different. For example. One might be <strong>Chain × Jewelry × Necklace Popcorn Necklace</strong> others are BLANK VALUES but I just want all of them that contain <strong>Chain × Jewelry × Necklace</strong></p>
<p>How can I use pandas to pull these specific rows containing this string? I am having trouble. Any help is greatly appreciated thank you.</p>
",17988682.0,17988682.0,2022-01-30 23:54:05,2022-01-30 23:59:20,Using pandas in python to pull a specific string from a column,<python><pandas><data-science>,2,0,N/A,CC BY-SA 4.0
70943238,1,70985221.0,2022-02-01 15:34:45,0,55,"<p>The main goal is to understand:</p>
<ol>
<li>How likely is a customer churn?</li>
<li>Identify the churn reason(s) <strong>per user</strong>.</li>
</ol>
<p>For now, I'm using a random forest model. I can see the most important features for all users. Is there a way I could get the important feature per user? E.g., maybe a customer is leaving since they don't like the product, and another one is leaving since it's an expensive product, etc.</p>
<p>Thanks in advance!</p>
",9137379.0,-1.0,N/A,2022-02-04 10:56:07,How to add some global and local explainability to a predictions to understand for which reasons a customer churns?,<machine-learning><data-science><churn>,1,0,N/A,CC BY-SA 4.0
71044764,1,-1.0,2022-02-09 06:11:11,-1,97,"<p><em><strong>Actually, I want to scrape the <code>'title'</code> and <code>'product description'</code> for all the products and from all the <code>pages</code>, and then save it into the <code>'.csv'</code> file.</strong></em></p>
<p><strong>URL</strong>:- <a href=""https://www.nykaa.com/makeup/body-art/c/3024?page_no=1&amp;sort=popularity&amp;ptype=lst&amp;id=3024&amp;root=nav_2&amp;dir=desc&amp;order=popularity&amp;eq=desktop"" rel=""nofollow noreferrer"">hhttps://www.nykaa.com/makeup/body-art/c/3024?page_no=1&amp;sort=popularity&amp;ptype=lst&amp;id=3024&amp;root=nav_2&amp;dir=desc&amp;order=popularity&amp;eq=desktop</a></p>
<p><strong>This is what, I have tried.</strong></p>
<pre><code>from msilib.schema import Error
from os import sep
from tkinter import ON
from turtle import goto
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import numpy as np
from random import randint
import pandas as pd
import requests
import csv

title_list = []
para_list = []
expiry_list = []
country_list = []
importer_list = []
address_list = []

myDict = {'body-art': 3024}


browser = webdriver.Chrome(
    r'C:\Users\paart\.wdm\drivers\chromedriver\win32\97.0.4692.71\chromedriver.exe')

browser.maximize_window()
browser.implicitly_wait(20)

for item_name in myDict:
    page_num = 1
    while True:
        try:
            page = f&quot;https://www.nykaa.com/makeup/{item_name}/c/{myDict[item_name]}?page_no={page_num}&amp;sort=popularity&amp;ptype=lst&amp;id={myDict[item_name]}&amp;root=nav_2&amp;dir=desc&amp;order=popularity&amp;eq=desktop&quot;

            print(page)

            requests.get(page)

            soup = BeautifulSoup(requests.get(page).content, 'html.parser')
            urls = [item.get(&quot;href&quot;)
                    for item in soup.find_all(&quot;a&quot;, class_=&quot;css-qlopj4&quot;)]
            # print(urls)
            if len(urls) == 0:
                break
            for i in range(0, 2): #Since, it's a huge amount of data, that's why I have taken 2 products on one page, otherwise it will be in the range(0,30). It will cover all the products from an individual pages.
                try:
                    url = urls[i]

                    browser.get(&quot;https://www.nykaa.com&quot; + url)

                    title_data = browser.find_elements(
                        By.CLASS_NAME, 'css-1gc4x7i').text

                    print(title_data)

                    for t in title_data:
                        title_list.append(t)

                    browser.execute_script(&quot;document.body.style.zoom='50%'&quot;)
                    browser.execute_script(&quot;document.body.style.zoom='100%'&quot;)

                    # Creates &quot;load more&quot; button object.
                    browser.implicitly_wait(20)
                    loadMore = browser.find_element(
                        By.XPATH, &quot;/html/body/div[1]/div/div[3]/div[1]/div[2]/div/div/div[2]&quot;)
                    loadMore.click()
                    browser.implicitly_wait(20)

                    desc_data = browser.find_elements(By.ID, 'content-details')

                    for desc in desc_data:
                        para_details = browser.find_element(By.XPATH,
                                                            '//*[@id=&quot;content-details&quot;]/p[1]').text
                        para_list.append(para_details)
                        expiry = browser.find_element(By.XPATH,
                                                      '//*[@id=&quot;content-details&quot;]/p[2]').text
                        expiry_list.append(expiry)
                        country = browser.find_element(By.XPATH,
                                                       '//*[@id=&quot;content-details&quot;]/p[3]').text
                        country_list.append(country)
                        importer = browser.find_element(By.XPATH,
                                                        '//*[@id=&quot;content-details&quot;]/p[4]').text
                        importer_list.append(importer)
                        address = browser.find_element(By.XPATH,
                                                       '//*[@id=&quot;content-details&quot;]/p[5]').text
                        address_list.append(address)

                except:
                    break
        except:
            break
        page_num += 1

title_list = [i.split('.css', 1)[0] for i in title_list]
print(*title_list, sep=&quot;\n&quot;)
print(*para_list, sep=&quot;\n&quot;)
print(*expiry_list, sep=&quot;\n&quot;)
print(*country_list, sep=&quot;\n&quot;)
print(*importer_list, sep=&quot;\n&quot;)
print(*address_list, &quot;\n&quot;)
data_new = {&quot;Title&quot;: title_list, &quot;Para&quot;: para_list, &quot;Expiry&quot;: expiry_list,
            &quot;Country&quot;: country_list, &quot;Importer&quot;: importer_list, &quot;Address&quot;: address_list}
df = pd.DataFrame(data_new)
df.to_csv(&quot;nykaa_makeup_bodyArt_new.csv&quot;)
# print(df)
</code></pre>
<p>The <em><strong>Output</strong></em>, I am receiving is as:</p>
<pre><code>DevTools listening on ws://127.0.0.1:30887/devtools/browser/a222842a-7ce3-4070-a684-7e8bb8772279
https://www.nykaa.com/makeup/body-art/c/3024?page_no=1&amp;sort=popularity&amp;ptype=lst&amp;id=3024&amp;root=nav_2&amp;dir=desc&amp;order=popularity&amp;eq=desktop
https://www.nykaa.com/makeup/body-art/c/3024?page_no=2&amp;sort=popularity&amp;ptype=lst&amp;id=3024&amp;root=nav_2&amp;dir=desc&amp;order=popularity&amp;eq=desktop
https://www.nykaa.com/makeup/body-art/c/3024?page_no=3&amp;sort=popularity&amp;ptype=lst&amp;id=3024&amp;root=nav_2&amp;dir=desc&amp;order=popularity&amp;eq=desktop
https://www.nykaa.com/makeup/body-art/c/3024?page_no=4&amp;sort=popularity&amp;ptype=lst&amp;id=3024&amp;root=nav_2&amp;dir=desc&amp;order=popularity&amp;eq=desktop
https://www.nykaa.com/makeup/body-art/c/3024?page_no=5&amp;sort=popularity&amp;ptype=lst&amp;id=3024&amp;root=nav_2&amp;dir=desc&amp;order=popularity&amp;eq=desktop



PS E:\Web Scraping - Nykaa&gt;
</code></pre>
<p>I think, <strong>due to the implicity_wait()</strong> function, it's <strong>not able to fetch the product's title &amp; description</strong>. After <strong>my code run</strong>s, the <strong>'.csv' file is created</strong>, but it's a <strong>blank file</strong>. Maybe, I am wrong. <strong>Please help me regarding this</strong>. Do I need change to add/change some parts of the code?</p>
<p><strong>Thanks 🙏🏻</strong></p>
",12755158.0,-1.0,N/A,2022-02-09 07:24:51,How to run 'implicity_wait()' in a 'for loop' with respect to Web Scraping using Python?,<python><web-scraping><selenium-chromedriver><data-science>,1,0,N/A,CC BY-SA 4.0
71072650,1,-1.0,2022-02-10 21:44:05,1,222,"<h2>Geom_smooth formula</h2>
<p>I am currently trying to fit a line to my data.</p>
<p>However, no fitted line is shown on the plot with none of the default methods of geom_smooth.</p>
<p>Here is the code that generated the plot:</p>
<pre><code>p1 &lt;- ggplot(data[ which(data$subject_id == &quot;P121&quot; &amp; !is.na(data$col1)), ], aes(x = date, y= col1)) + 
  geom_point(colour= met.brewer(&quot;Isfahan1&quot;, 1)) + 
  geom_smooth(method = &quot;stats::loess&quot;, se= FALSE) +
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
                      axis.ticks.x=element_blank(), axis.title.y=element_blank(), axis.text.y=element_blank(),
                      axis.ticks.y =element_blank(), legend.title=element_blank()) +
  scale_color_manual(values=met.brewer(&quot;Isfahan1&quot;, 13))
</code></pre>
<p><a href=""https://i.stack.imgur.com/Pdvit.png"" rel=""nofollow noreferrer"">This is the plot that is created by the code above</a>
I am aware that we can customize the formula for the model to be fitted, however, I'm new to R. Could you guide me on how to decide on the formula?</p>
",18162449.0,-1.0,N/A,2022-02-14 15:59:23,R geom_smooth custom best fitted line formula,<r><ggplot2><data-science><data-visualization>,1,1,N/A,CC BY-SA 4.0
71013762,1,-1.0,2022-02-07 05:06:28,0,48,"<p>I'm using Django(3.2.11) with the Postgres database. and we have 12k stocks symbols stored in a database. now we are updating their price in the database frequently with <code>https://cloud.iexapis.com/stable/stock/{symbols}/intraday-prices?token={iex_api_key}&amp;chartIEXOnly=true</code>.
now I am required to update all 12k stock prices in 1 minute. for now, I'm using the celery periodic task. but the task took more than 1 minute for completing.
so how to update all stocks in 1 minute</p>
",16509673.0,16509673.0,2022-02-07 05:12:22,2022-02-07 05:12:22,How to update 12k records in 1 minute in Django,<python><django><celery><data-science><background-process>,0,4,N/A,CC BY-SA 4.0
71073257,1,-1.0,2022-02-10 22:53:35,0,100,"<p>I am a beginner<br />
How can i use for loop to print all them</p>
<pre><code>['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',
   'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',
   'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',
   'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',
   'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
   'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',
   'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
   'PavedDrive', 'SaleType', 'SaleCondition']
</code></pre>
<p>Like I did in it</p>
<pre class=""lang-py prettyprint-override""><code>pd.concat([df['MSZoning'].value_counts()/df.shape[0] * 100, 
           df3['MSZoning'].value_counts()/df3.shape[0] * 100], axis=1,
          keys=['MSZoning_org','MSZoning_clean'])
</code></pre>
<p>output:-</p>
<p>![output][https://i.stack.imgur.com/trH9m.jpg]</p>
",17355512.0,1779532.0,2022-02-11 04:47:41,2022-02-11 12:22:22,How can i use loop to show the data distribution in a dataset,<python><pandas><loops><for-loop><data-science>,1,0,N/A,CC BY-SA 4.0
71076844,1,-1.0,2022-02-11 08:03:33,0,304,"<p>I have a pandas dataframe, Which has 3 columns : <code>clm A</code>, <code>clm B</code>, <code>clm C</code> and one target column <code>results.</code> I want to find out whether target is depending on all/any other column. If I can draw a graph, I might have some insights. I am new to data visualization, so I do not know how to do this.</p>
<p>I tried using 2 columns at a time, E.g.: I drew scatter plot for <code>Clm A, results</code>, and then for <code>Clm B, results</code> and so on but I think all 3 columns (A, B, C) collectively are determining values for results. Is there any way to visualize this?</p>
",18178559.0,-1.0,N/A,2022-02-11 08:03:33,Finding out relation between multiple columns through data visualization in python,<python><matplotlib><seaborn><data-science><data-visualization>,0,4,N/A,CC BY-SA 4.0
71080383,1,-1.0,2022-02-11 12:50:27,0,635,"<p>I would like to use lasso or ridge regression in my model to handle multicolinearity, but when I try to do so and to watch the summary, i get an error:</p>
<pre><code>print(lasso_r.summary())
  File &quot;C:\Users\aleks\PycharmProjects\statistics\venv\lib\site-packages\statsmodels\base\model.py&quot;, line 1177, in summary
    raise NotImplementedError
NotImplementedError
</code></pre>
<p>My code looks the next way:</p>
<pre><code>lr = sm.OLS.from_formula(&quot;Ozone~Q('Solar.R')+Wind+Temp&quot;,data=air)
lasso_r = lr.fit_regularized(method='sqrt_lasso')
print(lasso_r.summary())
</code></pre>
<p>Could you tell me, please, what could cause this problem and how i can fix it?</p>
",17417282.0,-1.0,N/A,2022-07-15 08:01:44,How to use lasso/ridge regression model in statsmodels?,<python><regression><data-science><data-analysis><statsmodels>,1,0,N/A,CC BY-SA 4.0
71088040,1,-1.0,2022-02-12 00:41:36,0,476,"<p>I'm new to python and would like to know how do I display the record with the highest value.</p>
<p>I already figured out how to display the largest value, but I want to display the value concatenated with name(id).</p>
<p>For now, I did the following:</p>
<pre><code>import pandas as pd

data = pd.read_csv('datasets/kc_house_data.csv')
#what is the most expensive house (highest sale value)
print(max(data['price']))
</code></pre>
<p>That way it's only displaying the value, I want to display the record.</p>
<p>I also tried as follows:</p>
<pre><code>print(data[['id', 'price']].sort_values('price', ascending=False).head(1))
</code></pre>
<p>This way it returned the name and value correctly, but I believe there is some way to do this using the max() function.</p>
<p>Can someone help me?</p>
",16383990.0,-1.0,N/A,2022-02-12 07:59:09,How to display the record with the highest value?,<python><data-science>,1,3,N/A,CC BY-SA 4.0
71054166,1,71054678.0,2022-02-09 17:33:30,3,596,"<p>I am working on a machine learning project with very sparsely labeled data. There are several categorical features, resulting in roughly one hundred different classes between the features.</p>
<p>For example:</p>
<pre><code>0    red
1    blue
2    &lt;missing&gt;

color_cat = pd.DataFrame(['red', 'blue', np.NAN])
color_enc = OneHotEncoder(sparse=True, handle_unknown='ignore')
color_one_hot = color_enc.fit_transform(color_cat)
</code></pre>
<p>After I put these through scikit's <code>OneHotEncoder</code> I am expecting the missing data to be encoded as <code>00</code>, since the docs state that <code>handle_unknown='ignore'</code> causes the encoder to return an all zero array. Substituting another value, such as with <code>[SimpleImputer][1]</code> is not an option for me.</p>
<p>What I expect:</p>
<pre><code>0    10
1    01
2    00
</code></pre>
<p>Instead <code>OneHotEncoder</code> treats the missing values as another category.</p>
<p>What I get:</p>
<pre><code>0    100
1    010
2    001
</code></pre>
<p>I have seen the related question: <a href=""https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o"">How to handle missing values (NaN) in categorical data when using scikit-learn OneHotEncoder?</a>
But the solutions do not work for me. I explicitly require a zero vector.</p>
",7440665.0,-1.0,N/A,2022-02-09 18:43:43,Missing categorical data should be encoded with an all-zero one-hot vector,<python><pandas><machine-learning><scikit-learn><data-science>,1,3,N/A,CC BY-SA 4.0
71069734,1,71069838.0,2022-02-10 17:34:26,0,46,"<p>Given two lists of 1s and 0s (1 represents the true label, and 0 represents the false false) of the same length, output a 2darrary of counts, each cell is defined as follows</p>
<p>Top left: Predicted true and actually true (True positive)
Top right: Predicted true but actually false (False positive)
Bottom left: Predicted false but actually true (False negative)
Bottom right: Predicted false and actually false (True negative)</p>
<p>Sample Input</p>
<pre><code>1 1 0 0
1 0 0 0
</code></pre>
<p>Sample Output</p>
<pre><code>[[1., 0.],
[1., 2.]]
</code></pre>
<p>My code outputs:</p>
<pre><code>[[1 0]
 [1 2]]
</code></pre>
<p>Where can I get those dots??? Don't care about commas, i don't know why, but the answer without them is correct.</p>
<p>My code:</p>
<pre><code>import numpy as np

y_true = [int(x) for x in input().split()]
y_pred =  [int(x) for x in input().split()]
y_true = np.array(list(map(lambda x: 0 if x == 1 else 1, y_true)))
y_pred = np.array(list(map(lambda x: 0 if x == 1 else 1, y_pred)))
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_pred, y_true))
</code></pre>
",18173653.0,-1.0,2022-02-10 17:43:31,2022-02-10 17:43:31,How can I add the dots into numpy.ndarray output?,<python><numpy><data-science>,1,1,N/A,CC BY-SA 4.0
71094459,1,-1.0,2022-02-12 17:57:20,0,168,"<p>Is there a package in Python that lets you combine means and standard deviations from multiple groups to a single group?</p>
",3607022.0,3607022.0,2022-02-12 18:13:58,2022-02-12 18:13:58,Combine mean and standard deviation in Python,<python><pandas><dataframe><scipy><data-science>,0,3,N/A,CC BY-SA 4.0
71103796,1,-1.0,2022-02-13 18:42:15,0,49,"<p>I am working on a project trying to predict the revenue over the next 12 months, but when I run the predictions and plot it, the future prediction show a consistant downward trend. I used the ARIMA model to get my data stationary but not sure why this is happening. Can someone explain what this means, and why the predictions are just falling. <a href=""https://i.stack.imgur.com/ZBr2l.png"" rel=""nofollow noreferrer"">Original Plot</a>
<a href=""https://i.stack.imgur.com/jVKXs.png"" rel=""nofollow noreferrer"">Original vs Predicted</a></p>
",14920388.0,-1.0,N/A,2022-02-13 18:42:15,Downward predictions in timeseries,<python><time-series><data-science><arima>,0,3,N/A,CC BY-SA 4.0
71111019,1,-1.0,2022-02-14 11:21:14,0,143,"<p>I have been trying to convert a dataset file in XML format to JSON. I tried many solutions, I always got the same error</p>
<p><code>ExpatError: not well-formed (invalid token): line 67667, column 103 </code></p>
<p>I tried <code>untangle</code>, <code>xmltodict</code>, <code>pandas_read_xml as pdx</code></p>
<p>This is my code for  my attempt with <code>pandas_read_xml as pdx </code> which was the last solution:</p>
<pre><code>df = pdx.read_xml(&quot;Ryiadh_utf_8.xml&quot;)```
</code></pre>
",10083463.0,-1.0,N/A,2022-02-14 11:21:14,a problem to Convert XML File dataset to JSON in python,<python><json><xml><dataset><data-science-experience>,0,3,N/A,CC BY-SA 4.0
71079373,1,-1.0,2022-02-11 11:28:52,1,2533,"<p>Often I compare two models using different metrics in the notebook. It would be pretty good if I can just split cell output into 2 columns, fill the first column, then fill the second column.</p>
<p>Now I am calling a function that prints all metrics one by one</p>
<pre><code>check_metrics(model_path)
check_metrics(producion_model_pathes[label])
</code></pre>
<p>The output of check metrics looks like <a href=""https://i.stack.imgur.com/IoEjX.png"" rel=""nofollow noreferrer"">output example</a></p>
<p>Is it possible to split the output into 2 columns, then set as default output the first column, then before calling the second function set the second column as default output? So the output must be looks like the 2 pictures (like above) stacked vertically</p>
",13257982.0,-1.0,N/A,2022-08-17 12:34:51,Jupyter notebook split output on 2 columns,<machine-learning><jupyter-notebook><output><data-science><ipython>,1,2,N/A,CC BY-SA 4.0
71079798,1,71106428.0,2022-02-11 12:03:54,2,1251,"<p>I have two nested tables one is source and another is target table. I wanted to compare nested columns of source and target table. I am comparing two tables to check weather data is being updated in source table or not. Is there any sql in BigQuery to achieve the same?</p>
<p><a href=""https://i.stack.imgur.com/0srqH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0srqH.png"" alt=""enter image description here"" /></a></p>
<p>Here are my approaches which I was previously doing to compare two tables with nested record:</p>
<p><strong>1. This is the first approach:</strong></p>
<pre><code>SELECT to_json_string(info) FROM database.nested_table_source
except distinct
SELECT to_json_string(info) FROM nested_table_target
</code></pre>
<p>to_json_string() is not working, since this function returns different sequence of source rows and target rows sometimes, even though data is same in both tables, it results out different records.</p>
<p><strong>2. This is the second approach:</strong></p>
<pre><code>select name
from dataset.nested_table_source a
join dataset.nested_table_target b
using(name)
where
a.name!=b.name  and
(select string_agg(format('%t', s) order by key) from a.info s) 
!= (select string_agg(format('%t', s) order by key ) from b.info s)
</code></pre>
<p>In this approach I am using string_agg function to compare two nested records. But I am not sure if that is the correct way to compare record fields.</p>
<p>What should I do in this case?</p>
",7718928.0,7718928.0,2022-02-13 14:32:37,2022-02-14 01:59:21,How to compare two tables having record type column in BigQuery,<sql><google-cloud-platform><google-bigquery><data-science><data-warehouse>,1,3,N/A,CC BY-SA 4.0
71113578,1,-1.0,2022-02-14 14:36:07,0,17,"<p>I have a dataframe with categorical value, numerical values and a counter.
I want to keep only rows with the highest counter per category.
So, if my dataframe is:</p>
<pre><code>df = Tr. N1. N2. counter
     T1.  0.  3.    0 
     T1.  2.  3.    0
     T1.  7.  1.    1 
     T2.  2.  7.    0
     T2.  0.  9.    1 
     T2.  5.  6.    2
     T2.  3.  2.    2 
     T3.  7.  8.    0
</code></pre>
<p>For each value of Tr, I want to keep only rows with the highest counter.
So, the new df will be:</p>
<pre><code>df = Tr. N1. N2. counter

     T1.  7.  1.    1  
     T2.  5.  6.    2
     T2.  3.  2.    2 
     T3.  7.  8.    0
</code></pre>
<p>What is the best way to do so?</p>
",6057371.0,-1.0,N/A,2022-02-14 14:36:07,"pandas dataframe how to take only rows with max value on one col, per group",<pandas><dataframe><pandas-groupby><data-science><data-munging>,0,3,2022-02-14 14:37:55,CC BY-SA 4.0
71119326,1,-1.0,2022-02-14 23:11:20,1,1435,"<p>I'm a windows user, I have been trying to use Jupyter-Notebook to perform operations that uses sklearn package. I have been running into module not found error even though I have it installed.</p>
<p>I installed it using</p>
<pre><code>pip install sklearn
</code></pre>
<p>The error I'm getting is:</p>
<pre><code>import sklearn.datasets as datasets

Getting following error:

---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
C:\Users\WINDOW~1\AppData\Local\Temp/ipykernel_12348/2369316483.py in &lt;module&gt;
      5 import matplotlib.pyplot as plt
      6 import seaborn as sns
----&gt; 7 import sklearn.datasets as datasets
      8 import csv

ModuleNotFoundError: No module named 'sklearn'
</code></pre>
<p>I tried checking if I have sklearn installed using</p>
<pre><code>pip list

cycler          0.11.0
fonttools       4.29.1
joblib          1.1.0
kiwisolver      1.3.2
matplotlib      3.5.1
numpy           1.22.2
packaging       21.3
pandas          1.4.0
Pillow          9.0.1
pip             22.0.3
pyparsing       3.0.7
python-dateutil 2.8.2
pytz            2021.3
scikit-learn    1.0.2
scipy           1.8.0
seaborn         0.11.2
setuptools      41.2.0
six             1.16.0
sklearn         0.0
threadpoolctl   3.1.0
Unidecode       1.3.2
</code></pre>
",18061379.0,-1.0,N/A,2023-07-16 11:45:52,Getting Module not found error when trying to import sklearn,<python><scikit-learn><data-science><jupyter><sklearn-pandas>,1,5,N/A,CC BY-SA 4.0
71093331,1,71093486.0,2022-02-12 15:42:14,0,134,"<p>I'm new to sql.
I have 3 datasets ,
patient (columns : id age Zip_Code, size, weight, sex)
blood_tests (columns : test_ID, test_date, blood_sugar, laboratory_ID, patient_ID)
laboratory (columns : id, name, Zip_code, departments)</p>
<p>how can i get the number of patients per center ?
i did this code but it doesnt give the number per</p>
<pre><code>select DISTINCT patient_ID, laboratory_ID from patient,blood_tests where patient.id = blood_tests.patient_ID AND blood_tests.laboratory_ID = laboratory.id;
</code></pre>
<p>but i don't know how to get the total number of patients per center, because some of them did more than one exam in the same center and they have done tests in many labs?</p>
<p>For the second question. he aks us to get the 4 tests that a specific patient carried out in a laboratory called 'NWB'.
and i did this and noticed that he is patient with and ID = 25 but how can i get that without specifying that the id is 25.</p>
<pre><code>select patient_ID, laboratory.name from patient, blood_tests,laboratory where patient_ID = blood_tests.patient_ID AND blood_tests.laboratory_ID = laboratory.id HAVING laboratory.name = &quot;NWB&quot;;
</code></pre>
<p>Thank You in advance.</p>
",7308799.0,7308799.0,2022-02-12 20:42:02,2022-02-13 04:33:44,How can i get a specific data from my dataset using SQL and PHPmyadmin?,<mysql><sql><database><phpmyadmin><data-science>,3,0,N/A,CC BY-SA 4.0
71101448,1,-1.0,2022-02-13 14:11:29,0,891,"<p>I have been working on the case study where data is highly imbalanced. we have been taught we can handle the imbalanced data by either under sampling the majority class or over sampling the minority class.
I wanted to ask if there is any other way/method that can be used to handle imbalanced data?</p>
<p>this question is more on conceptual side than programming.</p>
<p>for example,
I was thinking if we could put some weight on minority class (conceptually) to make the model emphasize on identifying pattern in minority class.
I don't know how that can be done but this concept theoretically should work.</p>
<p>feel free to put crazy ideas too.</p>
",18196108.0,-1.0,N/A,2023-05-18 07:12:27,How to handle imbalanced data in general,<machine-learning><data-science><imbalanced-data>,4,0,N/A,CC BY-SA 4.0
71120944,1,-1.0,2022-02-15 03:54:59,0,41,"<p>I have a data frame <code>df</code>. I am adding the last 120 rows at a time from beginning cell <code>i</code>, under the same column, and update the sum to the following cell (121th row). After that, the beginning cell becomes <code>i+1</code> and repeat. The problem with my code is for the first couple loops it works fine and the results are correct, but then it stops calculating based on the conditions and return just 0. What are the issues here?</p>
<pre><code>cul_precip_120 = 0
counter_120 = 120
for i in range(1, len(df)): # starting day 367, which is first day in 1971
    
    cul_precip_120 = df.Precip.iloc[127 + counter_120: 247 + counter_120 : 1].sum() # 30 days from day 337 to day 366

    temp_120 = np.append(temp_120, cul_precip_120)

    counter_120 += 120
    i += 1
</code></pre>
",17494248.0,-1.0,N/A,2022-02-15 04:20:25,Why does my for loop stop following the conditions and return just 0 after first couple iterations?,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
71121104,1,-1.0,2022-02-15 04:26:16,1,911,"<p>my code:</p>
<pre><code># opening CSV
with open('notables.csv', 'w') as notables:
    file_write=csv.writer(notables, delimiter = ',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)

    # write
    # TO DO: how to get the values to write as well?
    file_write.writerow(data1)
    
    # close file
    file_write.close()
</code></pre>
<p>my error:</p>
<pre><code> file_write.close()

 AttributeError: '_csv.writer' object has no attribute 'close'
</code></pre>
<p>Am I suppose to close a csv file when done? If so, how?</p>
",14390382.0,-1.0,N/A,2022-02-15 05:47:24,AttributeError: '_csv.writer' object has no attribute 'close',<python><csv><data-science>,1,1,N/A,CC BY-SA 4.0
71123454,1,-1.0,2022-02-15 08:47:50,-3,692,"<p>Here are few example input &amp; outputs to understand the question</p>
<p>example 1</p>
<ul>
<li>input  = 555</li>
<li>output should be  = 55</li>
</ul>
<p>example 2</p>
<ul>
<li>input  = 5455</li>
<li>output should be  = 545</li>
</ul>
<p>example 3</p>
<ul>
<li>input  = 6555</li>
<li>output should be  = 655</li>
</ul>
<p>example 4</p>
<ul>
<li>input  = 3675</li>
<li>output should be  = 367</li>
</ul>
<p>Kindly help me with code in python</p>
",16672929.0,4420967.0,2022-02-15 16:03:58,2022-02-15 16:03:58,How to remove a single number from input number in python,<python><python-2.7><google-analytics><data-science><data-science-experience>,2,2,N/A,CC BY-SA 4.0
71114323,1,-1.0,2022-02-14 15:33:11,1,362,"<p>I am stuck with my Data Science project. When I perform my code:</p>
<pre><code>log_clf = LogisticRegression(max_iter=10000, C=2)
svm_clf = OneVsRestClassifier(LinearSVC(C=0.01,loss = &quot;hinge&quot;, random_state = 42))
voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],voting='hard')
voting_clf.fit(X_train, y_train)
</code></pre>
<p>I get the warning:</p>
<blockquote>
<p>Output exceeds the size limit. Open the full output data in a text
editor</p>
<p>C:\anaconda3\lib\site-packages\sklearn\linear_model_logistic.py:763:
ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL
NO. of ITERATIONS REACHED LIMIT.</p>
<p>Increase the number of iterations (max_iter) or scale the data as
shown in:
<a href=""https://scikit-learn.org/stable/modules/preprocessing.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/preprocessing.html</a> Please also refer to the documentation for alternative solver options:
<a href=""https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a>
n_iter_i = _check_optimize_result(
C:\anaconda3\lib\site-packages\sklearn\svm_base.py:985:
ConvergenceWarning: Liblinear failed to converge, increase the number
of iterations.   warnings.warn(&quot;Liblinear failed to converge, increase
&quot; C:\anaconda3\lib\site-packages\sklearn\svm_base.py:985:
ConvergenceWarning: Liblinear failed to converge, increase the number
of iterations.</p>
</blockquote>
<p>I have increased the max_iter for LogisticRegression, this doesn't help.
I saw some recommendations to ignore such warnings, is it a good strategy or I can handle the issue?
Thank you for any help.</p>
",16128731.0,-1.0,N/A,2022-02-14 15:33:11,The warning: STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in:,<machine-learning><data-science><machine-learning-model>,0,1,N/A,CC BY-SA 4.0
71119984,1,-1.0,2022-02-15 00:54:17,0,694,"<pre><code>number =[1,2,3]
day = 4
year = 2022
with open('notables.csv', 'w') as notables:
    file_write=csv.writer(notables, delimiter = ',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)

    file_write.writerow=([number, day, year])
</code></pre>
<p>my error when running the above code is:
AttributeError: '_csv.writer' object attribute 'writerow' is read-only</p>
<p>I though csv.writer was used for writing? Am I not suppose to use .writerow here? What am I suppose to use?</p>
",14390382.0,-1.0,N/A,2022-02-15 01:01:15,AttributeError: '_csv.writer' object attribute 'writerow' is read-only,<python><data-science>,1,4,N/A,CC BY-SA 4.0
71134953,1,71152511.0,2022-02-16 00:13:07,0,1193,"<p>I have a dataframe that consists of 3 columns. Champion (categorical, holds string values), total damage done (numerical), win (holds Boolean values, True or False). I want to draw a line and I want its markers to be &quot;o&quot; if &quot;win == True&quot; and &quot;x&quot; if &quot;win == False&quot;. I tried the code that I have attached here but it doesn't work.It gives <code>ValueError: Filled and line art markers cannot be mixed.</code>I tried to do it with hue or style but it changes the line style rather than marker. And I tried giving style my win column and I tried to make markers to follow from that, but that didn't work either. Can anyone help?
Thanks
<a href=""https://i.stack.imgur.com/dkjEZ.png"" rel=""nofollow noreferrer"">Only with <code>style</code> ScreenShot</a></p>
<pre><code>fig = plt.figure(figsize=(12,8))

h = sns.lineplot(data=skyhill_all,x='champion',y='totalDamageDealt',style='win',markers=['o','x'])
h.yaxis.set_minor_locator(AutoMinorLocator())


h.tick_params(which='both',width=2)
h.tick_params(which='major',length=8)
h.tick_params(which='minor',length=4)

h.set_ylabel('Total Damage Done')
h.set_xlabel('Played Champions')
h.set_yticks(np.arange(5000,75000,5000))

print(h)
</code></pre>
",17712160.0,-1.0,N/A,2022-02-17 04:24:45,"Seaborn lineplot, different markers for different boolean values",<dataframe><matplotlib><seaborn><data-science><data-visualization>,1,5,N/A,CC BY-SA 4.0
71031361,1,-1.0,2022-02-08 09:22:13,0,94,"<p>I have a function returning two lists while applying it on a column. I want to map each list on a separate column, while calling the function only once, as I have millions of input rows:</p>
<p>I have tried below code however not getting the desired output:</p>
<pre><code>def NERTag_ent(a):
    n=nl.ner(a)
    pr=[x[0] for x in n if x[1]=='PERSON']
    org=[y[0] for y in n if y[0]=='ORGANIZATION']
    return(tuple((pr,org))

df['Per'],df['Org']=df['n1'].apply(NERTag_ent)
</code></pre>
<blockquote>
<p>df['Per'] column should contain all the person and df['Org'] should contain all organisation.</p>
</blockquote>
",16441515.0,-1.0,N/A,2022-02-08 09:40:01,Updating multiple columns using apply function on a single column. The apply function returns two lists,<python><data-science><nltk><stanford-nlp>,1,0,N/A,CC BY-SA 4.0
71044465,1,71044818.0,2022-02-09 05:33:54,0,713,"<p>I'm studying data science. I looked other references to get an clear answer to this. But no success  with that. Can someone explain me about this.</p>
",9182171.0,-1.0,N/A,2022-02-09 12:31:15,"What is the difference between data validation and data cleaning, And what arethe connections and similarities between them?",<validation><data-science><data-cleaning>,1,0,N/A,CC BY-SA 4.0
71085896,1,71086192.0,2022-02-11 19:58:15,-2,111,"<p>Define the function <code>getColumn()</code> below that takes a column name and returns the data in that column as a list of strings to print the items in a header.</p>
<p>My <code>getColumn(name)</code> function fails and gives the error message
<code>unhashable type:list</code></p>
<pre><code>import csv
from csv import DictReader
from collections import defaultdict

with open('training_set_features.csv') as new_csv_file:
    new_csv_file = csv.DictReader(new_csv_file,fieldnames=headers)
    data = list(new_csv_file)

column_names = list(data[0].keys())
print(&quot;List of column names : &quot;, column_names)

# This part of my code is where I am stuck so it is incomplete
def getColumn(name):
    empty_list=[]
    column=dict[column_names]
    for row in rows:
        return

 print(getColumn('doctor_recc_h1n1'))
</code></pre>
",16775447.0,355230.0,2022-02-11 20:10:10,2022-02-11 20:34:05,Using CSV module (*no pandas*): How to print the rows in a header column?,<python><csv><dictionary><data-science>,1,2,N/A,CC BY-SA 4.0
71086129,1,-1.0,2022-02-11 20:21:20,0,108,"<p>Here is a part of my dataset but I am not 100% sure how I am supposed to categorise my dataset. I have been thinking about dummy and one-hot encoding methods for Districts and Crime types and set month as an index but I don't see the pattern for it. My goal is to forecast the total amount of crimes in the city and per district and the total specific crime types in the city and per district. So for instance, the total amount of crimes from 2011-2025 in the city and the total amount of shoplifting in Stoke Bishop in 2011-2025.</p>
<pre><code>        Month          Districts                    Crime type
0  2018-01-01       Stoke Bishop  Violence and sexual offences
1  2018-01-01  St Philip's Marsh                   Shoplifting
2  2018-01-01        Barton Hill                         Drugs
3  2018-01-01  St Philip's Marsh                   Shoplifting
4  2018-01-01  St Philip's Marsh                   Shoplifting
</code></pre>
",12380950.0,-1.0,N/A,2022-02-12 16:14:14,How can I turn multiple categorical data for the forecasting using LSTM model?,<machine-learning><data-science><lstm><data-preprocessing>,1,0,N/A,CC BY-SA 4.0
71101956,1,-1.0,2022-02-13 15:08:20,-1,1028,"<p>For somebody super beginner how can one remember what to pass into model.fit?
I have to keep on looking up that we are supposed to pass X_train, y_train.
But I guess I don't fully understand why we don't pass in something else such as:
X_train, y_test.</p>
<p>Thank you!
Josh</p>
",13959879.0,-1.0,N/A,2023-04-15 08:18:38,fitting on X_train and y_train,<python><machine-learning><data-science>,3,0,N/A,CC BY-SA 4.0
71144912,1,-1.0,2022-02-16 15:44:56,1,736,"<p>I am trying to rename the new aggregated column name in the groupBy function.</p>
<p>My code:</p>
<pre><code>groupByColumns = ['clientId', 'state', 'branchId']
aggColumn = 'amount'
aggOperation = sum
comNewColName = totalSalesDone

result = df.groupby(groupByColumns)[aggColumn].agg(aggOperation)
</code></pre>
<p>This here it is working perfectly. Now I am trying to rename the aggeregated new column:</p>
<pre><code>result = df.groupby(groupByColumns, as_index=False).agg(comNewColName=(aggColumn,aggOperation))
</code></pre>
<p>But I am getting column name comNewColName but I need to get totalSalesDone.</p>
",3655069.0,472495.0,2022-02-23 22:02:29,2022-02-23 22:02:29,Pandas - Groupby rename column name,<python><pandas><data-science>,3,1,N/A,CC BY-SA 4.0
71130076,1,71167011.0,2022-02-15 16:33:01,1,482,"<p>Say I had three types of vehicle, which are all related by some similar attributes.</p>
<p><a href=""https://i.stack.imgur.com/rfAhO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rfAhO.png"" alt=""enter image description here"" /></a></p>
<p>What is the best way to show these relationships?</p>
",12307227.0,3723423.0,2022-02-18 00:21:26,2022-02-18 00:21:26,"An entity relationship diagram shows the relationship between entities, but how do you show the relationship between attributes?",<data-science><computer-science><entity-relationship><modeling><erd>,1,0,N/A,CC BY-SA 4.0
71135354,1,-1.0,2022-02-16 01:20:21,0,297,"<p>I am using neo4j GDS library and wondering is there any way I can mutate a property on the relationship which is already exist on the node.
e.g. I have nodes with label Person connected nodes Book using relationship read. I am using the Page Rank algorithm which gives me expected output but I want to use weighted algorithm and wants to use the property Price on the Book.
As per my document I could find that I can use the weight on the relationship using &quot;relationshipWeightProperty&quot; but couldn't find anything related to node.
So is there any way I can use the weight from the target nodes property or is there any way I can mutate the price property on the relationship from the node and then use it?</p>
",2813165.0,-1.0,N/A,2022-02-16 18:34:58,Is there any way to mutate the property on relationship which is already exist on node?,<neo4j><pagerank><graph-data-science>,1,0,N/A,CC BY-SA 4.0
71147093,1,-1.0,2022-02-16 18:06:01,0,42,"<p>new is the main name of the folder, there are still 3subfloders in that main folder. my question is - I would print like the &quot;path and data also&quot;?</p>
<pre><code>import OS

import pandas as pd

import glob

files = OS.listdir(&quot;/home/rugved/new&quot;)

print(files)

full_data = pd.DataFrame()

for file in files:

    path = &quot;/home/rugved/new&quot; +  file + &quot;/*.csv&quot;

    mod = pd.concat([pd.read_csv(f) for f in glob.glob(path)], ignore_index = True)

    full_data = full_data.append(mod)

print(full_data)
</code></pre>
<hr />
<h2>Traceback is-
['abc', 'ghi', 'def']</h2>
<p>ValueError                                Traceback (most recent call last)
/tmp/ipykernel_56324/2862949528.py in 
9 for file in files:
10     path = &quot;/home/rugved/new&quot; +  file + &quot;/*.csv&quot;
---&gt; 11     mod = pd.concat([pd.read_csv(f) for f in glob.glob(path)], ignore_index = True)
12     full_data = full_data.append(mod)
13</p>
<p>/usr/lib/python3/dist-packages/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)
242     ValueError: Indexes have overlapping values: ['a']
243     &quot;&quot;&quot;
--&gt; 244     op = _Concatenator(
245         objs,
246         axis=axis,</p>
<p>/usr/lib/python3/dist-packages/pandas/core/reshape/concat.py in <strong>init</strong>(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)
302
303         if len(objs) == 0:
--&gt; 304             raise ValueError(&quot;No objects to concatenate&quot;)
305
306         if keys is None:</p>
<p>ValueError: No objects to concatenate</p>
",4582234.0,16458798.0,2022-02-16 18:13:53,2022-02-16 18:14:33,I create main folder is new in that i also sub folders. I would to print sub dir data,<python><python-3.x><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
71149737,1,71156033.0,2022-02-16 21:50:13,2,231,"<p>I have the following table</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Path ID</th>
<th>Lane ID</th>
<th>Customer</th>
<th>Source</th>
<th>Destination</th>
<th>Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>Mumbai</td>
<td>Chicago</td>
<td>Berlin</td>
<td>Ship</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>Mumbai</td>
<td>Berlin</td>
<td>Mumbai</td>
<td>Air</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>Mumbai</td>
<td>Chicago</td>
<td>Berlin</td>
<td>Air</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>Mumbai</td>
<td>Berlin</td>
<td>Dubai</td>
<td>Air</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>Mumbai</td>
<td>Dubai</td>
<td>Mumbai</td>
<td>Ship</td>
</tr>
</tbody>
</table>
</div>
<p>I want the following table</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Path ID</th>
<th>Source</th>
<th>Site2</th>
<th>Site3</th>
<th>Destination</th>
<th>Lane1 Mode</th>
<th>Lane2 Mode</th>
<th>Lane3 Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Chicago</td>
<td>Berlin</td>
<td></td>
<td>Mumbai</td>
<td>Ship</td>
<td>Air</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Chicago</td>
<td>Berlin</td>
<td>Dubai</td>
<td>Mumbai</td>
<td>Air</td>
<td>Air</td>
<td>Ship</td>
</tr>
</tbody>
</table>
</div>
<p>How do I go about getting this table? I feel like <code>groupby</code> is obviously required but what after that? Not sure how to proceed from there. The dataset is really big so it also needs to be efficient. Any pointers would help :)</p>
",9867311.0,-1.0,2022-02-17 10:06:42,2022-03-11 20:48:42,Group by pandas to get path from source to end destination,<python><pandas><dataframe><pandas-groupby><data-science>,2,0,N/A,CC BY-SA 4.0
71142746,1,-1.0,2022-02-16 13:28:33,0,345,"<p>This question concerns the dataframes in Pandas. Here is my problem.
I have a DataFrame of this format:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-09-30 00:00:00.00</td>
<td>10</td>
</tr>
<tr>
<td>2021-09-30 00:30:00.00</td>
<td>12</td>
</tr>
<tr>
<td>2021-09-30 01:00:00.00</td>
<td>10</td>
</tr>
<tr>
<td>2021-09-30 01:30:00.00</td>
<td>13</td>
</tr>
</tbody>
</table>
</div>
<p>I would to create a new column which takes the <strong>mean over each hour by conserving my dataframe</strong>.
The result should be like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Weight</th>
<th>Mean</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-09-30 00:00:00.00</td>
<td>10</td>
<td>11</td>
</tr>
<tr>
<td>2021-09-30 00:30:00.00</td>
<td>12</td>
<td>11</td>
</tr>
<tr>
<td>2021-09-30 01:00:00.00</td>
<td>10</td>
<td>11.5</td>
</tr>
<tr>
<td>2021-09-30 01:30:00.00</td>
<td>13</td>
<td>11.5</td>
</tr>
</tbody>
</table>
</div>
<p>Here is an example code:</p>
<pre><code>timestamp = pd.date_range(start=&quot;2021-10-01T00:00:000Z&quot;, end=&quot;2021-10-02T00:00:000Z&quot;, freq=&quot;200L&quot;)
df = timestamp.to_frame(index=True, name='timestamp')
df['timestamp'] = pd.to_datetime(df['timestamp'])
df['weight'] = np.random.randint(1, 20, df.shape[0])
df = df.set_index(['timestamp'])
</code></pre>
<p>It creates a DataFrame like this:
<a href=""https://i.stack.imgur.com/IDapx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IDapx.png"" alt=""enter image description here"" /></a></p>
<p>Now, I can do a resample like:
<code> dfresample = df.resample('8h').mean()</code></p>
<p>which gives: <a href=""https://i.stack.imgur.com/9Cnm6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Cnm6.png"" alt=""enter image description here"" /></a></p>
<p>However, this new DataFrame is reduced to 4 rows with an average of 8 hours. I would like to conserve my 432001 rows of first DataFrame <strong>df</strong>, where a new column <strong>mean</strong> is added, where each row takes the mean of that corresponding <strong>'8h'</strong> average.
How could I do that?</p>
<p>Thanks</p>
",3231319.0,3231319.0,2022-02-16 14:13:05,2022-02-18 16:20:02,Average per time interval by keeping the rows of a DataFrame,<python><pandas><dataframe><data-science><data-preprocessing>,1,5,N/A,CC BY-SA 4.0
71918984,1,-1.0,2022-04-19 02:25:57,0,587,"<p>How can I set the output of each layer of keras to be transposed(x.T) and then passed to the next layer?</p>
<p>For Example, layer1: x1= [[1,1,1],[2,2,2], w1= [[1,1,1],[2,2,2], out1 =x1<em>w1+b1.  -&gt;&gt;  layer2: x2=out1.T, w2= [[1,1,1],[2,2,2], out2 =x2</em>w2+b2.</p>
<pre><code>import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as layers
from tensorflow.keras import Sequential
import matplotlib.pyplot as plt
import pandas as pd
from tensorflow.keras.utils import normalize
from tensorflow.keras.utils import to_categorical
import tensorflow.keras.optimizers as optimizers
import tensorflow.keras.losses as losses
import tensorflow.keras.metrics as metrics
import numpy as np


mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test, y_test)
</code></pre>
",16558311.0,-1.0,N/A,2022-04-19 02:25:57,How to transpose the output of each layer of keras and pass it to the next layer,<python><machine-learning><keras><data-science>,0,3,N/A,CC BY-SA 4.0
71919843,1,-1.0,2022-04-19 04:53:37,0,35,"<p>I'm a neophyte in R.</p>
<p>I have a data frame that consists of about ~4000 conversations between two people.  It's structured roughly like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Unique Identifier</th>
<th>column1</th>
<th>column2</th>
</tr>
</thead>
<tbody>
<tr>
<td>123456</td>
<td>blahblah</td>
<td>blahblah</td>
</tr>
<tr>
<td>789412</td>
<td>blahblah</td>
<td>blahblah</td>
</tr>
</tbody>
</table>
</div>
<p>My goal is to get a similarity score for message 1 and message 2 of each row.  So eventually the data frame would look like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Unique Identifier</th>
<th>column1</th>
<th>column2</th>
<th>cosine</th>
</tr>
</thead>
<tbody>
<tr>
<td>123456</td>
<td>blahblah</td>
<td>blahblah</td>
<td>.562</td>
</tr>
<tr>
<td>789412</td>
<td>blahblah</td>
<td>blahblah</td>
<td>.264</td>
</tr>
</tbody>
</table>
</div>
<p>Ultimately, I’d have ~4000 scores (one for each row). I’m assuming that costring is the correct command to run for this, but I keep getting errors.  I'm assuming it's because R doesn't know that I want to compare column1 &amp; 2 in each row.</p>
",18574801.0,-1.0,N/A,2022-04-19 07:09:55,How do I get LSAfun to compare two rows of data in R?,<r><nlp><data-science><lsa>,1,1,N/A,CC BY-SA 4.0
71128709,1,71129004.0,2022-02-15 15:03:22,0,619,"<p>I'm on Jupyter trying to plot a slice of a DataFrame with the following values: Female = 16092 and Male = 27895</p>
<pre><code>sexo = df2.groupby(['Sexo']).size()
sexo

Sexo
F    16092
M    27895
dtype: int64


fig, axs = plt.subplots(1, figsize=[8,8])
sns.histplot(data=sexo, x='Sexo', y=sexo)
plt.show()
</code></pre>
<p>But the graph doesn't plot bars with a size relative to the values, just two squares covering a large number of values. (I can't post pictures here in Stack Overflow yet).</p>
<p>I'm probably feeding the histplot with wrong data or should be using another graph.</p>
<p>Thanks in advance.</p>
",8162202.0,-1.0,N/A,2022-02-15 15:24:24,Ploting with seaborn histplot,<python><numpy><matplotlib><seaborn><data-science>,2,3,N/A,CC BY-SA 4.0
71138242,1,-1.0,2022-02-16 08:13:50,0,81,"<p>I am trying to run groupby with multiple columns and aggregate column and aggregate Operator.</p>
<p>I will get all of above as parameter to method. I have to do groupby:</p>
<pre><code>result = df.groupby([groupByColumns])[aggColumn].agg(aggOperation)
</code></pre>
<p>Here</p>
<pre><code>groupByColumns: clientId,state,branchId
aggColumn: amount
aggOperator: sum
</code></pre>
<p>But I am getting this error</p>
<pre><code>KeyError: ''
</code></pre>
<p>I am not good in Panda. How can I correct my statement above?</p>
",3655069.0,472495.0,2022-04-03 01:09:47,2022-04-03 01:09:47,KeyError: in Pandas,<python><pandas><data-science>,2,0,N/A,CC BY-SA 4.0
71148779,1,-1.0,2022-02-16 20:20:31,0,1457,"<p>I've seen several posts here all saying &quot;Go go Python › Data Science: Text Output Limit&quot; but that setting does not exist in my VSCode or I'm too dumb to find it.</p>
<p>Has something changed?</p>
<p>How do I make VSCode diplay the full .csv I've imported?</p>
<p>It's [232 rows x 32 columns] but VSCode is only showing 10 rows - I get 0-4 and 227-231, everything in the middle is missing.</p>
<p>The number of columns changes depending on how wide the console window is.</p>
<p>edit more info:
I am using pandas and python 3.10, MS Python extension, and outputting to a vscode termial window.</p>
<p><a href=""https://i.stack.imgur.com/7ZSBg.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",3928808.0,3928808.0,2022-02-16 20:51:58,2022-02-17 07:44:45,How to stop VSCode from truncating Python output to console?,<python><visual-studio-code><data-science>,1,2,2022-02-17 21:16:25,CC BY-SA 4.0
71915742,1,-1.0,2022-04-18 18:37:34,-1,36,"<p>so I have a table that looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>memory confidence</th>
<th>Test (1= correct, 2=incorrect)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>56</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>78</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>98</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>24</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>45</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>87</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>People answer questions on a test, and then rate their confidence.</p>
<p>I want to create a scatterplot (and ultimately run a regression analysis) on the relationship between a person's average level of confidence over the test, and the percentage that they got correct. Each person should be one point on the scatterplot.</p>
<p>Any ideas?</p>
",18366823.0,-1.0,N/A,2022-04-19 01:27:29,Compute an average for each participant in long-form data,<python><statistics><regression><seaborn><data-science>,1,2,N/A,CC BY-SA 4.0
71941260,1,-1.0,2022-04-20 14:14:01,0,770,"<p>I received a Pandas Dataframe I have to work with and optimize. There are a bunch of columns and my goal is to iterate over a specific column through all the rows.</p>
<p>I got e.g.:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Association</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr>
<td>National Basketball Association</td>
<td>1991</td>
</tr>
<tr>
<td>Major League Baseball</td>
<td>2001</td>
</tr>
</tbody>
</table>
</div>
<p>And I now want to iterate over the association column and mutate all &quot;National Basketball Association&quot; to &quot;NBA&quot; and all &quot;Major League Baseball&quot; to &quot;MLB&quot; and so on.</p>
<p>What would be the most efficient approach for this? I tried using IFs, which felt not that efficient.</p>
<p>Thank you guys in advance!</p>
",10491430.0,-1.0,N/A,2022-04-20 14:41:53,Iterate over single column inside Pandas DataFrame and mutate data,<python><pandas><dataframe><iteration><data-science>,2,0,N/A,CC BY-SA 4.0
71919898,1,71919948.0,2022-04-19 05:01:56,1,698,"<p>I have this Python Pandas DataFrame:</p>
<pre><code>Municipio
São Caetano do Sul (SP)
Florianópolis (SC)
Vitória (ES)    
</code></pre>
<p>How to extract the term between () and turn it into:</p>
<pre><code>Municipio                   UF
São Caetano do Sul (SP)     (SP)
Florianópolis (SC)          (SC)
Vitória (ES)                (ES)
</code></pre>
",18858014.0,-1.0,N/A,2022-04-19 06:16:00,How to extract a string between special character on a column dataframe in python?,<python><pandas><dataframe><data-science>,4,3,N/A,CC BY-SA 4.0
71955753,1,71955971.0,2022-04-21 13:57:50,1,2654,"<p>I am new to <code>sklearn</code> &amp; <code>XGBoost</code>.
I would like to use GridSearchCV to tune a XGBoost classifier. One of the checks that I would like to do is the graphical analysis of the loss from train and test. So far I have created the following code:</p>
<pre class=""lang-py prettyprint-override""><code># Create a new instance of the classifier
xgbr =  xgb.XGBClassifier()
# Create a new pipeline with preprocessing steps and model (imballanced learn)
pipeline  = imb_pipeline([
                          ('preprocess', preprocess), # Encode and transform categorical variables
                          ('re-sample', samplers[0]), # re-samples data to ballanced state
                          ('scale', scalers[0]), # scales the data
                          ('model', xgbr), # models
                          ])

# Create parameter values for gridsearch - carefull, &quot;model__&quot; prepended defined in pipeline
params = { 
    'model__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],
    'model__learning_rate': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],
    &quot;model__gamma&quot;:[0, 0.25, 0.5, 0.75,1],
    'model__n_estimators': [100, 500, 1000],
    &quot;model__subsample&quot;:[0.9],
    &quot;model__colsample_bytree&quot;:[0.5],
    &quot;model__early_stopping_rounds&quot;: [10], 
    &quot;model__random_state&quot;: [random_state], 
    &quot;model__eval_metric&quot; : [&quot;error&quot;], 
    &quot;model__eval_set&quot; : [[(X_train, Y_train), (X_test,Y_test)]]
}

# Use GridSearchCV for all combinations
grid = GridSearchCV(
    estimator = pipeline,
    param_grid = params,
    scoring = 'roc_auc',
    n_jobs = -1,
    cv = 5,
    verbose = 3,
)

# Model fitting
grid.fit(X_train, Y_train)
</code></pre>
<p>I have create in <code>params</code> a key-value pair for <code>eval_metric</code> and <code>eval_set</code>:<br />
My question is now, how to access those values and plot a curve of train and test loss (sorry I cannot post a figure here).
Another question: Are the values hand-over by <code>eval_set</code> also piped by the pipeline or do I have to create a separate pipeline for those?</p>
<p>I am using <code>xgb.__version == 0.90</code>, <code>sklearn.__version__ == 1.0.2</code>, <code>python == 3.7.13 @ (google colab)</code></p>
",13342999.0,-1.0,N/A,2022-04-21 14:53:15,Sklearn Pipelines + GridsearchCV + XGBoost + Learning Curve,<python><scikit-learn><data-science><xgboost><imbalanced-data>,1,0,N/A,CC BY-SA 4.0
71956654,1,71962851.0,2022-04-21 15:00:10,1,36,"<p>i am trying to pass an email to my <code>Pipeline</code> and throw some prob based on the training. For doing that i used a bunch of function to take from the email pass like</p>
<pre><code>from collections import Counter

import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin


class EmailLengthTransformer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return np.array([len(e[0].split(&quot;@&quot;)[0]) for e in X]).reshape(-1, 1)


class DomainLengthTransformer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return np.array([len(e[0].split(&quot;@&quot;)[-1]) for e in X]).reshape(-1, 1)


class NumberOfVoulsTransfomer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        vouls = &quot;aeiouAEIOU&quot;
        name = [e[0].split(&quot;@&quot;)[0] for e in X]
        return np.array(
            [sum(1 for char in name if char in vouls) for name in name]
        ).reshape(-1, 1)


class NumberOfCapitalsTransfomer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return np.array(
            [sum(1 for char in email[0] if char.isupper()) for email in X]
        ).reshape(-1, 1)


class NumberOfDigitsTransfomer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        digits = &quot;0123456789&quot;
        return np.array(
            [sum(1 for char in email[0] if char in digits) for email in X]
        ).reshape(-1, 1)
</code></pre>
<p>So after this i package them inside another class and send it to <code>Pipeline</code> like this</p>
<pre><code>class EmailsSuspicionModel:
    def __init__(self, X_train, X_valid, y_train, y_valid, model_params):
        self.X_train = X_train
        self.X_valid = X_valid
        self.y_train = y_train
        self.y_valid = y_valid
        self.model_params = model_params
        self.preprocesser = FeatureUnion(
            [
                (&quot;email_length&quot;, EmailLengthTransformer()),
                (&quot;domain_length&quot;, DomainLengthTransformer()),
                (&quot;number_of_vouls&quot;, NumberOfVoulsTransfomer()),
                (&quot;number_of_capitals&quot;, NumberOfCapitalsTransfomer()),
                (&quot;number_of_digits&quot;, NumberOfDigitsTransfomer()),
                (&quot;highest_char_frequency&quot;, HighestCharFrequencyTransfomer()),
                (&quot;number_of_different_chars&quot;, NumberOfDifferentChars()),
                (
                    &quot;number_of_consecutive_or_identical_chars&quot;,
                    NumberOfConsecutiveOrIdenticalCharsTransfomer()
                ),
            ]
        )

    def transform(self):
        logging.info(&quot;Transform validation data - Required for evaluation&quot;)
        valid_preprocesser = self.preprocesser.fit(self.X_train)
        return valid_preprocesser.transform(self.X_valid)

    def pipeline(self):
        logging.info(&quot;Build sklearn pipeline with XGBoost model&quot;)
        xgb_model = XGBClassifier(eval_metric=&quot;logloss&quot;, use_label_encoder=False)
        if self.model_params:
            logging.info(f&quot;XGBoost model params: {self.model_params}&quot;)
            xgb_model = XGBClassifier(**self.model_params)

        return Pipeline([(&quot;preproc&quot;, self.preprocesser), (&quot;classifier&quot;, xgb_model)])

    def fit(self):
        self.pipeline().fit(
            self.X_train, self.y_train, classifier__eval_set=[(self.transform(), self.y_valid)]
        )
</code></pre>
<p>So whenver i start using the classes in action</p>
<pre><code>X_valid_transformed = EmailsSuspicionModel(X_train.values, X_valid.values, y_train, y_valid, model_params=None).transform()
pipeline = EmailsSuspicionModel(X_train, X_valid, y_train, y_valid, model_params=None).pipeline()
pipeline.fit(
        X_train, y_train, classifier__eval_set=[(X_valid_transformed, y_valid)]
    )
</code></pre>
<p>My model is not yielding my expecting results ( i double check it against a notebook which i dont use pipeline ) and i think is because X_train is not being trained with the proper feature set since whenever i do</p>
<pre><code>pipeline['preproc'].transform(['lucasdresl@gmail.com'])
([1, 1, 0, 0, 0]) 
</code></pre>
<p>Clearly that transformation is not being applied correctly since result is <code>([10, 9, 3, 0, 0])</code> from the functions provided and i think the model is being trained with this same error</p>
",8627559.0,-1.0,N/A,2022-04-22 02:32:55,FeatureTransform from sklearn properly used,<scikit-learn><data-science><transform><pipeline>,1,1,N/A,CC BY-SA 4.0
71960613,1,-1.0,2022-04-21 20:38:30,1,56,"<p>Here is my task:</p>
<p>Import the Python csv module.</p>
<p>Create a Python file object in read mode for crime_sampler.csv called csvfile.</p>
<p>Create an empty list called crime_data.</p>
<p>Loop over a csv reader on the file object :</p>
<p>Inside the loop, append the date (first element), type of crime (third element),
location description (fifth element),
and arrest (sixth element) to the crime_data list.</p>
<p>Remove the first element (headers) from the crime_data list.</p>
<p>Print the first 10 records of the crime_data list. This has been done for you!</p>
<pre><code># Import the csv module
import csv

# Create the file object: csvfile
csvfile = open('crime_sampler.csv')

# Create an empty list: crime_data
crime_data = []

# Loop over a csv reader on the file object
for row in csvfile:

    # Append the date, type of crime, location description, and arrest
    crime_data.append((row[0], row[2], row[4], row[5]))

# Remove the first element from crime_data
crime_data.pop(0)

# Print the first 10 records
print(crime_data[:10])
</code></pre>
<p>I get unexpected result, i dont know where i'm making a mistake</p>
<p>expected result - [('05/23/2016 05:35:00 PM', 'ASSAULT', 'STREET', 'false'), ('03/26/2016 08:20:00 PM', 'BURGLARY', 'SMALL RETAIL STORE', 'false'), ('04/25/2016 03:05:00 PM', 'THEFT', 'DEPARTMENT STORE', 'true'), ('04/26/2016 05:30:00 PM', 'BATTERY', 'SIDEWALK', 'false'), ('06/19/2016 01:15:00 AM', 'BATTERY', 'SIDEWALK', 'false'), ('05/28/2016 08:00:00 PM', 'BATTERY', 'GAS STATION', 'false'), ('07/03/2016 03:43:00 PM', 'THEFT', 'OTHER', 'false'), ('06/11/2016 06:55:00 PM', 'PUBLIC PEACE VIOLATION', 'STREET', 'true'), ('10/04/2016 10:20:00 AM', 'BATTERY', 'STREET', 'true'), ('02/14/2017 09:00:00 PM', 'CRIMINAL DAMAGE', 'PARK PROPERTY', 'false')]</p>
<p>my result - [('0', '/', '3', '/'), ('0', '/', '6', '/'), ('0', '/', '5', '/'), ('0', '/', '6', '/'), ('0', '/', '9', '/'), ('0', '/', '8', '/'), ('0', '/', '3', '/'), ('0', '/', '1', '/'), ('1', '/', '4', '/'), ('0', '/', '4', '/')]</p>
<p>DATASET</p>
<p>Date,Block,Primary Type,Description,Location Description,Arrest,Domestic,District
05/23/2016 05:35:00 PM,024XX W DIVISION ST,ASSAULT,SIMPLE,STREET,false,true,14
03/26/2016 08:20:00 PM,019XX W HOWARD ST,BURGLARY,FORCIBLE ENTRY,SMALL RETAIL STORE,false,false,24
04/25/2016 03:05:00 PM,001XX W 79TH ST,THEFT,RETAIL THEFT,DEPARTMENT STORE,true,false,6
04/26/2016 05:30:00 PM,010XX N PINE AVE,BATTERY,SIMPLE,SIDEWALK,false,false,15
06/19/2016 01:15:00 AM,027XX W AUGUSTA BLVD,BATTERY,AGGRAVATED: HANDGUN,SIDEWALK,false,false,12
05/28/2016 08:00:00 PM,070XX S ASHLAND AVE,BATTERY,DOMESTIC BATTERY SIMPLE,GAS STATION,false,true,7
07/03/2016 03:43:00 PM,0000X N STATE ST,THEFT,RETAIL THEFT,OTHER,false,false,1
06/11/2016 06:55:00 PM,044XX W MAYPOLE AVE,PUBLIC PEACE VIOLATION,RECKLESS CONDUCT,STREET,true,false,11
10/04/2016 10:20:00 AM,016XX W 63RD ST,BATTERY,SIMPLE,STREET,true,false,7
02/14/2017 09:00:00 PM,018XX S WOOD ST,CRIMINAL DAMAGE,TO CITY OF CHICAGO PROPERTY,PARK PROPERTY,false,false,12</p>
",18739200.0,18739200.0,2022-04-21 20:57:20,2022-04-21 20:59:06,Unexpected result in csv file looping,<python><csv><data-science>,1,6,N/A,CC BY-SA 4.0
71924608,1,-1.0,2022-04-19 12:00:50,-1,538,"<p>Sometimes performing feature reduction reduces number of features with methods like PCA and then we could scale only the relevant variables. Is there a rule that we need to do normalization/scaling first and then the feature reduction?</p>
",5810845.0,-1.0,N/A,2022-04-21 14:18:52,Should we always first perform feature normalization and then the feature reduction?,<machine-learning><data-science><feature-engineering><machine-learning-model><feature-scaling>,1,1,N/A,CC BY-SA 4.0
71925163,1,71927351.0,2022-04-19 12:42:11,0,202,"<p>I'm using this code to cluster my documents.
The graph output looks like this:
<a href=""https://i.stack.imgur.com/dqayh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dqayh.png"" alt=""enter image description here"" /></a>
I'm trying to accomplish a way of printing out the most common words on which the clusters were based on. Is it possible with gensim d2v?</p>
<pre><code>kmeans_model = KMeans(n_clusters=3, init='k-means++', max_iter=100) 
X = kmeans_model.fit(d2v_model.dv.vectors)
labels=kmeans_model.labels_.tolist()
l = kmeans_model.fit_predict(d2v_model.dv.vectors)
pca = PCA(n_components=2).fit(d2v_model.dv.vectors)
datapoint = pca.transform(d2v_model.dv.vectors)

data['cluster'] = kmeans_model.labels_
clusters = data.groupby('cluster')    


for i, cluster in enumerate(clusters.groups):
    with open('cluster'+str(cluster)+ '.csv', 'w', encoding=&quot;utf-8&quot;, newline='') as f:
        data = data.replace(r'\r+|\n+|\t+',' ', regex=True)
        data = clusters.get_group(cluster)[['NR_SOLICITACAO','DS_ANALISE','PRE_PROCESSED']] # get title and overview columns
        f.write(data.to_csv(index_label='id')) # set index to id

import matplotlib.pyplot as plt
label1 = [&quot;#0000FF&quot;, &quot;#006400&quot;, &quot;#FFFF00&quot;, &quot;#CD5C5C&quot;, &quot;#FF0000&quot;, &quot;#FF1493&quot;]
color = [label1[i] for i in labels]
plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)
centroids = kmeans_model.cluster_centers_
centroidpoint = pca.transform(centroids)
plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')
plt.show()
</code></pre>
",17322005.0,-1.0,N/A,2022-04-19 15:18:21,How to see on what words the clusters were based on,<data-science><cluster-analysis><k-means><word2vec>,1,0,N/A,CC BY-SA 4.0
71962201,1,-1.0,2022-04-22 00:30:20,0,51,"<p>I'm trying to oversample an imbalanced dataset with a continuous target variable using <code>SMOGNRegress</code> from the <code>UBL</code> package in <code>R</code>.</p>
<p>When I run the code:</p>
<pre><code>SMOGNRegress(Deceased~., normalized_data, rel = &quot;auto&quot;, thr.rel = 0.9999, C.perc = &quot;balance&quot;, k = 2, repl = F, dist = &quot;Euclidean&quot;, pert = 0.01)
</code></pre>
<p>I get the following error:</p>
<blockquote>
<p>'names' attribute [35563] must be the same length as the vector [1]</p>
</blockquote>
<p>From similar questions regarding this error, I saw that it is an issue of lengths not matching, but I cannot wrap my head around how to fix that in order to generate the new oversampled data set.</p>
",18862379.0,14829703.0,2022-04-22 08:21:29,2023-03-07 16:14:16,Error 'names' attribute [35563] must be the same length as the vector [1] after running SMOGNRegress in R,<r><data-science><oversampling>,1,0,N/A,CC BY-SA 4.0
71974438,1,-1.0,2022-04-22 20:43:56,2,1951,"<blockquote>
<p>Getting this error: AttributeError: 'GPT2Tokenizer' object has no
attribute 'train_new_from_iterator'</p>
</blockquote>
<p>Very similar to hugging face documentation. I changed the input and that's it (shouldn't affect it). It worked once. Came back to it 2 hrs later and it doesn't... nothing was changed NOTHING. Documentation states train_new_from_iterator only works with 'fast' tokenizers and that AutoTokenizer is supposed to pick a 'fast' tokenizer by default. My best guess is, it is having some trouble with this. I also tried downgrading transformers and reinstalling to no success. df is just one column of text.</p>
<pre><code>from transformers import AutoTokenizer
import tokenizers

def batch_iterator(batch_size=10, size=5000):
    for i in range(100): #2264
        query = f&quot;select note_text from cmx_uat.note where id &gt; {i * size} limit 50;&quot;
        df = pd.read_sql(sql=query, con=cmx_uat)

        for x in range(0, size, batch_size):
            yield list(df['note_text'].loc[0:5000])[x:x + batch_size]

old_tokenizer = AutoTokenizer.from_pretrained('roberta')
training_corpus = batch_iterator()
new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 32000)
</code></pre>
",13967671.0,-1.0,N/A,2023-05-08 06:57:43,Training New AutoTokenizer Hugging Face,<python><nlp><data-science><huggingface-transformers><transformer-model>,1,2,N/A,CC BY-SA 4.0
71917792,1,-1.0,2022-04-18 22:44:04,-1,1520,"<p>I have a simple line plot on Seaborn -</p>
<blockquote>
<p>ax = sns.lineplot(data=df2, x=&quot;screen&quot;, y=&quot;bmi&quot;)</p>
</blockquote>
<p>And I want to know how to label the coordinates of certain points like <a href=""https://i.stack.imgur.com/Imlzg.jpg"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/Imlzg.jpg</a>. I have tried a few methods but they have not worked. Thanks a lot for helping.</p>
<p>Similar questions asked wanted a way to label a third variable on their graph, but I just want to label the y-values at certain points. No other questions solved this problem.</p>
",18776968.0,18776968.0,2022-04-19 20:58:35,2022-04-19 20:58:35,How do I Label the Coordinates on a Seaborn Line Plot?,<python><seaborn><data-science><graph-data-science>,1,0,2022-04-19 01:46:23,CC BY-SA 4.0
71924056,1,-1.0,2022-04-19 11:16:41,1,663,"<p>I want to do image segmention with k-means clustering, and i want to:</p>
<ol>
<li>Calculate distances between all points and the initial centroids.</li>
<li>Assign all points to their closest centroid.</li>
</ol>
<p>Here is my code:</p>
<pre><code>def init(ds, k, random_state=42):
np.random.seed(random_state)
centroids = [ds[0]]

for _ in range(1, k):
    dist_sq = np.array([min([np.inner(c-x,c-x) for c in centroids]) for x in ds])
    probs = dist_sq/dist_sq.sum()
    cumulative_probs = probs.cumsum()
    r = np.random.rand()
    
    for j, p in enumerate(cumulative_probs):
        if r &lt; p:
            i = j
            break
    
    centroids.append(ds[i])

return np.array(centroids)

k = 4
centroids = init(pixels, k, random_state=42)
print(centroids)

# First centroid
centroids[0]

#Calculate distances between all points and the initial centroids.


# Assign all points to their closest centroid.
</code></pre>
",15659041.0,671366.0,2022-04-19 11:45:06,2022-04-19 11:45:06,k-mean clustering calculate distance between all points and the initial centroids,<python><data-science><cluster-analysis><k-means>,0,0,N/A,CC BY-SA 4.0
71936723,1,-1.0,2022-04-20 08:49:57,1,124,"<p>I'm doing some statistical data analysis and I'm using a moving average to &quot;cancel out&quot; some noise! I was wondering if there is any process or a &quot;right way&quot; to determine the size of the window.</p>
<p>For example, in a specific data set, I have 22 883 data points over 90 seconds. What could be a good window size?</p>
<p>Thank you all in advance!</p>
",18864156.0,18864156.0,2022-04-20 10:04:52,2022-04-20 10:04:52,Choosing an appropriate window for a sliding average,<python><statistics><data-science><data-analysis><sliding-window>,0,4,N/A,CC BY-SA 4.0
71959971,1,71974924.0,2022-04-21 19:29:54,0,256,"<p>I have been running seasonal_decompose() from the statsmodels on about 20 totally different datasets. Is it standard that the seasonality is 7 when looking at a dataset with day frequency?</p>
<p>Here is a picture as an example of one dataset decomp. I zoomed in on the seasonality so that you can see that it is again 7 days:</p>
<p><a href=""https://i.stack.imgur.com/GEZDw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GEZDw.png"" alt=""enter image description here"" /></a></p>
<p>Why is it always 7 days though? I wouldn't expect it to be always 7 days and the datasets are all different from each other, so by now I think that either this is total coincidence or this is because of seasonal_decompose().</p>
<p>But looking at how seasonal_decompose() in the statsmodels documentation , it uses LOESS to figure out the seasonality. If I look at the formula, it should be able to find different frequencies of the seasonality. I just need to verify that I am not wrong here: Is it pure coincidence that all of my datasets produce a 7 day frequency of the seasonality?</p>
",10438528.0,-1.0,N/A,2022-04-22 21:39:30,Seasonality is always 7 when running seasonal_decompose(). Why is that?,<python><statistics><data-science><statsmodels>,1,1,N/A,CC BY-SA 4.0
71976360,1,-1.0,2022-04-23 02:31:51,1,41,"<p>The program is supposed to take the rows in column [newAssetCode] and compare them to the rows in column ['allAssetCodes'] to then find the assets in [newAssetCode] that already exist in ['allAssetCodes']. The program will then count how many times the assets exist in ['allAssetCodes']. for example if If ['allAssetCodes'] contains 'XYZ','XYZ1', 'XYZ2' it will count that and then add a digit to that count and append that number in a new list creating 'XYZ3'. my question is did I implement this correctly? and Also I do have an error. <a href=""https://i.stack.imgur.com/HI0vO.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>'''</p>
<pre><code>from itertools import count
import pandas as pd
import openpyxl
from csv import reader

data = pd.read_excel('Codes.xlsx')
new_assets =  data['newAssetCode'].tolist()
allAssetCodes = data['allAssetCodes'].tolist()

print(len(new_assets))
print(len(allAssetCodes))

assets_real = list()
for asset in new_assets:
    if not isinstance(asset,str):
        continue
    if asset not in allAssetCodes:
        assets_real.append(asset)
        continue
    existing = [exasset for exasset in allAssetCodes if exasset.startswith(asset)]
    assets_real.append(f'{asset}{len(existing)+1}')

print(assets_real)
print(existing)
</code></pre>
<p>'''</p>
",11909398.0,11909398.0,2022-04-23 06:05:51,2022-04-23 06:05:51,Python list comparing implementation and error,<python><excel><list><data-science>,0,3,N/A,CC BY-SA 4.0
71958210,1,-1.0,2022-04-21 16:58:39,0,33,"<p>I want to save my data in the CSV format, I have some sentences and I want to save every sentence in a different row, but the output is like this:
<a href=""https://i.stack.imgur.com/Vjo3s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vjo3s.png"" alt=""enter image description here"" /></a></p>
<p>This is my code:</p>
<pre><code>with jsonlines.open('/content/data.jsonl') as reader:
    with open('/content/sample_data/sents.csv', 'w') as csv_file:
      writer = csv.writer(csv_file, delimiter=',')
      for obj in reader:
          if obj['label']=='ENOUGH':
              writer.writerow(obj['sent'])
</code></pre>
<p>and this is a piece of my data:</p>
<pre><code>1 Cameroon lists three definitely endangered languages, 13 severely endangered, and 16 critically endangered from among its at least 250 languages.
2 Ryan Mathews of the 2003 Kansas State Wildcats football team posted a higher rushing average than his teammate Ell Roberson.
3 Alan Lowry only played games with his close relatives in his entire life.
</code></pre>
<p>I want to save them in differenet rows and a single column. what should I do?</p>
",11098263.0,-1.0,N/A,2022-04-23 06:33:37,Writing charcters in the csv files instead of writing the sentence,<python><csv><data-science><jsonlines>,1,10,N/A,CC BY-SA 4.0
71958283,1,-1.0,2022-04-21 17:04:38,0,24,"<p>I have a productivity app that I'm currently working on which requires a user analytics panel that logs how much time they've studied, including how much time spent on app navigation, how much time spent while timer is running, how much time spent while timer is paused etc...</p>
<p>I've built the app in react native and set up servers written in python for backend processing. My current model is that every user interaction is logged into an array, for example,</p>
<p><code>[{&quot;data&quot;: {&quot;screen&quot;: &quot;Settings&quot;, &quot;type&quot;: &quot;6&quot;}, &quot;date&quot;: 1650558805913}, {&quot;data&quot;: {&quot;screen&quot;: &quot;Home&quot;, &quot;type&quot;: &quot;6&quot;}, &quot;date&quot;: 1650558808312}, {&quot;data&quot;: {&quot;screen&quot;: &quot;Spaced Repetition&quot;, &quot;type&quot;: &quot;6&quot;}, &quot;date&quot;: 1650558809615}]</code></p>
<p>where type corresponds to the type of event and date is the timestamp. Right now, I plan on  sending this data to the backend. The backend will loop the array and everytime it reaches a specific defined element, such as navigating to a specific screen, it will create an object that corresponds to that screen. All the elements that come after the creation of the object will be processed and logged accordingly into that object until reaching another predefined element in the array.</p>
<p>This, I've found out is a very rudimentary way of doing it and is nowhere near as scalable as I want it to be.</p>
",9602675.0,8239061.0,2022-04-21 17:09:40,2022-04-21 17:09:40,How do I process user interactions and make meaningful analytics for my app?,<javascript><python><react-native><data-science><analytics>,0,2,N/A,CC BY-SA 4.0
71975236,1,71976482.0,2022-04-22 22:23:17,0,96,"<p>Hi is there a way to filter out unique values ina a pandas data frame.
I am using the code below to filter out the unique values. However, I am getting different ordered combinations. For example, ['Creative, Modern Cuisine', 'Modern Cuisine, Creative'] is there a way to filter this out.</p>
<p>[<img src=""https://i.stack.imgur.com/bWad7.png"" alt=""Part of the data"" />]</p>
<pre><code>cuisine = df.Cuisine.unique()
cuisine_count = df.Cuisine.nunique()
print(cuisine, cuisine_count)
</code></pre>
",18913248.0,-1.0,2022-04-22 23:06:43,2022-04-23 02:59:12,Trying to filter out unique values in a pandas data frame,<python><pandas><dataframe><data-science>,1,5,N/A,CC BY-SA 4.0
72005875,1,72006187.0,2022-04-25 21:21:21,0,691,"<p>I'm good in Python but new to Pandas and know almost nothing about stats so forgive me if this is a simple or ignorant question.</p>
<p>Say I have a dataframe with two columns like, e.g., <code>Jobs</code> and <code>Cars</code> where the entries in both are one of a finite set of strings, e.g. <code>[Software Engineer, Sysadmin, Product Manager]</code> and <code>[Tesla, Hummer, Ford Focus]</code>.</p>
<p>I want to produce a table of some sort showing the correlation between jobs and cars, like this:</p>
<pre><code>|     | Tesla | Hummer | Ford Focus |
| SWE | ###   | ###    | ###        |
| SA  | ###   | ###    | ###        |
| PM  | ###   | ###    | ###        |
</code></pre>
<p>What's the most pythonic way of doing this? Honestly this is a one off query and my data set is pretty small so it doesn't need to be the most efficient.</p>
<p>Edit: A sample dataframe could be generated like this</p>
<pre><code>from random import choice

jobs = ['SWE', 'Data Scientist', 'Product Manager', 'Sysadmin', 'Data Engineer']
cars = ['Tesla', 'Hummer', 'Ford Focus', 'Chevy Volt', 'Toyota Tercel']

df = pd.DataFrame({
    'jobs': [choice(jobs) for _ in range(1000) ], 
    'cars': [choice(cars) for _ in range(1000) ]
})
</code></pre>
<p>The expected output would be similar to that of <code>DataFrame.corr()</code> but that function only operates on numbers and these are strings.</p>
",7949500.0,7949500.0,2022-04-25 21:45:06,2022-04-25 22:05:12,Calculating correlation of two text columns in Pandas,<python><pandas><numpy><matplotlib><data-science>,1,7,N/A,CC BY-SA 4.0
71993019,1,71993159.0,2022-04-24 23:00:48,0,84,"<p>every time i run it it gives me only the last value of social_score, while I want to get all the social_score values in the json API and store the output to an #iterated list to calculate with the valueI</p>
<blockquote>
<p>help on this JSON</p>
<pre><code>url = requests.get(&quot;https://api2.lunarcrush.com/v2?data=assets&amp;symbol=xrp&amp;data_points=730&amp;interval=day&amp;change=max&quot;).json()

def get_data():
    for data in url['data']:
        result = data['social_score']
        print(result)
        
get_data()
</code></pre>
</blockquote>
",18932935.0,-1.0,N/A,2022-04-24 23:30:00,python JSON for loop,<python><json><data-science>,1,0,N/A,CC BY-SA 4.0
71997293,1,72003003.0,2022-04-25 09:27:23,1,3227,"<p>Currently using gensim 4.0 library to write the code. However, I don't know why it keeps failing in finding a similar word. At first, when I set up min_count = 5, the error is, that it wants me to build a vocab first, but after I reduce it to min_count = 1, it says, key error not present...Full code with datasets over here: <a href=""https://github.com/JYjunyang/FYPDEMO"" rel=""nofollow noreferrer"">https://github.com/JYjunyang/FYPDEMO</a> Am I writing something wrong or missing some important steps? Everything works fine but just this word2vec implementation...Will appreciate for every guidance provided...
<strong>Take note: LemmaColumn is a dataframe after lemmatization</strong></p>
<pre><code>def FeaturesExtraction():
    word2vec = 
Word2Vec(sentences=LemmaColumn,vector_size=100,window=5,min_count=1,workers=8,sg=1)
    b1 = time.time()
    train_time = time.time() - b1
    print(word2vec.wv.most_similar('virus', topn=10))
</code></pre>
<p>And I not sure why, after training with 10k data, unique words in vocabulary only have 7:<br />
word #0/7 is t<br />
word #1/7 is l<br />
word #2/7 is x<br />
word #3/7 is e<br />
word #4/7 is _<br />
word #5/7 is u<br />
word #6/7 is f</p>
",14811578.0,14811578.0,2022-04-25 13:06:08,2022-04-25 16:47:31,"Word2vec raise KeyError(f""Key '{key}' not present"")",<python><machine-learning><data-science><word2vec>,1,0,N/A,CC BY-SA 4.0
72004956,1,-1.0,2022-04-25 19:43:19,2,367,"<p>I am constructing a map with meaningful data using Folium, on Python. But, I need to extract the information(for example an image which is bounded by max-min lat-long values). I tried several different ways. However, I don't get the data I desired.</p>
<p>A sample map, constructed using Folium, in an html file.
<a href=""https://i.stack.imgur.com/Yet7Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yet7Z.png"" alt=""enter image description here"" /></a>
I need to use this as an RGB image rather than an interactive map. As much as I see, there is no such functionality. At least, I could not find. Is there a way?
Assuming that there is no such way, and I decided to crop this image using selenium browser method. So, I firstly had to indicate the boundaries in order to capture the image with corresponding latitude, longitude values. I applied fit_bounds(), but it is not bounded by given max/min lat-long values. There is padding-like expansion outside of the boundaries. Therefore, this way also failed. Could you please let me know if there is a solution for this purpose? Simply, briefly, I need to have the data that includes the RGB image, lat-long values(at least the boundaries) and these are retrieved directly from a folium map if possible.
Thank you in advance for any support.</p>
",5540297.0,-1.0,N/A,2022-04-25 19:43:19,Is there a way to extract visual data from a folium map?,<python><data-science><folium>,0,0,N/A,CC BY-SA 4.0
72013546,1,-1.0,2022-04-26 11:59:00,0,124,"<p>A recursive mean computation using D&amp;C is easy when the length of a list is known beforehand.</p>
<pre><code>def mv_recursive (X):
    if len(X)==1:
        mu = X[0]
        var = 0
    else:
        mu = (int(len(X)/2)/len(X))*mv_recursive(X[0:int(len(X)/2)])[0] + ((len(X)-int(len(X)/2))/len(X))*mv_recursive(X[int(len(X)/2):len(X)])[0]
        var = mv_recursive(X[0:int(len(X)/2)])[1] + mv_recursive(X[int(len(X)/2):len(X)])[1] + ((int(len(X)/2)*(len(X)-int(len(X)/2)))/len(X))*(mv_recursive(X[0:int(len(X)/2)])[0]-mv_recursive(X[int(len(X)/2):len(X)])[0])*(mv_recursive(X[0:int(len(X)/2)])[0]-mv_recursive(X[int(len(X)/2):len(X)])[0])
    return mu, var
</code></pre>
<p>The above comes from the following realization:</p>
<p><a href=""https://i.stack.imgur.com/dsAPc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dsAPc.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/g0928.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g0928.png"" alt=""enter image description here"" /></a></p>
<p>How could one tackle the same problem for a stream of integers of unknown length (online mean calculation), using recursion and D&amp;C?</p>
<p>Thanks for any tips!</p>
",3880357.0,3880357.0,2022-04-26 12:53:23,2022-04-26 12:53:23,Can a mean & variance computation algorithm in Python be implemented recursively using the divide and conquer approach?,<python><data-science><divide-and-conquer>,0,9,N/A,CC BY-SA 4.0
72013747,1,-1.0,2022-04-26 12:12:50,0,23,"<p>I am trying to build a scatter plot with the average time between now and the last DateTime stamp on X-axis and Location on the Y-axis. However, I am struggling to write the correct query for this. I have tried everything but datediff() returns only days, timediff() returns milliseconds (if works) plus I cannot use avg() on the results. My recent attempt is to create timeframes instead of average but can't get my query working. I am a newbie.</p>
<pre><code>SELECT order_reference AS count_of_orders
    ,loc
    ,time_diff
FROM (
    SELECT order_reference
        ,loc
        ,CASE 
            WHEN mindif &gt; 0
                AND mindif &lt; 5
                THEN 'Less than 5min'
            WHEN mindif &gt; 5
                AND mindif &lt; 15
                THEN '5min-15min'
            WHEN mindif &gt; 15
                AND mindif &lt; 30
                THEN '15min-30min'
            WHEN mindif &gt; 30
                AND mindif &lt; 45
                THEN '30min-45min'
            WHEN mindif &gt; 45
                AND mindif &lt; 60
                THEN '45min-1h'
            WHEN mindif &gt; 60
                AND mindif &lt; 75
                THEN '1h-1h15min'
            WHEN mindif &gt; 75
                AND mindif &lt; 90
                THEN '1h15min - 1h30min'
            WHEN mindif &gt; 90
                AND mindif &lt; 105
                THEN '1h30min-1h45min'
            WHEN mindif &gt; 105
                AND mindif &lt; 120
                THEN '1h45min - 2h'
            WHEN mindif &gt; 120
                THEN 'Over 2h'
            END TIME
    FROM (
        SELECT minute(now() - mth.date_time_stamp) AS mindif
            ,soh.order_reference AS order_reference
            ,mth.location AS loc
        FROM sales_order_header soh
        LEFT JOIN sales_order_summary sos
            ON sos.sales_order_header_id = soh.id
        LEFT JOIN sales_order_status sost
            ON sost.id = sos.current_status_id
        LEFT JOIN manufacturing_tracking_history mth
            ON mth.id = sos.current_manufacturing_tracking_history_id
        WHERE sost.STATUS NOT IN (
                &quot;Completed&quot;
                ,&quot;Cancelled&quot;
                )
        ) table1
    ) tab2
WHERE table1.loc IS NOT NULL;
</code></pre>
",18659357.0,-1.0,N/A,2022-04-26 12:12:50,I am trying to build a scatter plot with the average time between now and the last DateTime stamp on X-axis and Location on the Y-axis,<sql><data-science>,0,4,N/A,CC BY-SA 4.0
72021549,1,-1.0,2022-04-26 23:14:39,0,194,"<p>I'm new in machine learning and I'm trying to train a model.
I'm using this Keras oficial example as a guide to set my dataset and feed it into the model: <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence</a></p>
<p>From the training data I have an sliding windows created for a single column and for the labels I have a binary classification (1 or 0).</p>
<p>This is the model creation code:</p>
<pre><code>n = 200
hidden_units = n
dense_model = Sequential()
dense_model.add(Dense(hidden_units, input_shape=([200,1])))
dense_model.add(Activation('relu'))
dense_model.add(Dropout(dropout))
print(hidden_units)

while hidden_units &gt; 2:
    hidden_units = math.ceil(hidden_units/2)
    dense_model.add(Dense(hidden_units))
    dense_model.add(Activation('relu'))
    dense_model.add(Dropout(dropout))
    print(hidden_units)
dense_model.add(Dense(units = 1, activation='sigmoid'))
</code></pre>
<p>This is the functions I'm using to compile the model:</p>
<pre><code>def compile_and_fit(model, window, epochs, patience=2):
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                      patience=patience,
                                                      mode='min')
    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    model.fit(window.train , epochs=epochs)
    return history
</code></pre>
<p>This is the model training:</p>
<pre><code>break_batchs = find_gaps(df_train, 'date_diff', diff_int_value)
for keys, values in break_batchs.items():
    dense_window = WindowGenerator(data=df_train['price_var'],
                                   data_validation=df_validation['price_var'],
                                   data_test=df_test['price_var'],
                                   input_width=n,
                                   shift=m,
                                   start_index=values[0],
                                   end_index=values[1], 
                                   class_labels=y_buy,
                                   class_labels_train=y_buy_train,
                                   class_labels_test=y_buy_test,
                                   label_width=1,
                                   label_columns=None,
                                   classification=True,
                                   batch_size=batch_size,
                                   seed=None)
    history = compile_and_fit(dense_model, dense_window)

</code></pre>
<p>and those are the shapes of the batches:</p>
<pre><code>(TensorSpec(shape=(None, 200, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 1), dtype=tf.float64, name=None))
</code></pre>
<p>The problem is (I guess) that, from the model summary the model is training from the last dimension when it should be working in the second one:</p>
<pre><code>dense_model.summary()

Model: &quot;sequential_21&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
                                          |Model is being applied here
                                          |
                                          v
dense_232 (Dense)            (None, 200, 200)          400       
_________________________________________________________________
                                     |When it should be applied here
                                     |
                                     v
activation_225 (Activation)  (None, 200, 200)          0         
_________________________________________________________________
dropout_211 (Dropout)        (None, 200, 200)          0         
_________________________________________________________________
dense_233 (Dense)            (None, 200, 100)          20100     
_________________________________________________________________
activation_226 (Activation)  (None, 200, 100)          0         
_________________________________________________________________
dropout_212 (Dropout)        (None, 200, 100)          0         
_________________________________________________________________
dense_234 (Dense)            (None, 200, 50)           5050      
_________________________________________________________________
activation_227 (Activation)  (None, 200, 50)           0         
_________________________________________________________________
dropout_213 (Dropout)        (None, 200, 50)           0         
_________________________________________________________________
dense_235 (Dense)            (None, 200, 25)           1275      
_________________________________________________________________
activation_228 (Activation)  (None, 200, 25)           0         
_________________________________________________________________
dropout_214 (Dropout)        (None, 200, 25)           0         
_________________________________________________________________
dense_236 (Dense)            (None, 200, 13)           338       
_________________________________________________________________
activation_229 (Activation)  (None, 200, 13)           0         
_________________________________________________________________
dropout_215 (Dropout)        (None, 200, 13)           0         
_________________________________________________________________
dense_237 (Dense)            (None, 200, 7)            98        
_________________________________________________________________
activation_230 (Activation)  (None, 200, 7)            0         
_________________________________________________________________
dropout_216 (Dropout)        (None, 200, 7)            0         
_________________________________________________________________
dense_238 (Dense)            (None, 200, 4)            32        
_________________________________________________________________
activation_231 (Activation)  (None, 200, 4)            0         
_________________________________________________________________
dropout_217 (Dropout)        (None, 200, 4)            0         
_________________________________________________________________
dense_239 (Dense)            (None, 200, 2)            10        
_________________________________________________________________
activation_232 (Activation)  (None, 200, 2)            0         
_________________________________________________________________
dropout_218 (Dropout)        (None, 200, 2)            0         
_________________________________________________________________
dense_240 (Dense)            (None, 200, 1)            3         
=================================================================
Total params: 27,306
Trainable params: 27,306
Non-trainable params: 0
_________________________________________________________________


</code></pre>
<p>And because of that Im getting <code>ValueError: logits and labels must have the same shape ((None, 200, 1) vs (None, 1, 1))</code></p>
<p>How can I tell Keras to apply the training in the second dimension and not the last one?</p>
<h1>EDIT</h1>
<p><a href=""https://i.stack.imgur.com/66wlN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/66wlN.png"" alt=""Diagram explanation"" /></a></p>
<p>This is what I understand is happening, is this right? How I fixed it?</p>
<h1>Edit 2</h1>
<p>I tried to modify as suggested, using:</p>
<p><code>dense_model.add(Dense(hidden_units, input_shape=(None,200,1)))</code></p>
<p>but I'm getting the following warning:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (None, None, 200, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 200, 1), dtype=tf.float32, name='dense_315_input'), name='dense_315_input', description=&quot;created by layer 'dense_315_input'&quot;), but it was called on an input with incompatible shape (None, 200, 1, 1).
</code></pre>
",6442390.0,6442390.0,2022-05-04 02:19:18,2022-05-04 02:19:18,"Keras Dense Model ValueError: logits and labels must have the same shape ((None, 200, 1) vs (None, 1, 1))",<machine-learning><deep-learning><data-science><tf.keras><densenet>,1,2,N/A,CC BY-SA 4.0
72021709,1,-1.0,2022-04-26 23:41:59,0,78,"<p>I am trying to implement a method in Python which calculates the mean and variance for a stream of integers of an unknown length:</p>
<pre><code>def mv_online (X):
    buffer = [0]
    i = 0
    for x in X:
        if i%2==0: # level 0 always empty
            buffer[0] = (x,0)
            if buffer.count(0)==0: # add new level when buffer is full
                buffer.append(0)
        else: # combination step
            j = buffer.index(0) # find level where combination will occur
            temp = (x,0)
            for k in range (0,j):
                mu1 = buffer[k][0]
                mu2 = temp[0]
                mu = (mu1+mu2)/2
                var1 = buffer[k][1]
                var2 = temp[1]
                var = var1+var2+0.5*(mu1-mu2)*(mu1-mu2)
                temp = (mu,var)
                buffer[k]=0 # clean buffer at respective levels
            buffer[j]=temp
        i+=1
    return buffer[-1] if buffer[-1]!=0 else buffer[-2]
</code></pre>
<p>While the mean is correct, the variance is much higher as compared to the naive variance calculation, i.e. the sum over <code>(x_i - mean)^2 * 1/(n-1)</code>.</p>
<p>What am I missing here?</p>
",3880357.0,355230.0,2022-04-27 01:58:16,2022-04-27 02:30:43,Why is my variance calculation method so far off?,<python><data-science>,2,0,N/A,CC BY-SA 4.0
72025373,1,72025465.0,2022-04-27 08:10:09,1,659,"<p>I cannot find solutions to that anywhere. What I am trying to achieve is: <strong>dataframe</strong> grouped by <strong>'Machine'</strong> column, next would be average column <strong>'datetime'</strong> and last one as a count of <strong>'Barcode'</strong>.</p>
<p><strong>Not working attempt:</strong></p>
<pre><code> result = df.groupby('Machine')['datetime'].aggregate('mean')['Barcode'].aggregate('count')
</code></pre>
<p>[1]: <a href=""https://i.stack.imgur.com/h8HiV.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/h8HiV.png</a>  <strong>&lt;-- dateFrame before</strong> operation</p>
",18659357.0,-1.0,N/A,2022-04-27 08:18:07,"Is there a way in pandas to aggregate one column as mean, and another as count?",<python><pandas><dataframe><data-science><aggregate-functions>,1,0,2022-04-27 08:25:09,CC BY-SA 4.0
72029440,1,72112559.0,2022-04-27 13:14:43,0,164,"<p>Following is my sample data:</p>
<blockquote>
<p>data = {850.0: 6, -852.0: 5, 992.0: 29, -993.0: 25, 990.0: 27, -992.0: 28,
965.0: 127, 988.0: 37, -994.0: 24, 996.0: 14, -996.0: 19, -998.0: 19, 995.0: 17, 954.0: 71, -953.0: 64, 983.0: 48, 805.0: 20, 960.0: 97, 811.0: 23, 957.0: 98, 818.0: 9, -805.0: 10, -962.0: 128, 822.0: 5, 970.0: 115, 823.0: 6, 977.0: 86, 815.0: 11, 972.0: 118, -809.0: 3, -982.0: 77, 963.0: 129, 816.0: 15, 969.0: 131, 809.0: 13, -973.0: 115, 967.0: 141, 964.0: 110, 966.0: 141, -801.0: 11, -990.0: 33, 819.0: 8, 973.0: 113, -981.0: 71, 820.0: 16, 821.0: 10, -988.0: 42, 833.0: 7, 958.0: 92, -980.0: 98, 968.0: 138, -808.0: 5, -984.0: 57, 976.0: 108, 828.0: 3, -807.0: 6, 971.0: 134, -814.0: 3, 817.0: 13, -975.0: 112, 814.0: 12, 825.0: 6, 974.0: 90, -974.0: 125, -824.0: 2, -966.0: 131, -822.0: 4, 962.0: 108, -967.0: 121, -810.0: 3, 810.0: 11, 826.0: 7, 953.0: 74, -970.0: 140, -804.0: 6, -813.0: 2, 812.0: 18, 961.0: 126, -965.0: 159, -806.0: 5, 955.0: 74, -958.0: 93, -818.0: 6, 813.0: 18, 824.0: 6, 937.0: 25, -946.0: 51, -802.0: 8, 950.0: 48, -957.0: 91, 808.0: 11, 959.0: 116, -821.0: 3, -959.0: 108, 827.0: 4, -817.0: 4, 944.0: 47, -971.0: 126, -972.0: 104, -977.0: 96, 956.0: 92, 807.0: 10, 806.0: 21, 952.0: 60, 948.0: 51, 951.0: 67, 945.0: 47, -986.0: 37, 892.0: 13, 910.0: 23, 876.0: 6, -912.0: 18, 891.0: 8, 911.0: 22, -913.0: 13, 894.0: 7, 895.0: 12, 925.0: 15, 887.0: 6, 915.0: 16, 877.0: 7, 905.0: 14, 889.0: 7, -899.0: 10, 916.0: 17, -907.0: 11, -919.0: 17, 900.0: 20, 898.0: 9, 918.0: 16, 914.0: 18, 906.0: 18, 908.0: 17, -889.0: 7, 903.0: 16, 888.0: 5, -905.0: 9, -911.0: 19, 904.0: 20, -908.0: 12, 840.0: 2, -906.0: 16, 896.0: 11, -910.0: 17, -863.0: 3, 907.0: 27, -904.0: 10, -898.0: 13, 909.0: 19, -916.0: 20, 924.0: 24, 919.0: 20, -887.0: 6, 920.0: 12, 921.0: 12, 922.0: 15, 899.0: 14, -902.0: 9, -917.0: 12, 902.0: 14, 942.0: 46, 931.0: 23, 901.0: 22, -923.0: 14, -927.0: 15, 913.0: 18, -918.0: 16, 929.0: 22, 928.0: 13, -922.0: 7, -921.0: 16, 933.0: 22, 926.0: 13, 917.0: 18, 923.0: 16, 936.0: 24, 803.0: 30, -930.0: 10, 939.0: 33, -939.0: 24, 893.0: 8, 830.0: 5, 897.0: 8, 886.0: 8, -897.0: 4, -903.0: 12, -920.0: 9, -894.0: 3, -934.0: 14, 932.0: 23, -928.0: 16, 943.0: 40, 946.0: 45,
801.0: 17, -944.0: 35, 935.0: 23, 941.0: 30, -926.0: 11, -940.0: 38, 802.0: 16, 940.0: 43, -943.0: 38, -935.0: 24, 804.0: 23, -933.0: 9, -945.0: 36, 949.0: 56, 858.0: 2, -839.0: 3, -964.0: 108, -969.0: 111, -815.0: 2, 881.0: 3, -955.0: 74, -803.0: 3, 947.0: 50, -948.0: 57, -950.0: 58, -961.0: 133, -947.0: 43, -949.0: 54, -936.0: 20, 980.0: 75, -848.0: 3, -941.0: 27, -827.0: 5, -816.0: 7, -942.0: 37, 938.0:
29, -956.0: 81, -951.0: 59, -932.0: 11, -954.0: 71, -952.0: 64,
-811.0: 3, 979.0: 89, -963.0: 128, -892.0: 4, -960.0: 109, 871.0: 4, 978.0: 85, -968.0: 136, 865.0: 1, -856.0: 3, 930.0: 11, 843.0: 5, -844.0: 1, -929.0: 24, -925.0: 19, -931.0: 11, 981.0: 65, 912.0: 19, 927.0: 10, -924.0: 8, -938.0: 25, 989.0: 31, -819.0: 4, 934.0: 16, -976.0: 92, -915.0: 14, 975.0: 92, 869.0: 5, 998.0: 9, 870.0: 1, -826.0: 2, 834.0: 2, 882.0: 5, 839.0: 4, 829.0: 3, 846.0: 2, -978.0: 117, -991.0: 39, -983.0: 59, -989.0: 48, 832.0: 4, 860.0: 5, -937.0:
25, 859.0: 1, 842.0: 5, -857.0: 4, -891.0: 8, 837.0: 4, -868.0: 3,
-884.0: 4, 851.0: 4, 874.0: 8, 852.0: 6, 997.0: 14, -888.0: 3, 866.0: 6, -893.0: 6, -890.0: 6, 982.0: 45, 863.0: 2, 835.0: 3, -834.0: 3,
-979.0: 73, 853.0: 3, 984.0: 44, -985.0: 30, 985.0: 36, 991.0: 25, 986.0: 35, -987.0: 29, 994.0: 24, 993.0: 29, -995.0: 16, -997.0: 17, -880.0: 4, -830.0: 3, 847.0: 1, 884.0: 4, -877.0: 5, -840.0: 1, -846.0: 2, -896.0: 8, -866.0: 2, -851.0: 2, -871.0: 2, -885.0: 3, -832.0: 3, -878.0: 1, 890.0: 6, 987.0: 22, -847.0: 2, 878.0: 5, 879.0: 3, 885.0: 5, 848.0: 2, 841.0: 5, 856.0: 3, 857.0: 4, 864.0: 1, 831.0:
5, 849.0: 3, 844.0: 3, 875.0: 3, 836.0: 3, 999.0: 6, -999.0: 6,
-900.0: 7, 845.0: 2, 862.0: 1, 880.0: 4, 855.0: 2, -876.0: 1, -882.0: 2, -835.0: 2, -831.0: 5, -812.0: 1, -825.0: 2, -860.0: 3, -914.0: 12,
-855.0: 5, -870.0: 5, -881.0: 4, -823.0: 3, -901.0: 5, -909.0: 15, -886.0: 2, 873.0: 3, -879.0: 1, -869.0: 4, -883.0: 4, -895.0: 8, 868.0: 3, -836.0: 2, 883.0: 4, -861.0: 2, -859.0: 2, -837.0: 1, -864.0: 2, -829.0: 2, -875.0: 4, -858.0: 2, -843.0: 1, -862.0: 1, -872.0: 2, 854.0: 2, -842.0: 1, -845.0: 3, -833.0: 1, -853.0: 3, 861.0: 3, -820.0: 2, -850.0: 2, -867.0: 2, -854.0: 1, -841.0: 3, 867.0: 1, -865.0: 3, -849.0: 2, 838.0: 1, -838.0: 1, -873.0: 1}</p>
</blockquote>
<p>It is the <code>Key/Value</code> of a dictionary in Python. The <code>Keys</code> are the sensors data and the <code>Values</code> are the number of occurrences. I need to find if the two <code>Key/Value</code> match as in the following example:</p>
<p><code>959.0: 116</code> and <code>-959.0: 108</code></p>
<p>Here, the sensor data <code>959.0</code> and <code>-959.0</code> are repeated (occurred) <code>116</code> and <code>108</code> times, respectively. In my system, I can assume that <code>959.0</code> is good data. But it's not always the ideal case. The sensor data can be <code>958, -955, 952, etc</code> with their respective occurrence number. I need to find the good sensor data from my DB such that each data has similar opposite value and close number of occurrences are present.</p>
<p>My attempts:</p>
<p>At this moment, I'm solving it manually by plotting the data (x being the sensor data and y being the number of occurrence) and filtering it horizontally and vertically. For example:</p>
<pre><code>    for key in list(data.keys()):  ## Filtering sensor data based on their difference on occurance times
        if ((-1*key) in data.keys() and abs(data[key]-data[(-1*key)])&lt;2): 
        #if (-1*key) in data.keys(): 
            pass
        else: del data[key]
        
    #print(data)    
    for key in list(data.keys()):  ##Horizontal filter (based on number of occurance)
         if data[key] &gt;20 or abs(key)&gt;1000:
            pass
         else: del data[key]

lists = sorted(data.items()) # sorted by key, return a list of tuples


x, y = zip(*lists) # unpack a list of pairs into two tuples

plt.plot(x, y,marker=&quot;*&quot;)
plt.grid()
plt.show()
</code></pre>
<p>Is there any better statistical way to solve my problem in Python? Thank you.</p>
",4336593.0,-1.0,N/A,2022-05-05 10:11:09,How to estimate similarity between sensor data based on the number of occurrence?,<python><pandas><dataframe><dictionary><data-science>,2,3,N/A,CC BY-SA 4.0
72030012,1,72031704.0,2022-04-27 13:52:53,0,185,"<p>I have the following 2 dfs:</p>
<h3><code>diag</code></h3>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th><strong>id</strong></th>
<th><strong>encounter_key</strong></th>
<th><strong>start_of_period</strong></th>
<th><strong>end_of_period</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>AAA</td>
<td>2020-06-12</td>
<td>2021-07-07</td>
</tr>
<tr>
<td>1</td>
<td>BBB</td>
<td>2021-12-31</td>
<td>2022-01-04</td>
</tr>
</tbody>
</table>
</div><h3><code>drug</code></h3>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th><strong>id</strong></th>
<th><strong>start_datetime</strong></th>
<th><strong>drug</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2020-06-16</td>
<td>Mel</td>
</tr>
<tr>
<td>1</td>
<td>2020-06-18</td>
<td>Mel</td>
</tr>
<tr>
<td>1</td>
<td>2020-06-18</td>
<td>Flu</td>
</tr>
<tr>
<td>1</td>
<td>2022-01-01</td>
<td>Mel</td>
</tr>
</tbody>
</table>
</div>
<p>I want to combine (?merge/?join/?concatenate) the cols of <code>drug</code> where the <code>start_datetime</code> is within the start and end periods (inclusive) of <code>diag</code>, ending up with more rows in <code>diag</code> like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th><strong>id</strong></th>
<th><strong>encounter_key</strong></th>
<th><strong>start_of_period</strong></th>
<th><strong>end_of_period</strong></th>
<th><strong>drug</strong></th>
<th><strong>start_datetime</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>AAA</td>
<td>2020-06-12</td>
<td>2021-07-07</td>
<td>Mel</td>
<td>2020-06-16</td>
</tr>
<tr>
<td>1</td>
<td>AAA</td>
<td>2020-06-12</td>
<td>2021-07-07</td>
<td>Mel</td>
<td>2020-06-18</td>
</tr>
<tr>
<td>1</td>
<td>AAA</td>
<td>2020-06-12</td>
<td>2021-07-07</td>
<td>Flu</td>
<td>2020-06-18</td>
</tr>
<tr>
<td>1</td>
<td>BBB</td>
<td>2021-12-31</td>
<td>2022-01-04</td>
<td>Mel</td>
<td>2022-01-01</td>
</tr>
</tbody>
</table>
</div>
<p>Hope this makes sense and apologies for not using the correct terms - I'm unsure of them. Thanks in advance.</p>
",13726792.0,-1.0,N/A,2022-05-01 11:17:41,"How to merge two dfs in pandas (based on datetime period), and add rows if duplicates",<python><pandas><dataframe><merge><data-science>,1,1,N/A,CC BY-SA 4.0
72033765,1,-1.0,2022-04-27 18:36:37,1,78,"<p>I have a code that counts how many times every word is used in text:</p>
<pre><code>from collections import Counter
import pandas as pd
import string


stoplist = ['able', 'about', 'above', 'abroad', 'according', 'accordingly', 'across', 'actually', 'adj', 'after', 'afterwards', 'again', 'against', 'ago', 'ahead', &quot;ain't&quot;, 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'always', 'am', 'amid', 'amidst', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', &quot;aren't&quot;, 'around', 'as', &quot;a's&quot;, 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'back', 'backward', 'backwards', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'came', 'can', 'cannot', 'cant', &quot;can't&quot;, 'caption', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', &quot;c'mon&quot;, 'co', 'co.', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', &quot;couldn't&quot;, 'course', &quot;c's&quot;, 'currently', 'dare', &quot;daren't&quot;, 'definitely', 'described', 'despite', 'did', &quot;didn't&quot;, 'different', 'directly', 'do', 'does', &quot;doesn't&quot;, 'doing', 'done', &quot;don't&quot;, 'down', 'downwards', 'during', 'each', 'edu', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'evermore', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'fairly', 'far', 'farther', 'few', 'fewer', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forever', 'former', 'formerly', 'forth', 'forward', 'found', 'four', 'from', 'further', 'furthermore', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'had', &quot;hadn't&quot;, 'half', 'happens', 'hardly', 'has', &quot;hasn't&quot;, 'have', &quot;haven't&quot;, 'having', 'he', &quot;he'd&quot;, &quot;he'll&quot;, 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', &quot;here's&quot;, 'hereupon', 'hers', 'herself', &quot;he's&quot;, 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'hundred', &quot;i'd&quot;, 'ie', 'if', 'ignored', &quot;i'll&quot;, &quot;i'm&quot;, 'immediate', 'in', 'inasmuch', 'inc', 'inc.', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'inside', 'insofar', 'instead', 'into', 'inward', 'is', &quot;isn't&quot;, 'it', &quot;it'd&quot;, &quot;it'll&quot;, 'its', &quot;it's&quot;, 'itself', &quot;i've&quot;, 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'known', 'knows', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', &quot;let's&quot;, 'like', 'liked', 'likely', 'likewise', 'little', 'look', 'looking', 'looks', 'low', 'lower', 'ltd', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', &quot;mayn't&quot;, 'me', 'mean', 'meantime', 'meanwhile', 'merely', 'might', &quot;mightn't&quot;, 'mine', 'minus', 'miss', 'more', 'moreover', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', &quot;mustn't&quot;, 'my', 'myself', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', &quot;needn't&quot;, 'needs', 'neither', 'never', 'neverf', 'neverless', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'no-one', 'nor', 'normally', 'not', 'nothing', 'notwithstanding', 'novel', 'now', 'nowhere', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', &quot;one's&quot;, 'only', 'onto', 'opposite', 'or', 'other', 'others', 'otherwise', 'ought', &quot;oughtn't&quot;, 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provided', 'provides', 'que', 'quite', 'qv', 'rather', 'rd', 're', 'really', 'reasonably', 'recent', 'recently', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 'round', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', &quot;shan't&quot;, 'she', &quot;she'd&quot;, &quot;she'll&quot;, &quot;she's&quot;, 'should', &quot;shouldn't&quot;, 'since', 'six', 'so', 'some', 'somebody', 'someday', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 'take', 'taken', 'taking', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', &quot;that'll&quot;, 'thats', &quot;that's&quot;, &quot;that've&quot;, 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', &quot;there'd&quot;, 'therefore', 'therein', &quot;there'll&quot;, &quot;there're&quot;, 'theres', &quot;there's&quot;, 'thereupon', &quot;there've&quot;, 'these', 'they', &quot;they'd&quot;, &quot;they'll&quot;, &quot;they're&quot;, &quot;they've&quot;, 'thing', 'things', 'think', 'third', 'thirty', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'till', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', &quot;t's&quot;, 'twice', 'two', 'un', 'under', 'underneath', 'undoing', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'upwards', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'v', 'value', 'various', 'versus', 'very', 'via', 'viz', 'vs', 'want', 'wants', 'was', &quot;wasn't&quot;, 'way', 'we', &quot;we'd&quot;, 'welcome', 'well', &quot;we'll&quot;, 'went', 'were', &quot;we're&quot;, &quot;weren't&quot;, &quot;we've&quot;, 'what', 'whatever', &quot;what'll&quot;, &quot;what's&quot;, &quot;what've&quot;, 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', &quot;where's&quot;, 'whereupon', 'wherever', 'whether', 'which', 'whichever', 'while', 'whilst', 'whither', 'who', &quot;who'd&quot;, 'whoever', 'whole', &quot;who'll&quot;, 'whom', 'whomever', &quot;who's&quot;, 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', &quot;won't&quot;, 'would', &quot;wouldn't&quot;, 'yes', 'yet', 'you', &quot;you'd&quot;, &quot;you'll&quot;, 'your', &quot;you're&quot;, 'yours', 'yourself', 'yourselves', &quot;you've&quot;, 'zero', 'a', &quot;how's&quot;, 'i', &quot;when's&quot;, &quot;why's&quot;, 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'uucp', 'w', 'x', 'y', 'z', 'I', 'www', 'amount', 'bill', 'bottom', 'call', 'computer', 'con', 'couldnt', 'cry', 'de', 'describe', 'detail', 'due', 'eleven', 'empty', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'forty', 'front', 'full', 'give', 'hasnt', 'herse', 'himse', 'interest', 'itse”', 'mill', 'move', 'myse”', 'part', 'put', 'show', 'side', 'sincere', 'sixty', 'system', 'ten', 'thick', 'thin', 'top', 'twelve', 'twenty', 'abst', 'accordance', 'act', 'added', 'adopted', 'affected', 'affecting', 'affects', 'ah', 'announce', 'anymore', 'apparently', 'approximately', 'aren', 'arent', 'arise', 'auth', 'beginning', 'beginnings', 'begins', 'biol', 'briefly', 'ca', 'date', 'ed', 'effect', 'et-al', 'ff', 'fix', 'gave', 'giving', 'heres', 'hes', 'hid', 'home', 'id', 'im', 'immediately', 'importance', 'important', 'index', 'information', 'invention', 'itd', 'keys', 'kg', 'km', 'largely', 'lets', 'line', &quot;'ll&quot;, 'means', 'mg', 'million', 'ml', 'mug', 'na', 'nay', 'necessarily', 'nos', 'noted', 'obtain', 'obtained', 'omitted', 'ord', 'owing', 'page', 'pages', 'poorly', 'possibly', 'potentially', 'pp', 'predominantly', 'present', 'previously', 'primarily', 'promptly', 'proud', 'quickly', 'ran', 'readily', 'ref', 'refs', 'related', 'research', 'resulted', 'resulting', 'results', 'run', 'sec', 'section', 'shed', 'shes', 'showed', 'shown', 'showns', 'shows', 'significant', 'significantly', 'similar', 'similarly', 'slightly', 'somethan', 'specifically', 'state', 'states', 'stop', 'strongly', 'substantially', 'successfully', 'sufficiently', 'suggest', 'thered', 'thereof', 'therere', 'thereto', 'theyd', 'theyre', 'thou', 'thoughh', 'thousand', 'throug', 'til', 'tip', 'ts', 'ups', 'usefully', 'usefulness', &quot;'ve&quot;, 'vol', 'vols', 'wed', 'whats', 'wheres', 'whim', 'whod', 'whos', 'widely', 'words', 'world', 'youd', 'youre']
text1 = str(input(&quot;Paste text here: &quot;))
words1 = [s.lower() for s in text1.split() if s.lower() not in stoplist]
words1 = [''.join(c for c in s if c not in string.punctuation) for s in words1]
data = {'words': words1}
df = pd.DataFrame(data)
df = df['words'].value_counts()
display(df)
</code></pre>
<p>And the output looks something like this:</p>
<pre><code>lorem          5
ipsum          5
dummy          2
text           2
typesetting    2
type           2
sheets         1
unchanged      1
popularised    1
1960s          1
release        1
</code></pre>
<p>I want to add a third column that counts the frequency of an individual word appearing in the text, so it would look smth like this:</p>
<pre><code>lorem          5     3%
ipsum          5     3%
dummy          2     1% 
text           2     1%
</code></pre>
<p>I know I have to calculate the sum of every element in second column then divide it by every corresponding number, but I'm not sure how to to this in a dataframe</p>
",8359437.0,10315163.0,2022-04-28 05:24:08,2022-04-28 05:47:18,How to find how popular each word in dataframe is?,<python><pandas><dataframe><data-science>,3,0,N/A,CC BY-SA 4.0
72041293,1,-1.0,2022-04-28 09:30:39,0,108,"<p>I have to add a new row at the end of each person information. In the new row which we will add all the information will be same as last row like <code>name</code>, <code>last_update</code>, and new <code>date</code> will have same value as last row. But visited column should always have value <code>abc</code> and start date will have same value as <code>last_update</code> and date will have same value as new date.</p>
<p>lets say dataframe looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>last_update</th>
<th>New date</th>
<th>visited</th>
<th>start_date</th>
<th>date</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ravi</td>
<td>22-04-2010</td>
<td>22-04-2010</td>
<td>abc</td>
<td>22-09-1987</td>
<td>24-04-2010</td>
</tr>
<tr>
<td>Ravi</td>
<td>27-04-2010</td>
<td>28-04-2010</td>
<td>xyz</td>
<td>24-07-2001</td>
<td>2-08-2015</td>
</tr>
<tr>
<td>Rajesh</td>
<td>22-06-2012</td>
<td>22-07-2012</td>
<td>yyy</td>
<td>12-08-2005</td>
<td>25-08-2012</td>
</tr>
<tr>
<td>Rajesh</td>
<td>24-02-2014</td>
<td>25-04-2014</td>
<td>zzz</td>
<td>18-06-2002</td>
<td>26-06-2014</td>
</tr>
</tbody>
</table>
</div>
<p>New dataframe will be</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>last_update</th>
<th>New date</th>
<th>visited</th>
<th>start_date</th>
<th>date</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ravi</td>
<td>22-04-2010</td>
<td>22-04-2010</td>
<td>abc</td>
<td>22-09-1987</td>
<td>24-04-2010</td>
</tr>
<tr>
<td>Ravi</td>
<td>27-04-2010</td>
<td>28-04-2010</td>
<td>xyz</td>
<td>24-07-2001</td>
<td>2-08-2015</td>
</tr>
<tr>
<td>Ravi</td>
<td>27-042010</td>
<td>28-04-2010</td>
<td>abc</td>
<td>27-042010</td>
<td>28-04-2010-----newly added row</td>
</tr>
<tr>
<td>Rajesh</td>
<td>22-06-2012</td>
<td>22-07-2012</td>
<td>yyy</td>
<td>12-08-2005</td>
<td>25-08-2012</td>
</tr>
<tr>
<td>Rajesh</td>
<td>24-02-2014</td>
<td>25-04-2014</td>
<td>zzz</td>
<td>18-06-2002</td>
<td>26-06-2014</td>
</tr>
<tr>
<td>Rajesh</td>
<td>24-02-2014</td>
<td>25-04-2014</td>
<td>abc</td>
<td>24-02-2014</td>
<td>25-04-2014----newly added row</td>
</tr>
</tbody>
</table>
</div>",11566030.0,-1.0,2022-05-09 21:03:51,2022-05-09 21:03:51,How to add a new row after every unique entries in pandas dataframe,<python><python-3.x><pandas><machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
71982025,1,71985700.0,2022-04-23 17:24:16,0,670,"<p>I am wondering whether I can plot a graph in which I show a range of best and worst results using matplotlib. The result should look something like this:</p>
<p><a href=""https://i.stack.imgur.com/D47OI.png"" rel=""nofollow noreferrer"">Image of the graph I want to replicate here.</a></p>
<p>You see the ranges around each point that specify what the best and worst measure is? This is exactly what I am looking for.</p>
",18921083.0,-1.0,N/A,2022-04-24 05:21:57,How to render line plot with uncertainty in matplotlib,<python><matplotlib><plot><data-science>,1,0,N/A,CC BY-SA 4.0
71994719,1,71997017.0,2022-04-25 05:06:50,3,64,"<pre><code>from matplotlib import markers
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap

plt.rcParams['figure.figsize'] = [9,9]
plt.rcParams.update({'font.size' : 16})

#domain definition
dx = 0.01 #input(&quot;input the step size: &quot;)
x = np.pi*np.arange(-1+ float(dx),1+float(dx),float(dx))
n = len(x)
nquart = int(np.floor(n/4))

#hat funtion
f = np.zeros_like(x)
f[nquart : 2*nquart] = (4/n)*np.arange(1,nquart+1)
f[2*nquart:3*nquart] = np.ones(nquart) - (4/n)*np.arange(0,nquart)

#subplot creation
fig,ax = plt.subplots(1)
ax.plot(x,f)

#core fourier series
name = 'accent'
cmap = get_cmap('tab10')
colors = cmap.colors
ax.set_prop_cycle(color = colors)

# sum of values with an array of ones with the same shape and type as a given array.
Ao = np.sum(f*np.ones_like(x))*dx
ffs = Ao/2

A = np.zeros(20)
B = np.zeros(20)

for k in range(20):
    #the inner products
    A[k] = np.sum(f*np.cos(np.pi*(k+1)*(x/np.pi)))*dx
    B[k] = np.sum(f*np.sin(np.pi*(k+1)*(x/np.pi)))*dx

    ffs = ffs + A[k]*np.cos((k+1)*np.pi*(x/np.pi)) + B[k]*np.sin((k+1)*np.pi*(x/np.pi))
    ax.plot(x,ffs,markers = 'o',LineWidth = 1.5)
    plt.show()
</code></pre>
<p>when running the code i get an error <strong>AttributeError: 'Line2D' object has no property'markers ,'LineWidth'</strong>
if i do not use the markers and LineWidth the code runs but the expected outcome is not what i wanted
<em>i am getting around 15 graphs but that is not what i wanted the color style is also not getting applied</em></p>
",17623211.0,-1.0,N/A,2022-04-25 09:08:11,Trying to plot Fourier sines,<python><numpy><matplotlib><data-science><fft>,1,2,N/A,CC BY-SA 4.0
72012320,1,72025116.0,2022-04-26 10:24:25,1,60,"<p>I have a dataframe  like given below:</p>
<pre><code>ID1 ID2 YrMonth Class
1   p1  Feb-19  PE5
1   p1  Feb-19  PE5
1   p1  Feb-19  PE5
1   p1  Feb-19  SC

1   p2  Feb-19  SC
1   p2  Feb-19  SC
1   p2  Feb-19  SC

1   p3  Feb-19  EA
1   p3  Feb-19  EA
1   p3  Feb-19  PE5

1   p4  Feb-19  EA
1   p4  Feb-19  PE5
1   p4  Feb-19  SC
</code></pre>
<p>I want to convert it into another dataframe or pivot such that in a given month for a particular ID2 if there is a transition in class it should be reflected in a row as given in output
table.
For ex - In ID2 for p1 class changes from PE5 to SC. In output I have represented as PE5-&gt;SC but it could other convenient representation also.</p>
<p>If there is not change in class for a particular ID2, class should come as it is as in second row of  output table class is SC only.</p>
<p>For ID2 p3 there is transition in class from EA to PE5 so it is represented as EA-&gt;PE5.</p>
<p>For ID2 p4 there is transition in class from EA-PE5-SC so it is represented as EA-&gt;PE5-&gt;SC</p>
<p>Output pivot/dataframe</p>
<pre><code>ID1    ID2  YrMonth  Class                   
1      p1   Feb-19   PE5-&gt;SC
1      p2   Feb-19   SC
1      p3   Feb-19   EA-&gt;PE5
1      p4   Feb-19   EA-&gt;PE5-&gt;SC
</code></pre>
",18951121.0,4420967.0,2022-04-27 08:18:48,2022-04-27 08:18:48,Changing python data-frame to pivot as per need,<python><pandas><dataframe><pivot><data-science>,1,0,N/A,CC BY-SA 4.0
72044317,1,-1.0,2022-04-28 13:12:44,0,61,"<p>parent_folder /
subfolder1 /
subsubfolder1/
a.py
b.py
subsubfolder2/
c.py
d.py
e.py
subfolder2 /
subsubfolder2/
f.py
g.py
subfolder3 /
h.py
i.py
g.py</p>
<p>I want to import the functions from files h,i,g to all the subsubfolders py files. Can any one help me how do I do that.
I have tried using sys.path.insert(0 , 'path')
from h import funct1  , from g import funct1. It works
II get this error ImportError: attempted relative import with no known parent package when I do from .h import funct1 and I also want to Implement this in docker files. Is there any other way?
Thanks in advance !!</p>
",18975659.0,-1.0,N/A,2022-04-29 09:09:41,Cannot import functions in from one directory's subfolder to other pyhton files in same parent directories sub folders. How do I use init.py files,<python-3.x><dockerfile><data-science><docker-container>,1,0,N/A,CC BY-SA 4.0
72046074,1,-1.0,2022-04-28 15:05:33,1,146,"<p>Using the Poisson regression importing from <code>statsmodels.api</code> as sm.</p>
<p>I am doing something similar to this, but a different dataset.
<a href=""https://timeseriesreasoning.com/contents/poisson-regression-model/"" rel=""nofollow noreferrer"">https://timeseriesreasoning.com/contents/poisson-regression-model/</a></p>
<p>In my model:</p>
<pre><code>y_train.shape = (52, 52)

X_train.shape = (52, 503)
</code></pre>
<p>I get an error when I run:</p>
<pre><code>poisson_training_results = sm.GLM(y_train, X_train, family = sm.families.Poisson()).fit()
</code></pre>
<p><code>&quot;ValueError: operands could not be broadcast together with shapes (52,1,52) (52,503)&quot;</code></p>
<p>It just looks like it randomly adds '1' to y_train shape.</p>
<p>Does anyone know why it is doing this?</p>
<p>Edit: CODE.</p>
<pre><code>mask = np.random.rand(len(final_delta_df)) &lt; 0.8
df_train = final_delta_df[mask]
df_test = final_delta_df[~mask]
print('Training data set length = '+str(len(df_train)))
print('Testing data set length = '+str(len(df_test)))
</code></pre>
<p>Training data set length=52
Testing data set length=12</p>
<pre><code>expr = &quot;&quot;&quot;deaths_sum ~ positive_auc + vax_pop + hospital_beds_avg + diabetes_prevalence + aged_70_older + stringency_avg + Avg_temp + Avg_UV_index + Avg_water_vapour + Delta_prop&quot;&quot;&quot;

y_train, X_train = dmatrices(expr, df_train, return_type = 'dataframe')
y_test, X_test = dmatrices(expr, df_test, return_type = 'dataframe')

poisson_training_results = sm.GLM(y_train, X_train, family = sm.families.Poisson()).fit()
</code></pre>
",1774800.0,1774800.0,2022-04-28 17:29:34,2022-04-28 17:29:34,Poisson - ValueError: operands could not be broadcast together with shapes,<python><data-science><data-analysis><statsmodels><poisson>,0,7,N/A,CC BY-SA 4.0
72046819,1,72047541.0,2022-04-28 15:56:26,0,140,"<p>I am writing a for loop to try to do an encoding for all of my values in a dataset. I have plenty of categorical values and initially the for loop works for the label encoder but I am trying to include a onehotencoder instead of using get_dummies on a separate line.</p>
<p>sample data:</p>
<pre><code>               STYP_DESC  Gender       RACE_DESC DEGREE               MAJR_DESC1 FTPT  Target
0                   New  Female           White     BA  Business Administration   FT       1
1  New 1st Time Freshmn  Female           White     BA               Studio Art   FT       1
2                   New    Male           White   MBAX  Business Administration   FT       1
3                   New  Female         Unknown     JD             Juris Doctor   PT       1
4                   New  Female  Asian-American   MBAX  Business Administration   PT       1       

</code></pre>
<pre><code>from sklearn.preprocessing import OneHotEncoder, LabelEncoder

le = LabelEncoder()
enc = OneHotEncoder(handle_unknown='ignore',drop='first')
le_count = 0
enc_count = 0
for col in X_train.columns[1:]:
    if X_train[col].dtype == 'object':
        if len(list(X_train[col].unique())) &lt;= 2:
            le.fit(X_train[col])
            X_train[col] = le.transform(X_train[col])
            le_count += 1
        else:
            enc.fit(X_train[[col]])
            X_train[[col]] = enc.transform(X_train[[col]])
            enc_count +=1
print('{} columns were label encoded and {} columns were 1-hot encoded'.format(le_count, enc_count))
</code></pre>
<p>but when I run it, I don't get errors but the encoding is super weird with a slew of tuples being inserted into my new dataset.</p>
<p>When I run the code without the everything in the else clause, it runs fine and I can simply use get_dummies to encode the other variables.</p>
<p>The only issue is when I use get_dummies, I drop_first is set to true; but I lose track of what is supposed to be 0 and what's supposed to be 1. (i.e. this problem is a major issue for tracking Gender and FTPT.</p>
<p>Any suggestions on this? I would use get_dummies but since I'm doing the preprocessing stage <em>after</em> splitting my data I'm worried about a category possibly being dropped out.</p>
",12797331.0,12797331.0,2022-04-28 16:03:46,2022-04-28 17:09:28,Labelencoder and OneHotEncoder within the same for loop,<python><pandas><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
72016494,1,-1.0,2022-04-26 15:19:57,0,288,"<p>I have a folder with 38000 images and I'm trying to create a dataset from them to use in a Convolutional LSTM, which supports input in the format of a 5d array (B,T,W,H,C) where B is the number of samples, T is the size of the samples and the rest of the array are the image properties.</p>
<p>So far I've managed to create a numpy array using the following code:</p>
<pre><code>path = input(&quot;folder: &quot;)
training_data = []
for img in os.listdir(path):
    pic = cv2.imread(os.path.join(path,img))
    pic = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)
    pic = cv2.resize(pic,(64,64))
    training_data.append([pic])
np.save(os.path.join(path,'dataset'),np.array(training_data))
</code></pre>
<p>With this code I can create an array with the shape <strong>(38000, 1, 64, 64, 3)</strong>.</p>
<p>Using as an example data from other datasets such as Moving MNIST I could see that it is an array with the shape <strong>(10000, 20, 64, 64, 1)</strong> which means that it contains 10000 sequences of images each of length 20.</p>
<p>How can I create an array from my data that contains image sequences like Moving MNIST?</p>
<p>To clarify, I need to create an array with my 38000 images divided into sequences of 10 resulting in an array with the shape <strong>(3800, 10, 64, 64, 3)</strong> <em>- 3800 sequences each of length 10 -</em></p>
",18953914.0,18953914.0,2022-04-26 15:39:29,2022-04-26 15:39:29,Creating a image dataset,<python><arrays><machine-learning><deep-learning><data-science>,0,3,N/A,CC BY-SA 4.0
72027961,1,-1.0,2022-04-27 11:31:05,0,28,"<p>I want my code to perform semantic analysis and create a csv table:</p>
<pre><code>from collections import Counter
import pandas as pd


stoplist = ['.', 'and', 'was', 'in', 'a', 'the', ',', '?', ':', 'of']
text1 = str(input(&quot;Paste text here: &quot;))

words1 = [s.lower() for s in text1.split() if s.lower() not in stoplist]
data = {'quantity': words1}
df = pd.DataFrame(data)
df = df['quantity'].value_counts()
df.to_csv('seo.csv')
</code></pre>
<p>Stoplist works for words, however it does not for punctuation:
<a href=""https://i.stack.imgur.com/MvpDq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MvpDq.jpg"" alt=""enter image description here"" /></a></p>
<p>Many people suggested using <code>.str.replace(r'[^\w\s]+', '')</code>, but it doesn't work here:</p>
<pre><code>AttributeError: Can only use .str accessor with string values!
</code></pre>
",8359437.0,8359437.0,2022-04-27 11:59:18,2022-04-27 11:59:18,How to remove punctuation in pandas table without converting it into string? (str.replace doesn't work here),<python><pandas><data-science>,0,6,2022-04-27 11:33:28,CC BY-SA 4.0
72048963,1,72049702.0,2022-04-28 19:01:50,0,36,"<p>I have a dataset on which I need to apply binary classification to predict target value. I applied 5 algorithms on training set such as Logistic regression, Naive Bayes, KNN, SVM, Decision Trees. Out of which Binary Classification using Logistic Regression is giving me highest accuracy but the thing is I did not preprocess my dataset. Now should I again train my model using all five algorithms or Is it sure that Binary Classification using Logistic Regression will again give my highest accuracy after pre processing training dataset?</p>
",13782834.0,-1.0,N/A,2022-04-28 20:13:36,Does highest accuracy giving algorithm still give highest accuracy out of all after preprocessing data set?,<machine-learning><regression><data-science><classification><logistic-regression>,1,1,2022-04-29 09:16:21,CC BY-SA 4.0
72053739,1,-1.0,2022-04-29 06:39:33,0,212,"<p>I have a mining dataset which has a following features Rock_type, Gold in grams(AU). Rock type has 8 different rock types and Gold (AU) has presence of gold in grams in those particular rocktypes and size of dataset is around 30k. With varying value of gold presence in those rock types. Here we have many outliers and I cannot ignore them, so let me know how I can convert mean value of every rocktype and impute to corresponding rocktype</p>
<p>EX:</p>
<pre><code>Rock_type: saprolite, margilite, saprolite, saprolite, mafic, mafic, UD, margilite
Gold(AU) :  25.0     , 0.7,     12.0   ,    14.0    ,  1.5   , 1.7  ,   6.7 , 0.9
</code></pre>
<p>Need solution like this in pandas dataframe:</p>
<pre><code>Rock_type: saprolite, margilite, saprolite, saprolite, mafic, mafic,          UD,        margilite
Gold(AU) :  41.6   ,     1.15,         41.6   ,    41.6    ,  2.35  , 2.35  ,   6.7 , 1.15
</code></pre>
<p>Also let me know is it good practice to have mean value here or do we need to consider mean or mode to get better prediction value.</p>
<p>Thanks in advance</p>
",18985022.0,13897851.0,2022-04-30 14:16:15,2022-04-30 14:16:15,How to convert mean value of each column variable and fill this mean value to corresponding variable in dataframe?,<python><pandas><statistics><data-science>,2,1,2022-04-29 07:03:16,CC BY-SA 4.0
71990419,1,-1.0,2022-04-24 16:42:38,0,49,"<p>i have saved my dataset in two lists</p>
<blockquote>
<p>hi_list=df['hindi_lyrics'] and
en_list=df['english_lyrics']</p>
</blockquote>
<p>the output for these list are (en_lyrics)</p>
<pre><code> Mere ishaare pe
1       Tak dhina dhin naacha jo
2       Twitter pe trend ho gaya
3          Hey Nashik ke dhol pe
4      Dhan dhana dhan naacha to
</code></pre>
<p>HERE i put my data for word alignment using multilingual BERT</p>
<pre><code>   sent_src, sent_tgt = hi_list.strip().split(), en_list.strip().split()
    token_src, token_tgt = [tokenizer.tokenize(word) for word in sent_src], [tokenizer.tokenize(word) for word in sent_tgt]
    wid_src, wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_src], [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]
    ids_src, ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids'], tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']
    sub2word_map_src = []
    for i, word_list in enumerate(token_src):
      sub2word_map_src += [i for x in word_list]
    sub2word_map_tgt = []
    for i, word_list in enumerate(token_tgt):
      sub2word_map_tgt += [i for x in word_list]
</code></pre>
<p>here is the error i get
**</p>
<blockquote>
<p>AttributeError: 'Series' object has no attribute 'strip'</p>
</blockquote>
<p>should i convert my object into a different atrribute ...what should i do</p>
",16287475.0,-1.0,N/A,2022-04-24 16:42:38,how do i convert my series object into string? i think thats the prblem,<python><pandas><nlp><data-science><bert-language-model>,0,2,N/A,CC BY-SA 4.0
72022441,1,-1.0,2022-04-27 01:58:30,0,98,"<p>I have a dataset of used cars, there is a column of prices in the dataset. I want to introduce a new <strong>ordinal</strong> column with the values (high, medium, and low) considering the prices of cars like so:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>price</th>
<th>ordinal</th>
</tr>
</thead>
<tbody>
<tr>
<td>higher than 20,000</td>
<td>high</td>
</tr>
<tr>
<td>10,000-20,000</td>
<td>medium</td>
</tr>
<tr>
<td>below 10,000</td>
<td>low</td>
</tr>
</tbody>
</table>
</div>
<p>Dataset:
<a href=""https://i.stack.imgur.com/53GNq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/53GNq.png"" alt=""screenshot of dataset"" /></a></p>
",18959943.0,355230.0,2022-04-27 02:19:22,2022-04-27 13:44:04,How to add an ordinal data column based on prices to dataframe?,<python><pandas><dataframe><data-science><ordinal-classification>,2,1,2022-04-27 13:52:56,CC BY-SA 4.0
72030718,1,-1.0,2022-04-27 14:41:36,1,1549,"<p>I have a Fan Speed (RPM) dataset of 192.405 Values (train+test values). I am training the ARIMA model and trying to predict the rest of the future values of our dataset and comparing the results.</p>
<p>While fitting the model in test data I am getting straight line for predictions</p>
<pre><code>from sklearn.model_selection import train_test_split 
from statsmodels.tsa.arima_model import ARIMA

dfx = df[(df['Tarih']&gt;'2020-07-23') &amp; (df['Tarih']&lt;'2020-10-23')]

X_train = dfx[:int(dfx.shape[0]*0.8)] #2 months
X_test = dfx[int(dfx.shape[0]*0.8):] # rest, 1 months

model = ARIMA(X_train.Value, order=(4,1,4))
model_fit = model.fit(disp=0)
print(model_fit.summary())

test = X_test
train = X_train
</code></pre>
<p>What could i do now ?</p>
<p><a href=""https://i.stack.imgur.com/3WsFc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3WsFc.png"" alt=""enter image description here"" /></a></p>
",10844552.0,10844552.0,2022-04-27 14:53:15,2022-04-27 15:31:20,Getting straight line while creating ARIMA model,<python><data-science><prediction><forecasting><arima>,1,0,N/A,CC BY-SA 4.0
72055788,1,72057230.0,2022-04-29 09:30:52,1,78,"<p>I am trying to extract the data in the table at <strong><a href=""https://www.ecoregistry.io/emit-certifications/ra/10"" rel=""nofollow noreferrer"">https://www.ecoregistry.io/emit-certifications/ra/10</a></strong></p>
<p>Using the google developer tools&gt;network tab, I am able to get the json link where the data for this table is stored: <strong><a href=""https://api-front.ecoregistry.io/api/project/10/emitcertifications"" rel=""nofollow noreferrer"">https://api-front.ecoregistry.io/api/project/10/emitcertifications</a></strong></p>
<p>I am able to manually copy this json data and extract the information using this code I've written:</p>
<pre><code>import json
import pandas as pd
data = '''PASTE JSON DATA HERE'''
info = json.loads(data)
columns = ['# Certificate', 'Carbon offsets destination', 'Final user', 'Taxpayer subject','Date','Tons delivered']
dat = list()
for x in info['emitcertifications']:
dat.append([x['consecutive'],x['reasonUsingCarbonOffsets'],x['userEnd'],x['passiveSubject'],x['date'],x['quantity']])
df = pd.DataFrame(dat,columns=columns)
df.to_csv('Data.csv')
</code></pre>
<p>I want to automate it such that I can extract the data from the json link: <strong><a href=""https://api-front.ecoregistry.io/api/project/10/emitcertifications"" rel=""nofollow noreferrer"">https://api-front.ecoregistry.io/api/project/10/emitcertifications</a></strong> directly instead of manually pasting json data in:</p>
<pre><code>data = '''PASTE JSON DATA HERE'''
</code></pre>
<p>The link is not working in python or even in browser directly:</p>
<pre><code>import requests
import json
url = ('https://api-front.ecoregistry.io/api/project/10/emitcertifications')
response = requests.get(url)
print(json.dumps(info, indent=4))
</code></pre>
<p>The error output I get is:
{'status': 0, 'codeMessages': [{'codeMessage': 'ERROR_401', 'param': 'invalid', 'message': 'No autorizado'}]}</p>
<p>When I download the data from the developer tools then this dictionary has 'status':1 and after that all the data is there.</p>
<p>Edit: I tried adding request headers to the url but it still did not work:</p>
<pre><code>import requests
import json
url = ('https://api-front.ecoregistry.io/api/project/10/emitcertifications')
hdrs = {&quot;accept&quot;: &quot;application/json&quot;,&quot;accept-language&quot;: &quot;en-IN,en;q=0.9,hi-IN;q=0.8,hi;q=0.7,en-GB;q=0.6,en-US;q=0.5&quot;,&quot;authorization&quot;: &quot;Bearer null&quot;, &quot;content-type&quot;: &quot;application/json&quot;,&quot;if-none-match&quot;: &quot;W/\&quot;1326f-t9xxnBEIbEANJdito3ai64aPjqA\&quot;&quot;, &quot;lng&quot;: &quot;en&quot;, &quot;platform&quot;: &quot;ecoregistry&quot;,&quot;sec-ch-ua&quot;: &quot;\&quot; Not A;Brand\&quot;;v=\&quot;99\&quot;, \&quot;Chromium\&quot;;v=\&quot;100\&quot;, \&quot;Google Chrome\&quot;;v=\&quot;100\&quot;&quot;, &quot;sec-ch-ua-mobile&quot;: &quot;?0&quot;, &quot;sec-ch-ua-platform&quot;: &quot;\&quot;Windows\&quot;&quot;, &quot;sec-fetch-dest&quot;: &quot;empty&quot;,&quot;sec-fetch-mode&quot;: &quot;cors&quot;, &quot;sec-fetch-site&quot;: &quot;same-site&quot; }
response = requests.get(url, headers = hdrs)
print(response)
info = response.json()
print(json.dumps(info, indent=4))
</code></pre>
<p>print(response) give output as '&lt;Response [304]&gt;' while info = response.json() gives traceback error 'Expecting value: line 1 column 1 (char 0)'</p>
<p>Can someone please point me in the right direction?</p>
<p>Thanks in advance!</p>
",8036266.0,8036266.0,2022-04-29 10:23:07,2022-04-29 11:26:31,JSON link from google developer tools not working in Python (or in browser),<python><json><web-scraping><data-science><data-extraction>,1,14,N/A,CC BY-SA 4.0
72068580,1,-1.0,2022-04-30 12:34:34,0,115,"<p>I have a need for generating data for performance testing for an application which has data with lot of relations between entities. Here is example.</p>
<pre><code>DivA                                     
DivA[Payroll,HR,IT] 
Payroll[Location,Classification,files]
HR[Location,Training,Compliance]
IT[Clearance,Experience,Compliance]
Location[City,Country]
Classification[ExemptionType,Expiry date]
....
</code></pre>
<p>From above &quot;schema&quot;</p>
<p>I need to generate data using following algorithm</p>
<ol>
<li>Create parent entity (Ex: Consumer Electronics Division )</li>
<li>Populate all children (Ex: Consumer Electronics Division [Payroll,HR,IT] )</li>
<li>Check if children has more children (Ex: Consumer Electronics Division [Payroll[Location,Classification,files],HR [Location,Training,Compliance],IT[Clearance,Experience,Compliance]]
....
keep going until you don't find any more children.</li>
</ol>
<p>Is there any algorithm/Data structure that helps to create data like this easily?</p>
<p>Thank you!</p>
",9564452.0,-1.0,N/A,2022-05-05 22:19:24,How can i create a graph/tree programmatically to generate test data,<python><algorithm><tree><data-science>,1,0,N/A,CC BY-SA 4.0
72076162,1,-1.0,2022-05-01 11:10:47,-1,169,"<p>I'm trying to plot confusion matrix, but the plot seems unable to show values at the center.</p>
<pre><code>Code : 
plt.figure(figsize=(4,2))
    sns.heatmap(cm, annot=True, annot_kws={&quot;size&quot;:20}, 
            cmap='Blues', square=True, fmt='.2f')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.title('Confusion matrix for:\n{}'.format(model.__class__.__name__));
</code></pre>
<p>Output :
<a href=""https://i.stack.imgur.com/549lG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/549lG.png"" alt=""enter image description here"" /></a></p>
<p>How to have the values 0.45, 0.94, 0.06 and 0.55 to be at the center of square box for better readability ?</p>
",9079461.0,9079461.0,2022-05-01 12:30:24,2022-05-01 18:02:09,confusion matrix plot is unable to show values within plot,<data-science><data-analysis><confusion-matrix>,1,0,N/A,CC BY-SA 4.0
72079973,1,72081600.0,2022-05-01 19:53:16,1,1751,"<p>I need to achieve the following in my python module.</p>
<ol>
<li>Get a directory as an argument through the argparse module</li>
<li>Use that directory to iterate through all the files present in that directory.</li>
<li>Later, append only the CSV files to a dictionary and store them there.</li>
</ol>
<p>I have tried os.walk, but it does not return anything.</p>
<p>I have written code to get a directory as input and then iterate through it for files. But it throws errors.</p>
<p>Below is the code</p>
<pre class=""lang-py prettyprint-override""><code> import argparse
 import os
 import re
    
 # Accepting arguments for source_directory, my_sql connection details and table name to 
   update the MySQL entries to
    
parser = argparse.ArgumentParser(
        description='Utility to update MySQL tables on remote host')
parser.add_argument('-s', '--source_dir', help='Enter Path For CSV File')
    
args = parser.parse_args()
    
# Empty dictionary to store list CSV files in directory
files_in_dir = {}
    
# Function to iterate through the source_directory and read-only CSV files and append them 
   to files_in_dir dictionary
    
if args.source_dir:
    for file in args.source_dir:
    
        if os.path.abspath(file.name).endswith('.csv'):
            files_in_dir[(os.path.abspath(file.name))] = 0
    
        else:
            print(&quot;The file %s is not a .csv file and it will be ignored&quot; % (file.name))
    
print(files_in_dir)
</code></pre>
<p>And this is the error when executed through the terminal.</p>
<pre class=""lang-sh prettyprint-override""><code> python3 sourcecheck.py -s /home/thepredator/Desktop/VOIS/csv_files/*
usage: sourcecheck.py [-h] [-s SOURCE_DIR]
sourcecheck.py: error: unrecognized arguments: /home/thepredator/Desktop/VOIS/csv_files/Book1.csv /home/thepredator/Desktop/VOIS/csv_files/Build.py /home/thepredator/Desktop/VOIS/csv_files/car_data.csv /home/thepredator/Desktop/VOIS/csv_files/db.py /home/thepredator/Desktop/VOIS/csv_files/Edited.csv /home/thepredator/Desktop/VOIS/csv_files/Iris.csv /home/thepredator/Desktop/VOIS/csv_files/Lab_Python_for_Data_Analytics.ipynb /home/thepredator/Desktop/VOIS/csv_files/startupNaN.csv /home/thepredator/Desktop/VOIS/csv_files/startupog.csv /home/thepredator/Desktop/VOIS/csv_files/text.csv
</code></pre>
<p>I have written the above code by watching different videos. Please help me fix the errors. Also, I'm not a python expert.</p>
",14849205.0,-1.0,N/A,2022-05-03 05:48:12,Write code to iterate all files in a directory and append each file's path to a dictionary,<python><loops><oop><data-science><argparse>,2,3,N/A,CC BY-SA 4.0
72072506,1,-1.0,2022-04-30 22:04:17,0,43,"<p>I have accelerometer data, X, Y, Z and Type (this is the label) which can be 'fast', 'slow', 'normal'. I should use the data for supervised learning, classification. I am a new to time series data and I found this function to process the data but I am not quite sure how it works. Can anyone help me get the concept of it? Or any other algorithms to process the data and then pass the features and label to a model?</p>
<pre><code>def  get_frames(df, frame_size, hop_size):
N_FEATURES=3

frames=[]
labels=[]
for i in range(0, len(df)-frame_size, hop_size):
    x=df['X'].values[i: i+frame_size]
    y=df['Y'].values[i: i+frame_size]
    z=df['Z'].values[i: i+frame_size]
    label=stats.mode(df['Type'][i:i+frame_size])
    frames.append([x,y,z])
    labels.append(label)
    
frames=np.asarray(frames).reshape(-1, frame_size, N_FEATURES)
labels=np.asarray(labels)
    
return frames, labels
</code></pre>
",13130892.0,-1.0,N/A,2022-04-30 22:04:17,Time series data processing algorithm problem,<python><jupyter-notebook><time-series><data-science>,0,2,N/A,CC BY-SA 4.0
72096379,1,-1.0,2022-05-03 08:00:19,-2,64,"<pre><code>import requests

from requests.auth import HTTPBasicAuth

a=requests.get('https://api.github.com/user',auth=HTTPBasicAuth('email','pass'))
print(a.status_code)
</code></pre>
<p>I am entering right GitHub email and password but getting error 401</p>
",19022229.0,3626138.0,2022-05-03 08:44:31,2022-05-03 08:44:31,"HTTP basic authorisation ERROR 401. showing , but I am entering right email and password",<python><data-science>,1,1,N/A,CC BY-SA 4.0
72100135,1,-1.0,2022-05-03 13:27:54,0,90,"<p>I have been tasked with analyzing the input flow in a water tank in relation to a number of weather parameters. In a narrower sense, I have to investigate any possible effect that these variables might have on the variable of interest.
That being said, I don't know which method(s) to apply as I'm thinking only of Pearson's correlation coefficient. Even with this one, the sampling rate is different as the weather conditions are measured every 3 hours while input flow every 5 minutes. Should I average over 3 hours, disregard data not corresponding to weather dataset timestamp or would you suggest something else?<br><br>
<code>weather = [ (1.21,0) , (1.08, 0.5), (1.04, 1), (1.02, 1.5)]</code><br>
<code>input_flow =  [ (120,0), (124,1)]</code><br><br>
<em>A representation of such data where the first index is the value of the parameter while the second one is time in seconds</em></p>
",19024577.0,-1.0,N/A,2022-05-06 15:12:05,Analyzing unevenly spaced timeseries,<python><time-series><data-science>,1,1,N/A,CC BY-SA 4.0
72100586,1,72100953.0,2022-05-03 14:00:37,0,237,"<p>I have two dataframes with labels, I want to append or concat them at bottom of each other</p>
<pre><code>d = {}
d['first_level'] = pd.DataFrame(columns=['idx', 'a', 'b', 'c'],
                                 data=[[10, 1, 2, 3],
                                       [20, 4, 5, 6]]).set_index('idx')

d['first_level2'] = pd.DataFrame(columns=['idx', 'a', 'b', 'c'],
                                 data=[[10, 1, 2, 3],
                                       [20, 4, 5, 6]]).set_index('idx')

    df_final = pd.concat(d, axis=1,)
    writer = pd.ExcelWriter('test_file.xlsx')
            # df_final.reset_index(drop=True, inplace=True)
            # ddf_finalf=df_final.drop(['idx'],axis=1)
            df_final.to_excel(writer, sheet_name='my_analysis', index=True,)
        
        writer.save()
</code></pre>
<p>I have this code, this results in excel output like this:</p>
<pre><code> first_level  first_level2 
 idx a b c     idx a b c
  10 1 2 3      10 1 2 3
  20 4 5 5      20 4 5 5
</code></pre>
<p>However I want <code>first_level2</code> at bottom of <code>first_level</code>,and the excel output should be like this:</p>
<pre><code>first_level  
idx a b c 
10 1 2 3 
20 4 5 6 

first_level2 
idx a b c
10 1 2 3
20 4 5 5
</code></pre>
<p>I tried with MultiIndex too, but got the same results,How can I achieve this using pandas I want to such output in excel, I tried looking into docs but couldn't find anything relevant.</p>
",10293085.0,16343464.0,2022-05-03 14:33:28,2022-05-03 14:33:28,append several Pandas dataframe on top of each other in the same excel sheet,<python><pandas><dataframe><data-science><data-processing>,2,8,N/A,CC BY-SA 4.0
72109778,1,-1.0,2022-05-04 08:13:41,1,636,"<p>I have a .CSV file containing Arabic data and I need to view this file in jupyter using python and pandas.
But I have a problem with the encoding
What should I do ? any ideas please ?
<a href=""https://i.stack.imgur.com/FVeA2.png"" rel=""nofollow noreferrer"">This is my code</a></p>
<p><a href=""https://i.stack.imgur.com/z6qPs.png"" rel=""nofollow noreferrer"">And this is the error</a></p>
",19031266.0,-1.0,N/A,2022-05-05 23:37:13,Import arabic CSV file using python,<python><csv><data-science><data-visualization><jupyter>,3,2,N/A,CC BY-SA 4.0
72123229,1,-1.0,2022-05-05 07:10:57,1,443,"<p>After fitting AutoTS model over some time series data, how can I save &amp; load the best model trained? Though, the AutoTS object has export_template() &amp; import_template() functions to save best model, but while loading best model from this template, it requires re-fitting. How can such a solution be used in production? My code:</p>
<pre><code>from autots import AutoTS

model = AutoTS(
frequency='infer',
prediction_interval=0.9,
ensemble=None,
model_list=&quot;fast&quot;,  # &quot;superfast&quot;, &quot;default&quot;, &quot;fast_parallel&quot;
transformer_list=&quot;fast&quot;,  # &quot;superfast&quot;,
drop_most_recent=1,
max_generations=4,
num_validations=2,
validation_method=&quot;backwards&quot;) 

 model.fit(df_day,date_col='xyz',value_col='abc')
 model.export_template(&quot;unique_user_1&quot;, models='best', n=1, max_per_model_class=3)
</code></pre>
<p>Now, in some new instance, when I do</p>
<pre><code> model = model.import_template('unique_user_1.csv',method='only')
</code></pre>
<p>The model required retraining.</p>
",8422170.0,-1.0,N/A,2022-12-09 04:27:14,Save & load best model in AutoTS python,<python><machine-learning><time-series><data-science><forecasting>,1,0,N/A,CC BY-SA 4.0
72131241,1,72131586.0,2022-05-05 17:20:24,0,40,"<p>The code below creates a Scatter plot from <code>X</code> and based on values of <code>w,b</code>, creates lines over X.</p>
<p>I have tried a couple of combinations such as:</p>
<pre><code>fig.canvas.draw()
fig.canvas.flush_events()

plt.clf
plt.cla
</code></pre>
<p>But they either seem to plot multiple lines over the plot or Delete the figure  / axes.</p>
<p><strong>Is it possible to plot the Scatter plot only once but the Lines keep changing based on  <code>w,b</code>?</strong>.</p>
<p>Below is the code that I have used:</p>
<pre><code>from sklearn import datasets
import matplotlib.pyplot as plt
import numpy as np
import time
from IPython.display import display, clear_output

def get_hyperplane_value(x, w, b, offset):
    '''
    Generate Hyperplane for the plot
    '''
    return (-w[0] * x + b + offset) / w[1]


def plot_now(ax, W,b):
    '''
    Visualise the results
    '''
    x0_1 = np.amin(X[:, 0])
    x0_2 = np.amax(X[:, 0])

    x1_1 = get_hyperplane_value(x0_1, W, b, 0)
    x1_2 = get_hyperplane_value(x0_2, W, b, 0)

    x1_1_m = get_hyperplane_value(x0_1, W, b, -1)
    x1_2_m = get_hyperplane_value(x0_2, W, b, -1)

    x1_1_p = get_hyperplane_value(x0_1, W, b, 1)
    x1_2_p = get_hyperplane_value(x0_2, W, b, 1)

    ax.plot([x0_1, x0_2], [x1_1, x1_2], &quot;y--&quot;)
    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], &quot;k&quot;)
    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], &quot;k&quot;)

    x1_min = np.amin(X[:, 1])
    x1_max = np.amax(X[:, 1])
    ax.set_ylim([x1_min - 3, x1_max + 3])
    
    ax.scatter(X[:, 0], X[:, 1], marker=&quot;o&quot;, c = y)
    return ax



X, y = datasets.make_blobs(n_samples=50, n_features=2, centers=2, cluster_std=1.05, random_state=40)
y = np.where(y == 0, -1, 1)


fig = plt.figure(figsize = (7,7))
ax = fig.add_subplot(1, 1, 1)

    
for i in range(50):
    
    W = np.random.randn(2)
    b = np.random.randn()
    
    ax.cla()
    ax = plot_now(ax, W, b)
    
    display(fig)    
    clear_output(wait = True)
    plt.pause(0.25) 
</code></pre>
",11725056.0,11725056.0,2022-05-05 17:43:44,2022-05-05 17:49:27,"Efficient way to erase and re create a (part of, if possible) subplot inside loop using matplotlib?",<python><matplotlib><data-science><data-visualization><analytics>,1,0,N/A,CC BY-SA 4.0
72050701,1,-1.0,2022-04-28 22:02:02,0,30,"<p><a href=""https://i.stack.imgur.com/aA9ji.png"" rel=""nofollow noreferrer"">starter Q</a>: How do I know if the data is filtered? Or how to filter a data?</p>
<p><a href=""https://i.stack.imgur.com/Rr9Hp.png"" rel=""nofollow noreferrer"">Part1</a></p>
<p><a href=""https://i.stack.imgur.com/AWvi1.jpg"" rel=""nofollow noreferrer"">part 2</a></p>
<p>Can someone please explain how to start the first part of my project. I've never used R before and had this project thrown at us. Thank you</p>
",18981457.0,18981457.0,2022-04-28 22:29:50,2022-04-28 22:29:50,pData Analysis using language R,<r><database><data-science><computer-science><economics>,0,6,N/A,CC BY-SA 4.0
72101295,1,-1.0,2022-05-03 14:51:20,5,9630,"<p>I'm attempting to do a grid search to optimize my model but it's taking far too long to execute. My total dataset is only about 15,000 observations with about 30-40 variables. I was successfully able to run a random forest through the gridsearch which took about an hour and a half but now that I've switched to SVC it's already ran for over 9 hours and it's still not complete. Below is a sample of my code for the cross validation:</p>
<pre><code>from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.svm import SVC

SVM_Classifier= SVC(random_state=7)



param_grid = {'C': [0.1, 1, 10, 100],
              'gamma': [1,0.1,0.01,0.001],
              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
              'degree' : [0, 1, 2, 3, 4, 5, 6]}

grid_obj = GridSearchCV(SVM_Classifier,
                        
                        return_train_score=True,
                        param_grid=param_grid,
                        scoring='roc_auc',
                        cv=3,
                       n_jobs = -1)

grid_fit = grid_obj.fit(X_train, y_train)
SVMC_opt = grid_fit.best_estimator_

print('='*20)
print(&quot;best params: &quot; + str(grid_obj.best_estimator_))
print(&quot;best params: &quot; + str(grid_obj.best_params_))
print('best score:', grid_obj.best_score_)
print('='*20)

</code></pre>
<p>I have already reduced the cross validation from 10 to 3, and I'm using n_jobs=-1 so I'm engaging all of my cores. Is there anything else I'm missing that I can do here to speed up the process?</p>
",12797331.0,-1.0,N/A,2022-05-06 03:47:40,Python : GridSearchCV taking too long to finish running,<python><machine-learning><scikit-learn><data-science><cross-validation>,2,4,N/A,CC BY-SA 4.0
72134294,1,-1.0,2022-05-05 22:35:53,1,379,"<p>I'm trying to run algorithms on Neo4j's Aura DS databases.</p>
<p>It seems like I've by and large understood how to connect to an Aura DS database, project a particular graph, then apply one of the algorithms from the graphdatascience (GDS) library in order to do node classification or solve some other machine learning problem.</p>
<p>However, can I somehow connect to an Aura DS database and retrieve the data in a format like pandas dataframe/tensor/numpy array/etc. and use other libraries besides GDS to train?</p>
<p>Apologies if this is trivial. I've tried searching for this, but got no satisfactory answer.</p>
",6071007.0,-1.0,N/A,2022-05-06 06:58:13,Can a neo4j graph projected in Python be then transformed to pandas dataframe/tensor/numpy/etc to be used with pytorch/etc.?,<python><neo4j><graph-data-science>,1,1,N/A,CC BY-SA 4.0
72138359,1,-1.0,2022-05-06 08:23:17,0,33,"<p>this is my first question on stack overflow so i am not sure how to post in the right way, i hope i do well :).</p>
<p>I have a question regarding my prediction model in python.
I have made my first prediction model and now that i have made the prediction i would also like to have a note or message when the prediction has reached a value =&gt; 14,5. So just a small message with &quot;Please check machine&quot; when reached 14,5.</p>
<p>see prediction graph in figure and small portion of the code i have written.
<a href=""https://i.stack.imgur.com/8sVky.png"" rel=""nofollow noreferrer"">Prediction graph</a></p>
<p><a href=""https://i.stack.imgur.com/CK5Gz.png"" rel=""nofollow noreferrer"">Prediction</a></p>
<pre><code>y_pred = model.predict(X_test)

y_train_inv = cnt_transformer.inverse_transform(y_train.reshape(1, -1))
y_test_inv = cnt_transformer.inverse_transform(y_test.reshape(1, -1))
y_pred_inv = cnt_transformer.inverse_transform(y_pred)

plt.plot(np.arange(0, len(y_train)), y_train_inv.flatten(), 'g', label=&quot;history&quot;)
plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_test_inv.flatten(), marker='.', label=&quot;true&quot;)
plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_pred_inv.flatten(), 'r', label=&quot;prediction&quot;)
plt.ylabel('DP inlet')
plt.xlabel('Time Step')
plt.legend()
plt.show();

#Output see figure

plt.plot(y_test_inv.flatten(), marker='.', label=&quot;true&quot;)
plt.plot(y_pred_inv.flatten(), 'r', label=&quot;prediction&quot;)
plt.ylabel('DP inlet')
plt.xlabel('timestamp')
plt.legend()
plt.show();

#Output see figure
</code></pre>
",19050895.0,19050895.0,2022-05-06 08:31:21,2022-05-06 08:31:21,Warning note when data has reached 14.5,<python><pandas><matplotlib><data-science>,0,2,N/A,CC BY-SA 4.0
72062475,1,72062551.0,2022-04-29 18:46:07,0,274,"<pre><code>df['gender'] = df['gender'].map({&quot;2&quot;: &quot;man&quot;, &quot;1&quot;: &quot;woman&quot;})
</code></pre>
<p>Got <code>NaN</code> instead of man&amp;woman</p>
<p>What is wrong?</p>
",18992751.0,16343464.0,2022-04-29 18:59:43,2022-04-29 19:08:25,Got Nan while mapping the values in dataframe,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
72070367,1,72080463.0,2022-04-30 16:45:56,1,1159,"<p>So, as evident from the title, I want to convert a pdf to a csv so that I could use that data in my project. The problem is that the pdf formatting is not at all suitable for conversion to a csv file. For a human reader, the file makes complete sense but for a computer, it is extremely difficult to comprehend. It is difficult for me to explain here but I would encourage my fellow data scientists to help me find a solution for the same.</p>
<p>The pdf can be found here:</p>
<pre><code>https://mospi.gov.in/documents/213904/533217//Appendix-II1602843196372.pdf/7da592e8-0da1-abd0-3b15-da3227f76fea
</code></pre>
<p>Any ideas/techniques would be extremely helpful.</p>
",13958291.0,-1.0,N/A,2022-05-01 23:37:37,Converting a pdf to a csv with steps using python,<python><csv><pdf><data-science>,1,1,N/A,CC BY-SA 4.0
72089991,1,72090330.0,2022-05-02 17:04:58,0,29,"<p>I was going through ensemblling methods and was wondering what is the difference between the optimization techniques like gradient descent etc. And ensembling techniques like bagging, boosting etc.</p>
",15822504.0,404970.0,2022-05-03 06:02:22,2022-05-03 06:02:22,What is the difference between optimization algorithms and Ensembling methods?,<optimization><methods><data-science><difference>,1,0,N/A,CC BY-SA 4.0
72105705,1,72105847.0,2022-05-03 21:35:21,1,168,"<p>I am new to python and my objective is to create a basic script capable of filtering a csv database through user input.
I searched through various posts and answers and came up with this code that doesn't give the desired responses for some reason, can somebody tell me if it is correct or an erroneous approach? Thank you very much for your help and here is the code below :</p>
<pre><code>import csv

from encodings import utf_8
from csv import DictReader
from multiprocessing import Value
from typing import Dict
with open('Book.csv', newline='', ) as file :
    reader= DictReader(file)
  
    for row in reader :
     print(row)
    
  

    ask_age=(input(&quot;enterage:&quot;))
    
    for row in reader :
     for key,val in row.items : 
        if ask_age==str in row.items['Age',str] is True : 
         print(row.items['Name',str])
</code></pre>
<p>Here is a sample of the database (it's a mock database as I am still figuring out the script)</p>
<pre><code>Age,Name,Sex
10,brian,male
30,amilia,female
40,raylie,female 
</code></pre>
",18354493.0,18354493.0,2022-05-03 21:59:21,2022-12-27 06:58:38,Using input to filter parsed csv through Dictreader,<python><csv><dictionary><data-science>,2,2,N/A,CC BY-SA 4.0
72135509,1,-1.0,2022-05-06 02:24:01,-1,40,"<p>I am very new to R programming and have been provided the following data to implement a non-parametric test on. My issue lies in being able to turn this data (in R) into a long format data frame, so I may then conduct a histo/box plot. We aren't allowed to simply turn data into csv then read in, it has to be done in R.</p>
<p>A:1361,1466,1319,1426,1437,1541,1474,1386,1510,1373,1463,1305,1571,1224,1372
B:1581,1515,1606,1518,1395,1584,1671,1573,1454,1674,1459,1647
C:1482,1570,1575,1634,1542,1651,1189,1678,1391,1525
D:2084,1566,1990,1996,2052,1436,1808,1679,1981,2014,1759,1842,1603,1670,1845,2016,1621,2050,1690,1933</p>
<p>I've turned these into vectors but keep spitting error mssgs when I try to turn into data frame (vectors different lengths). Any pointers would be much help, I've been trying to troubleshoot for hours and my prof is no help.</p>
<p>Thanks</p>
",19048683.0,-1.0,N/A,2022-05-06 07:17:02,How to turn four vectors of differing lengths into a long format dataframe?,<r><dataframe><vector><data-science>,2,3,N/A,CC BY-SA 4.0
72140069,1,72140139.0,2022-05-06 10:31:34,1,146,"<p>I have a pandas data frame like given below</p>
<pre><code>Id1     YEAR    CLAIM_STATUS   no_of_claims
 1  2019-01       4               1
 1  2019-01       5               1
 1  2019-02       4               1
 1  2019-02       5               1
 1  2019-03       4               6
 1  2019-03       5               2
 1  2019-04       5               1
 1  2019-04       6               1
 1  2019-05       5               2
 1  2019-06       4               1
 1  2019-06       5               1
</code></pre>
<p>Here claim_STATUS status 4 means claim is rejected I need to find percentage of claims rejected for each Id1 at each year where percentage of claims rejected is calculated like</p>
<pre><code>claim-status-4/(claim-status-4+claim-status-5+claim-status-6)
</code></pre>
<p>in above table claim rejected percentage is <code>(1/(1+6+3))*100 = 10%</code></p>
<p>I need to convert above dataframe into output given below:</p>
<pre><code>Id1   YEAR_MO    % of claims rejected
1   2019-01         50%
1   2019-02         50%
1   2019-03         75%
1   2019-04         0%
1   2019-05         0%
1   2019-06         50%
</code></pre>
",18951121.0,18951121.0,2022-05-06 11:56:30,2022-05-06 12:16:31,Finding percentage of rejection in pandas dataframe,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
72087895,1,72089943.0,2022-05-02 14:05:51,-2,41,"<p>It looks like most predicted values are close to 0.5. How can the predicted values follow closer the original values?</p>
<pre><code>normalizer = layers.Normalization()
normalizer.adapt(np.array(X_train))

model = keras.Sequential([
        normalizer,
        layers.Dense(8, activation='relu'),
        layers.Dense(1, activation='linear'),
        layers.Normalization()
    ])
</code></pre>
<p><a href=""https://i.stack.imgur.com/YahrF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YahrF.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/erqyJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/erqyJ.png"" alt=""enter image description here"" /></a></p>
",7311228.0,-1.0,N/A,2022-05-02 17:00:58,Why predicted values are towards center?,<tensorflow><keras><neural-network><data-science>,1,6,2022-05-02 18:41:05,CC BY-SA 4.0
72095659,1,-1.0,2022-05-03 06:42:37,0,546,"<p>I have two large datasets. Let's say few thousands rows for V dataset with 18 columns. I would need to find correlations between individual rows (e.g., row V125 is similar to row V569 across the 18 columns). But since it's large I don't know how to filter it after. Another problem is that I have B dataset (different information on my 18 columns) and I would like to find similar pattern between the two datasets (e.g., row V55 and row B985 are similar, V3 is present only if B45 is present, etc...). Is there a way to find out? I'm open to any solutions. PS: this is my first question so let me know if it needs to be edited or I'm not clear. Thank you for any help.</p>
",19016154.0,-1.0,N/A,2022-05-05 23:26:20,Data science: How to find patterns and correlations in two datasets in python,<python><statistics><data-science><correlation>,2,2,N/A,CC BY-SA 4.0
72141358,1,-1.0,2022-05-06 12:17:47,-1,85,"<p><strong>Solved in comment</strong></p>
<p>I have this struggle with a dataheavy project. I can run a file that uses a query file -- Al the query's and converters are in here -- without problems, but when I run a file that runs that file again, I get an error that the db cant be found.</p>
<pre><code>so: 
Queryfile() : No problem,
Queryfile() -&gt; Actionfile() : No problem,
Queryfile() -&gt; Actionfile() -&gt; action_manager(): Error, cant find db,
Queryfile() -&gt; Actionfile() -&gt; action_manager() -&gt; startupfile(): Giant error, no way to solve it.
</code></pre>
<p>My goal is to build a startup file in the main dir and trigger all needed scripts from there, but I constantly receive this error:</p>
<pre><code>    (sqlite3.OperationalError) unable to open database file
(Background on this error at: https://sqlalche.me/e/14/e3q8)
('ERROR_LOADING_ANALYSES', OperationalError('(sqlite3.OperationalError) unable to open database file'))
</code></pre>
<p>This is really bad and I don't know what i need to to make this project run. If there is any alternative way, for example move the workdirectory to this file, run it and move back, I would like to know it. Please let me know how you are dealing with this.</p>
<p>this is my project tree.</p>
<pre><code>├───core_data
│   ├───data_temp
│   ├───**DATABASE**
├───core_update
│   ├───update_main
│   │   └───__pycache__
│   ├───update_additional_scripts
│   │   └───__pycache__
├───core_utils
│   ├───database_querys
│   │   └───__pycache__
│   ├───database_tables
│   │   └───__pycache__
├───startup file 
├───constants. 
</code></pre>
<p>Look forward to your reply.</p>
",17665172.0,17665172.0,2022-10-24 12:39:43,2023-03-29 16:58:46,Python cant find database if script is run from other file,<python><sqlite><data-science>,1,1,N/A,CC BY-SA 4.0
72164599,1,-1.0,2022-05-08 19:48:37,0,133,"<pre><code>from sklearn.tree import DecisionTreeRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

model = DecisionTreeRegressor()
my_pipeline = Pipeline(steps=[
    (&quot;scale&quot;, StandardScaler),
    (&quot;preprocessor&quot;, preprocessor),
    (&quot;model&quot;, model)
])

my_pipeline.fit(X_train, y_train)

</code></pre>
<p><strong>Raising the error</strong> <code>AttributeError: 'DataFrame' object has no attribute 'fit'</code>.</p>
<p>When I run the code without scaling it shows no error but with the scaling, it shows a weird (for me) error.</p>
<p>So my questions are:</p>
<ul>
<li>How can I properly add a scale to my pipeline?</li>
<li>Why the variable my_pipeline is a pandas object?</li>
</ul>
<p><strong>Edit</strong></p>
<pre><code>#Creation of imputer to deal with missing value for numerical_cols
from sklearn.impute import SimpleImputer
numerical_transformer = SimpleImputer(strategy=&quot;mean&quot;)

#Creation of pipe to deal with categorical value and their missing value
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

categorical_cols = [cname for cname in X.columns if data[cname].nunique() &lt; 10 and data[cname].dtype == &quot;object&quot;]
numerical_cols = [cname for cname in X.columns if data[cname].dtypes != &quot;object&quot;]
numerical_cols.remove(&quot;PassengerId&quot;)

categorical_transformer = Pipeline(steps=[
    (&quot;imputer&quot;, SimpleImputer(strategy='most_frequent')),
    (&quot;onehot&quot;, OneHotEncoder(handle_unknown='ignore'))
])

from sklearn.compose import ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
    (&quot;num&quot;, numerical_transformer, numerical_cols),
    (&quot;cat&quot;, categorical_transformer, categorical_cols)
])


</code></pre>
<ul>
<li>The error is line 12 at <code>my_pipeline.fit(X_train, y_train)</code></li>
</ul>
",11414634.0,11414634.0,2022-05-08 20:12:15,2022-05-08 20:12:15,Adding scale to my pipeline showing me weird error,<python><scikit-learn><data-science><pipeline><data-preprocessing>,0,3,N/A,CC BY-SA 4.0
72157515,1,-1.0,2022-05-08 01:44:54,0,705,"<p><strong>I have gotten a trouble with multi layers in the same plot with geopandas</strong>. I got plot both successfully separately, however to plot two legends (one to each) I could not. Below it is the code to read the datasets.</p>
<pre><code>dfGeo = pd.read_csv('drive/MyDrive/dfGeo.csv', sep=',').dropna()
dfGeo.drop_duplicates(subset=['Link'], inplace=True)
gdf = gpd.GeoDataFrame(dfGeo, geometry=gpd.points_from_xy(dfGeo.long, dfGeo.lat))
bairros = gpd.read_file('drive/MyDrive/bairros.geojson')
</code></pre>
<p>Below it is the code to plot the first map with legend to it.</p>
<pre><code>fig, ax = plt.subplots(figsize=(10,20))
bairros.plot(column='rpa', legend=True, categorical=True, ax=ax)
</code></pre>
<p><img src=""https://i.stack.imgur.com/SmM0F.png"" alt=""first plot"" /></p>
<p>This is the first code to plot both layers without legend to each one</p>
<pre><code>fig, ax = plt.subplots(figsize=(10,20))
bins = mapc.Quantiles(gdf['Preco'], k=5).bins
ax.set_aspect('equal')
bairros.plot(ax=ax, color='gray', edgecolor='silver')
gdf.plot(ax=ax, marker='o', markersize=12, color='gold')
plt.show()
</code></pre>
<p><img src=""https://i.stack.imgur.com/BJKHw.png"" alt=""second plot"" /></p>
<p>At end it is the code trying to plot both with legend to each one, but unsuccessfully the only one legend appeared from the second plot.</p>
<pre><code>fig, ax = plt.subplots(figsize=(10,20))
bins = mapc.Quantiles(gdf['Preco'], k=5).bins
ax.set_aspect('equal')

# legend this doesn't appear
bairros.plot(column='rpa', legend=True, categorical=True, ax=ax)

# legend this appear    
gdf.plot(column='Preco', cmap='inferno', ax=ax, marker='o', markersize=12, legend=True, scheme=&quot;User_Defined&quot;, classification_kwds=dict(bins=bins))

plt.show()
</code></pre>
<p><img src=""https://i.stack.imgur.com/50MLt.png"" alt=""third plot"" /></p>
<p>I would like to plot and putting the legend of both in the same plot. How could I do it?
<strong>Obs:</strong> I tried a solution from this <a href=""https://stackoverflow.com/questions/63005381/how-to-show-both-colormaps-in-the-legend-in-geopandas"">issue</a>, but the output changed without any legend.</p>
<p><a href=""https://i.stack.imgur.com/BI77C.png"" rel=""nofollow noreferrer"">fourth plot</a></p>
<p>Thanks in advance!</p>
",19060921.0,19060921.0,2022-05-08 23:43:30,2022-05-08 23:43:30,Trying plot two geodataframes with two legends in the same plot in Geopandas,<python><matplotlib><plot><data-science><geopandas>,0,5,N/A,CC BY-SA 4.0
72190624,1,-1.0,2022-05-10 17:22:48,-4,211,"<p>I tried loading my csv file using pd.read_csv. It has 33 million records and takes too much time for loading and querying also.</p>
<p>I have data of 200k customers.
This is the code I have written for sampling</p>
<p>Data is loading quickly when using a dask dataframe but takes much time for queries.</p>
<pre><code>df_s = df.sample(frac = 300000/33819106,replace = None,random_state = 10)
</code></pre>
<p>This works fine but the customers have ordered many products. In the sample how to include all the products of the customers. How to sample based on customer id?</p>
",17615572.0,4420967.0,2022-05-12 08:45:10,2022-05-12 08:45:10,How to take sample from dask dataframe having all the products ordered by certain number of customers alone?,<pandas><dataframe><machine-learning><jupyter-notebook><data-science>,1,1,N/A,CC BY-SA 4.0
72191377,1,-1.0,2022-05-10 18:28:31,0,13,"<p>Data at hand: 1000 questionnaires with a finite database of questions, say 100 questions about name, gender, income etc. Each questionnaire contains 10 to 30 questions from this question database. The wording of a certain question remains identical across different questionnaires. The 100 questions have their unique label (Q1 to Q100) in the database.</p>
<p>Task: creating a new questionnaire. Assuming I know which questions (say 20 questions including Q1, Q5, Q10, Q22 etc) I need to ask on the new questionnaire, I need to know what order should I place these questions.</p>
<p>Machine learning question: how do I learn the patterns from the existing data to help myself order the 20 questions on my new questionnaire?</p>
",4427381.0,4427381.0,2022-05-10 18:37:06,2022-05-10 18:37:06,How to learn the order of content by machine learning?,<machine-learning><nlp><data-science>,1,0,N/A,CC BY-SA 4.0
72155142,1,-1.0,2022-05-07 17:56:44,1,72,"<p>I'm learning Pytorch so I decided to participate in kaggle competition <a href=""https://www.kaggle.com/competitions/digit-recognizer/overview"" rel=""nofollow noreferrer"">DigitRecognition</a>. It seems to me that eveything is correct but during the training the model accuracy was get worse and worst. I think that I've made some unnoticable mistake which spoils the model maybe connected with custom DataSet class (<em>MyDataset</em>) but I can't find it myself. Please, somebody help me.
Here is <a href=""https://colab.research.google.com/drive/1th6k4Iw06ywTjE_myCw4j_O7umpqylfW?usp=sharing"" rel=""nofollow noreferrer"">colab notebook with all solution</a>.</p>
",18085239.0,404970.0,2022-05-07 19:19:32,2022-05-07 20:11:06,Pytorch model doesn't fit for some reason,<python><pytorch><data-science>,0,1,N/A,CC BY-SA 4.0
72158678,1,-1.0,2022-05-08 06:36:23,0,60,"<p>I'm taking a Data Science class that uses Python and this is a questions that stumped me today. &quot;How many babies are named “Oliver” in the state of Utah for all years?&quot;
To answer this question we were supposed to use data from this set <a href=""https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv</a></p>
<p>So I started by loading in pandas.</p>
<pre><code>import pandas as pd
</code></pre>
<p>Then I loaded in the data set and created a data frame</p>
<pre><code>url='https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv'

names=pd.read_csv(url)
</code></pre>
<p>Finally I used the .query() method to single out the data type that I wanted, the name Oliver.</p>
<pre><code>oliver=names.query(&quot;name == 'Oliver'&quot;)
</code></pre>
<p>I eventually found the total number of babies that had been named Oliver in Utah using this code</p>
<pre><code>total=pd.DataFrame.sum(quiz)

print(total)
</code></pre>
<p>but I wasn't sure how to single out the data for both the name and the state, or if that is even possible. Is there anyone out there that knows of a better way to find this answer?</p>
",19066348.0,-1.0,N/A,2022-07-01 20:41:56,How do I query more than one column in a data frame?,<python><pandas><data-science>,1,3,N/A,CC BY-SA 4.0
72171205,1,72171521.0,2022-05-09 11:25:40,0,67,"<p>I am using or-tools to solve an optimization problem. I have many constraints that I am storing in a list. Unfortunately, that takes a lot of memory space.</p>
<pre><code>constraints_list = []
for i in range(rows):
    constraints_list.append([0] * (4 * i) + [1, 1, 1, 1] + [0] * ((rows * (columns - 1)) -  ((columns - 1) * (i + 1))))
</code></pre>
<p><code>rows</code> is in the order of hundreds of thousands, and <code>columns</code> is five. Is there a better way to do this?</p>
",16532158.0,68587.0,2022-05-09 11:35:15,2022-05-09 14:41:24,Is there a way that uses less memory in python to do this,<python><data-science>,1,3,N/A,CC BY-SA 4.0
72174950,1,72175845.0,2022-05-09 16:02:50,0,288,"<p>I am trying to run a <a href=""https://keras.io/"" rel=""nofollow noreferrer"">Keras</a> sequential model but can't get the right shape for the model to train on.</p>
<p>I reshaped <code>x</code> and <code>y</code> to:</p>
<pre><code>x = x.reshape(len(x), 500)
y = y.reshape(len(y), 500)
</code></pre>
<p>Currently, both the input shape and output shape are:</p>
<pre><code>(9766, 500)
(9766, 500)
</code></pre>
<p>The dataset consists of 9766 inputs and 9766 outputs respectively. Each input is a single array of 500 values and each output is also an array of 500 values.</p>
<p>So here is one single input array:</p>
<pre><code>[0.99479668 0.99477965 0.99484778 0.99489887 0.99483926 0.99451565
 0.99458378 0.99457526 0.99453268 0.99468597 0.99466042 0.99449862
 0.99453268 0.99454971 0.99463487 0.99461784 0.99451565 0.99463487
 0.99467745 0.99502661 0.99480519 0.99493294 0.99493294 0.99522248
 0.99526506 0.99528209 0.99527358 0.99515435 0.99529913 0.99488184
 0.99508623 0.99512881 0.99522248 0.99497552 0.9954439  0.99554609
 0.99581861 0.99573345 0.9957079  0.99626144 0.99626144 0.99592932
 0.99558867 0.99541835 0.99524803 0.99586119 0.99601448 0.99588674
 0.99584416 0.99559719 0.995495   0.99520545 0.99552055 0.99510326
 0.9951799  0.99560571 0.99561422 0.99541835 0.99586119 0.995759
 0.9957079  0.99583564 0.9959208  0.99578454 0.99604854 0.99612519
 0.99609112 0.99630402 0.9961337  0.99672983 0.99655099 0.99643176
 0.99643176 0.99648286 0.99649138 0.99645731 0.99670428 0.99654247
 0.99647435 0.99607409 0.99589525 0.99600596 0.99596338 0.99621035
 0.99633809 0.99632106 0.99583564 0.99581009 0.99574196 0.9959719
 0.99557164 0.99567383 0.99572493 0.9958697  0.99568235 0.9959208
 0.99598893 0.99620183 0.99611667 0.99620183 0.9959719  0.9957079
 0.99612519 0.99558867 0.99569938 0.99518842 0.99553758 0.99552055
 0.99576751 0.99577603 0.99583564 0.99602299 0.99630402 0.99637215
 0.99701937 0.99701086 0.99731744 0.99700234 0.99696828 0.99668725
 0.99703641 0.99725782 0.99684054 0.99605706 0.99608261 0.99581861
 0.9958697  0.99583564 0.99566532 0.99585267 0.99566532 0.99604003
 0.99540984 0.99473707 0.995231   0.99441346 0.9942261  0.99397914
 0.99367256 0.99409836 0.99415797 0.99420907 0.99398765 0.99356185
 0.99382585 0.99428571 0.9945412  0.99444752 0.99436236 0.99404726
 0.9938003  0.99424313 0.99483074 0.99474558 0.99457526 0.99457526
 0.99465191 0.99466042 0.99467745 0.99448158 0.99454971 0.99479668
 0.994703   0.99455823 0.99472855 0.99507771 0.99529913 0.99515435
 0.99525655 0.99621886 0.99586119 0.99576751 0.9962359  0.99614222
 0.99723228 0.99685757 0.99680647 0.99689163 0.99644028 0.99701937
 0.99675538 0.99637215 0.99614222 0.99628699 0.9964488  0.99641473
 0.99652544 0.99652544 0.99664467 0.99698531 0.99712157 0.99703641
 0.99799872 0.99859485 0.99876517 0.99950607 0.99902065 0.99891846
 0.99804982 0.99839898 0.99857782 0.99850117 0.99891846 0.99912284
 0.99919097 0.99919949 0.99896956 0.99896104 0.99877369 0.99898659
 0.99918246 0.99890994 0.9990462  0.99895252 0.99885033 0.99871407
 0.99871407 0.99871407 0.99864594 0.99854375 0.9983564  0.9985693
 0.99870556 0.99868001 0.9987822  0.99877369 0.99900362 0.99882478
 0.99896956 0.99885885 0.99880775 0.99890994 0.99906323 0.99908026
 0.9990462  0.99921652 0.99920801 0.99936129 0.99937833 0.99943794
 0.99935278 0.99943794 0.99967639 0.99956568 0.99960826 0.99962529
 0.99942942 0.99940387 0.9992591  0.99908878 0.99912284 0.99913988
 0.99905472 0.99914839 0.99913136 0.99933575 0.99935278 0.99929317
 0.99931871 0.99905472 0.99965084 0.99995742 1.         0.99962529
 0.999472   0.99939536 0.99932723 0.99929317 0.99931871 0.99931871
 0.99950607 0.99953162 0.99942942 0.99919097 0.99902917 0.99913988
 0.99915691 0.9990462  0.9990973  0.99923355 0.99940387 0.99954865
 0.99958271 0.99940387 0.99943794 0.99928465 0.9990973  0.99905472
 0.99915691 0.99921652 0.99913988 0.99913136 0.99912284 0.9992591
 0.99916542 0.99917394 0.99918246 0.99906323 0.99905472 0.99907175
 0.99901214 0.9990462  0.99913988 0.9990462  0.9990462  0.99880775
 0.99890994 0.99868852 0.99868852 0.99889291 0.99896956 0.99886736
 0.99932723 0.99943794 0.99932723 0.99931871 0.99931871 0.99921652
 0.99874814 0.99871407 0.99915691 0.99969342 0.99962529 0.99916542
 0.99902917 0.99887588 0.99919097 0.99943794 0.99847562 0.9988333
 0.99905472 0.99913988 0.99931871 0.99936129 0.99893549 0.99869704
 0.99842453 0.99868001 0.99868852 0.9987822  0.9987311  0.99871407
 0.99860336 0.99826272 0.99805834 0.99785395 0.99792208 0.99804982
 0.99797317 0.99797317 0.99778582 0.99749627 0.99751331 0.99758143
 0.99732595 0.99741111 0.99699383 0.99733447 0.99728337 0.99686608
 0.99714712 0.9973515  0.99753885 0.99753034 0.99762402 0.99774324
 0.99781989 0.99765808 0.99739408 0.9974026  0.99723228 0.99737705
 0.99728337 0.99728337 0.99736002 0.99726634 0.99732595 0.99721524
 0.99728337 0.99701937 0.99715563 0.99715563 0.99744518 0.99753034
 0.99747073 0.99765808 0.9978284  0.99726634 0.99724931 0.99776879
 0.99746221 0.9976666  0.9976666  0.99744518 0.99734298 0.99833085
 0.99866298 0.99800724 0.99714712 0.99648286 0.99588674 0.99598041
 0.99563125 0.99595486 0.99626144 0.99601448 0.99456674 0.9947541
 0.99499255 0.99483926 0.9950181  0.99497552 0.99484778 0.99424313
 0.99416649 0.99416649 0.9942772  0.99288908 0.99266766 0.99293166
 0.99248031 0.99312753 0.99269321 0.99307643 0.99286353 0.99319566
 0.99346817 0.99337449 0.99322972 0.99302534 0.99322121 0.99307643
 0.99295721 0.99344262 0.99262508 0.99259953 0.99246327 0.99254844
 0.99265063 0.99288908 0.99288908 0.9930594  0.9933234  0.99340004
 0.99320417 0.99331488 0.99319566 0.99335746 0.99322121 0.99271876
 0.99271024 0.99270172 0.99259102 0.99308495 0.99331488 0.9930083
 0.99285501 0.99289759 0.99276134 0.99259102 0.99266766 0.99221631
 0.99216521 0.99225889 0.99227592 0.99196934 0.99162018 0.99147541
 0.99134767 0.99159463 0.99152651 0.99166276 0.99169683 0.99168831
 0.99175644 0.99178199 0.99161167 0.99165425 0.99170534 0.9915776
 0.9915776  0.99144135 0.99169683 0.99170534 0.99144986 0.99170534
 0.99187567 0.99192676 0.99183308 0.99177347 0.99173941 0.99176496
 0.99170534 0.9917905  0.99178199 0.99144986 0.99147541 0.99142431
 0.99149244 0.99139877]
</code></pre>
<p>And here is one output array:</p>
<pre><code>[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.99449862
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.99731744 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.99356185
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         1.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.99686608
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.99866298 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.99134767 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
</code></pre>
<p>And this is the model I am trying to train the data on (most likely with a bad architecture):</p>
<pre><code>model = Sequential()
model.add(LSTM(128, input_shape=(x.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(128, input_shape=(x.shape[1:])))
model.add(Dropout(0.1))

model.add(LSTM(32, input_shape=(x.shape[1:]) ,activation = 'relu'))
model.add(Dropout(0.2))

model.add (Dense(1 ,activation = 'sigmoid'))
opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-3)
model.compile(loss='mse',optimizer=opt, metrics=['accuracy'])
model.fit(x,y,epochs=20,validation_split=0.20)
</code></pre>
<p>How I would like the model to train is to see the input and produce an array of 500 values like the output array shown above.
But no matter what shape I try, I get an error like the following:</p>
<pre><code>ValueError: Input 0 of layer &quot;lstm&quot; is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 500)
</code></pre>
<p>What shape is the proper shape here and what am I doing wrong with the model architecture?</p>
<p>UPDATE 1:</p>
<p>I also tried reshaping <code>x</code> and <code>y</code> to:</p>
<pre><code>(9766, 1, 500)
(9766, 1, 500)
</code></pre>
<p>still no luck.</p>
",10295417.0,10295417.0,2022-05-09 17:40:01,2022-05-09 17:40:01,proper input and output shape of a keras Sequential model,<python><numpy><tensorflow><keras><data-science>,1,5,N/A,CC BY-SA 4.0
72192677,1,-1.0,2022-05-10 20:39:45,0,189,"<p>It is said that variance of the difference of two independent variables is the sum of variances (topic - Confidence intervals. Two means. Independent samples)</p>
<p>I tried to experiment with small dataset of two variables. i found that sum of variance of 2 independent variable is not equal to variance of difference. the experiment is below:-</p>
<ul>
<li>
<ol>
<li>new york apple price [$3.80, $3.76, $3.87, $3.99, $4.02, $4.25,$4.13, $3.98]
its variance = 0.027</li>
</ol>
</li>
</ul>
<ol start=""2"">
<li><p>LA apple price - [$3.02, $3.22, $3.24, $3.02, $3.06, $3.15, $3.81, $3.44], variance =  0.071</p>
</li>
<li><p>Their difference in prices = [0.78, 0.54, 0.63, 0.97, 0.96, 1.10, 0.32, 0.54] - variance = 0.0715</p>
</li>
</ol>
<p>sum of LA apple price variance and NY apple price variance = 0.098 which not equal to variance of difference = 0.0715</p>
<p>can someone explain me why is it so please?</p>
",17654441.0,-1.0,N/A,2022-05-11 00:06:43,variance of the difference of two independent variables is the sum of variances,<statistics><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
72172462,1,72172513.0,2022-05-09 13:03:35,1,312,"<p>I have a dataframe with 2 columns that represent a list:</p>
<pre><code>a. b.  vals.        locs
1. 2. [1,2,3,4,5].  [2,3]
5  1. [1,7,2,4,9].  [0,1]
8. 2. [1,9,4,7,8].  [3]
</code></pre>
<p>I want, for each row, exclude from the columns vals all the locations that are in locs.
so I will get:</p>
<pre><code>a. b.  vals.        locs.   new_vals
1. 2. [1,2,3,4,5].  [2,3].  [1,2,5]
5  1. [1,7,2,4,9].  [0,1].  [2,4,9]
8. 2. [1,9,4,7,8].  [3].    [1,9,4,8]
</code></pre>
<p>What is the best way to do so?</p>
<p>Thanks!</p>
",6057371.0,-1.0,N/A,2022-05-09 13:30:24,pandas dataframe how to remove values from cell that is a list based on other column,<pandas><dataframe><data-science><data-munging>,3,0,N/A,CC BY-SA 4.0
72176655,1,72176761.0,2022-05-09 18:27:28,-3,3484,"<p>My task is to replace all the elements whose both indexes are odd with 1, and  all the elements whose both indexes are even with -1.</p>
",8359437.0,-1.0,N/A,2022-05-09 19:23:59,How do I replace elements in two-dimensional array in python?,<python><arrays><data-science>,3,4,N/A,CC BY-SA 4.0
72188059,1,72188431.0,2022-05-10 14:19:58,0,582,"<pre><code>917 1st St, Dallas, TX 75001
682 Chestnut St, Boston, MA 02215
669 Spruce St, Los Angeles, CA 90001
669 Spruce St, Los Angeles, CA 90001
</code></pre>
<p><strong>so, i'm trying to extract the city and state from the given data...</strong></p>
<pre><code>def get_city_state(address):
    asplit = address.split(&quot;,&quot;)
    ssplit = address.split(&quot; &quot;)
    city = asplit[1].split()[-1]
    state = asplit[2].split()[0]
    return city , state


all_data['City'] = all_data['Purchase Address'].apply(lambda x: f&quot;{get_city_state(x)}&quot;)
all_data.head()
</code></pre>
",16582274.0,-1.0,N/A,2022-05-10 14:48:01,Extracting City and State from a column in CSV using python,<python><pandas><csv><data-science>,2,2,N/A,CC BY-SA 4.0
72199969,1,72201284.0,2022-05-11 11:08:32,2,91,"<p>first of all, I'm quite new to programming overall (&lt; 2 Months), so I'm sorry if that's an '<em>simple, no need to ask for help, try it yourself until you get it done'</em> problem.</p>
<p>I have two data-frames with partially the same content (general overview of mobile-numbers including their cost centers in the company <strong>and</strong> monthly invoices with the affected mobile-numbers and their invoice amount).</p>
<p>I'd like to compare the content of the 'mobile-numbers' column of the <em>monthly invoices DF</em> to the content of the 'mobile-numbers' column of the <em>general overview DF</em> and if matching, assign the respective cost center to the mobile-number in the <em>monthly invoices DF</em>.</p>
<p>I'd love to share my code with you, but unfortunately I have absolutely zero clue how to solve that problem in any way.</p>
<p>Thanks</p>
<p><strong>Edit: I'm from germany, I tried my best to explain the problem in english. If there is anything I messed up (so u dont get it) just tell me :)</strong></p>
<p><a href=""https://i.stack.imgur.com/FJQ34.png"" rel=""nofollow noreferrer"">Example of desired result</a></p>
",19093075.0,19093075.0,2022-05-11 11:28:28,2022-05-11 12:49:35,Compare columns (per row) of two DataFrames in Python,<python><pandas><dataframe><data-science>,1,3,N/A,CC BY-SA 4.0
72200115,1,-1.0,2022-05-11 11:19:05,1,256,"<p>I have 75 columns, and 300k captured network traffic CSV file.
I am playing with data to apply ML. I need to convert IP addresses to 1 and 0 according to internal and external.
So if it is</p>
<pre><code>10.0.2.* &gt; 0
others &gt; 1
</code></pre>
<p>Is there an easy way to do this?
I was doing the manually replace method.</p>
<pre><code>df['SrcAddr'] = df['SrcAddr'].replace(['10.0.2.15','10.0.2.2'],[0,0,0])
</code></pre>
",18964569.0,-1.0,N/A,2022-05-11 11:27:00,How to replace values to binary(0-1) in Pandas for Network data?,<pandas><machine-learning><data-science><netflow>,1,0,N/A,CC BY-SA 4.0
72203805,1,-1.0,2022-05-11 15:33:38,0,565,"<p>I need to switch the rows and columns of a dataframe without using the transpose function (df.T), this is the code I wrote so far.</p>
<pre><code>def transpose(matrix):
    
    transposed = pd.DataFrame()
    
    for i,j in matrix.iteritems():
        transposed.iloc[j,i] = matrix.iloc[j,i]
    
    return transposed
</code></pre>
<p>This code is giving the error: IndexError: positional indexers are out-of-bounds</p>
<p>After trying for a while, this is my new code:</p>
<pre><code>def transpose(matrix):
   
    transposed = matrix.copy()
    
    for i,j in matrix.iteritems():
        transposed.iat[i, j] = matrix.iat[j, i]
    
    return transposed
</code></pre>
<p>This is giving ValueError: iAt based indexing can only have integer indexers.</p>
<p>Changing iat to iloc, loc or at doesn't work.</p>
",15222930.0,15222930.0,2022-05-11 16:33:17,2022-05-11 16:58:55,"Switch rows and columns of dataframe, without using transpose function",<python><pandas><dataframe><data-science><transpose>,2,9,N/A,CC BY-SA 4.0
72170082,1,-1.0,2022-05-09 09:52:59,0,276,"<p>I have 2 PDF files which I want to compare with each other and get similarity score, from what I have been able to research online and find is a sentence or a paragraph being compared to each other, but I want to compare the between the pdfs as a whole and not a paragraph. I want to do this using BERT.</p>
<p>Any help is appreciated, Thanks.</p>
",16899087.0,-1.0,N/A,2022-05-09 09:52:59,Comparing 2 pdf files to check for their similarity using BERT,<python><nlp><data-science><bert-language-model>,0,2,N/A,CC BY-SA 4.0
72183414,1,-1.0,2022-05-10 09:00:26,1,156,"<p>A noob question.</p>
<p>As I understand, the pipeline library of scikit learn is a kind of automation helper, which brings the data through a defined data processing cycle. But in this case I don't see any sense in it.</p>
<p>Why can't I implement data preparation, model training, score estimation, etc. via functional or OOP programming in python? For me it seems much more agile and simple, you can control all inputs, adjust complex dynamic parameter grids, evaluate complex metrics, etc.</p>
<p>Can you tell me, why should anyone use sklearn.pipelines? Why does it exist?</p>
",18345231.0,-1.0,N/A,2022-05-10 10:04:06,Scikit learn pipelines. Does it make any sense?,<python><scikit-learn><data-science><pipeline>,3,2,N/A,CC BY-SA 4.0
72186991,1,-1.0,2022-05-10 13:11:33,1,145,"<p>This is more of an architecture question. I have a data engineering background and have been using airflow to orchestrate ETL tasks using airflow for a while. I have limited knowledge of containerization and kuberentes. I have a task to come up with a good practice framework for productionalizting our Data science models using an orchestration engine namely airflow.</p>
<p>Our Data science team creates many NLP models to process different text documents from various resources. Previously the model was created by an external team which requires us to create an anacoda environment install libraries on it and run the model. The running of model was very manual where a data engineer would spin us a EC2 instance, and setup the model download the files to the ec2 instance and process the files using the model and take the output for further processing.</p>
<p>We are trying to move away from this to an automated pipeline where we have an airflow dag that basically orchestrates this all. The point where I am struggling is the running the model part.</p>
<p>This is the logical step I am thinking of doing. Please let me know if you think this would be feasible. All of these will be down in airflow. Step 2,3,4 are the ones I am totally unsure how to achieve.</p>
<ol>
<li>Download files from ftp to s3</li>
<li>**Dynamically spin up a kubernetes cluster and create parallel pod based on number of files to be process.</li>
<li>Split files between those pods so each pod can only process its subset of files</li>
<li>Collate output of model from each pod into s3 location**</li>
<li>Do post processing on them</li>
</ol>
<p>I am unsure how I can spin up a kuberentes cluster in airflow on runtime and especially how I split files between pods so each pod only processes on its own chunk of files and pushes output to shared location.</p>
<p>The running of the model has two methods. Daily and Complete. Daily would be a delta of files that have been added since last run whereas complete is a historical reprocessing of the whole document catalogue that we run every 6 months. As you can imagine the back catalogue would require alot of parallel processing and pods in parallel to process the number of documents.</p>
<p>I know this is a very generic post but my lack of kuberentes is the issue and any help would be appreciated in pointing me in the right direction.</p>
",8375624.0,-1.0,N/A,2022-05-10 13:56:35,Orchestration of an NLP model via airflow and kubernetes,<kubernetes><airflow><data-science><orchestration>,1,0,N/A,CC BY-SA 4.0
72218136,1,-1.0,2022-05-12 15:13:57,0,78,"<p>How do I target a specific row and column and delete or replace that value with <code>NaN</code> with pandas data frames?</p>
<p>I have this data Filtering Problem.
I wrote my magnetic sensors data in a CSV file, but <strong>now I have to find the outliers and count them</strong> in order to compare diffrent location on an object.</p>
<p>As soon as I start moving the sensor the values jump all over the place. So there is a connction beweeen the values. I cannot simply assume my values are fixed and say bigger/smaller than x is an outlier.
As Method of choice I inspect the diffrence betweeen the values on one axis/column. If the difference is too big, it's an outlier.
Now I have used the pandas Library to read the data from my CSV file and I would like to delete the data in grey. Since I have three axis, so three columns, I cannot simply drop one row, because that I only want count the outliers. Deletion of rows would result in deletion of non outliers.</p>
<p>My Idea is now to remove the outliers and compare lengths of the Series or replace the outliers with NaN and count NaNs. For that I would have to remember all outliers somehow and target them specifically.</p>
<p>This is a graph of my values and following is the code without the visualization part
<a href=""https://i.stack.imgur.com/ELIIv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ELIIv.png"" alt=""Data with Outliers"" /></a></p>
<pre><code>def analyse( filename):
df = pd.read_csv(filename, float_precision='round_trip') # df= dfframe
listOfNames=[&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;]
df.to_csv('temp.csv', index=False)
upperBorder=1
for x in listOfNames:
    sigma=df[x].diff().std() # Calculates the standardderivation of difference of a Dataframe element compared with element in previous row). https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html
    df['is Noise'+str(x)] = df[x].diff().apply(lambda val: 'Green' if val &lt; upperBorder*sigma and val &gt; (-1)*upperBorder*sigma else 'Grey' ) 
</code></pre>
<p>Thank you for your time</p>
",-1.0,-1.0,N/A,2022-05-12 15:13:57,Pandas Delete/replace Specific Value according to location in dataframe,<python><pandas><data-science><signal-processing>,0,2,N/A,CC BY-SA 4.0
72200573,1,-1.0,2022-05-11 11:52:50,0,23,"<p>I have a data structured into table and I'm wondering if there is any way I could find connection based on this data so I got some kind of conditions/filters based on which data is presented?</p>
<p>Which tools could be useful ?</p>
",10801469.0,-1.0,N/A,2022-05-11 11:52:50,Finding conditions filters based on data,<python><machine-learning><data-science>,0,2,N/A,CC BY-SA 4.0
72200996,1,72201272.0,2022-05-11 12:22:03,0,341,"<pre><code>%%time

glove_embeddings = np.load('https://www.kaggle.com/code/authman/pickled-glove-840b-300d', allow_pickle=True)
fasttext_embeddings = np.load('../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl', allow_pickle=True)
</code></pre>
<p>Getting error :</p>
<p>FileNotFoundError: [Errno 2] No such file or directory: 'https://www.kaggle.com/code/authman/pickled-glove-840b-300d'</p>
",9429989.0,-1.0,N/A,2022-05-11 12:42:49,Unable to load Pickled Glove 840b 300d in Jupyter Notebook,<jupyter-notebook><nlp><data-science><kaggle>,1,0,N/A,CC BY-SA 4.0
72212341,1,-1.0,2022-05-12 08:29:17,0,64,"<p>I have a dataset in which one of its columns is <code>Ex-Showroom_Price</code>, and I'm trying to convert its values to integers but I'm getting an error.</p>
<pre><code>import pandas as pd

#reading the dataset
cars = pd.read_csv('cars_engage_2022.csv')

cars[&quot;Ex-Showroom_Price&quot;]  = int(cars[&quot;Ex-Showroom_Price&quot;] .split()[-1].replace(',',''))
</code></pre>
<p>Error:</p>
<pre class=""lang-none prettyprint-override""><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-40-d65bfedf76a4&gt; in &lt;module&gt;
----&gt; 1 cars[&quot;Ex-Showroom_Price&quot;]  = int(cars[&quot;Ex-Showroom_Price&quot;] .split()[-1].replace(',',''))

TypeError: 'int' object is not subscriptable
</code></pre>
<p>Values of <code>Ex-Showroom_Price</code>:</p>
<pre class=""lang-none prettyprint-override""><code>Rs. 2,92,667
Rs. 2,36,447
Rs. 2,96,661
Rs. 3,34,768
Rs. 2,72,223
:
</code></pre>
",16854609.0,404970.0,2022-05-13 05:46:13,2022-05-13 05:46:13,How to convert the values of an attribute having categorical values to integer type?,<python><pandas><dataframe><data-science>,2,7,N/A,CC BY-SA 4.0
72217485,1,72235150.0,2022-05-12 14:30:44,0,172,"<p>I merged 3 different CSV(D1,D2,D3) Netflow datasets and created one big dataset(df), and applied KMeans clustering to this dataset.
To merge them I did not use pd.concat because of memory error and solved with Linux terminal.</p>
<pre><code>df = pd.read_csv('D.csv')
#D is already created in a Linux machine from terminal

........
KMeans Clustering
........

As a result of clustering, I separated the clusters into a dataframe
then created a csv file.
cluster_0 = df[df['clusters'] == 0]
cluster_1 = df[df['clusters'] == 1]
cluster_2 = df[df['clusters'] == 2]

cluster_0.to_csv('cluster_0.csv')
cluster_1.to_csv('cluster_1.csv')
cluster_2.to_csv('cluster_2.csv')

#My goal is to understand the number of same rows with clusters
#and D1-D2-D3
D1 = pd.read_csv('D1.csv')
D2 = pd.read_csv('D2.csv')
D3 = pd.read_csv('D3.csv')
</code></pre>
<p>All these datasets contain the same column names, they have 12 columns(all numerical values)</p>
<p>Example expected result:</p>
<p>cluster_0 has xxxx numbers of same rows from D1, xxxxx numbers of same rows from D2, xxxxx numbers of same rows from D3?</p>
",18964569.0,18964569.0,2022-05-13 21:01:30,2022-05-13 21:01:30,How to count the same rows between multiple CSV files in Pandas?,<python><pandas><data-science><cluster-analysis><netflow>,2,5,N/A,CC BY-SA 4.0
72221986,1,72224805.0,2022-05-12 20:57:50,0,241,"<p>Hello I am struggling to find a solution to probably a very common problem.</p>
<p>I want to merge two csv-files with soccer data. They basically store different data of the same games. Normally I would do a merge with <code>.merge</code>, but the problem is, that the nomenclature differs for some teams in the two Datasets. So for example <code>Manchester City</code> is called <code>Man. City</code> in the second data frame.</p>
<p>Here's roughly what df1 and df2 look like:</p>
<p>df:</p>
<pre><code>team1            team2     date                      some_value_i_want_to_compare
Manchester City  Arsenal   2022-05-20 22:00:00 0.2812  5
</code></pre>
<p>df2:</p>
<pre><code>team1       team2     date                      some_value_i_want_to_compare
Man. City   Arsenal   2022-05-20 22:00:00 0.2812  3
</code></pre>
<p>Note that in the above case there are only differences in <code>team1</code> but there could also be cases where <code>team2</code> is slightly different. So for example in this case <code>Arsenal</code> could be called <code>FC Arsenal</code> in the second data set.</p>
<p>So my main question is: <strong>How could I automatically analyse the differences in the two datasets naming?</strong></p>
<p>My second question is: <strong>How do I scale this for more than 2 data sets so that the number of data sets ultimately doesn't matter?</strong></p>
",18301773.0,18301773.0,2022-05-12 22:12:40,2022-05-13 05:26:58,how to merge multiple datasets with differences in merge-index strings?,<python><pandas><dataframe><merge><data-science>,2,1,N/A,CC BY-SA 4.0
72194482,1,-1.0,2022-05-11 01:32:14,0,43,"<p>I have a question regarding what approach to building a predictive model in R would be best for my data.</p>
<p>Say I have a series of orders per month for the past 5 years. The data have three variables- month, year and sum or orders.</p>
<p>What is the best way to build a model that will predict the number of orders for next month based on the number of orders over the past 6 months <strong>and</strong> the normal seasonal peaks and troughs for the number of orders? What is the best way to approach this problem using R?</p>
<p>Unfortunately I do not have the data at hand, but am just asking generally how to approach this problem in R.</p>
<p>Thanks in advance.</p>
",18800160.0,18800160.0,2022-05-11 04:32:21,2022-05-11 04:32:21,Approach to predictive modelling of monthly orders using R,<r><machine-learning><regression><data-science><predictive>,0,5,N/A,CC BY-SA 4.0
72206296,1,-1.0,2022-05-11 19:00:49,1,122,"<p>I have a database of documents where searching quickly for keywords and patterns would be very useful to have.</p>
<p>I know of &quot;Burrows–Wheeler transform&quot;/FM-index. I wonder if there are any programs or database programs based on BWT or similar methods in order to search a corpus in O(1) and hopefully more advantages.</p>
<p>Any ideas?</p>
",12603110.0,-1.0,N/A,2022-05-12 15:22:08,Database that allows full text search in O(1),<database><data-structures><data-science><full-text-search><corpus>,1,2,N/A,CC BY-SA 4.0
72206460,1,72208990.0,2022-05-11 19:15:10,0,1068,"<p>I used pandas to read a CSV file of the stock data of Tesla and plotted a line chart of the price in 2021. Also, I have  <code>dates = []</code> with a collection of dates, which I want to plot in the same line chart with vertical lines.</p>
<p>My code:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt


&quot;&quot;&quot;
Stock Data
&quot;&quot;&quot;
path_to_stock_file = &quot;/path/to/file/data/TSLA-USD.csv&quot;
stock_df = pd.read_csv(path_to_stock_file)
in_year = stock_df[stock_df[&quot;Date&quot;].str.contains(&quot;2021&quot;)]

dates = [] # this list consists of a collection of dates 

&quot;&quot;&quot;
Plot
&quot;&quot;&quot;
in_year[&quot;Close&quot;].plot()
plt.figure(figsize=(100, 10))
in_year.index = in_year[&quot;Date&quot;]
plt.plot(in_year.index, in_year[&quot;Close&quot;])

for d in dates:
    plt.axvline(x=f&quot;{d}&quot;, color=&quot;green&quot;)


plt.xlabel(&quot;date&quot;)
plt.ylabel(&quot;$ price&quot;)
plt.xticks(rotation=45)
plt.title(&quot;Tesla Stock Price 2021&quot;)
plt.show()
</code></pre>
<p>Data used for plot:</p>
<pre><code># stock date/price for line chart
{
    &quot;2021-01-04&quot;: 729.77002,
    &quot;2021-01-05&quot;: 735.109985,
    &quot;2021-01-06&quot;: 755.97998,
    &quot;2021-01-07&quot;: 816.039978,
    &quot;2021-01-08&quot;: 880.02002,
    &quot;2021-01-11&quot;: 811.190002,
    &quot;2021-01-12&quot;: 849.440002,
    &quot;2021-01-13&quot;: 854.409973,
    &quot;2021-01-14&quot;: 845.0,
    &quot;2021-01-15&quot;: 826.159973,
    &quot;2021-01-19&quot;: 844.549988,
    &quot;2021-01-20&quot;: 850.450012,
    &quot;2021-01-21&quot;: 844.98999,
    &quot;2021-01-22&quot;: 846.640015,
    &quot;2021-01-25&quot;: 880.799988,
    &quot;2021-01-26&quot;: 883.090027,
    &quot;2021-01-27&quot;: 864.159973,
    &quot;2021-01-28&quot;: 835.429993,
    &quot;2021-01-29&quot;: 793.530029,
    &quot;2021-02-01&quot;: 839.809998,
    &quot;2021-02-02&quot;: 872.789978,
    &quot;2021-02-03&quot;: 854.690002,
    &quot;2021-02-04&quot;: 849.98999,
    &quot;2021-02-05&quot;: 852.22998,
    &quot;2021-02-08&quot;: 863.419983,
    &quot;2021-02-09&quot;: 849.460022,
    &quot;2021-02-10&quot;: 804.820007,
    &quot;2021-02-11&quot;: 811.659973,
    &quot;2021-02-12&quot;: 816.119995,
    &quot;2021-02-16&quot;: 796.219971,
    &quot;2021-02-17&quot;: 798.150024,
    &quot;2021-02-18&quot;: 787.380005,
    &quot;2021-02-19&quot;: 781.299988,
    &quot;2021-02-22&quot;: 714.5,
    &quot;2021-02-23&quot;: 698.840027,
    &quot;2021-02-24&quot;: 742.02002,
    &quot;2021-02-25&quot;: 682.219971,
    &quot;2021-02-26&quot;: 675.5,
    &quot;2021-03-01&quot;: 718.429993,
    &quot;2021-03-02&quot;: 686.440002,
    &quot;2021-03-03&quot;: 653.200012,
    &quot;2021-03-04&quot;: 621.440002,
    &quot;2021-03-05&quot;: 597.950012,
    &quot;2021-03-08&quot;: 563.0,
    &quot;2021-03-09&quot;: 673.580017,
    &quot;2021-03-10&quot;: 668.059998,
    &quot;2021-03-11&quot;: 699.599976,
    &quot;2021-03-12&quot;: 693.72998,
    &quot;2021-03-15&quot;: 707.940002,
    &quot;2021-03-16&quot;: 676.880005,
    &quot;2021-03-17&quot;: 701.809998,
    &quot;2021-03-18&quot;: 653.159973,
    &quot;2021-03-19&quot;: 654.869995,
    &quot;2021-03-22&quot;: 670.0,
    &quot;2021-03-23&quot;: 662.159973,
    &quot;2021-03-24&quot;: 630.27002,
    &quot;2021-03-25&quot;: 640.390015,
    &quot;2021-03-26&quot;: 618.710022,
    &quot;2021-03-29&quot;: 611.289978,
    &quot;2021-03-30&quot;: 635.619995,
    &quot;2021-03-31&quot;: 667.929993,
    &quot;2021-04-01&quot;: 661.75,
    &quot;2021-04-05&quot;: 691.049988,
    &quot;2021-04-06&quot;: 691.619995,
    &quot;2021-04-07&quot;: 670.969971,
    &quot;2021-04-08&quot;: 683.799988,
    &quot;2021-04-09&quot;: 677.02002,
    &quot;2021-04-12&quot;: 701.97998,
    &quot;2021-04-13&quot;: 762.320007,
    &quot;2021-04-14&quot;: 732.22998,
    &quot;2021-04-15&quot;: 738.849976,
    &quot;2021-04-16&quot;: 739.780029,
    &quot;2021-04-19&quot;: 714.630005,
    &quot;2021-04-20&quot;: 718.98999,
    &quot;2021-04-21&quot;: 744.119995,
    &quot;2021-04-22&quot;: 719.690002,
    &quot;2021-04-23&quot;: 729.400024,
    &quot;2021-04-26&quot;: 738.200012,
    &quot;2021-04-27&quot;: 704.73999,
    &quot;2021-04-28&quot;: 694.400024,
    &quot;2021-04-29&quot;: 677.0,
    &quot;2021-04-30&quot;: 709.440002,
    &quot;2021-05-03&quot;: 684.900024,
    &quot;2021-05-04&quot;: 673.599976,
    &quot;2021-05-05&quot;: 670.940002,
    &quot;2021-05-06&quot;: 663.539978,
    &quot;2021-05-07&quot;: 672.369995,
    &quot;2021-05-10&quot;: 629.039978,
    &quot;2021-05-11&quot;: 617.200012,
    &quot;2021-05-12&quot;: 589.890015,
    &quot;2021-05-13&quot;: 571.690002,
    &quot;2021-05-14&quot;: 589.73999,
    &quot;2021-05-17&quot;: 576.830017,
    &quot;2021-05-18&quot;: 577.869995,
    &quot;2021-05-19&quot;: 563.460022,
    &quot;2021-05-20&quot;: 586.780029,
    &quot;2021-05-21&quot;: 580.880005,
    &quot;2021-05-24&quot;: 606.440002,
    &quot;2021-05-25&quot;: 604.690002,
    &quot;2021-05-26&quot;: 619.130005,
    &quot;2021-05-27&quot;: 630.849976,
    &quot;2021-05-28&quot;: 625.219971,
    &quot;2021-06-01&quot;: 623.900024,
    &quot;2021-06-02&quot;: 605.119995,
    &quot;2021-06-03&quot;: 572.840027,
    &quot;2021-06-04&quot;: 599.049988,
    &quot;2021-06-07&quot;: 605.130005,
    &quot;2021-06-08&quot;: 603.590027,
    &quot;2021-06-09&quot;: 598.780029,
    &quot;2021-06-10&quot;: 610.119995,
    &quot;2021-06-11&quot;: 609.890015,
    &quot;2021-06-14&quot;: 617.690002,
    &quot;2021-06-15&quot;: 599.359985,
    &quot;2021-06-16&quot;: 604.869995,
    &quot;2021-06-17&quot;: 616.599976,
    &quot;2021-06-18&quot;: 623.309998,
    &quot;2021-06-21&quot;: 620.830017,
    &quot;2021-06-22&quot;: 623.710022,
    &quot;2021-06-23&quot;: 656.570007,
    &quot;2021-06-24&quot;: 679.820007,
    &quot;2021-06-25&quot;: 671.869995,
    &quot;2021-06-28&quot;: 688.719971,
    &quot;2021-06-29&quot;: 680.76001,
    &quot;2021-06-30&quot;: 679.700012,
    &quot;2021-07-01&quot;: 677.919983,
    &quot;2021-07-02&quot;: 678.900024,
    &quot;2021-07-06&quot;: 659.580017,
    &quot;2021-07-07&quot;: 644.650024,
    &quot;2021-07-08&quot;: 652.809998,
    &quot;2021-07-09&quot;: 656.950012,
    &quot;2021-07-12&quot;: 685.700012,
    &quot;2021-07-13&quot;: 668.539978,
    &quot;2021-07-14&quot;: 653.380005,
    &quot;2021-07-15&quot;: 650.599976,
    &quot;2021-07-16&quot;: 644.219971,
    &quot;2021-07-19&quot;: 646.219971,
    &quot;2021-07-20&quot;: 660.5,
    &quot;2021-07-21&quot;: 655.289978,
    &quot;2021-07-22&quot;: 649.26001,
    &quot;2021-07-23&quot;: 643.380005,
    &quot;2021-07-26&quot;: 657.619995,
    &quot;2021-07-27&quot;: 644.780029,
    &quot;2021-07-28&quot;: 646.97998,
    &quot;2021-07-29&quot;: 677.349976,
    &quot;2021-07-30&quot;: 687.200012,
    &quot;2021-08-02&quot;: 709.669983,
    &quot;2021-08-03&quot;: 709.73999,
    &quot;2021-08-04&quot;: 710.919983,
    &quot;2021-08-05&quot;: 714.630005,
    &quot;2021-08-06&quot;: 699.099976,
    &quot;2021-08-09&quot;: 713.76001,
    &quot;2021-08-10&quot;: 709.98999,
    &quot;2021-08-11&quot;: 707.820007,
    &quot;2021-08-12&quot;: 722.25,
    &quot;2021-08-13&quot;: 717.169983,
    &quot;2021-08-16&quot;: 686.169983,
    &quot;2021-08-17&quot;: 665.710022,
    &quot;2021-08-18&quot;: 688.98999,
    &quot;2021-08-19&quot;: 673.469971,
    &quot;2021-08-20&quot;: 680.26001,
    &quot;2021-08-23&quot;: 706.299988,
    &quot;2021-08-24&quot;: 708.48999,
    &quot;2021-08-25&quot;: 711.200012,
    &quot;2021-08-26&quot;: 701.159973,
    &quot;2021-08-27&quot;: 711.919983,
    &quot;2021-08-30&quot;: 730.909973,
    &quot;2021-08-31&quot;: 735.719971,
    &quot;2021-09-01&quot;: 734.090027,
    &quot;2021-09-02&quot;: 732.390015,
    &quot;2021-09-03&quot;: 733.570007,
    &quot;2021-09-07&quot;: 752.919983,
    &quot;2021-09-08&quot;: 753.869995,
    &quot;2021-09-09&quot;: 754.859985,
    &quot;2021-09-10&quot;: 736.27002,
    &quot;2021-09-13&quot;: 743.0,
    &quot;2021-09-14&quot;: 744.48999,
    &quot;2021-09-15&quot;: 755.830017,
    &quot;2021-09-16&quot;: 756.98999,
    &quot;2021-09-17&quot;: 759.48999,
    &quot;2021-09-20&quot;: 730.169983,
    &quot;2021-09-21&quot;: 739.380005,
    &quot;2021-09-22&quot;: 751.940002,
    &quot;2021-09-23&quot;: 753.640015,
    &quot;2021-09-24&quot;: 774.390015,
    &quot;2021-09-27&quot;: 791.359985,
    &quot;2021-09-28&quot;: 777.559998,
    &quot;2021-09-29&quot;: 781.309998,
    &quot;2021-09-30&quot;: 775.47998,
    &quot;2021-10-01&quot;: 775.219971,
    &quot;2021-10-04&quot;: 781.530029,
    &quot;2021-10-05&quot;: 780.590027,
    &quot;2021-10-06&quot;: 782.75,
    &quot;2021-10-07&quot;: 793.609985,
    &quot;2021-10-08&quot;: 785.48999,
    &quot;2021-10-11&quot;: 791.940002,
    &quot;2021-10-12&quot;: 805.719971,
    &quot;2021-10-13&quot;: 811.080017,
    &quot;2021-10-14&quot;: 818.320007,
    &quot;2021-10-15&quot;: 843.030029,
    &quot;2021-10-18&quot;: 870.109985,
    &quot;2021-10-19&quot;: 864.27002,
    &quot;2021-10-20&quot;: 865.799988,
    &quot;2021-10-21&quot;: 894.0,
    &quot;2021-10-22&quot;: 909.679993,
    &quot;2021-10-25&quot;: 1024.859985,
    &quot;2021-10-26&quot;: 1018.429993,
    &quot;2021-10-27&quot;: 1037.859985,
    &quot;2021-10-28&quot;: 1077.040039,
    &quot;2021-10-29&quot;: 1114.0,
    &quot;2021-11-01&quot;: 1208.589966,
    &quot;2021-11-02&quot;: 1172.0,
    &quot;2021-11-03&quot;: 1213.859985,
    &quot;2021-11-04&quot;: 1229.910034,
    &quot;2021-11-05&quot;: 1222.089966,
    &quot;2021-11-08&quot;: 1162.939941,
    &quot;2021-11-09&quot;: 1023.5,
    &quot;2021-11-10&quot;: 1067.949951,
    &quot;2021-11-11&quot;: 1063.51001,
    &quot;2021-11-12&quot;: 1033.420044,
    &quot;2021-11-15&quot;: 1013.390015,
    &quot;2021-11-16&quot;: 1054.72998,
    &quot;2021-11-17&quot;: 1089.01001,
    &quot;2021-11-18&quot;: 1096.380005,
    &quot;2021-11-19&quot;: 1137.060059,
    &quot;2021-11-22&quot;: 1156.869995,
    &quot;2021-11-23&quot;: 1109.030029,
    &quot;2021-11-24&quot;: 1116.0,
    &quot;2021-11-26&quot;: 1081.920044,
    &quot;2021-11-29&quot;: 1136.98999,
    &quot;2021-11-30&quot;: 1144.76001,
    &quot;2021-12-01&quot;: 1095.0,
    &quot;2021-12-02&quot;: 1084.599976,
    &quot;2021-12-03&quot;: 1014.969971,
    &quot;2021-12-06&quot;: 1009.01001,
    &quot;2021-12-07&quot;: 1051.75,
    &quot;2021-12-08&quot;: 1068.959961,
    &quot;2021-12-09&quot;: 1003.799988,
    &quot;2021-12-10&quot;: 1017.030029,
    &quot;2021-12-13&quot;: 966.409973,
    &quot;2021-12-14&quot;: 958.51001,
    &quot;2021-12-15&quot;: 975.98999,
    &quot;2021-12-16&quot;: 926.919983,
    &quot;2021-12-17&quot;: 932.570007,
    &quot;2021-12-20&quot;: 899.940002,
    &quot;2021-12-21&quot;: 938.530029,
    &quot;2021-12-22&quot;: 1008.869995,
    &quot;2021-12-23&quot;: 1067.0,
    &quot;2021-12-27&quot;: 1093.939941,
    &quot;2021-12-28&quot;: 1088.469971,
    &quot;2021-12-29&quot;: 1086.189941,
    &quot;2021-12-30&quot;: 1070.339966,
    &quot;2021-12-31&quot;: 1056.780029,
}

# specific dates for vertical lines
[
    &quot;2021-12-24&quot;,
    &quot;2021-12-23&quot;,
    &quot;2021-12-23&quot;,
    &quot;2021-12-23&quot;,
    &quot;2021-12-18&quot;,
    &quot;2021-12-14&quot;,
    &quot;2021-12-06&quot;,
    &quot;2021-12-01&quot;,
    &quot;2021-11-06&quot;,
    &quot;2021-11-01&quot;,
    &quot;2021-10-08&quot;,
    &quot;2021-10-08&quot;,
    &quot;2021-10-02&quot;,
    &quot;2021-09-10&quot;,
    &quot;2021-09-10&quot;,
    &quot;2021-08-05&quot;,
    &quot;2021-07-29&quot;,
    &quot;2021-07-10&quot;,
    &quot;2021-07-02&quot;,
    &quot;2021-06-24&quot;,
    &quot;2021-06-21&quot;,
    &quot;2021-06-11&quot;,
    &quot;2021-05-20&quot;,
    &quot;2021-05-19&quot;,
    &quot;2021-05-13&quot;,
    &quot;2021-05-11&quot;,
    &quot;2021-04-22&quot;,
    &quot;2021-04-22&quot;,
    &quot;2021-04-22&quot;,
    &quot;2021-04-18&quot;,
    &quot;2021-04-15&quot;,
    &quot;2021-04-08&quot;,
    &quot;2021-03-24&quot;,
    &quot;2021-03-24&quot;,
    &quot;2021-03-14&quot;,
    &quot;2021-03-07&quot;,
    &quot;2021-03-06&quot;,
    &quot;2021-02-07&quot;,
    &quot;2021-01-02&quot;,
]
</code></pre>
<p>Resulting chart:</p>
<p><a href=""https://i.stack.imgur.com/iAPeO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iAPeO.png"" alt=""enter image description here"" /></a></p>
<p>As you can see, only the dates from <code>dates = []</code>, which also have an associated date in the stock data have been plotted properly. All dates from <code>dates = []</code>, which <strong>doesn't have</strong> an associated date on the x-axis (stock price dates) have been plotted at the end of the chart.</p>
<p>Since the stock market has trading hours the data will always have some gaps with no date/price available.</p>
<p>Does anyone have an idea how I can solve the problem otherwise?</p>
",17309720.0,17309720.0,2022-05-11 20:23:59,2023-06-14 18:14:02,Python/Matplotlib plot vertical line for specific dates in line chart,<python><pandas><matplotlib><data-science>,1,4,2022-05-12 05:02:59,CC BY-SA 4.0
72211153,1,-1.0,2022-05-12 06:46:16,-2,50,"<p>I'm trying to convert Rs. 2,92,667 to integer type but I'm confused like how to remove Rs. and commas from it .</p>
<p><code> cars = [int(x.split('Rs. ')[-1]) for x in cars['Ex-Showroom_Price']]</code></p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-14-b9318b3d5174&gt; in &lt;module&gt;
----&gt; 1 cars = [int(x.split('Rs. ')[-1]) for x in cars['Ex-Showroom_Price']]

&lt;ipython-input-14-b9318b3d5174&gt; in &lt;listcomp&gt;(.0)
----&gt; 1 cars = [int(x.split('Rs. ')[-1]) for x in cars['Ex-Showroom_Price']]

ValueError: invalid literal for int() with base 10: '2,92,667'
</code></pre>
<p><a href=""https://i.stack.imgur.com/9wZkB.png"" rel=""nofollow noreferrer""><strong>This is what I tried</strong></a></p>
",16854609.0,-1.0,N/A,2022-05-12 08:03:47,"How to convert categorical data like Rs. 2,92,667	to integer and float type?",<python><machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
72229631,1,72229891.0,2022-05-13 12:29:47,0,37,"<p>I'm an Environmental Engineer, trying to make a leap change to the data science area which interests me more.</p>
<p>I'm new to Python, I work at a company that evaluates air quality data and I think that if I automate the analysis, I should save some time.</p>
<p>I've imported the CSV files with environmental data from the past month, did some filter in that just to make sure that the data were okay and did a groupby just analyse the data day-to-day (I need that in my report for the regulatory agency).</p>
<p>The step by step of what I did:</p>
<pre><code>medias = tabela.groupby(by=[&quot;Data&quot;]).mean()
display (tabela)
</code></pre>
<p><a href=""https://i.stack.imgur.com/hgvpq.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>As you can see there's a column named <code>Data</code>, but when I do the info check it not recognizes the <code>Data</code> as a column.</p>
<pre><code>print (medias.info())
</code></pre>
<p><a href=""https://i.stack.imgur.com/BEUyL.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>How can I solve this? I need to plot some graphs with the concentration of rain and dust per day.</p>
",19109820.0,13302.0,2022-05-15 12:08:47,2022-05-15 12:08:47,How do i plot a bar graphic with the groupby made column,<python><plotly><data-science>,1,0,N/A,CC BY-SA 4.0
72219528,1,72222945.0,2022-05-12 16:59:43,3,125,"<p>I'm trying to implement Integer Programming for Nearest Neighbor Classifier in python using <code>cvxpy</code>.</p>
<h2>Short intro</h2>
<p>Given a dataset of <code>n</code> points with a color (red or blue) we would like to choose the minimal number of candidate points, s.t for each point that isn`t a candidate, its closest candidate has the same color.</p>
<h2>My flow</h2>
<p>Given a set of <code>n</code> points (with colors) define an indicator vector <code>I</code> (<code>|I| = n</code>),</p>
<pre><code>I_i = 1 if and only if vertex i is chosen as a candidate
</code></pre>
<p>In addition, I defined two more vectors, named as <code>A</code> and <code>B</code> (<code>|A| = |B| = n</code>) as follow:</p>
<pre><code>A_i = the distance between v_i to it's closest candidate with the **same** color
</code></pre>
<pre><code>B_i = the distance between v_i to it's closest candidate with a **different** color
</code></pre>
<p>Therefore, I have <code>n</code> constrains which are:
<code>B_i &gt; A_i</code>
for any <code>i</code></p>
<p>My target is to minimize the sum of vector <code>I</code> (which represents the number of candidates)</p>
<h2>My Issue</h2>
<p>Its seems that the vectors <code>A</code>, <code>B</code> are changing because they affected by <code>I</code>, since when a candidate is chosen, it is affecting its entry in <code>I</code> which affects <code>A</code> and <code>B</code> and the constrains are dependent on those vectors..</p>
<p>Any suggestions?</p>
<p>Thanks !</p>
",9547104.0,-1.0,N/A,2022-05-14 15:03:36,Integer Programming for NNC,<python><algorithm><data-science><linear-programming><cvxpy>,1,0,N/A,CC BY-SA 4.0
72219542,1,-1.0,2022-05-12 17:00:30,0,108,"<h1>assignment:Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</h1>
<pre><code>&lt;p&gt;&lt;pre&gt;Initialize the weight_vector and intercept term to zeros (Write your code in def initialize_weights())

Create a loss function (Write your code in def logloss())

logloss=−1∗1nΣforeachYt,Ypred(Ytlog10(Ypred)+(1−Yt)log10(1−Ypred))

for each epoch:

    for each batch of data points in train: (keep batch size=1)

        calculate the gradient of loss function w.r.t each weight in weight vector (write your code in def gradient_dw())

        dw(t)=xn(yn−σ((w(t))Txn+bt))−λNw(t))

        Calculate the gradient of the intercept (write your code in def gradient_db()) check this

        db(t)=yn−σ((w(t))Txn+bt))

        Update weights and intercept (check the equation number 32 in the above mentioned pdf):
        w(t+1)←w(t)+α(dw(t))

        b(t+1)←b(t)+α(db(t))

    calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)

    And if you wish, you can compare the previous loss and the current loss, if it is not updating, then you can stop the training

    append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )&lt;/pre&gt;&lt;/p&gt;
</code></pre>
<p>importing libraries</p>
<pre><code>  import numpy as np
  import pandas as pd
  from sklearn.datasets import make_classification
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn import linear_model
</code></pre>
<p>Creating custom dataset</p>
<pre><code>    X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,
                               n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)
</code></pre>
<p>Splitting data into train and test</p>
<pre><code>    # you need not standardize the data as it is already standardized
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)



    def initialize_weights(row_vector):
        ''' In this function, we will initialize our weights and bias'''
        #initialize the weights as 1d array consisting of all zeros similar to the dimensions of row_vector
        #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html
        #initialize bias to zero
        w = np.zeros_like(X_train[0])
        b=0
        return w,b
    dim=X_train[0] 
    w,b = initialize_weights(dim)
    print('w =',(w))
    print('b =',str(b))
    def sigmoid(z):
        ''' In this function, we will return sigmoid of z'''
        # compute sigmoid(z) and return
        return 1 /(1+np.exp(-z))
    def logloss(y_true,y_pred):
        # you have been given two arrays y_true and y_pred and you have to calculate the logloss
        #while dealing with numpy arrays you can use vectorized operations for quicker calculations as compared to using loops
        #https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html
        #https://www.geeksforgeeks.org/vectorized-operations-in-numpy/
        #write your code here
        
        sum=0
        for i in range(len(y_true)):
             sum += (y_true[i] * np.log10(y_pred[i])) + ((1 - y_true[i]) * np.log10(1 - y_pred[i]))
        loss = -1 * (1 / len(y_true)) * sum
         
        return loss
    #make sure that the sigmoid function returns a scalar value, you can use dot function operation
    def gradient_dw(x,y,w,b,alpha,N):
        '''In this function, we will compute the gardient w.r.to w '''
        dw = x * (y - sigmoid(np.dot(w,x) + b) - (alpha / N) * w)
        
        return dw
    #sb should be a scalar value
    def gradient_db(x,y,w,b):
         '''In this function, we will compute gradient w.r.to b '''
         db = y - sigmoid(np.dot(w,x)+ b)  
         
         return db
    # prediction function used to compute predicted_y given the dataset X
    def pred(w,b, X):
        N = len(X)
        predict = []
        for i in range(N):
            z=np.dot(w,X[i])+b
            predict.append(sigmoid(z))
        return np.array(predict)
    def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):
      
        ''' In this function, we will implement logistic regression'''
        #Here eta0 is learning rate
        #implement the code as follows
        
        # for every epoch
            # for every data point(X_train,y_train)
               #compute gradient w.r.to w (call the gradient_dw() function)
               #compute gradient w.r.to b (call the gradient_db() function)
               #update w, b
            # predict the output of x_train [for all data points in X_train] using pred function with updated weights
            #compute the loss between predicted and actual values (call the loss function)
            # store all the train loss values in a list
            # predict the output of x_test [for all data points in X_test] using pred function with updated weights
            #compute the loss between predicted and actual values (call the loss function)
            # store all the test loss values in a list
            # you can also compare previous loss and current loss, if loss is not updating then stop the process 
            # you have to return w,b , train_loss and test loss
            
        train_loss = []
        test_loss = []
      
        # initalize the weights (call the initialize_weights(X_train[0]) function)
        w,b = initialize_weights(X_train[0]) # Initialize the weights
        #write your code to perform SGD
        # for every epoch
        for i in range(epochs):
            
            train_pred = []
            test_pred = []
            # for every data point(X_train,y_train)
            for j in range(N):
              #compute gradient w.r.to w (call the gradient_dw() function)
              dw= gradient_dw(X_train[j],y_train[j],w,b,alpha,N)
              
              #compute gradient w.r.to b (call the gradient_db() function)
              db = gradient_db(X_train[j],y_train[j],w,b)
              
              #update w, b
              w= w+ (eta0* dw)
              b =b+ (eta0* db)
    
    #predict the output of x_train [for all data points in X_train] using pred function with updated weights
    
            for k in range(0,N):
                
                w1_predict = pred(w,b,X_train[k])
                train_pred.append(w1_predict)
    ##compute the loss between predicted and actual values (call the loss function)
            loss_pred1 = logloss(y_train,train_pred)
            train_loss.append(loss_pred1)
    # predict the output of x_test [for all data points in X_test] using pred function with updated weights
            for k in range(len(X_test)):
                
                w2_predict = pred(w,b,X_test[k])
                test_pred.append(w2_predict)
    #compute the loss between predicted and actual values (call the loss function)        
            loss_pred2 = logloss(y_test,test_pred)
            test_loss.append(loss_pred2)
    
        return w,b,train_loss,test_loss
    alpha=0.001
    eta0=0.001
    N=len(X_train)
    epochs=6
    w,b,train_loss,test_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)

    from matplotlib import pyplot as plt
    epoch = [i for i in range(1,epochs+1,1)]
    plt.figure(figsize=(8,6))
    plt.grid()
    plt.plot(epoch,train_loss , label='train log loss')
    plt.plot(epoch,test_loss, label='test log loss')
    plt.xlabel(&quot;epoch number&quot;)
    plt.ylabel(&quot;log loss&quot;)
    plt.title(&quot;log loss curve of logistic regression&quot;)
    plt.legend()
    plt.show
</code></pre>
<p>I am getting this below error. I used grader function check all the function, its coming true. However. I am getting an below error while running the code. I tried use reshape to change the shape of train loss. still getting the error. Please help  </p></p>
<pre><code>    ---------------------------------------------------------------------------
    
    ValueError                                Traceback (most recent call last)
    
    &lt;ipython-input-28-52c352e46321&gt; in &lt;module&gt;()
          1 plt.figure(figsize=(8,6))
          2 plt.grid()
    ----&gt; 3 plt.plot(epoch,train_loss , label='train log loss')
          4 plt.plot(epoch,test_loss, label='test log loss')
          5 plt.xlabel(&quot;epoch number&quot;)
    
    3 frames
    
    /usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py in _plot_args(self, tup, kwargs)
        340 
        341         if x.shape[0] != y.shape[0]:
    --&gt; 342             raise ValueError(f&quot;x and y must have same first dimension, but &quot;
        343                              f&quot;have shapes {x.shape} and {y.shape}&quot;)
        344         if x.ndim &gt; 2 or y.ndim &gt; 2:

ValueError: x and y must have same first dimension, but have shapes (6,) and (1350,)
</code></pre>
",18948317.0,18948317.0,2022-05-12 17:21:47,2022-05-12 17:21:47,"ValueError: x and y can be no greater than 2-D, but have shapes (6,) and (6, 15, 15)",<python><data-science><linear-regression><sgd>,0,2,N/A,CC BY-SA 4.0
72223744,1,-1.0,2022-05-13 02:07:58,1,674,"<p>I am trying to collect the community feed data from a channel for analytics.
I couldn't find a way using the YouTube Data API v3. Is there a way to extract such data?</p>
<p>The data I would like to extract is in the community feed. For instance, it's a community feed from <a href=""https://www.youtube.com/user/westlandindia"" rel=""nofollow noreferrer"">Dr. Bharatendra Rai channel</a>. I would like to collect all his feed.
<a href=""https://i.stack.imgur.com/DQgGs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DQgGs.png"" alt=""Dr. Bharatendra Rai community post"" /></a></p>
",12832956.0,7123660.0,2022-10-16 02:44:31,2022-12-07 00:19:53,Extract YouTube Channel Community Feed Data,<youtube><youtube-api><data-science><youtube-data-api>,1,2,N/A,CC BY-SA 4.0
72239362,1,-1.0,2022-05-14 10:28:33,0,147,"<p>My y going in and both <code>y_train</code> and <code>y_eval</code> are binary int, what am I doing wrong?</p>
<p>I noticed the predictions going out are like this <code>[0.,1.,0. ...]</code> which is probably the culprit of the problem; but I'm not sure what is causing it.</p>
<pre><code>lgbm_clf = lgbm.LGBMClassifier(
    objective=&quot;binary&quot;,
    random_state=1,
    n_estimators=10000,
    boosting=&quot;gbdt&quot;,
    is_unbalance = True,
    metric= None)

lgbm_clf.fit(
        X_train,
        y_train,
        eval_set=[(X_eval, y_eval)],
        eval_metric=evalerror,
        early_stopping_rounds=150)
    
preds = lgbm_clf.predict(X_eval)
print(f&quot;LightGBM AUC on the evaluation set: {roc_auc_score(y_eval, preds):.5f}&quot;)
</code></pre>
<p>my custom metric is:</p>
<pre><code>def evalerror(preds, y_eval):
  tn, fp, fn, tp = confusion_matrix(y_eval, preds).ravel()
  return 'credimiMetric',((3*tp - (3*fp) - (50*fn))*1000), True
</code></pre>
<p>Any idea why I'm getting this error when fitting?</p>
<pre><code>Classification metrics can't handle a mix of continuous and binary targets
</code></pre>
",12150292.0,1573267.0,2022-05-14 11:01:24,2022-05-14 11:01:24,"""Classification metrics can't handle a mix of continuous and binary targets"" when trying to set a custom eval_metric using LGBMClassifier",<scikit-learn><data-science><lightgbm>,0,2,N/A,CC BY-SA 4.0
73054567,1,73054799.0,2022-07-20 15:43:39,1,56,"<p>I have two data frames where first dataframe has index starting from zero. The second dataframe has repeated indexes starting from zero. I want to to join the two dataframes based on their indexes.
First dataframe is like this</p>
<pre><code>      Start_Year    End_Year
0      1500      1500
1      1500      1501
2      1500      1700
3      1500      1800
4      1500      1800
... ... ...
3409    2018    2018
3410    2018    2018
3411    2019    2019
3412    2019    2022
3413    2020    2020
3414 rows × 2 columns
</code></pre>
<p>The second dataframe is</p>
<pre><code>0                       [KingdomofPoland, Georgia]
0                 [GrandDuchyofLithuania, Georgia]
1                   [NorthernYuanDynasty, Georgia]
2                 [SpanishEmpire, ChechenRepublic]
2       [CaptaincyGeneralofChile, ChechenRepublic]
                           ...                    
3411             [SyrianOpposition, SpanishEmpire]
3412                 [UnitedStates, SpanishEmpire]
3412                [UnitedKingdom, SpanishEmpire]
3412                  [SaudiArabia, SpanishEmpire]
3413                              [Turkey, Russia]
Length: 31170, dtype: object
</code></pre>
<p>I want to join these two dataframes based on index i.e the new dataframe should look like</p>
<pre><code>      Start_Year    End_Year        new_col
0      1500         1500        [KingdomofPoland, Georgia]
0      1500         1500        [GrandDuchyofLithuania, Georgia]
1      1500         1501        [NorthernYuanDynasty, Georgia]
2      1500         1700        [SpanishEmpire, ChechenRepublic]
2      1500         1700        [CaptaincyGeneralofChile, ChechenRepublic]
......
3411    2019        2019        [SyrianOpposition, SpanishEmpire]
3412    2019        2022        [UnitedStates, SpanishEmpire]
3412    2019        2022        [UnitedKingdom, SpanishEmpire]
3412    2019        2022        [SaudiArabia, SpanishEmpire]
.......
</code></pre>
<p>What this essentially is I need to replicate the rows of dataframe 1 based on how many times the same index is repeated in the second dataframe. As we can see, in second dataframe, zero index appears twice, so we replicate the rows of zero index of dataframe 1 twice and then join the dataframes and so on. In the end we can reset the index(that I know about).</p>
<p>I am attaching the links of both the dataframes for the reference.
Link for first dataframe <a href=""https://drive.google.com/file/d/1DqxhnMM8R21Olm9zeRJeDgua_ozoRp8P/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1DqxhnMM8R21Olm9zeRJeDgua_ozoRp8P/view?usp=sharing</a></p>
<p>Link for second dataframe <a href=""https://drive.google.com/file/d/1sX5xcTeovVqXtZgSZ5cTC5JRdUvaw7gd/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1sX5xcTeovVqXtZgSZ5cTC5JRdUvaw7gd/view?usp=sharing</a></p>
<p>I cant figure out how to proceed with such tasks. Please help me out.</p>
",19486982.0,-1.0,N/A,2022-07-20 16:01:55,Join to dataframes based on index where the second dataframe has repeated indexes related to the first dataframe,<python><pandas><dataframe><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
73061491,1,-1.0,2022-07-21 06:15:08,-1,579,"<p>Typically, when assessing customer churn, static data is used, for example, recent or average customer spending data, customer characteristics, etc. This approach does not allow tracking the activity of one client in dynamics: decrease/increase in his expenses, changes in location, tariffs, etc.</p>
<p>Question: Is there a modeling approach that takes into account such changes? My dataset looks like this:
<a href=""https://i.stack.imgur.com/HXn5i.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Each client has a different number of lines showing the dynamics of their activity. How can you predict whether a client will leave next month given all his data (all the lines that refer to him)?</p>
",19592016.0,-1.0,N/A,2022-07-21 07:41:54,Churn prediction with time series. Analyze client behavior in dynamic,<machine-learning><data-science><prediction><churn>,1,0,N/A,CC BY-SA 4.0
72241097,1,-1.0,2022-05-14 14:24:50,1,405,"<p>I ran a series of simulations and want to create a response surface of the performance based off my two parameters, tol and eta. The issue I'm having is actually plotting this in python. I have the vectors vec_tol (of dimension nx1) and vec_eta (mx1), and two matrices of the performance t_osa_adj (dimension nxm) and sdn_osa_sol (also nxm). I would like to plot a surface plot of [x,y,z] = [vec_tol,vec_eta,t_osa_adj] colored by sdn_osa_sol and I keep running into this error in python:</p>
<p>&quot;ValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (16,) and arg 1 with shape (10,).&quot;</p>
<p>My understanding is that this is because for the plot_surface command to work n must be equal to m. How do I fix this? I have put a snippet of the code below.</p>
<pre><code>fig = plt.figure(figsize = (8, 7), facecolor='white')
ax = fig.add_subplot(projection='3d')
surf = ax.plot_surface(vec_tol, vec_eta, t_osa_adj, facecolors = cm.jet(np.log(sdn_osa_sol)), linewidth=0, antialiased=False)
</code></pre>
<p>Thank you.</p>
",3726556.0,-1.0,N/A,2022-05-14 14:24:50,Plot Python surface with non-square data,<python-3.x><matplotlib><data-science>,0,3,N/A,CC BY-SA 4.0
73052929,1,-1.0,2022-07-20 13:52:18,0,215,"<pre><code>bach_sal = bach['Salary']
masters_sal = masters['Salary']
phd_sal = phd['Salary']
deg_category_sal = [assoc_sal, bach_sal, masters_sal, phd_sal]

x = np.array(assoc['Person'])
y = np.array(assoc_sal)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.7)
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(x_train.reshape(-1,1),y_train)
print (lr.coef_)
print (lr.intercept_)
</code></pre>
<p>I received the coefficient and y intercept but everytime I run the code, they change to a different number. Please help. Thank you.</p>
",19559886.0,-1.0,N/A,2022-08-06 15:23:29,Linear Regression's coefficient and intercept kept changing,<python><machine-learning><scikit-learn><statistics><data-science>,2,1,N/A,CC BY-SA 4.0
73065674,1,-1.0,2022-07-21 11:52:15,0,182,"<p>I'm a data scientist, and I make apps using python. Where I make a lot of connections with databases, build and deploy my code on the cloud and I use models training and visualization.
My question is:
Is Django useful for making non-web apps?
if not are they any other frameworks? (similar to spring-boot for java)
Best regards</p>
",7875029.0,-1.0,N/A,2022-07-21 12:24:35,Can I use Django to make non-web python apps as a data scientist?,<python><django><data-science>,2,1,N/A,CC BY-SA 4.0
73065972,1,-1.0,2022-07-21 12:13:25,-2,59,"<p>I am trying to extract the table from this website  When I am scraping, it is not giving the full html script. also the table tag has no class or id in it. Can anyone suggest how to extract? <a href=""https://i.stack.imgur.com/irOrv.jpg"" rel=""nofollow noreferrer"">https://www.jaivikkheti.in/inputsupplier</a></p>
<p><a href=""https://i.stack.imgur.com/irOrv.jpg"" rel=""nofollow noreferrer"">enter image description here</a>
[url]: <a href=""https://www.jaivikkheti.in/inputsupplier"" rel=""nofollow noreferrer"">https://www.jaivikkheti.in/inputsupplier</a></p>
<p>Code</p>
<pre><code>from bs4 import BeautifulSoup
import requests
from bs4 import BeautifulSoup

URL = url =&quot;https://www.jaivikkheti.in/inputsupplier&quot;
page = requests.get(URL)

soup = BeautifulSoup(page.content, &quot;html.parser&quot;)
  
# Reading the file and storing in a variable
contents = HTMLFileToBeOpened.read()
  
# Creating a BeautifulSoup object and
# specifying the parser 
beautifulSoupText = BeautifulSoup(contents, 'lxml')
  
  
# Using the prettify method to modify the code
#  Prettify() function in BeautifulSoup helps
# to view about the tag nature and their nesting
print(soup.body.prettify())
</code></pre>
<p>I see the data is inside some java file. because of that it is not able to extract.</p>
",17550164.0,7752296.0,2022-08-29 11:34:52,2022-08-29 11:34:52,Not scraping whole html and table,<python><html><database><escaping><data-science>,2,2,N/A,CC BY-SA 4.0
73057909,1,-1.0,2022-07-20 20:41:46,0,390,"<p>I've been trying to cluster my graph of jobs.</p>
<p>The edges weights are the count of the transitions between 2 nodes(jobs).</p>
<p>I've been reading about and I've based my code in this paper: <a href=""https://hal.archives-ouvertes.fr/hal-01887680/document"" rel=""nofollow noreferrer"">https://hal.archives-ouvertes.fr/hal-01887680/document</a></p>
<p>Code:</p>
<pre><code>G = nx.DiGraph() #Full transitions graph
G.add_weighted_edges_from(list(transitions_df.itertuples(index=False, name=None)))

H = nx.subgraph(G, list(df.query(&quot;sub_family_desc == 'ClientSupport' | sub_family_desc == 'Consulting' &quot;).code.unique())) #Gruph with only two subfamily_jobs(clusters)
pos = nx.kamada_kawai_layout(H)
weights = nx.get_edge_attributes(H, &quot;weight&quot;)

a = nx.spectral_graph_forge(H, 0.7)
adj_mat = nx.to_numpy_matrix(a)
sc = SpectralClustering(2, affinity='precomputed', n_init=100,assign_labels=&quot;kmeans&quot;,random_state=np.random.seed(1234))
sc.fit(adj_mat)
</code></pre>
<p>I also tryed to add random walks, but I have faild and couldn't pass the random walk to the SKlean Spectral Cluster</p>
<pre><code>from stellargraph import StellarGraph

#converting it to stellar graph format so we can leverage biased random walk from this library
sg_graph = StellarGraph.from_networkx(H)
print(sg_graph.info())

from stellargraph.data import BiasedRandomWalk
from gensim.models import Word2Vec

#from each singular node/job we generate 40 biased (weight-biased) random walks with a max length of 10
rw = BiasedRandomWalk(sg_graph)
#40 sequences per node
weighted_walks = rw.run(nodes=sg_graph.nodes(),length=10, n=100, p=5, q=0.05, weighted=True, seed=1234)
print(&quot;Number of random walks: {}&quot;.format(len(weighted_walks)))
</code></pre>
<p>Should I have added the random_walk in the model? How can I make it?</p>
",19590047.0,-1.0,N/A,2022-07-23 22:28:46,How can I cluster my weighted directed graph?,<machine-learning><graph><data-science><cluster-analysis><random-walk>,1,1,N/A,CC BY-SA 4.0
73060055,1,-1.0,2022-07-21 02:33:39,-1,101,"<p>I have 2 items whose purchase counts are as follows.</p>
<p>item1 = 100, 200, 300, 400, 500<br />
item2 = 500, 400, 300, 200, 100</p>
<p>The values are in order day-5, day-4, day-3, day-2, day-1. The last value is the latest.
Though the total counts are same, item1 seems to more trending. I want a technique to calculate the trending score.</p>
<p>I tried using z-score, but couldnt get the right score. I have tried exponential moving weighted average and it is shown below. But it doesnt produce the result I need. Here is the code</p>
<pre><code>alpha=0.9
item1_view_counts = [100,200,300,400,500]
item2_view_counts = [500,400,300,200,100]
count_values1 = pd.DataFrame({'count_values': item1_view_counts})
ema1 = count_values1.ewm(alpha=alpha, adjust=True).mean()
sum1 = count_values1.ewm(alpha=alpha, adjust=True).mean().tail(5).sum()
count_values2 = pd.DataFrame({'count_values': item2_view_counts})
ema2 = count_values2.ewm(alpha=alpha, adjust=True).mean()
sum2 = count_values2.ewm(alpha=alpha, adjust=True).mean().tail(5).sum()
print(&quot;ema1:&quot; + str(ema1))
print(&quot;ema2:&quot; + str(ema2))
print(&quot;score1:&quot; + str(sum1))
print(&quot;score2:&quot; + str(sum2))
</code></pre>
<p>Result</p>
<pre><code>ema1:   count_values
0    100.000000
1    190.909091
2    289.189189
3    388.928893
4    488.893889
ema2:   count_values
0    500.000000
1    409.090909
2    310.810811
3    211.071107
4    111.106111
score1:count_values    1457.921062
dtype: float64
score2:count_values    1542.078938
dtype: float64
</code></pre>
<p>I am looking for some technique that scores the item1 higher than item2.</p>
<p>Appreciate your help here.</p>
",4109433.0,-1.0,N/A,2022-07-21 04:32:15,what is good technique to calculate trending score?,<python><pandas><machine-learning><statistics><data-science>,1,4,N/A,CC BY-SA 4.0
73078080,1,-1.0,2022-07-22 09:25:32,0,32,"<p>I'm using ArangoDB for Graph-Versioning and would be looking for a faster method to evaluate whether or not a Node is the same in two different collections.
Apart from hashing each node before I write it - does ArangoDB have any mechanism that lets me read the Hash of the node?
I usually access the Database with Python-Arango.</p>
<p>If hashing it by myself is the only viable option what would be a reasonable Hash-Function for these types of documents in a Graph-DB? <code>_id</code> should not be included as the same node in two different collections would still differ. <code>_rev</code> would not really matter, and I am not sure if <code>_key</code> is in fact required as the node is identified by it any way.</p>
",2516892.0,-1.0,N/A,2022-07-23 01:24:05,Node Hash in ArangoDB?,<graph-databases><arangodb><graph-data-science><python-arango>,1,0,N/A,CC BY-SA 4.0
73052682,1,73052804.0,2022-07-20 13:37:29,3,64,"<p>From dataframe sructured like this</p>
<pre><code>   A B
 0 1 2
 1 3 4
</code></pre>
<p>I need to get list like this:</p>
<pre><code>[{&quot;A&quot;: 1, &quot;B&quot;: 2}, {&quot;A&quot;: 3, &quot;B&quot;: 4}]
</code></pre>
",19547474.0,19547474.0,2022-08-10 11:46:48,2022-08-10 11:46:48,Divide dataframe into list of rows containing all columns,<python><pandas><dataframe><data-science><rows>,1,0,N/A,CC BY-SA 4.0
73067795,1,-1.0,2022-07-21 14:14:12,0,411,"<p>Network visual with dynamic selection</p>
<p>Is anyone aware of a network association visual with dynamic selection formatting? Essentially, something like network navigator (or zoom charts drill down graph pro) but with the ability to select nodes and have associated nodes highlighted in some way. I’ve not been able to find this functionality using the zoom charts free trial or with any other visual. TIA</p>
",19595065.0,-1.0,N/A,2022-08-12 20:27:47,Power BI: a visualization to dynamically show a network interaction,<graph><powerbi><data-science><data-visualization>,1,0,N/A,CC BY-SA 4.0
73079270,1,-1.0,2022-07-22 11:00:48,0,524,"<p>otherwise when i remove the inplace=true, it works just fine
here is the code:
in: x=df1.drop('Outcome', axis=1, inplace=True)
y=df1['Outcome']</p>
<p>out: KeyError                                  Traceback (most recent call last)
~\anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
3360             try:
-&gt; 3361                 return self._engine.get_loc(casted_key)
3362             except KeyError as err:</p>
<p>~\anaconda3\lib\site-packages\pandas_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()</p>
<p>~\anaconda3\lib\site-packages\pandas_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()</p>
<p>pandas_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()</p>
<p>pandas_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()</p>
<p>KeyError: 'Outcome'</p>
",19182368.0,-1.0,N/A,2022-07-22 13:55:04,how do i get rid of a key error in jupyter notebook?,<python-3.x><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
73081631,1,-1.0,2022-07-22 14:04:15,1,87,"<p>I'm trying to break up Product names into categories, for example if the product is &quot;Demi Baguette&quot;, the category should be &quot;Baguette&quot; and sub category &quot;Demi&quot;. I have looked at NLP articles but nothing seems to be what I need as they all focus on sentences and text.
I've seen other questions answered by saying to use a dict, however there is over 15 thousand rows in the excel file so that's not really possible.</p>
<p>Any ideas as to how I can tackle this or where I can look?</p>
<p>Here is an example of my data.</p>
<p><a href=""https://i.stack.imgur.com/H64rp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H64rp.png"" alt=""enter image description here"" /></a></p>
<p>So I would want the category to be &quot;Soup&quot; and then sub categories based on flavour e.g&quot;Chicken&quot;, and misc labels &quot;Cream&quot;.</p>
",16607026.0,-1.0,N/A,2022-07-22 14:04:15,Excel Product Categorization in Python,<python><excel><machine-learning><data-science><data-analysis>,0,0,N/A,CC BY-SA 4.0
73088018,1,-1.0,2022-07-23 04:22:19,1,65,"<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Age</th>
<th>Gender</th>
<th>BusinessTravel</th>
<th>Department</th>
<th>Distance</th>
<th>Education</th>
<th>MaritalStatus</th>
<th>Salary</th>
<th>YearsWorked</th>
<th>Satisfaction</th>
</tr>
</thead>
<tbody>
<tr>
<td>41</td>
<td>Female</td>
<td>Frequent</td>
<td>Sales</td>
<td>12</td>
<td>5</td>
<td>Married</td>
<td>5000</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>24</td>
<td>Male</td>
<td>Rarely</td>
<td>HR</td>
<td>22</td>
<td>4</td>
<td>Single</td>
<td>3400</td>
<td>1</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>Satisfaction - Scale from 1 to 5, 5 is the most satisfied.</p>
<p>Distance - Distance from home to workplace</p>
<p>Above is a sample of the data.</p>
<p><em>Would Kmeans or Kmodes be appropriate for such a dataset?</em></p>
<p>Thank you for any answers in advance.</p>
",19337698.0,-1.0,N/A,2022-07-24 14:17:07,What is the best python approach/model for clustering dataset with many discrete and categorical variables?,<python><scikit-learn><data-science><cluster-analysis>,1,0,N/A,CC BY-SA 4.0
73083132,1,73083287.0,2022-07-22 16:02:57,-1,221,"<p>I am using the Flower code example to try POC of Federated Learning but I keep getting the error below when I run the client.py file:</p>
<pre class=""lang-bash prettyprint-override""><code>INFO flower 2022-07-04 15:27:37,301 | connection.py:102 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flower 2022-07-04 15:27:37,323 | connection.py:39 | ChannelConnectivity.IDLE
DEBUG flower 2022-07-04 15:27:37,389 | connection.py:39 | ChannelConnectivity.CONNECTING
DEBUG flower 2022-07-04 15:27:37,708 | connection.py:39 | ChannelConnectivity.READY
Training finished for round 5
DEBUG flower 2022-07-04 15:27:41,178 | connection.py:121 | gRPC channel closed
Traceback (most recent call last):
File &quot;C:\Users\HP\Development\Federated-Learning-sklearn\sklearnff\client.py&quot;, line 52, in
fl.client.start_numpy_client(
File &quot;C:\Users\HP.virtualenvs\sklearnff-UZKgGLZy\lib\site-packages\flwr\client\app.py&quot;, line 173, in start_numpy_client
start_client(
File &quot;C:\Users\HP.virtualenvs\sklearnff-UZKgGLZy\lib\site-packages\flwr\client\app.py&quot;, line 94, in start_client
client_message, sleep_duration, keep_going = handle(
File &quot;C:\Users\HP.virtualenvs\sklearnff-UZKgGLZy\lib\site-packages\flwr\client\grpc_client\message_handler.py&quot;, line 61, in handle
return _fit(client, server_msg.fit_ins), 0, True
File &quot;C:\Users\HP.virtualenvs\sklearnff-UZKgGLZy\lib\site-packages\flwr\client\grpc_client\message_handler.py&quot;, line 117, in _fit
fit_res = client.fit(fit_ins)
File &quot;C:\Users\HP.virtualenvs\sklearnff-UZKgGLZy\lib\site-packages\flwr\client\numpy_client.py&quot;, line 203, in fit
raise Exception(EXCEPTION_MESSAGE_WRONG_RETURN_TYPE_FIT)
Exception:
NumPyClient.fit did not return a tuple with 3 elements.
The returned values should have the following type signature:

Tuple[List[np.ndarray], int, Dict[str, Scalar]]
Example
model.get_weights(), 10, {&quot;accuracy&quot;: 0.95}
</code></pre>
",12221987.0,-1.0,N/A,2022-07-22 16:16:53,I'm getting an **NumPyClient.fit did not return a tuple with 3 elements.** error when I run client.py file. What could be the problem?,<python><scikit-learn><data-science><flower><federated-learning>,1,0,2022-07-25 09:32:18,CC BY-SA 4.0
73088340,1,-1.0,2022-07-23 05:45:09,0,200,"<p>I am trying to replace null values with 0 by applying certain conditions. Here is the code to generate the dataset.</p>
<pre><code>data = {'month': ['2022-01-01', '2022-02-01', '2022-03-01', '2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01', '2022-05-01', '2022-06-01', '2022-07-01', '2022-08-01'], 'Date1': ['2022-01-01', '2022-01-01', '2022-01-01', '2022-01-01', '2022-01-01', '2022-01-01', '2022-01-01', '2022-05-01', '2022-05-01', '2022-05-01', '2022-05-01'], 'Date2': ['2022-02-01', '2022-02-01', '2022-02-01', '2022-04-01', '2022-04-01', '2022-04-01', '2022-04-01', np.nan, np.nan, np.nan, np.nan], 'Name': ['A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'], 'num': [1234, 1234, 1234, 456, 456, 456, 456, np.nan, np.nan, np.nan, np.nan], 'sales': ['MN', 'MN', 'MN', 'CA', 'CA', 'CA', 'CA', 'FL', 'FL', 'FL', 'FL'], 'Num1': [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 44.0, 44.0, 44.0, 44.0], 'Num2': [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 29.0, 29.0, 29.0, 29.0]}

df = pd.DataFrame(data)
df

         month       Date1       Date2 Name   num sales  Num1  Num2
0   2022-01-01  2022-01-01  2022-02-01    A  1234    MN   NaN   NaN
1   2022-02-01  2022-01-01  2022-02-01    A  1234    MN   NaN   NaN
2   2022-03-01  2022-01-01  2022-02-01    A  1234    MN   NaN   NaN
3   2022-01-01  2022-01-01  2022-04-01    B   456    CA   NaN   NaN
4   2022-02-01  2022-01-01  2022-04-01    B   456    CA   NaN   NaN
5   2022-03-01  2022-01-01  2022-04-01    B   456    CA   NaN   NaN
6   2022-04-01  2022-01-01  2022-04-01    B   456    CA   NaN   NaN
7   2022-05-01  2022-05-01         NaN    C   NaN    FL  44.0  29.0
8   2022-06-01  2022-05-01         NaN    C   NaN    FL  44.0  29.0
9   2022-07-01  2022-05-01         NaN    C   NaN    FL  44.0  29.0
10  2022-08-01  2022-05-01         NaN    C   NaN    FL  44.0  29.0
</code></pre>
<p>So,  I want to replace the null values in the num column for certain rows if month-date1 &lt;=2, and I want to replace null values in Num1, and Num2 column for certain rows if month-date2 &lt;=2. If it is greater than 2, then don't replace it with 0.</p>
<p>So, I was wondering how can I implement these steps in python?</p>
<p>This is the expected outcome
<a href=""https://i.stack.imgur.com/iEZxz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iEZxz.png"" alt=""enter image description here"" /></a></p>
",16875907.0,11865956.0,2022-07-23 07:10:58,2022-07-23 10:08:59,How to fill NA values with applying condition?,<python><pandas><dataframe><imputation><data-science-experience>,1,5,N/A,CC BY-SA 4.0
73067665,1,-1.0,2022-07-21 14:04:26,0,62,"<p>I'm trying to create a graph projection and use different GDS algorithms on the latest version of Neo4j - the previous algorithms were made using the pre-GDS syntax.</p>
<p>I'd like to project a new graph to perform community detection from the in-memory graph but am unsure how to translate the previous syntax into a graph projection in order to then perform the LPA.</p>
<p><em>Previous syntax (works on pre-GDS version):</em></p>
<pre><code>CALL algo.labelPropagation.stream(
'MATCH (p:Publication) RETURN id(p) as id',
'MATCH (p1:Publication)-[r1:HAS_WORD]-&gt;(w)&lt;-[r2:HAS_WORD]-(p2:Publication) 
WHERE r1.occurrence &gt; 5 AND r2.occurrence &gt; 5
RETURN id(p1) as source, id(p2) as target, count(w) as weight',
{graph:'cypher',write:false, weightProperty : &quot;weight&quot;})
YIELD nodeId, label
WITH label, collect(algo.asNode(nodeId)) AS nodes WHERE size(nodes) &gt; 2
MERGE (c:PublicationLPACommunity {id : label})
FOREACH (n in nodes |
   MERGE (n)-[:IN_LPA_COMMUNITY]-&gt;(c)
)
return label, nodes
</code></pre>
<p><em>My attempt:</em></p>
<ol>
<li>Graph projection</li>
</ol>
<pre><code>CALL gds.labelPropagation.stream('pbRef')
YIELD nodeId, communityId
RETURN gds.util.asNode(nodeId).name AS name, communityId;
</code></pre>
<p>I may be missing something here ^^^</p>
<ol start=""2"">
<li>LPA stream</li>
</ol>
<pre><code>CALL gds.labelPropagation.stream('pbRef')
YIELD nodeId, communityId AS Community
RETURN gds.util.asNode(nodeId).name AS Name, Community
ORDER BY Community, Name
</code></pre>
<p><em>The following part works but maybe it has to be within one of the previous steps?</em></p>
<pre><code>MATCH (p1:Publication)-[r1:HAS_WORD]-&gt;(w)&lt;-[r2:HAS_WORD]-(p2:Publication) 
WHERE r1.occurrence &gt; 5 AND r2.occurrence &gt; 5
RETURN id(p1) as source, id(p2) as target, count(w) as weight
</code></pre>
<p>I'm missing something to get it working. Does anyone have any insight into this?</p>
<p>Thanks in advance</p>
",19480934.0,-1.0,N/A,2022-07-21 14:04:26,Using GDS LPA on a projected graph,<neo4j><cypher><graph-data-science>,0,5,N/A,CC BY-SA 4.0
73084338,1,-1.0,2022-07-22 17:57:32,0,65,"<p>I create a ngram from Python and the output its something like this:</p>
<pre><code>'vida': 113, 'sistema': 104, 'economía': 91, 'nacional': 84, 'mujeres': 76, 'derechos': 75, 'salud': 75, 'paz': 67, 'colombia': 66, 'social': 66, 'trabajo': 63, 'protección': 62, 'política': 61, 'país': 55, 'acceso': 53, 'cambio': 49, 'sociedad': 49, 'derecho': 49, 'educación': 47, 'cuidado': 46, 'así': 45, 'productiva': 44, 'condiciones': 44, 'cultura': 41, 'participación': 39, 'agua': 39, 'gobierno': 38, 'desarrollo': 38, 'integral': 37, 'construcción': 37, 'personas': 37, 'naturaleza': 35, 'territorios': 35, 'popular': 35, 'público': 35, 'pública': 34, 'territorio': 33, 'pueblos': 33, 'servicios': 33, 'todas': 33, 'población': 33, 'indígenas': 32, 'garantizaremos': 32, 'climático': 31, 'vivienda': 31, 'justicia': 31, 'políticas': 31, 'través': 31, 'producción': 30, 'cultural': 30, 'empleo': 30, 'calidad': 29, 'internacional': 29, 'manera': 29, 'pacto': 28, 'públicos': 28, 'mediante': 28, 'impulsaremos': 28, 'plan': 27, 'procesos': 27, 'territorial': 26, 'seguridad': 26, 'reconocimiento': 26, 'igualdad': 25, 'conocimiento': 25, 'afrodescendientes': 25, 'economías': 25, 'toda': 25, 'sociales': 25, 'nivel': 25, 'garantizar': 25, 'comunidades': 25, 'atención': 25, 'uso': 25, 'territoriales': 25, 'tierra': 24, 'víctimas': 24, 'transición': 24, 'trabajadores': 24, 'mayor': 24, 'programas': 24, 'enfoque': 24, 'base': 24, 'democratización': 23, 'productividad': 23, 'arte': 23, 'negros': 23, 'avanzaremos': 23, 'rurales': 23, 'públicas': 23, 'formas': 23, 'sector': 23, 'formación': 23, 'garantía': 22, 'lucha': 22, 'palenqueros': 22, 'pensión': 22, 'fin': 22, 'transformación': 22, 'marco': 22, 'culturales': 22, 'género': 21, 'sistemas': 21, 'ambiental': 21, 'modelo': 21, 'mundial': 20, 'niños': 20, 'gran': 20, 'alimentaria': 20, 'zonas': 20, 'bajo': 20, 'reforma': 20, 'potencia': 19, 'crédito': 19, 'primera': 19, 'raizales': 19, 'mayores': 19, 'fundamental': 19, 'organizaciones': 19, 'educativo': 19, 'comunitarias': 19, 'ambientales': 19, 'espacio': 18, 'industria': 18, 'jóvenes': 18, 'infancia': 18, 'niñas': 18, 'patrimonio': 18, 'alimentación': 18, 'económica': 18, 'instrumentos': 18, 'bienes': 18, 'generación': 18, 'recursos': 18, 'proyectos': 18, 'promoveremos': 18, 'laboral': 18, 'deporte': 17, 'realidad': 17, 'diversidad': 17, 'humana': 17, 'reparación': 17, 'programa': 17, 'soberanía': 17, 'garantizando': 17, 'diálogo': 17, 'mercado': 17, 'regional': 17, 'económico': 16, 'saber': 16, 'campo': 16, 'campesinado': 16, 'tiempo': 16, 'parte': 16, 'familias': 16, 'energía': 16, 'autonomía': 16, 'local': 16, 'rural': 16, 'populares': 16, 'apoyo': 16, 'fortalecimiento': 16, 'libre': 15, 'oportunidades': 15, 'haremos': 15, 'rrom': 15, 'adultos': 15, 'basada': 15, 'capacidad': 15, 'millones': 15, 'libertad': 15, 'acuerdo': 15, 'superar': 15, 'valor': 15, 'energética': 15, 'fortaleceremos': 15, 'centros': 15, 'gestión': 15, 'ser': 15, 'garantías': 14, 'agenda': 14, 'corrupción': 14, 'nuevo': 14, 'crearemos': 14, 'hombres': 14, 'superior': 14, 'general': 14, 'tierras': 14, 'espacios': 14, 'capacidades': 14, 'ingresos': 14, 'nacionales': 14, 'mejorar': 14, 'prácticas': 14, 'servicio': 14, 'control': 14, 'alrededor': 13, 'desigualdad': 13, 'convivencia': 13, 'nación': 13, 'diversidades': 13, 'saberes': 13, 'puedan': 13, 'grandes': 13, 'institucionalidad': 13, 'áreas': 13, 'riesgo': 13, 'infraestructura': 13, 'financiamiento': 13, 'transporte': 13, 'cada': 13, 'particular': 13, 'red': 12, 'vivir': 12, 'defensa': 12, 'fundamentales': 12, 'democracia': 12, 'efectiva': 12, 'permita': 12, 'colombiana': 12, 'productores': 12, 'universal': 12, 'creación': 12, 'mínimo': 12, 'permitan': 12, 'especial': 12, 'articulación': 12, 'distribución': 12, 'regionales': 12, 'hacer': 12, 'actividades': 12, 'locales': 12, 'productos': 12, 'intercultural': 12, 'tecnologías': 12, 'entidades': 12, 'sectores': 12, 'memoria': 12, 'negocio': 11, 'física': 11, 'poblaciones': 11, 'violencia': 11, 'trabajos': 11, 'impuestos': 11, 'promoción': 11, 'frente': 11, 'cuidados': 11, 'bienestar': 11, 'digno': 11, 'reducción': 11, 'dentro': 11, 'investigación': 11, 'debe': 11, 'medidas': 11, 'implica': 11, 'comunitarios': 11, 'cobertura': 11, 'incluyendo': 11, 'nuevas': 11, 'autoridades': 11, 'aprovechamiento': 11
</code></pre>
<p>I want to create a cdv file from it, that cotain two columns &quot;words&quot; and &quot;Count&quot; in a csv file, Im looking for a way fo how to do it in Python o R</p>
<p>Thanks for the help!</p>
",18473320.0,6461462.0,2022-09-22 19:40:50,2022-09-22 19:40:50,how to create a data table from a not estructured text file,<python><r><data-science>,3,4,N/A,CC BY-SA 4.0
73091762,1,73098817.0,2022-07-23 15:01:30,0,467,"<pre class=""lang-py prettyprint-override""><code>    trf1=ColumnTransformer([(&quot;Infuse_val&quot;,SimpleImputer(strategy=&quot;mean&quot;),[0])],remainder=&quot;passthrough&quot;)
    trf4=ColumnTransformer([(&quot;One_hot&quot;,OneHotEncoder(sparse=False,handle_unknown=&quot;ignore&quot;),[1,4])],remainder=&quot;passthrough&quot;)
    trf2=ColumnTransformer([(&quot;Ord_encode&quot;,OrdinalEncoder(categories=[&quot;Strong&quot;,&quot;Mild&quot;]),[3])],remainder=&quot;passthrough&quot;)
    trf3=ColumnTransformer([(&quot;scale&quot;,StandardScaler(),[0,2])],remainder=&quot;passthrough&quot;)
    pipe = Pipeline([
        ('trf1',trf1),
        ('trf2',trf2),
        ('trf3',trf3),
        ('trf4',trf4),
    ])
    pipe.fit(x_train,y_tarin)
</code></pre>
<p>Error</p>
<pre><code>ValueError: Shape mismatch: if categories is an array, it has to be of shape (n_features,).
</code></pre>
<p>The table is</p>
<p><a href=""https://i.stack.imgur.com/EDveN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EDveN.png"" alt=""enter image description here"" /></a></p>
<p>I don't understand what's the error here in my code?</p>
",19517102.0,19517102.0,2022-07-23 15:09:40,2022-07-24 13:28:19,"ValueError: Shape mismatch: if categories is an array, The error is not resolved even after specifying the columns as indexes",<python><machine-learning><scikit-learn><data-science><one-hot-encoding>,1,1,N/A,CC BY-SA 4.0
73106971,1,-1.0,2022-07-25 09:33:04,0,162,"<p>i Have a following dataframe</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>A-Party</th>
<th>B-Party</th>
<th>Duration</th>
<th>Call Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>B</td>
<td>2</td>
<td>In</td>
</tr>
<tr>
<td>B</td>
<td>A</td>
<td>2</td>
<td>out</td>
</tr>
<tr>
<td>B</td>
<td>A</td>
<td>3</td>
<td>in</td>
</tr>
<tr>
<td>c</td>
<td>B</td>
<td>10</td>
<td>in</td>
</tr>
<tr>
<td>B</td>
<td>c</td>
<td>10</td>
<td>out</td>
</tr>
</tbody>
</table>
</div>
<p>The desired output is</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>A-Party</th>
<th>B-Party</th>
<th>Duration</th>
<th>Call Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>B</td>
<td>2</td>
<td>In</td>
</tr>
<tr>
<td>B</td>
<td>A</td>
<td>2</td>
<td>out</td>
</tr>
<tr>
<td>c</td>
<td>B</td>
<td>10</td>
<td>in</td>
</tr>
<tr>
<td>B</td>
<td>c</td>
<td>10</td>
<td>out</td>
</tr>
</tbody>
</table>
</div>
<p>i want to match to get rows in which different rows have same pair values of columns A-Party and B-Party(like first two rows have pair (A,B) and (B,A)) as well their Duration values shoud match and Call Type should be opposite.Provide me the efficient solution for this.</p>
",16411269.0,10315163.0,2022-07-25 09:34:23,2022-07-25 09:34:23,Find rows by matching pair values in pandas dataframe,<python><pandas><dataframe><data-science>,0,2,N/A,CC BY-SA 4.0
73107018,1,73107820.0,2022-07-25 09:35:49,0,528,"<p>I'm using <code>scipy.stats.ttest_ind</code> and <code>scipy.stats.wilcoxon</code> to perform t test and the Wilcoxon test, but I have no idea how can I set the significant level for them. In the official documents, nothing about it is mentioned and it seems like no arguments of these two methods are about it. Does anyone know how to set it?</p>
<p>The official documents:</p>
<p><a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html</a>
<a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html</a></p>
",14602367.0,-1.0,N/A,2022-07-25 10:37:34,How to set significance level for scipy.stats test,<python><scipy><statistics><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
73107184,1,73109925.0,2022-07-25 09:47:39,0,79,"<p>I'm trying to work on a dataset with 510,000 rows and 636 columns. I loaded it into a dataframe using the dask dataframe method, but the entries can't be displayed. When i try to get the shape, it results in delays. Is there a way for me to analyze the whole dataset without using big data technologies like Pyspark?</p>
<pre><code>from dask import dataframe
import requests
import zipfile
import os
import pandas as pd

if os.path.exists('pisa2012.zip') == False:
    r = requests.get('https://s3.amazonaws.com/udacity-hosted-downloads/ud507/pisa2012.csv.zip', allow_redirects=True)
    open('pisa2012.zip', 'wb').write(r.content)

if os.path.exists('pisa2012.csv') == False:
    with zipfile.ZipFile('pisa2012.zip', 'r') as zip_ref:
        zip_ref.extractall('./')

df_pisa = dataframe.read_csv('pisa2012.csv')

df_pisa.shape #Output:(Delayed('int-e9d8366d-1b9e-4f8e-a83a-1d4cac510621'), 636)
</code></pre>
",14657890.0,-1.0,N/A,2022-07-25 13:20:00,How can I work on a large dataset without having to use Pyspark?,<python-3.x><pandas><data-science><bigdata><dask>,1,1,N/A,CC BY-SA 4.0
73109421,1,-1.0,2022-07-25 12:41:00,0,41,"<p>Here, all_data is a dataframe and I grouped it in result. plt is used to import matplotlib.
...</p>
<pre><code> result = all_data.groupby(&quot;Months&quot;).sum() 
    result
    
    Quantity Ordered   Price Each  Sales
    Months          
    1   10903   1811768.38  1822256.73
    2   13449   2188884.72  2202022.42
    3   17005   2791207.83  2807100.38
    4   20558   3367671.02  3390670.24
    5   18667   3135125.13  3152606.75
    6   15253   2562025.61  2577802.26
    7   16072   2632539.56  2647775.76
    8   13448   2230345.42  2244467.88
    9   13109   2084992.09  2097560.13
    10  22703   3715554.83  3736726.88
    11  19798   3180600.68  3199603.20
    12  28114   4588415.41  4613443.34
    
    plt.bar(result[&quot;Months&quot;], result[&quot;Sales&quot;])
    plt.show
    KeyError: 'Months'
    
    The above exception was the direct cause of the following exception:
</code></pre>
<p>...</p>
",16300378.0,16300378.0,2022-07-25 12:44:54,2022-07-25 12:44:54,"I am trying plot two column(Months , sales) of dataframe in bar graph using matplotlib but it shows error as KeyError for Months column. why?",<python><pandas><matplotlib><data-science>,0,2,N/A,CC BY-SA 4.0
73114040,1,-1.0,2022-07-25 18:53:18,2,328,"<p>I have dataframe where a column named teams which the dtype is &quot;O&quot;(str) but inside it there is list:
eg : &quot;['Australia', 'Sri Lanka']&quot;
Now i want to split this two team names into two columns, how to do it?</p>
",18176092.0,-1.0,N/A,2022-07-25 19:08:47,"Splitting dataframe column where the dtype is Object but inside it there is list , how to split?",<python><pandas><dataframe><data-science><data-analysis>,1,3,N/A,CC BY-SA 4.0
73115367,1,-1.0,2022-07-25 21:08:16,1,102,"<p>I need to find matches and mismatches of Level + Part for the Names. If the Part and the Level of a Name are the same for another Name, it is a match. What I'm thinking is, split the df to separate frames. One per name. then concat the frames together on a groupby([&quot;Level&quot;, &quot;Part&quot;]). The problem is, I wont always have the same amount of names. Some datasheets will have 3, some will have 8. Is this the best way to go about this? If there's a better way, please let me know. I'm still learning.</p>
<pre><code>df = pd.DataFrame({'Name':[&quot;A&quot;,&quot;ABC&quot;,&quot;A&quot;,&quot;ABC&quot;,&quot;ABC&quot;,&quot;AAB&quot;,&quot;AAB&quot;] ,'Level': [1,1,2,1,3,4,2],'Part':[&quot;Upper&quot;,&quot;Upper 2&quot;,&quot;Upper 2&quot;,&quot;Upper&quot;,&quot;Middle&quot;, &quot;Deck&quot;, &quot;Ceiling&quot;]})

df2 = pd.DataFrame({'Part':[&quot;Upper&quot;,&quot;Upper 2&quot;,&quot;Middle&quot;, &quot;Deck&quot;, &quot;Ceiling&quot;],'A Level': [1,2,&quot;NaN&quot;,&quot;NaN&quot;,&quot;NaN&quot;],'ABC Level': [1,1,3,&quot;NaN&quot;,&quot;NaN&quot;],'AAB Level': [&quot;NaN&quot;,&quot;NaN&quot;,&quot;NaN&quot;,4,2]})

    Name    Level   Part
0   A         1     Upper
1   ABC       1     Upper 2
2   A         2     Upper 2
3   ABC       1     Upper
4   ABC       3     Middle
5   AAB       4     Deck
6   AAB       2     Ceiling
</code></pre>
<p>I want it to look something like this, but if there's a better output format, let me know.</p>
<pre><code>    Part     A Level    ABC Level   AAB Level
0   Upper    1          1           NaN
1   Upper 2  2          1           NaN
2   Middle   NaN        3           NaN
3   Deck     NaN        NaN         4
4   Ceiling  NaN        NaN         2
</code></pre>
",14352335.0,11865956.0,2022-07-25 22:45:37,2022-07-27 06:38:40,How to split a dataframe then merge to see matches?,<python><pandas><data-science>,2,7,N/A,CC BY-SA 4.0
73078231,1,73078527.0,2022-07-22 09:37:46,2,2382,"<p>How to get all stop words from <code>spacy.lang.en</code> and don't get any errors?</p>
<pre><code>from spacy.lang.en import stop_words as stop_words


def tokenize(sentence):
    sentence = nlp(sentence)
    # lemmatizing
    sentence = [ word.lemma_.lower().strip() if word.lemma_ != &quot;-PRON-&quot; else word.lower_ for word in sentence ]
    # removing stop words
    sentence = [ word for word in sentence if word not in stop_words and word not in punctuations ]        
    return sentence
</code></pre>
<pre><code>tokenize(&quot;Hallo ik ben leyla en &quot;) and then i get 
</code></pre>
<p>Then I got the following error and This is the error that I got</p>
<pre><code>TypeError: argument of type 'module' is not iterable
</code></pre>
",11359133.0,1740577.0,2022-07-22 10:28:49,2023-01-12 16:04:40,How to get all stop words from Spacy and don't get any errors? TypeError: argument of type 'module' is not iterable,<python><nlp><data-science><spacy>,1,3,N/A,CC BY-SA 4.0
73100659,1,-1.0,2022-07-24 17:37:33,1,65,"<p>I'm working on a scientific database that contains model statements such as:</p>
<p>&quot;A possible cause of Fibromyalgia is Microglial hyperactivity, as supported by these 10 studies: [...] and contradicted by 1 study [...].&quot;</p>
<p>I need to specify a source for statements in Neo4j and be able to do 2 ways operations, like:</p>
<ul>
<li>Find all statements supported by a study</li>
<li>Find all studies supporting a statement</li>
</ul>
<p>The most immediate idea I had is to use the DOI of studies as unique identifiers in the relationship property. The big con of this idea is that I have to scan all the relationships to find the list of all statements supported by a study.</p>
<p><a href=""https://i.stack.imgur.com/sG6rG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sG6rG.png"" alt=""Fibromyalgia: has cause: microglial hyperactivity"" /></a></p>
<p>So, since it is impossible to make a link between a study and a relationship, I had the idea to make 2 links, at each extremity of the relationship. The obvious con is that it does not give information about the relationship, like &quot;support&quot; or &quot;contradict&quot;.</p>
<p><a href=""https://i.stack.imgur.com/xo8hq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xo8hq.png"" alt=""Study has cause"" /></a></p>
<p>So, I came to the conclusion that I need a node for the hypothesis:</p>
<p><a href=""https://i.stack.imgur.com/vrVOq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vrVOq.png"" alt=""The hypothesis is sourced and linked to concepts"" /></a></p>
<p>However, it overloads the graph and we are not anymore in the classical node -relationship-&gt; node design that makes property graphs so easy to understand.</p>
<p>Using RDF, it is possible to add properties to relationships using subgraphs, however there we enter semantic graphs and quad stores, which is a more complex tool.</p>
<p>So I'm wondering if there is a &quot;correct&quot; design pattern for Neo4j to support this type of need that I may not have imagined instead?</p>
<p>Thanks</p>
",9927519.0,-1.0,N/A,2022-07-26 05:54:37,Neo4j data modeling: correct way to specify a source for a statement?,<neo4j><rdf><graph-databases><knowledge-graph><graph-data-science>,1,0,N/A,CC BY-SA 4.0
73118611,1,-1.0,2022-07-26 06:28:05,1,524,"<p>I am having a very hard time figuring this out. Im trying to create a real time satellite tracker and using the sky field python module. It reads in TLE data and then gives a LAT and LON position relative to the earth. The sky filed module creates satrec objects which cannot be pickled (even tried using dill). I am using a for loop to loop over all the satellites but this is very slow so I want to speed it up using multiprocessing with the pool method, but as above this is not working since multiprocessing uses pickle. Is there any way around this or does anyone have suggestions on other ways to use multiprocessing so speed up this for loop?</p>
<pre><code>from skyfield.api import load, wgs84, EarthSatellite
import numpy as np
import pandas as pd
import time
import os 
from pyspark import SparkContext
from multiprocessing import Pool
import dill

data = pd.read_json('tempSatelliteData.json')
print(data.head())

newData = data.filter(['tle_line0', 'tle_line1', 'tle_line2'])
newData.to_csv('test.txt', sep='\n', index=False)

stations_file = 'test.txt'
satellites = load.tle_file(stations_file)
ts = load.timescale()
t = ts.now()

#print(satellites)
#data = pd.DataFrame(data=satellites)
#data = data.to_numpy()

def normal_for():
# this for loop takes 9 seconds to comeplete TOO SLOW
    ty = time.time()
    for satellite in satellites:
        geocentric = satellite.at(t)
        lat,lon = wgs84.latlon_of(geocentric)
        print('Latitude:', lat)
        print('Longitude:', lon)
    print(np.round_(time.time()-ty,3),'sec')

def sat_lat_lon(satellite):
    geocentric = satellite.at(t)
    lat,lon = wgs84.latlon_of(geocentric)

p = Pool()
result = p.map(sat_lat_lon, satellites)
p.close()
p.join()
    
</code></pre>
",19551646.0,-1.0,N/A,2022-07-28 22:27:50,Is there a way to not use Pickling when using the python multi processing module?,<python><multiprocessing><data-science><pickle><dill>,2,7,N/A,CC BY-SA 4.0
73091067,1,-1.0,2022-07-23 13:15:50,0,86,"<p><a href=""https://i.stack.imgur.com/xZhja.jpg"" rel=""nofollow noreferrer"">You can See in this screenshot that what i'm trying to say</a></p>
",9855428.0,-1.0,N/A,2022-07-23 13:15:50,"Kite for Jupyterlab not running (Installed Jupyterlab version : 3.0.14, jupyter-kite version: 2.0.2)",<python><data-science><jupyter-lab><kite>,0,2,N/A,CC BY-SA 4.0
73108037,1,-1.0,2022-07-25 10:53:28,1,377,"<p>Newbie using Altair/Vega-lite and struggling a bit to &quot;get&quot; the transformations and calculations and encoding way of thinking, especially for more complex/nested data.</p>
<p>Specifically, I am trying to create a super simple layered histogram, that shows the salary distribution of different countries.</p>
<p>So far I was able to get on the Y Axis the percentage of occurrences compared to the total:</p>
<pre><code>
salaries = {
    'NL': np.random.normal(loc=80000, scale=30000, size=(500,)),
    'ES': np.random.normal(loc=80000, scale=30000, size=(50,))
}
source = pd.DataFrame({k:pd.Series(v) for k,v in salaries.items()})

c = alt.Chart(source).transform_fold(
   ['NL', 'ES'],
   as_=['Benchmark', 'Salaries']
   ).transform_joinaggregate(
       total='count(*)',
       groupby=['Benchmark']
   ).transform_calculate(
       pct='1/ datum.total'
   ).mark_bar(opacity=0.3, binSpacing=0
   ).encode(
       alt.Color('Benchmark:N'),
       x=alt.X('Salaries:Q', bin=alt.Bin(maxbins=20)),
       y=alt.Y('sum(pct):Q', axis=alt.Axis(format='%'), stack=None)
   )
</code></pre>
<p>which results in:</p>
<p><a href=""https://i.stack.imgur.com/2o2Vv.png"" rel=""nofollow noreferrer"">total percentage</a></p>
<p>However, I'd like the percentage to be applicable to each category instead of the total. So, in this example, on the Y axis the second distribution should show percentages in the same level as the first one, as they are identical normal distributions.</p>
<p>I hope it's clear enough, apologies for probably lacking the statistical theory and glossary to explain things better.</p>
",3489010.0,-1.0,N/A,2022-07-25 14:27:03,In Altair/Vega-lite how to show percentage of grouped category instead of total?,<data-science><data-visualization><altair>,1,0,N/A,CC BY-SA 4.0
73120668,1,-1.0,2022-07-26 09:11:10,1,164,"<p>I am gettin an attribute error when using graphdatascience to construct a graph from a simple dataframe. I am unsure what is missing from my code to make it work</p>
<pre><code>import pandas as pd
import neo4j
from graphdatascience import GraphDataScience

gds = GraphDataScience(&quot;bolt://localhost:7687&quot;, auth=None)

nodes = pandas.DataFrame(
    {
        &quot;nodeId&quot;: [0, 1, 2, 3],
        &quot;labels&quot;:  [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;],
        &quot;prop1&quot;: [42, 1337, 8, 0],
        &quot;otherProperty&quot;: [0.1, 0.2, 0.3, 0.4]
    }
)

relationships = pandas.DataFrame(
    {
        &quot;sourceNodeId&quot;: [0, 1, 2, 3],
        &quot;targetNodeId&quot;: [1, 2, 3, 0],
        &quot;relationshipType&quot;: [&quot;REL&quot;, &quot;REL&quot;, &quot;REL&quot;, &quot;REL&quot;],
        &quot;weight&quot;: [0.0, 0.0, 0.1, 42.0]
    }
)

G = gds.alpha.graph.construct(
    &quot;my-graph&quot;,      # Graph name
    nodes,           # One or more dataframes containing node data
    relationships    # One or more dataframes containing relationship data
)


UnableToConnectError: Couldn't connect to localhost:7687 (resolved to ('[::1]:7687', '127.0.0.1:7687')):
Failed to establish connection to ResolvedIPv6Address(('::1', 7687, 0, 0)) (reason [Errno 61] Connection refused)
Failed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [Errno 61] Connection refused)
</code></pre>
",13912118.0,13912118.0,2022-07-26 09:36:43,2022-07-26 17:54:55,Attribute error when trying to construct GDS graph from a data frame,<python><neo4j><graph-data-science>,1,0,N/A,CC BY-SA 4.0
73121009,1,-1.0,2022-07-26 09:35:58,0,67,"<p>I am trying to create a software with interface, but first I need to create the backend. As pyomo or other library can construct models,
I want to do it in my class. So the main idea is to first 1): create the object; 2):declare my variables and parameters within the object;
3): add ordinary differential equations to my object; 4) add optional equations to my object.</p>
<p>As pyomo, for example, to add an equation you can use a method for example, called Constraint, where it works as follows, e.g.: <code>model.C1 = Constraint(expr = (a*x**2 == 3))</code>.
Also in pyomo for you to create a parameter and put into the model you use for example: <code>model.P = Param(initialize=value)</code>. A variable is created as <code>model.variable = Var(bounds=(0, None))</code>.</p>
<p>I just need to understand these basic things, I mean, how to create methods in a class to do the same. Just these basic. To clarify what I want to do, here is a
reverse engineering, my main script is:</p>
<pre><code>model = my_class()

#DECLARING PARAMETERS

model.parameter(P1)
model.parameter(P2)
model.parameter(P3)
model.parameter(P4)
model.parameter(P5)

#DECLARING VARIABLES

model.variable(X[0])  # declaring X and W as variables, obs.: the argument is an object to the class and other methods inside this class?

model.variable(X[1])  

model.variable(W[0])  

model.variable(W[1])  

#PROCESS MODEL CONSTRUCTION:

#use model.ODE to add an ODE to the model

model.ODE( expr= (  Y[0] ==  P1*X[0] - P2*X[1] + P3*W[1] )    ) # Y[0] is related as being the derivative

model.ODE( expr= ( Y[1] ==  P4*W[1] - P5*X[0] )    ) # Y[1] is related as being the derivative 

model.solve() # &lt;&lt; ignore this line, since I still will create the method
</code></pre>
<p>THE MOST important thing is that &quot;expr&quot; must be an argument for other future method creations inside the class! So the python must recognize it as being an argument
output as well, so I can insert them into other methods. Once the user declare these expressions I can play with other methods...</p>
",18935522.0,18935522.0,2022-07-26 10:04:33,2022-07-26 10:04:33,Python Class creation to attach mathematical model and play with variables,<python><class><data-science><backend><pyomo>,0,2,N/A,CC BY-SA 4.0
73094874,1,-1.0,2022-07-23 23:38:48,0,325,"<p>I was able to convert an XPT file to CSV through the anaconda command prompt by installing the xport library and then typing something like this:</p>
<p><code>python -m xport file.xpt &gt; file.csv</code></p>
<p>However, when I try to convert the file in spyder using the code in the top answer to the post <a href=""https://stackoverflow.com/questions/7716999/xpt-to-csv-conversion"">XPT to CSV Conversion?</a>, I keep getting the error message &quot;'str' object has no attribute 'read'&quot;. My code is as follows:</p>
<pre><code>import xport, csv
with xport.XportReader(r'file.xpt') as reader:
    with open(r'file.csv', 'rb') as out:
        writer = csv.DictWriter(out, [f['name'] for f in reader.fields])
        for row in reader:
            writer.writerow(row)
</code></pre>
<p>I've tried to debug this by adding a break statement to the bottom of the for loop to see if it could at least print the first line of the file, but it didn't work. I also tried getting rid of the for loop entirely and instead manually printing the first line, which didn't help either. I'm not sure where the problem is.</p>
",19401567.0,19401567.0,2022-07-23 23:41:38,2022-07-24 00:13:38,"Error in converting XPT file to CSV using xport module in spyder (python), but no error in anaconda prompt",<python><anaconda><data-science><spyder>,1,0,N/A,CC BY-SA 4.0
73101496,1,73101637.0,2022-07-24 19:41:53,1,247,"<p>I am building a histogram from a pd.DataFrame.
I am attempting to customize the legend, however, any call to plt.legend() throws a TypeError.  Everything else regarding the graph is working as intended.  Any thoughts as to why?</p>
<p>Code:</p>
<pre><code>df.plot(x='Bucket', 
             kind='bar', 
             stacked=True,
             figsize=(10, 10),
          )


plt.ylabel('Emails', fontsize=20)
plt.xlabel('Character Length of Emails', fontsize=20)
plt.title('Email Topics vs Email Length', fontsize=30)
plt.xticks(rotation = 25, fontsize=15)
plt.yticks(rotation = 25, fontsize=15)

plt.legend()
</code></pre>
<p>Error :</p>
<pre><code> ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-554-876a190bbc34&gt; in &lt;module&gt;
     22 plt.yticks(rotation = 25, fontsize=15)
     23 
---&gt; 24 plt.legend()
     25 
     26 

TypeError: 'list' object is not callable
</code></pre>
<p>The same error is thrown when I call</p>
<pre><code>plt.legend([])
plt.legend(loc='upper center')
</code></pre>
<p>etc.</p>
",6377942.0,6377942.0,2022-07-24 20:02:00,2022-07-24 20:04:15,Matplotlib : plt.legend() Throwing TypeError,<python><pandas><matplotlib><data-science>,1,6,2022-07-25 01:35:34,CC BY-SA 4.0
73101770,1,-1.0,2022-07-24 20:24:04,0,65,"<p>I'm looking for tips or tricks on how to connect full text research papers to a graph in Neo4j.</p>
<p>I've researched some APIs that allow you to connect to scholarly articles - are there any best practices for doing this?</p>
<p>Cheers!</p>
",19480934.0,-1.0,N/A,2022-09-06 13:23:30,Connecting full text research papers to a graph GDS (Neo4j),<neo4j><cypher><graph-data-science>,1,0,N/A,CC BY-SA 4.0
73108275,1,-1.0,2022-07-25 11:12:12,0,34,"<p>I have a csv data that contains names of people from different countries like Croatia, Norway, Serbia, Turkey etc. Their names includes symbols like this 'Ødegaard', 'Cédric', 'Kolašinac'. Upon reading the csv data on python, it converts the names like this '�degaard' or 'Filip ?uri?i?'.</p>
<p>How can i fix this?</p>
",13017203.0,13017203.0,2022-07-25 11:13:28,2022-07-25 11:13:28,How to read languages in Python,<python><pandas><csv><text><data-science>,0,3,2022-07-25 11:23:26,CC BY-SA 4.0
73149464,1,-1.0,2022-07-28 08:18:57,0,207,"<p>I am trying to learn the use of gravityai and frankly i am a bit new to this. For that i followed <a href=""https://www.youtube.com/watch?v=i6qL3NqFjs4"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=i6qL3NqFjs4</a> from Ania Kubow. When i do this, at the end i encounter the error message. This message appears in gravity ai, when trying to run the job, i.e. after uploading all zipped files three .pkl files, one .py file, one .txt file, one .json file), after docker is initialized and run:</p>
<p><code>Error running executable: usage: classify_financial_articles.py [-h] {run,serve} ... classify_financial_articles.py: error: argument subcommand: invalid choice: '/tmp/gai_temp/0675f15ca0b04cf98071474f19e38f3c/76f5cdc86a1241af8c01ce1b4d441b0c' (choose from 'run', 'serve')</code>.</p>
<p>I do not understand the error message and therefore cannot fix it. Is it an error in the code? or in the configuration on the gravityai platform?  At no point do i run the .py file explicitly so i conclude, that it must be from the gravityai. Yet i dont get the error. Can anyone help me?</p>
<p>i added the .py file, as it is the one throwing the error</p>
<pre><code>from gravityai import gravityai as grav
import pickle
import pandas as pd

model = pickle.load(open('financial_text_classifier.pkl', 'rb'))
tfidf_vectorizer = pickle.load(open('financial_text_vectorizer.pkl','rb'))
label_encder = pickle.load(open('financial_text_encoder.pkl', 'rb'))

def process(inPath, outPath):
    # read csv input  file
    input_df = pd.read_csv(inPath)
    # read the data
    features = tfidf_vectorizer.transform(input_df['body'])
    # predict classes
    predictions = model.predict(features)
    #convert outpulabels to categories
    input_df['category'] = label_encder.inverse_transform(predictions)
    #save results to csv
    output_df = input_df(['id', 'category'])
    output_df.csv(outPath, index=False)

    grav.wait_for_requests(process) 
</code></pre>
<p>I can't find any errors in the .py file</p>
",11532018.0,4685471.0,2022-07-28 08:33:43,2022-10-12 12:58:50,"GravityAI deployment of AI throws ""error: argument subcommand: invalid choice:""",<python><docker><machine-learning><data-science>,2,0,N/A,CC BY-SA 4.0
73161533,1,73162218.0,2022-07-29 04:06:09,0,152,"<p>I have a large data set (100,000+ users) that needs to be cached and sorted for use at all times during a game's runtime. The system is of users in a game that have the ability to earn points. The system is broken up into parts for completing game tasks as follows:</p>
<p><strong>Category Score:</strong> Positive integer score earned from  completing tasks in a category of the game.<br>
<strong>Category Rank</strong>: Rank (0 -&gt; n) of users sorted by top score in a category.<br>
<strong>Category Worth</strong>: The worth for the category rank in a given category. I.E: &quot;Fighting&quot; rank 0 gives you 10 global points while &quot;Fighting&quot; rank 1 give you 9 global points.<br>
<strong>Global Score</strong>: A summation of points earned by each user depending on the category rank and score. I.E: If you earn a 0th place in fighting (worth 10 points), but 2nd place in defense (worth 8 points), your global score is 18th points.
<br>
<strong>Global Rank</strong>: Rank (0 -&gt; n) sorted by global score.</p>
<p>In the MySQL table keeping this data, each entry is a kept in a row of <code>(player [UUID], category [String], score [Integer])</code>. However, since data is modified every second based on user gameplay and the user needs to be able to access the global and individual category leaderboards at any time, this information needs to be cached in memory and pre-sorted to remain efficient. As an additional caveat, both category order and global order need to be kept available as separate data points considering a player may want to view an individual category rank and score as well as their global rank and score.</p>
<p>Currently, the system achieves this with the following steps:</p>
<ol>
<li>Get all entries from MySQL and log them into individual <code>User</code> objects with just the raw score for each category</li>
<li>Put all users into a category cache <code>(HashMap&lt;Category, ArrayList&lt;User&gt;&gt;)</code> with the list being sorted by their category score (this will be used for individual category leaderboards)</li>
<li>Loop through each category in the aforementioned category cache and put that user into a global cache <code>(ArrayList&lt;User&gt;)</code> with points summed. This is then sorted by total points which gives them their rank based on their index in the list.</li>
</ol>
<p>The current means of sorting are by using Java 8's built in <code>List#sort()</code> with comparators using the category's score and the global score respectively.</p>
<p>Currently, the sorting of this data with 100,000+ players and 25 categories is around 40-60 seconds with a massive amount of memory and CPU power being used to sort each pass through. This refreshes every 10 minutes and is severely impacting server performance.</p>
<p>Any idea on how to re-work this system to alleviate some of the pressure on the server?</p>
<p>Edit: <a href=""https://stackoverflow.com/questions/1883264/database-sort-vs-programmatic-java-sort"">this post</a> shows that for larger data sets, database sorting is more efficient, but I it doesn't fully cover how the ranking would be approached considering ranks are not based by one column, but rather a 25 calculations done based on the rank.</p>
",19644840.0,19644840.0,2022-07-29 06:28:56,2022-07-30 23:00:20,Keeping a large sorted User cache with an intensive comparator,<java><mysql><algorithm><sorting><data-science>,2,7,N/A,CC BY-SA 4.0
73174875,1,-1.0,2022-07-30 10:37:12,0,124,"<blockquote>
<p>I get the Used the following error when trying to do the score or mean squared error of a single sample :&quot;Found input variables with inconsistent number of samples&quot; for a decision tree regressor model from sklearn to find the chance
of having a heart attack based on 13 other parameters. The model seems
to work but the metrics tests always give me this kind of error
regardless of how I transform the data. Its because the sample test is 1 row whilst the training data is 303 rows but I don't know how to fit them.</p>
</blockquote>
<pre><code>import pandas as pd

from sklearn import tree

from sklearn.metrics import mean_squared_error


heart = pd.read_csv('heart.csv')
test = pd.read_csv('Heart_test.csv')


X = heart.iloc[:,0:13]
Y = heart.iloc[:,13:14]

test = test.iloc[:,0:13]

#print(X.head(), '\n', test.head(),'\n')

model = tree.DecisionTreeRegressor()
model = model.fit(X,Y)

y_prediction = model.predict(test)

print(mean_squared_error(Y,y_prediction)
</code></pre>
",16174740.0,16174740.0,2022-07-30 14:58:27,2022-07-30 14:58:27,Found input variables with inconsistent number of samples,<python><regression><data-science><metrics><decision-tree>,0,2,N/A,CC BY-SA 4.0
73128321,1,-1.0,2022-07-26 18:43:20,0,148,"<p>I have a datafame:</p>
<pre><code>df = A.  B.  C.  D. 
     1.  2.  3.  4
     1.  2.  3.  5
     8.  1.  6.  5
     1.  2.  4.  3
</code></pre>
<p>And I want to apply a function between each combination of two rows into a new matrix, such that in the cell (i,j) I will have the value of applying the function on rows #i and #j.
So if my function is <code>scipy.stats.pearsonr</code> , in location (0,0) I will get 1, and the value of (0,1) is <code>pearsonr([1,2,3,4] , [1,2,3,5])[0] = 0.98</code>.</p>
<p>What is the best way to do so?</p>
",6057371.0,-1.0,N/A,2022-07-26 18:43:20,pandas how to apply pairwise function between each two rows,<python><pandas><dataframe><data-science>,0,5,N/A,CC BY-SA 4.0
73174520,1,-1.0,2022-07-30 09:44:42,2,67,"<p>My main Excel Sheet:- <a href=""https://i.stack.imgur.com/G34to.png"" rel=""nofollow noreferrer"">Excel Sheet</a>
My code:</p>
<pre><code>from pathlib import Path

import pandas as pd
p = Path(r'/.')
df = pd.concat([pd.read_excel(f) 
                for f in p.rglob('test.xlsx')],
               ignore_index=True)
df
df.to_excel(&quot;final.xlsx&quot;)
</code></pre>
<p>The output is:- <a href=""https://i.stack.imgur.com/aHhO6.png"" rel=""nofollow noreferrer"">My 'NA' strings get convert to NaN</a></p>
<p><a href=""https://i.stack.imgur.com/9p630.png"" rel=""nofollow noreferrer"">and after converting the dataframe to excel all my NA sting to blank.
</a></p>
<p>The solution to this was :- <strong>keep_default_na=False</strong>
while reading the datasheet.</p>
",19642484.0,19642484.0,2022-07-30 10:38:36,2022-07-30 10:38:36,Pandas Convert 'NA' to NaN during converting dataframe to .csv/.xlsx,<python><excel><pandas><dataframe><data-science>,0,5,N/A,CC BY-SA 4.0
73179954,1,-1.0,2022-07-31 00:35:09,4,1407,"<p>Hi everyone I was trying to create a flow using prefect for python but I get the error <strong>TypeError: 'fn' must be callable</strong>, I already install all the packages and I have tried in different IDE like vs code and colab  but the result is the same.</p>
<p>the code is:</p>
<pre><code>from prefect import task, Flow


@task()
def load():
    print(&quot;Loading data&quot;)

with Flow(&quot;Hello-world&quot;) as flow:
    load()

flow.run() 


#ERROR
TypeError: 'fn' must be callable
</code></pre>
<p>I tried changing the code to this but I get a different error:</p>
<pre><code>from prefect import task, Flow


@task()
def load():
    print(&quot;Loading data&quot;)

with Flow(name = &quot;Hello-world&quot;) as flow:
    load()

flow.run() 

#ERROR
TypeError: Flow.__init__() missing 1 required positional argument: 'fn'
</code></pre>
",13939406.0,-1.0,N/A,2022-08-09 04:18:38,Error to create a flow in prefect for python using with and object Flow(),<python><data-science><etl><prefect>,2,0,N/A,CC BY-SA 4.0
73190014,1,-1.0,2022-08-01 07:14:22,0,45,"<p>i am having a problem turning a given .txt-file into a .csv-file.
I need to turn:</p>
<pre><code>Ch   Len  Typ   Delta t                 Time                   Data

00   08   16    00:18:41.728.560.640    00:18:41.728.560.640   9D A1 31 29

00   08   00    00:00:00.000.993.280    00:18:41.729.553.920   1D A0 32 2B
</code></pre>
<p>into:</p>
<pre><code>Ch,Len,Typ,Delta t,Time,Data

00,08,16,00:18:41.728.560.640,00:18:41.728.560.640,9D A1 31 29

00,08,00,00:00:00.000.993.280,00:18:41.729.553.920,1D A0 32 2B
</code></pre>
<p>I have tried reading the .txt-file with pandas but I don't know how to seperate the different tabbings. For example the spaces between the data bytes need to be ignored.</p>
<pre><code>df = pd.read_csv(self.complete_filename, sep='\s+')
df.to_csv('this_csv.csv')
</code></pre>
<p>Maybe you can help me out.
Thank you!</p>
",16352715.0,-1.0,N/A,2022-08-01 07:14:22,Pandas .txt-file into .csv-file -- Delimiter/Seperator,<python><pandas><dataframe><data-science>,0,5,N/A,CC BY-SA 4.0
73202695,1,-1.0,2022-08-02 06:08:36,-1,581,"<p>I have a target object and a lot of other objects in an image. Target object is pre-defined with a known shape in advance as shown in figure.</p>
<p><img src=""https://i.stack.imgur.com/C0r8o.png"" alt=""Target image"" /></p>
<p>My task is to detect all the target object present in the image and find the angle at which the detected object is oriented with respect to target object. For the object detection purpose I am using YOLO-V5-OBB model which gives me detection confidence and the rotated bounding box coordinates.See the result below</p>
<p><a href=""https://i.stack.imgur.com/cqnEZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cqnEZ.jpg"" alt=""detected result"" /></a></p>
<p>I would like to know how rotation angle is predicted by yolo-obb model in order to make rotated bounding boxes around the detected objects?</p>
",8673453.0,8673453.0,2022-08-24 05:16:44,2022-08-24 05:16:44,Image comparison and Angle Estimation,<data-science><artificial-intelligence><angle><yolov5><image-comparison>,2,1,2022-08-02 20:42:38,CC BY-SA 4.0
73158095,1,-1.0,2022-07-28 19:12:12,0,51,"<p>I'm using Decision Tree Classifier with class weights {0:0.2, 1:0.8}, but I got an error &quot;Class label 1 not present.&quot;, can anyone help me to fix this problem? thank you.</p>
<p>There is the link to my code:
<a href=""https://colab.research.google.com/drive/1riuGXWXKnfH8GbqlpFV1RC41gn2mMtIn?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1riuGXWXKnfH8GbqlpFV1RC41gn2mMtIn?usp=sharing</a></p>
<pre><code># Drop dependent variable from dataframe and create the X(independent variable) matrix
x = data.drop(columns = 'BAD')
# Create dummy variables for the categorical variables - Hint: use the get_dummies() function
x = pd.get_dummies(x, drop_first = True)
# Create y(dependent varibale)
y = data['BAD']

# Split the data into training and test set
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1) 

#Defining Decision tree model with class weights class_weight={0: 0.2, 1: 0.8}
d_tree =  DecisionTreeClassifier(class_weight = {0:0.2, 1:0.8})

#fitting Decision tree model
d_tree.fit(x_train, y_train)
</code></pre>
<p><a href=""https://i.stack.imgur.com/eCDpo.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",19643031.0,-1.0,N/A,2022-07-28 19:12:12,Class label 1 not present,<python><data-science>,0,5,N/A,CC BY-SA 4.0
73210803,1,-1.0,2022-08-02 16:27:28,0,380,"<p>I have time series data or and one group has multiple rows. So, I need to transform that multiple rows into a single row for a group in python.
I have this dummy dataset.</p>
<pre><code>data = [
  ['A','test1',1,2,3,4],
  ['A','test1',5,6,7,8],
  ['A','test1',9,10,11,12],
  ['B','test2',13,14,15,16],
  ['B','test2',17,18,19,20],
  ['B','test2',21,22,23,24],
  ['C','test3',29,30,31,32],
  ['C','test3',33,34,35,36],
  ['C','test3',37,38,39,40],

]

df = pd.DataFrame(data, columns=['Name', 'cate','num1', 'num2', 'num3', 'num4'])
df
</code></pre>
<p>And I want to change to this format in Python. Can somebody help me to change multiple rows into single rows?
<a href=""https://i.stack.imgur.com/K1jwW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K1jwW.png"" alt=""enter image description here"" /></a></p>
",16875907.0,-1.0,N/A,2022-08-02 16:39:45,how to convert multiple rows into single column for a time series data?,<python><pandas><dataframe><feature-engineering><data-science-experience>,2,0,N/A,CC BY-SA 4.0
73216470,1,-1.0,2022-08-03 04:57:34,0,69,"<p>I can't see environments in my Conda as in this picture. I used Anaconda Prompt and type <code>conda info --envs</code> and shows this error message</p>
<pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

Traceback (most recent call last):
  File &quot;C:\Users\Acer\anaconda3\lib\site-packages\conda\exceptions.py&quot;, line 1082, in __call__
    return func(*args, **kwargs)
  File &quot;C:\Users\Acer\anaconda3\lib\site-packages\conda\cli\main.py&quot;, line 87, in _main
    exit_code = do_call(args, p)
  File &quot;C:\Users\Acer\anaconda3\lib\site-packages\conda\cli\conda_argparse.py&quot;, line 84, in do_call
    return getattr(module, func_name)(args, parser)
  File &quot;C:\Users\Acer\anaconda3\lib\site-packages\conda\cli\main_info.py&quot;, line 317, in execute
    info_dict = get_info_dict(args.system)
  File &quot;C:\Users\Acer\anaconda3\lib\site-packages\conda\cli\main_info.py&quot;, line 163, in get_info_dict
    pkgs_dirs=context.pkgs_dirs,
  File &quot;C:\Users\Acer\anaconda3\lib\site-packages\conda\base\context.py&quot;, line 530, in pkgs_dirs
    fixed_dirs += user_data_dir(APP_NAME, APP_NAME),
  File &quot;C:\Users\Acer\anaconda3\lib\site-packages\conda\_vendor\appdirs.py&quot;, line 67, in user_data_dir
    path = os.path.join(_get_win_folder(const), appauthor, appname)
  File &quot;C:\Users\Acer\anaconda3\lib\site-packages\conda\_vendor\appdirs.py&quot;, line 284, in _get_win_folder_with_pywin32
    from win32com.shell import shellcon, shell
ImportError: DLL load failed while importing shell: The specified procedure could not be found.
</code></pre>
<p>Can someone help me and know the reason this happening? <a href=""https://i.stack.imgur.com/fCJ1O.jpg"" rel=""nofollow noreferrer"">Screenshot of trouble</a></p>
",19524997.0,19524997.0,2022-08-04 03:32:52,2022-08-04 03:32:52,Not able to see Conda environments,<python><jupyter-notebook><data-science><anaconda3>,0,3,N/A,CC BY-SA 4.0
73128955,1,-1.0,2022-07-26 19:44:36,1,89,"<p>i am quite new on R ! I am a student in econometrics, but still quite new to data science, and i am currently trying to manipulate a dataset and see what interesting facts i can get out of it.  It is mostly on the econometrics part, trying to find an appropriate model, that i am looking for help (i will be happy to receive any kind of advice on any part of my code though, as i am still learning how to use R !).</p>
<p>My dataset is one of employees of a firm, which has levels (hierarchy), and different wages. It is a dataset on one year but i have a similar dataset for each of the previous years and ultimately i would like to group them to work on the evolution of wages in each level.</p>
<p>I started by computing basic statistics on the dataset, following the advice i had been given on a previous post :</p>
<pre><code>df%&gt;%
  select(level, sex)%&gt;%
  group_by(level)%&gt;%
  summarise(mean = mean(sex == &quot;F&quot;))
</code></pre>
<p>And cleaning my dataset :</p>
<pre><code> df &lt;- transform(df, 
                sex=factor(ifelse(sex==&quot;NA&quot;, NA, sex)),
                age=as.numeric(age),
                time_passed=as.numeric(time_passed),
                level=factor(ifelse(level==&quot;&quot;, NA, level), ordered = TRUE),
                wage=as.numeric(wage))


 summary(df)
</code></pre>
<p>I plotted age across levels :</p>
<pre><code>plot(age ~ level, data=df)

 library(ggplot2)
 ggplot(df, aes(x=level, y=age, fill=sex)) +
   geom_boxplot() 
</code></pre>
<p>I tried to remove the NA, but failed when coding this (it did not remove the NA) :</p>
<pre><code>ggplot(df[!is.na(df$level),], aes(x=level, y=age, fill=sex)) +
  geom_boxplot() 
</code></pre>
<p>The linear regression showed level as a significant covariate, yet i would like to do it the other way round (level may be explained by age, not the other way round : i but &quot;level&quot; is a categorical variable...)</p>
<pre><code> lm(age ~ level, data=df)
</code></pre>
<p>I then ran an alternative test for correlation of ranked (ordinal) data, the Spearman rank test, to establish whether there is a significant correlation :</p>
<pre><code> library(pspearman)
 with(df, spearman.test(age, level))
</code></pre>
<p>I also tried to look at the differences in the mean age for each level (i am telling this so you might see the different things i have tried, i do not really have a pre established workflow so it may seem a bit messy, i am quite new to data science) :</p>
<pre><code>(library dplyr)
df %&gt;%
  filter(sex == &quot;F&quot; | sex == &quot;H&quot;) %&gt;%
  ggplot(aes(level, age)) +
  geom_point() +
  facet_wrap(~sex) + 
  ggtitle (&quot;age per level, for each sex&quot;)
</code></pre>
<p>I have also tried to follow this workflow, but i think it is limited for my dataset :
<a href=""https://data.library.virginia.edu/understanding-ordered-factors-in-a-linear-model/"" rel=""nofollow noreferrer"">https://data.library.virginia.edu/understanding-ordered-factors-in-a-linear-model/</a></p>
<p>I think a categorical regression model would be nice, i am thinking about a logistic model : i have tried the following method, yet it gives me non significant coefficients which seems wrong (wages are in fact an increasing function of level, which was not rendered by the model plotted then) :
<a href=""https://www.geeksforgeeks.org/regression-with-categorical-variables-in-r-programming/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/regression-with-categorical-variables-in-r-programming/</a></p>
<p>So you can see how i translated the model in the above article to my dataset, i will put it here :</p>
<pre><code> df$sex = as.factor(df$sex)
 df$level = as.factor(df$level)

 # two-way table of factor variable

 xtabs(~level + age, data = df)

 # partitioning the data
 str(df)

 set.seed(1234)
 data1&lt;-sample(2, nrow(df), 
          replace = T, 
          prob = c(0.6, 0.4))
train&lt;-df[data1 == 1,]
test&lt;-df[data1 == 2,]

# Now build a logistic regression model for our data. glm() function helps us to establish a 
neural network for our data. 


mymodel&lt;-glm(wage ~ age + sex + time_passed + level, 
         data = train, 
         family = 'binomial')
summary(mymodel)
</code></pre>
<p>So my main question is : what do you recommend to tackle this dataset (i am also grateful for any reading advice, and not afraid to tackle more elaborate models !) ?
Also, i have the same dataset for other years and i wonder if there is a function in R that could enable me to visualise and manipulate the levels over the year (i would like to see if the mean wages for the people in one level evolves throughout the years or not, for example, or the standard deviation).</p>
<p>My dataset has the following structure :</p>
<pre><code>structure(list(sex = c(&quot;F&quot;, &quot;H&quot;, &quot;F&quot;, &quot;F&quot;, &quot;H&quot;, &quot;F&quot;), age = c(&quot;24&quot;, 
&quot;33&quot;, &quot;53&quot;, &quot;32&quot;, &quot;38&quot;, &quot;21&quot;), time_passed = c(&quot;0&quot;, &quot;3&quot;, &quot;4&quot;, 
&quot;0&quot;, &quot;2&quot;, &quot;0&quot;), level = c(&quot;N7  &quot;, &quot;N7  &quot;, &quot;N9  &quot;, &quot;N7  &quot;, &quot;N8  &quot;, 
&quot;    &quot;), wage = c(&quot;2605&quot;, &quot;4931&quot;, &quot;11123&quot;, &quot;3750&quot;, &quot;6180&quot;, &quot;858,31&quot;
)), row.names = c(NA, 6L), class = &quot;data.frame&quot;)
</code></pre>
<p>I hope i haven't been too long, and thank you in advance for the help you might provide !</p>
",19627807.0,5784757.0,2022-07-27 10:20:13,2022-07-27 10:20:13,fitting the good regression for categorical data on R,<r><statistics><regression><data-science>,0,4,N/A,CC BY-SA 4.0
73129297,1,73129641.0,2022-07-26 20:18:42,0,511,"<p>Here is part of my data:
<a href=""https://i.stack.imgur.com/KiF4a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KiF4a.png"" alt=""data"" /></a></p>
<p>I want to create a dataframe with every row after the row with L = ball_snap in it. So not only that specific row but every row after it as well.</p>
<p>How would I do that in pandas?</p>
",16030581.0,-1.0,N/A,2022-07-27 00:12:17,Every row after a certain value,<pandas><data-science>,3,3,N/A,CC BY-SA 4.0
73141157,1,-1.0,2022-07-27 15:58:13,1,24,"<p>As the title suggests, I want to create a filtered Twitter based on the sentiment of the tweets in my feed.</p>
<p>I believe I can do the first bit of obtaining the Twitter data and then running a ML model from Hugging Face. I've done that using Quix and this tutorial (<a href=""https://quix.io/how-to-build-no-code-pipeline-sentiment-analysis/"" rel=""nofollow noreferrer"">https://quix.io/how-to-build-no-code-pipeline-sentiment-analysis/</a>) But I don't know whether I have the ability to publish the feed to a web application.</p>
<p>Any pointers would be greatly appreciated! FYI I'm very junior!!</p>
",19634212.0,-1.0,N/A,2022-07-27 15:58:13,"Creating a ""good vibes only"" Twitter feed",<twitter><data-science><streaming>,0,0,N/A,CC BY-SA 4.0
73159420,1,73159667.0,2022-07-28 21:33:57,0,75,"<p>I need to separate the rows between when Level is 1 until it is 1 again. These groups of rows will be separate dfs and grouped by their part name at level 1. But I will take care of that later. For now, I can't seem to figure out how to get all the rows between the systems as the amount of levels in a system are inconsistent.</p>
<p>My current work in progress is getting all the indexes where level is 1, then retrieving the rows in between. Is is very time consuming as some dfs will have about 50 systems.</p>
<pre><code>df = pd.DataFrame({'Name':[&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;ABC&quot;,&quot;ABC&quot;,&quot;AAB&quot;,&quot;AAB&quot;,&quot;AAB&quot;] ,'Level': [1,1,2,1,3,1,4,2],'Part':[&quot;Upper System&quot;,&quot;Upper 2 System&quot;,&quot;Upper 2 stock&quot;,&quot;Upper System&quot;,&quot;Middle&quot;,&quot;Limits System&quot;, &quot;Deck&quot;, &quot;Ceiling&quot;]})


    Name    Level   Part
0   A          1    Upper System
1   A          1    Upper 2 System
2   A          2    Upper 2 stock
3   ABC        1    Upper System
4   ABC        3    Middle
5   AAB        1    Limits System
6   AAB        4    Deck
7   AAB        2    Ceiling
</code></pre>
<p>I need the dfs to look like something like these</p>
<pre><code>    Name    Level   Part
0   A          1    Upper

    Name    Level   Part
0   A          1    Upper 2 System
1   A          2    Upper 2 stock

    Name    Level   Part
0   ABC        1    Upper System
1   ABC        3    Middle

    Name    Level   Part
0   AAB        1    Limits System
1   AAB        4    Deck
2   AAB        2    Ceiling
</code></pre>
",14352335.0,14352335.0,2022-07-28 21:48:53,2022-07-28 23:13:48,How to get rows between a value until the value appears again,<python><pandas><data-science>,4,2,N/A,CC BY-SA 4.0
73165936,1,-1.0,2022-07-29 11:23:03,0,48,"<p>How can I merge all the xlsx with a specific name in a given folder and subfolder. I mean it want it to recurse and find all the .xlsx with the same name</p>
",19642484.0,-1.0,N/A,2022-07-29 13:48:16,Merge all .xlsx of same name in all subdirectory,<python><pandas><dataframe><data-science>,1,4,N/A,CC BY-SA 4.0
73227760,1,-1.0,2022-08-03 21:00:20,0,24,"<p>I want to know if theres someway to only mantain one row if theres is some other rows that are similar, something that works similar as <code>stringsim</code>, doing it between all the roes, and deleting those duplicaste based in that criteria.</p>
",18473320.0,-1.0,N/A,2022-08-03 21:00:20,Remove almost identical data frame rows comparing them R,<r><data-science>,0,4,N/A,CC BY-SA 4.0
73240064,1,73246281.0,2022-08-04 17:26:51,1,146,"<p>I am trying to understand how dask.foldby works. Consider the following code.</p>
<pre><code>tasks = [{&quot;task_group&quot;: i // 10, &quot;numbers&quot;: list(range(i, i + 10))} for i in range(1, 1000, 10)]
tb = db.from_sequence(tasks)
</code></pre>
<p>I create a dask bag with 100 items.</p>
<pre><code>def aggregate_task(task):
    return np.array([sum(task['numbers'])] * 10000) # A relatively big result

def add(x, y):
    x = aggregate_task(x) if isinstance(x, dict) else x
    y = aggregate_task(y) if isinstance(y, dict) else y
    return x + y

res = tb.foldby(lambda task: task['task_group'] % 5, add, split_every=25)
</code></pre>
<p>I then fold the items by a certain grouping function, to calculate 5 sums.</p>
<pre><code>res.compute()
[(0, array([96100, 96100, 96100, ..., 96100, 96100, 96100])),
 (1, array([98100, 98100, 98100, ..., 98100, 98100, 98100])),
 (2, array([100100, 100100, 100100, ..., 100100, 100100, 100100])),
 (3, array([102100, 102100, 102100, ..., 102100, 102100, 102100])),
 (4, array([104100, 104100, 104100, ..., 104100, 104100, 104100]))]
</code></pre>
<p>When I look at the res task graph, I see this:
<a href=""https://i.stack.imgur.com/TvTB1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TvTB1.png"" alt=""enter image description here"" /></a></p>
<p>It seems like the result of folding is stored in a single worker / single partition (I am not too sure that 1 partition = 1 worker). I can confirm that <code>res</code> has only one partition.</p>
<pre><code>&gt;&gt; res.npartitions
1
</code></pre>
<p>My question is: Is it possible to ask dask to hold the result of each foldby group in a separate partition (and hence separate workers)? The reason I ask is that, each of the resultant array is massive and I would like to keep them at different workers for memory reason. It will also make it possible to write those results to different files in disk in parallel.</p>
",2105924.0,2105924.0,2022-08-04 17:34:32,2022-08-05 07:39:12,Make dask foldby create multiple partitions,<python-3.x><data-science><dask><data-analysis><dask-distributed>,1,0,N/A,CC BY-SA 4.0
73255821,1,73255907.0,2022-08-05 22:28:46,1,36,"<p>I am wanting to plot many datasets on one plot in python. Currently, I have hardcoded 20 of the datasets for the sake of testing my plot. Here is the shortened code below:</p>
<pre><code>import numpy as np
import matplotlib as plt

N_steps = 1000
N_plot = 20
xorb_list = np.empty((N_plot, N_steps))
yorb_list = np.empty((N_plot, N_steps))
xobs = np.empty((N_plot, N_steps))
yobs = np.empty((N_plot, N_steps))
zobs = np.empty((N_plot, N_steps))


for i in range(N_plot):
   for k in range(N_steps):
        xorb_list[i][k] = r[k] * cm_to_arcsec * cos_theta[k]
        yorb_list[i][k] = r[k] * cm_to_arcsec * sin_theta[k]
        xobs[i][k] = r[k] * (cos_lon * cos_w_plus_nu[k] - sin_lon * sin_w_plus_nu[k] * cos_i)
        yobs[i][k] = r[k] * (sin_lon * cos_w_plus_nu[k] + cos_lon * sin_w_plus_nu[k] * cos_i)
        zobs[i][k] = r[k] * sin_i * sin_w_plus_nu[k]

# Plotting the observed orbits

fig1 = plt.figure(1)
ax1 = fig1.add_subplot()
ax1.set_aspect(1)
ax1.set_title(&quot;Observed Stellar Orbits&quot;)

ax1.plot(xobs[0], yobs[0])
ax1.plot(xobs[1], yobs[1])
ax1.plot(xobs[2], yobs[2])
ax1.plot(xobs[3], yobs[3])
ax1.plot(xobs[4], yobs[4])
ax1.plot(xobs[5], yobs[5])
ax1.plot(xobs[6], yobs[6])
ax1.plot(xobs[7], yobs[7])
ax1.plot(xobs[8], yobs[8])
ax1.plot(xobs[9], yobs[9])
ax1.plot(xobs[10], yobs[10])
ax1.plot(xobs[11], yobs[11])
ax1.plot(xobs[12], yobs[12])
ax1.plot(xobs[13], yobs[13])
ax1.plot(xobs[14], yobs[14])
ax1.plot(xobs[15], yobs[15])
ax1.plot(xobs[16], yobs[16])
ax1.plot(xobs[17], yobs[17])
ax1.plot(xobs[18], yobs[18])
ax1.plot(xobs[19], yobs[19])

plt.show()
</code></pre>
<p>The total number of datasets will be around 100, which will not be fun to hardcode. Is there some way to generate these datasets for any arbitrary number of sets?</p>
",18402408.0,-1.0,N/A,2022-08-05 22:46:52,How can I add many datasets to a single plot in python?,<python><data-science><physics>,1,2,N/A,CC BY-SA 4.0
73232159,1,73269835.0,2022-08-04 07:46:28,-2,1049,"<p>I am training a random forest model and looking how the accuracy of the model gets affected over a range of <code>min_sample_split</code>. This the graph I got</p>
<p><a href=""https://i.stack.imgur.com/uXg05.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uXg05.png"" alt=""enter image description here"" /></a></p>
<p>With increase in minimum number of sample split the overfitting decreases but the f1 score on the test data also decreases from 0.755 to around 0.73.</p>
<p>So should I keep the overfit model as it gives better f1 score or should reduce overfit and reduce my overfit score?</p>
",13415630.0,4685471.0,2022-08-04 09:11:08,2022-08-07 17:59:06,Trade-off between overfitting and f1 score on test data,<machine-learning><data-science><random-forest>,1,2,N/A,CC BY-SA 4.0
73248114,1,73248242.0,2022-08-05 10:11:31,1,50,"<p>I have gotten the assignment to analyze a dataset of 1.000+ houses, build a multiple regression model to predict prices and then select the three houses which are the cheapest compared to the predicted price. Other than selecting specifically three houses, there is also the constraint of a &quot;budget&quot; of 7.000.000 total for purchasing the three houses.</p>
<p>I have gotten so far as to develop the regression model as well as calculate the predicted prices and the risiduals and added them to the original dataset. I am however completely stumped as to how to write a code to select the three houses, given the budget restraint and optimizing for highest combined risidual.</p>
<p>Here is my code so far:</p>
<pre><code>### modules
import pandas as pd
import statsmodels.api as sm


### Data import
df = pd.DataFrame({&quot;Zip Code&quot; : [94127, 94110, 94112, 94114],
                   &quot;Days listed&quot; : [38, 40, 40, 40],
                   &quot;Price&quot; : [633000, 1100000, 440000, 1345000],
                   &quot;Bedrooms&quot; : [0, 3, 0, 0],
                   &quot;Loft&quot; : [1, 0, 1, 1],
                   &quot;Square feet&quot; : [1124, 2396, 625, 3384],
                   &quot;Lotsize&quot; : [2500, 1750, 2495, 2474],
                   &quot;Year&quot; : [1924, 1900, 1923, 1907]})

### Creating LM
y = df[&quot;Price&quot;] # dependent variable
x = df[[&quot;Zip Code&quot;, &quot;Days listed&quot;, &quot;Bedrooms&quot;, &quot;Loft&quot;, &quot;Square feet&quot;, &quot;Lotsize&quot;, &quot;Year&quot;]]

x = sm.add_constant(x) # adds a constant
lm = sm.OLS(y,x).fit() # fitting the model

# predict house prices
prices = sm.add_constant(x)

### Summary
#print(lm.summary())

### Adding predicted values and risidual values to df
df[&quot;predicted&quot;] = pd.DataFrame(lm.predict(prices)) # predicted values
df[&quot;risidual&quot;] = df[&quot;Price&quot;] - df[&quot;predicted&quot;] # risidual values
</code></pre>
<p>If anyone has an idea, could you explain to me the steps and give a code example?
Thank you very much!</p>
",13297252.0,13297252.0,2022-08-05 15:13:39,2022-08-05 15:13:39,"Python data science: How to select three houses in dataset with budget constraint, optimizing for highest risidual between predicted and actual price",<python><data-science>,1,1,N/A,CC BY-SA 4.0
73256365,1,-1.0,2022-08-06 00:25:45,-2,63,"<p>This is a salary dataset composed of the following columns:</p>
<pre><code>['work_year', 'experience_level', 'employment_type',
   'job_title', 'salary', 'salary_currency', 'salary_in_usd',
   'employee_residence', 'remote_ratio', 'company_location',
   'company_size'],
  dtype='object')
</code></pre>
<ol>
<li><p>I want to look at the comparison between the features (experience_lvl, employment_type, job_title, salary_currency and remote ratio) and the label (salary).</p>
</li>
<li><p>I have to make the feature engineering part, which includes converting experience level, employment type and salary currency to suitable numerical values.</p>
</li>
<li><p>How can that be done? What is the optimal solution in this case?</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/BzUCs.jpg"" rel=""nofollow noreferrer"">The three columns that have to be converted</a></p>
",19471664.0,4685471.0,2022-08-06 03:57:51,2022-08-06 23:00:13,Converting objects into suitable numerical values,<python><machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
73265598,1,-1.0,2022-08-07 07:17:22,-1,67,"<p>I want to train a machine learning algorithm with two target variables y1 and y2. One of the solutions that i think of is to train each target separately and then combine the predictions of the two models, but i do not know how to combine the two models. And even I do not know if this is a good idea .. any suggestions ? <a href=""https://i.stack.imgur.com/4Huct.jpg"" rel=""nofollow noreferrer"">description</a></p>
",13139211.0,-1.0,N/A,2022-08-07 21:16:00,how to train machine learning algorithm with two target y1 and y2?,<machine-learning><data-science><artificial-intelligence>,1,1,N/A,CC BY-SA 4.0
73266721,1,-1.0,2022-08-07 10:37:44,0,127,"<p>I am trying to preprocess my data for NLP model. I wrote this code to remove numbers, symbols and hyper links. But now I want to delete every line that has a specific instance of the word 'system'. I don't seem to figure how to do that. <code>df</code> is my dataframe and <code>df['Content']</code> is where I have the text I want to delete the line from.</p>
<p>for example the text can be :</p>
<p>&quot;system: hi im the line that is meant to be deleted
Leena: this line must not be deleted
system: hi again im the line that is meant to be deleted &quot;</p>
<p>the output should be :
Leena: this line must not be deleted</p>
<pre><code>def CleaningTXT(df):
    Allchat=list()
    lines=df['Content'].values.tolist()
    for text in lines:
        text=text.lower()
        #remove links
        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        text = pattern.sub('', text)
        #remove session join/leave
        pattern = re.compile('new party join session')
        text = pattern.sub('', text)
        pattern = re.compile('new party leave session')
        text = pattern.sub('', text)
        #remove sympols
        text = re.sub(r&quot;[,.\&quot;!@#$%^&amp;*(){}?/;`~:&lt;&gt;+=-]&quot;, &quot;&quot;, text)
        #seperating words
        tokens = word_tokenize(text)
        table = str.maketrans('', '', string.punctuation)
        stripped = [w.translate(table) for w in tokens]
        #removing numbers
        words = [word for word in stripped if word.isalpha()]
        words = ' '.join(words)
        Allchat.append(words)
    return Allchat
</code></pre>
",-1.0,-1.0,2022-08-08 06:12:37,2022-08-08 06:12:37,deleting a specific line from a dataframe python NLP,<python><nlp><data-science><data-preprocessing>,1,0,N/A,CC BY-SA 4.0
73277236,1,-1.0,2022-08-08 11:34:48,0,87,"<p>I'm trying to make a seq2seq model following the Tensorflow documentation, which you can find here: <a href=""https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt"" rel=""nofollow noreferrer"">https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt</a>. However, while I'm trying to run the following lines of code:</p>
<pre class=""lang-py prettyprint-override""><code>class NMTDataset:
  def __init__(self, problem_type='en-ita'):
    self.problem_type = 'en-ita'
    self.inp_lang_tokenizer = None
    self.targ_lang_tokenizer = None
  
  def unicode_to_ascii(self, s):
    return &quot;&quot;.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')
  
  def preprocess_sentence(self, w):
    w = self.unicode_to_ascii(w.lower().strip())

    w = re.sub(r&quot;([?.!, ¿])&quot;, r&quot;\1&quot;, w)
    w = re.sub(r&quot;[^a-zA-Z?.!,¿]+&quot;, &quot; &quot;, w)

    w = w.strip()

    w = '&lt;start&gt;' + w + '&lt;end&gt;'
    return w

  def create_dataset(self, path, num_examples):
    lines = io.open(path, encoding='UTF-8').read().strip().split('\n')
    word_pairs = [[self.preprocess_sentence(w) for w in l.split('\t')] for l in lines[:num_examples]]

    return zip(*word_pairs)
    
  def tokenize(self, lang):
    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filter='', oov_token='&lt;OOV&gt;')
    lang_tokenizer.fit_on_texts(lang)

    tensor = lang_tokenizer.texts_to_sequences(lang)

    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')

    return tensor, lang_tokenizer

  def load_dataset(self, path, num_examples=None):
    targ_lang, inp_lang = self.create_dataset(path, num_examples)

    target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)
    input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)

    return input_tensor, target_tensor, self.targ_lang_tokenizer, self.inp_lang_tokenizer

  def call(self, num_examples, BATCH_SIZE, BUFFER_SIZE):
    file_path = download_NMT()
    input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)

    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)

    train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))
    train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

    val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))
    val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)

    return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer  

BUFFER_SIZE = 32000
BATCH_SIZE = 64
num_examples = 30000

dataset_creator = NMTDataset('en-ita')
train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BATCH_SIZE, BUFFER_SIZE)
</code></pre>
<p>the create_dataset function, and especially the <code>lines</code> variable returns me the following ValueError:</p>
<pre><code>ValueError: too many values to unpack (expected 2)
</code></pre>
<p>why is that? can you help me fix it?</p>
",15513487.0,15513487.0,2022-08-08 11:58:44,2022-08-08 11:58:44,ValueError: too many values to unpack (expected 2) in python while I try to run a function,<python><function><machine-learning><data-science>,0,3,N/A,CC BY-SA 4.0
73232044,1,-1.0,2022-08-04 07:38:09,0,235,"<p>In my dataframe, I have some Null values. I want to calculate the correlation, so does my Null values affect my correlation value or shall I replace the Null values with 0 and then find the correlation?</p>
",19689239.0,-1.0,N/A,2022-08-04 07:55:47,Correlation for Null Values,<python><statistics><data-science><data-analysis>,1,2,N/A,CC BY-SA 4.0
73246227,1,73250416.0,2022-08-05 07:34:08,1,345,"<p>How do I add existing Node properties of a node in the graph projection to the ML pipeline?</p>
<p>As far as I know, the <code>gds.beta.pipeline.linkPrediction.addNodeProperty</code> procedure takes different other procedures to creates node embeddings as new node properties to the pipeline but how do I add existing ones? Getting the below error even though the properties are projected in the memory graph.</p>
<p><code>Failed to invoke procedure gds.beta.pipeline.linkPrediction.train: Caused by: java.lang.IllegalArgumentException: Node properties [property1, property2, property3] defined in the feature steps do not exist in the graph or part of the pipeline</code></p>
",16688284.0,-1.0,N/A,2022-08-05 13:16:24,Add existing node properties to link prediction pipeline in neo4j?,<neo4j><graph-data-science>,1,0,N/A,CC BY-SA 4.0
73267037,1,73267222.0,2022-08-07 11:32:48,0,135,"<p>I have a data of two column like this</p>
<pre><code>data.head()
   GroupId  Planet
0    0008   Europa
1    0008   Europa
2    0009   Mars
3    0010   Earth
4    0011   Earth
5    0012   Earth
6    0012   NaN
</code></pre>
<p>Planet column has missing values but same groupid shares same planet. How can i create dictionary for mapping to fill na like { '0008' : 'Europa', '0009' : 'Mars ...}</p>
",13938389.0,13938389.0,2022-08-07 12:26:52,2022-08-08 10:52:49,How to add values of two column to dictionary for mapping,<python><pandas><data-science>,2,2,N/A,CC BY-SA 4.0
73272021,1,-1.0,2022-08-08 00:58:34,-2,46,"<p>I need help with the codes below:</p>
<p>First I do this:</p>
<pre><code>medias = []
for col in dataset_1:
    medias.append(dataset_1[col][~(dataset_1[col] == '?')].median())
</code></pre>
<p>After i tried to replace &quot;?&quot; for the median:</p>
<pre><code>for col in dataset_1:
    for media in medias:
        dataset_1[col].replace('?', media)
</code></pre>
<p>But didn't work! What i'm doing wrong?</p>
",18289072.0,18289072.0,2022-08-08 01:02:23,2022-08-08 01:31:34,Replace a string into numeric value,<python><pandas><dataframe><data-science>,1,2,N/A,CC BY-SA 4.0
73280153,1,-1.0,2022-08-08 15:11:42,3,218,"<p>I have a lot of unstructured data that conveys a set of certain actions. For example:</p>
<pre><code>Sentence 1: build and paint chain link fence with black coating, post and rail to be red coat
Sentence 2: new roll door and temp slide door - additional ac installation
Sentence 3: the worker shall : - remove exist fence panel and scrap
</code></pre>
<p>I have many such sentences in the dataset. I am trying to extract the main actions from these sentences, for example, the main actions in above sentences would be:</p>
<pre><code>Sentence 1: build and paint chain link fence
Sentence 2: additional ac installation
Sentence 3: remove exist fence panel and scrap
</code></pre>
<p>So far I have done <code>Topic Modelling</code> using this wonderful package called <a href=""https://maartengr.github.io/BERTopic/index.html"" rel=""nofollow noreferrer"">BERTopic</a>. It clusters the sentences and extracts topics using TF-IDF of the words in the sentences. Since it's a huge dataset, each cluster would have a lot of sentences and the topics obtained from those clusters will be less relevant.</p>
<p>For example, BERTopic would create more <strong>item</strong> clusters (such as clusters having sentences with words: fence, gate, house etc. since they occur more and has greater TF-IDF) instead of <strong>action</strong> clusters (such as paint, roofing, remodel, roll out, installation etc).</p>
<p>To extract main actions, I was thinking of <code>Named Entity Recognition</code> but I am not sure how the model would identify phrases of varying lengths.</p>
<p>Is there a way in which I can extract the main context/task/action in a sentence?</p>
",4404805.0,-1.0,N/A,2022-08-08 15:11:42,NLP - Extract main actions/tasks from unstructured sentences,<python><nlp><data-science><topic-modeling>,0,0,N/A,CC BY-SA 4.0
73175062,1,-1.0,2022-07-30 11:05:43,-2,45,"<pre><code>df4 = [] 
for i in (my_data.points.values.tolist()[0]): 
    df3 = pd.json_normalize(j) 
df4.append(df3)
df5 = pd.DataFrame(df4) 
df5.head()
</code></pre>
<p>When I run this code I get this error: <code>Must pass 2-d input. shape=(16001, 1, 3)</code></p>
",16597870.0,4390160.0,2022-07-30 11:08:27,2022-07-31 05:09:59,How to create a dataframe?,<python><data-science>,1,4,N/A,CC BY-SA 4.0
73190393,1,-1.0,2022-08-01 07:46:20,0,76,"<p>Here, I have multiple excel sheet in multiple subdirectories. The total number of rows here is close to 60k.</p>
<p><strong>My code is:-</strong></p>
<pre><code>df = pd.concat([pd.read_excel(path,keep_default_na=False,dtype=str) 
            for path in glob.iglob(f'{directory}/**/MUMBAI.xlsx', recursive=True)],
           ignore_index=True)
</code></pre>
<p>The problem is it takes a total of <strong>3:30 Minutes</strong> to execute. How can I load is much efficiently and fast?</p>
<p>I'm Thankful for the support from the community.</p>
",19642484.0,-1.0,N/A,2022-08-01 07:49:17,Process 60k rows of excel,<python><excel><pandas><data-science><xlrd>,1,2,N/A,CC BY-SA 4.0
73190416,1,73190481.0,2022-08-01 07:48:19,1,179,"<pre><code>RangeIndex: 381732 entries, 0 to 381731
Data columns (total 10 columns):
 #   Column           Non-Null Count   Dtype  
---  ------           --------------   -----  
 0   Unnamed: 0       381732 non-null  int64  
 1   tweet_id         378731 non-null  float64
 2   time             378731 non-null  object 
 3   tweet            378731 non-null  object 
 4   retweet_count    336647 non-null  float64
 5   Unnamed: 0.1     336647 non-null  float64
 6   User             3001 non-null    object 
 7   Date_Created     3001 non-null    object 
 8   Source of Tweet  3001 non-null    object 
 9   Tweet            3001 non-null    object 
dtypes: float64(3), int64(1), object(6)
memory usage: 29.1+ MB
</code></pre>
<pre><code>df = df.drop(['Unnamed: 0','Unnamed: 0.1','User','Date_Created','Source of Tweet'],axis =1)
df.head()
</code></pre>
<p>i wrote this code to drop unwanted columns from my dataframe but i am encountering keyError not found in axis</p>
<pre><code>KeyError: &quot;['Unnamed: 0', 'Unnamed: 0.1', 'User', 'Date_Created', 'Source of Tweet'] not found in axis&quot;
</code></pre>
",19500489.0,-1.0,N/A,2022-08-01 07:54:54,Key Error Raise when trying to delete an existing column,<pandas><dataframe><data-science><data-cleaning>,1,0,N/A,CC BY-SA 4.0
73196084,1,-1.0,2022-08-01 15:25:54,0,35,"<p>This is My General Cross Validation function:</p>
<pre><code>    &quot;&quot;&quot;
    General model cross validation
    &quot;&quot;&quot;
    warnings.filterwarnings('ignore')
    tscv = method(n_splits=n_splits)
    dt={&quot;Train RMSE&quot;:[],'CV RMSE':[], &quot;AIC&quot;:[], &quot;BIC&quot;:[]} 
    if 'exog' in model_params:
        exog = model_params['exog']
    for train_index, test_index in tqdm(tscv.split(df)):
        if 'exog' in model_params:
            X_train = exog.iloc[train_index]
            x_val = exog.iloc[test_index]
            model_params['exog'] = X_train
        else:
            X_train = None
            x_val = None
        train_fold = df.iloc[train_index]
        cv_fold = df.iloc[test_index]
        # if 'enforce_stationarity' in model_params:
        #     model_params['enforce_stationarity'] = True
        if 'use_exact_diffuse' in model_params:
            model_params['use_exact_diffuse'] = True

        forecast_model = model(train_fold, **model_params).fit()
        try:
            if X_train is not None:
                train_preds = forecast_model.predict(start=train_fold.index[0], end=train_fold.index[-1], exog=X_train)
            else:
                train_preds= forecast_model.predict(start=train_fold.index[0], end=train_fold.index[-1])
        except Exception as e:
            print(e)
            pass
        
        dt['Train RMSE'].append(mean_squared_error(train_fold[0:], train_preds,squared=False))
        if x_val is not None:
            test_preds = forecast_model.predict(start=cv_fold.index[0], end=cv_fold.index[-1], exog=x_val)
        else:
            test_preds = forecast_model.predict(start=cv_fold.index[0], end=cv_fold.index[-1])
        dt['CV RMSE'].append(mean_squared_error(cv_fold, test_preds,squared=False))
        dt['AIC'].append(forecast_model.aic)
        dt['BIC'].append(forecast_model.bic)
    if return_model:
        if 'exog' in model_params:
            model_params['exog'] = exog
        model_for_diagnostic = model(df, **model_params).fit()
        return pd.DataFrame(dt), model_for_diagnostic
    else:
        return pd.DataFrame(dt)
# To get summary info
def organize(df):
    cols = [pd.DataFrame(df[col]) for col in df.keys()]
    return pd.concat(cols, axis=1, keys=df.keys()).describe().T.drop(columns=['count', 'max' , '25%', '75%', '50%'])
</code></pre>
<p>This is my parallel helper function:</p>
<pre><code>    &quot;&quot;&quot;
    Helper function to return mean score of train and cv scores mainly for ARIMA related models
    &quot;&quot;&quot;
    try:
        df = general_cv(model,df,model_params=param_grid)
        train_rmse,cv_rmse =df[['Train RMSE', 'CV RMSE']].describe().loc['mean']
        return train_rmse,cv_rmse
    except Exception as e:
        return e
</code></pre>
<p>This is my Gridsearch function:</p>
<pre><code>    &quot;&quot;&quot;
    Gridsearch for ARIMA/etc
    &quot;&quot;&quot;
    warnings.filterwarnings('ignore')
    joblib_parallel = joblib.Parallel(n_jobs=n_jobs, verbose=verbose)
    try:
        train_rmse, cv_rmse = tuple(zip(*joblib_parallel((joblib.delayed(parallel_helper)(df, param, model) for param in param_grid))))
    except Exception as e:
        print(e)
    param_grid = np.array(param_grid)
    df_return = pd.DataFrame({
        &quot;Train RMSE&quot; : train_rmse,
        &quot;CV RMSE&quot; : cv_rmse,
        &quot;Order&quot; : [param['order'] if 'order' in param else np.nan for param in param_grid],
        &quot;Seasonal Order&quot; :  [param['seasonal_order'] if 'seasonal_order' in param else np.nan for param in param_grid],
        &quot;Trend param&quot; :  [param['trend'] if 'trend' in param else np.nan for param in param_grid],
        &quot;Autoregressive&quot; :  [param['autoregressive'] if 'autoregressive' in param else np.nan for param in param_grid],
        &quot;Seasonal&quot; :  [param['seasonal'] if 'seasonal' in param else np.nan for param in param_grid]
    })
    if sort:
        # display(df_return)
        return df_return.dropna(axis=1, thresh=.9).sort_values(by=['CV RMSE', 'Train RMSE'], na_position='last')
    else:
        return df_return.dropna(axis=1, thresh=0.9)
</code></pre>
<p>This is how I run my code:</p>
<pre><code>sarimax_param=list(dict (
    order = (p, 0, q),
    seasonal_order = (P, 0, Q, 7),
    exog=exogfull)
        for p in range(3)
                for q in range(3)
                    for P in range(3)
                        for Q in range(3))
sarimax_results = defaultdict ()
for gas in tqdm(gases) :
    sarimax_results[gas] = GridSearch (df[gas], param_grid = sarimax_param, model=SARIMAX, verbose=0, sort=True)
</code></pre>
<p>This is the error I get when I run my code:</p>
<pre><code>       'LinAlgError' object is not iterable

---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
C:\Users\ASHER~1.LAP\AppData\Local\Temp/ipykernel_25012/3656160125.py in &lt;module&gt;
     10 sarimax_results = defaultdict ()
     11 for gas in tqdm(gases) :
---&gt; 12     sarimax_results[gas] = GridSearch (df[gas], param_grid = sarimax_param, model=SARIMAX, verbose=0, sort=True)

C:\Users\ASHER~1.LAP\AppData\Local\Temp/ipykernel_25012/1404306223.py in GridSearch(df, param_grid, model, verbose, n_jobs, sort)
     11     param_grid = np.array(param_grid)
     12     df_return = pd.DataFrame({
---&gt; 13         &quot;Train RMSE&quot; : train_rmse,
     14         &quot;CV RMSE&quot; : cv_rmse,
     15         &quot;Order&quot; : [param['order'] if 'order' in param else np.nan for param in param_grid],

UnboundLocalError: local variable 'train_rmse' referenced before assignment
</code></pre>
<p>Anyone know what I did wrong?
This is the data: <a href=""https://github.com/MicroKryptx/TimeSeries"" rel=""nofollow noreferrer"">train.csv</a>
data is named: train.csv</p>
<p>When I change the value in the range loop from 2 to anything higher than 2, it returns me this error. But when it is set at 2 it works for some reason.
Another thing is when I run the code separately for each gas as I pivoted the train.csv dataset earlier, the gas CO works but the other 3 returns the error.</p>
",17808145.0,-1.0,N/A,2022-08-01 15:25:54,How to fix linalgerror object is not iterable?,<python><pandas><time-series><data-science><statsmodels>,0,3,N/A,CC BY-SA 4.0
73272953,1,-1.0,2022-08-08 04:31:32,2,37,"<p>I have data from <a href=""https://mitra.bukalapak.com/lite/grosir/kopi"" rel=""nofollow noreferrer"">https://mitra.bukalapak.com/lite/grosir/kopi</a></p>
<p>I am trying to extract information from this page. The page loads 7 items at a time, and I need to scroll to load all entries (for a total of 69). I am able to parse the HTML and get the information that I need for the first 10 entries, but I want to fully load all entries before parsing the HTML.</p>
<p>I am using python, selenium, and BeautifulSoup.</p>
",19714769.0,-1.0,N/A,2022-08-08 04:31:32,How to scrapping web(bukalapak) with auto scrolling,<python><data-science>,0,1,N/A,CC BY-SA 4.0
73278939,1,-1.0,2022-08-08 13:47:44,1,3088,"<p>How can we run Jupiter Notebook in PyCharm Community Edition? It says&quot; Once a notebook is opened, you can view all of its cells with both input and outputs. Editing and cell execution are not allowed.</p>
<p><img src=""https://i.stack.imgur.com/YMNYU.png"" alt=""1"" /></p>
",9471400.0,2395282.0,2022-08-08 16:22:08,2023-02-27 11:24:10,How to run and edit Jupyter Notebook Code in PyCharm Community Edition,<python><jupyter-notebook><pycharm><data-science>,2,0,N/A,CC BY-SA 4.0
73284029,1,-1.0,2022-08-08 21:06:54,1,167,"<p>Hi i have a pandas columns named YearWeekISO that has the YYYY- WNN date format, for instance 2020-W53. I want to convert that whole column to a date time format of
day-month-year.</p>
<p>This is the code i am using:
pd.to_datetime(df['YearWeekISO'], format=&quot;%Y-W%U&quot;).dt.strftime(&quot;%d/%m/%Y&quot;)</p>
<p>This is the error output:
Cannot use '%W' or '%U' without day and year</p>
<p>Thanks,best regards</p>
",19640755.0,-1.0,N/A,2022-08-08 21:23:39,"YYYY-WNN format to ""%d/%m/%Y"" date format",<python><pandas><data-science>,1,1,N/A,CC BY-SA 4.0
73281050,1,-1.0,2022-08-08 16:16:53,1,96,"<p>I have surface data Z over an [X,Y] mesh. In general Z = 0, but there will be peaks which stick up above this flat background, and these peaks will have roughly elliptical cross sections. These are diffraction intensity peaks, if anyone is curious. I would like to measure the elliptical cross section at about half the peak's maximum value.</p>
<p>So typically with diffraction, if it's a peak y = f(x), we want to look at the Full Width at Half Max (FWHM), which can be done by finding the peak's maximum, then intersecting the peak at that value and measuring the width. No problem.</p>
<p>Here I want to perform the analogous operation, but at higher dimension. If the peak had a circular cross section, then I could just do the FWHM = diameter of cross section. However, these peaks are elliptical, so I want to slice the peak at its half max and then fit an ellipse to the cross section. That way I can get the major and minor axes, inclination angle, and goodness of fit, all of which contain relevant information that a simple FWHM number would not provide.</p>
<p>I can hack together a way to do this, but it's slow and messy, so it feels like there must be a better way to do this.  So my question really just comes down to, has anyone done this kind of problem before, and if so, are there any modules that I could use to perform the calculation quickly and with a simple, clean code?</p>
",19719760.0,472495.0,2022-09-17 17:06:13,2022-09-17 17:06:13,"Python: How can I fit an ellipse to a cross-section of a peak in a 3D surface z = f(x,y)?",<python><math><data-science><curve-fitting><data-fitting>,0,5,N/A,CC BY-SA 4.0
73290699,1,-1.0,2022-08-09 10:58:57,0,28,"<pre><code>from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

model = LinearRegression()
model.fit(X_train.drop('assigned_room_type', axis=1), y_train)

# test set prediction results
predictions = model.predict(X_test)
print(f'MSE: {mean_squared_error(y_true=y_test, y_pred=predictions)}')
print(f'R-Squared: {r2_score(y_test, predictions)}')

</code></pre>
<p>whenever I am trying to build a model for almost every cleaned and uncleaned dataset throwing an issue like I couldn't convert from string to float
please help me out I need to change my code or do I need to do anything while cleaning a dataset?</p>
",19726043.0,-1.0,N/A,2022-08-09 10:58:57,could not convert string to float: 'City Hotel',<python><data-science>,0,2,N/A,CC BY-SA 4.0
73301384,1,73314570.0,2022-08-10 06:06:55,0,230,"<p>The <a href=""https://maartengr.github.io/BERTopic/"" rel=""nofollow noreferrer"">BerTopic</a> model resulted the below Topics:</p>
<p><a href=""https://i.stack.imgur.com/CNkp6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CNkp6.png"" alt=""enter image description here"" /></a></p>
<p>As you can see from the above, the model is finetuned to generate lesser outliers '-1' which has the count of 3 and it appears in the last.</p>
<p>While visualizing the <a href=""https://maartengr.github.io/BERTopic/getting_started/topicsperclass/topicsperclass.html"" rel=""nofollow noreferrer"">Topics per class</a>,</p>
<p><code>topic_model.visualize_topics_per_class(topics_per_class)</code></p>
<p>the below interactive visual is generated, and however it ignored the <code>0th</code> index, to be precise the Topic 0. The Global Topic Representations are displayed from <code>1, 2, 3, 4, 5, 6, -1</code></p>
<p><a href=""https://i.stack.imgur.com/fAdtt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fAdtt.png"" alt=""enter image description here"" /></a></p>
<p>Is the BerTopic designed in a way that it always assumes the very first index will be an outlier (<code>-1</code>), and eliminates it blindly?</p>
<p>Are the generated topics always accessed based on the count size, may be in descending order?</p>
",1793799.0,-1.0,N/A,2022-08-11 02:14:49,BerTopic Model - Visualization ignores 0th index,<python><data-science><artificial-intelligence><bert-language-model><topic-modeling>,1,0,N/A,CC BY-SA 4.0
73310178,1,-1.0,2022-08-10 17:03:08,0,114,"<p>Can someone explain the code in the photo, I do not understand how tqdm is used here. From my knowledge tqdm is used for iteration, so what does it iterate in the generator??
<a href=""https://i.stack.imgur.com/q2SYn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q2SYn.png"" alt=""enter image description here"" /></a></p>
",19737236.0,15521392.0,2022-08-10 17:06:30,2022-08-10 17:41:39,tqdm function with image data generator in a loop,<image><machine-learning><deep-learning><computer-vision><data-science>,0,2,N/A,CC BY-SA 4.0
73278108,1,-1.0,2022-08-08 12:44:39,-2,45,"<p>I want to check how many values are lower than 2500</p>
<p>1)Using .count(</p>
<pre class=""lang-py prettyprint-override""><code>df[df.price&lt;2500][&quot;price&quot;].count()
</code></pre>
<ol start=""2"">
<li>Using .values_counts()</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>df[df.price&lt;2500][&quot;price&quot;].value_counts()
</code></pre>
<p><a href=""https://i.stack.imgur.com/6vv0u.jpg"" rel=""nofollow noreferrer"">this ise code view</a></p>
<p>First one results 27540 and second 2050. Which one is correct count?</p>
",19718120.0,4765442.0,2022-08-08 16:05:54,2022-08-08 16:05:54,How to check dataframe column that contains value lower than 2500,<python><data-science>,1,2,N/A,CC BY-SA 4.0
73280368,1,-1.0,2022-08-08 15:25:58,1,253,"<p>I have a dataset which contains multiple of the same graph just with different node values.</p>
<p><a href=""https://i.stack.imgur.com/AG2pi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AG2pi.png"" alt=""enter image description here"" /></a></p>
<p>How would I create a model with PyTorch Geometric to predict a single missing value on the graph?</p>
<p><a href=""https://i.stack.imgur.com/wAPE8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wAPE8.png"" alt=""enter image description here"" /></a></p>
<p>I was thinking of training a GCN with a random mask on an individual example but from the document action it seems that a) <code>Data</code> only expects to take one graph with a mask as a training input b) <code>GCN</code> will make predictions on an entire graph not a single node.</p>
",13899026.0,-1.0,N/A,2022-08-08 15:25:58,How to make single node prediction regression model from training data of multiple graphs with PyTorch Geometric?,<python><machine-learning><pytorch><data-science><pytorch-geometric>,0,0,N/A,CC BY-SA 4.0
73291022,1,-1.0,2022-08-09 11:25:05,-1,268,"<p>I have a multi-index dataframe in Pandas with two index values like this:</p>
<pre><code>week_of_year    day_of_week

 1              0
 1              1
 .              .
 1              6

 2              0
 .              .
 .              .
 2              6
 .              .
</code></pre>
<p>I want to map the day_of_week index to <code>{'0': 'Monday', '1': 'Tuesday', ...}</code> but not sure how to access and map the second index.</p>
",19404770.0,18694400.0,2022-08-09 15:54:23,2022-08-09 15:54:23,Map multi-index column to new values in Pandas DataFrame,<python><pandas><dataframe><data-science><multi-index>,1,0,N/A,CC BY-SA 4.0
73297032,1,73321216.0,2022-08-09 19:19:01,1,97,"<p>iris dataset</p>
<p>data.describe()</p>
<p>#WE USE DISCRETIZATION BECAUSE IT CONVERT CONTINUOUS DATA INTO DICRETE DATA
#WE DOING DISTRETIZATION FOR EACH COLUMN
data['Sepal.Length'] = pd.cut(data['Sepal.Length'], bins = [data['Sepal.Length'].min(), data['Sepal.Length'].mean(), data['Sepal.Length'].max()], labels = [&quot;low&quot;,&quot;high&quot;])</p>
<p>data['Sepal.Width'] = pd.cut(data['Sepal.Width'], bins = [data['Sepal.Width'].min(), data['Sepal.Width'].mean(), data['Sepal.Width'].max()], labels = [&quot;low&quot;,&quot;high&quot;])</p>
<p>data['Petal.Length'] = pd.cut(data['Petal.Length'], bins = [data['Petal.Length'].min(), data['Petal.Length'].mean(), data['Petal.Length'].max()], labels = [&quot;low&quot;,&quot;high&quot;])</p>
<p>data['Petal.Width'] = pd.cut(data['Petal.Width'], bins = [data['Petal.Width'].min(), data['Petal.Width'].mean(), data['Petal.Width'].max()], labels = [&quot;low&quot;,&quot;high&quot;])</p>
<hr />
<p>#is there any method or short cut for this or by using for loop to discretized all columns at once</p>
<hr />
",19721122.0,-1.0,N/A,2022-08-15 16:53:30,how to do binning in discretization method and label it high low i have done this following method is there any short cut for binning and label itt,<data-science><data-cleaning>,1,0,N/A,CC BY-SA 4.0
73312085,1,73312384.0,2022-08-10 20:01:33,1,311,"<p>I have two pandas dataframes and I want to compare them to see what the differences are. df1 has a column of all unique IDs, a column of text data, and a column of numeric data. df2 has the same structure but it contains multiple records of the same IDs. I want to take a specific ID from df1 and its corresponding columns then compare it to all the matching IDs in df2 and their corresponding columns. Then i want to take the difference and put them into new df3.</p>
<p>EDIT: df3 should not have rows from df1 if it does not exist in df2</p>
<pre><code>import pandas as pd
data1 = {'ID':['L1', 'L2', 'L3', 'L4'], 'Text':['1A', '1B','1C','1D'], 'Num':[1, 2, 3, 4]}
df1 = pd.DataFrame(data1)
print(df1)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Text</th>
<th>Num</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1</td>
<td>1A</td>
<td>1</td>
</tr>
<tr>
<td>L2</td>
<td>1B</td>
<td>2</td>
</tr>
<tr>
<td>L3</td>
<td>1C</td>
<td>3</td>
</tr>
<tr>
<td>L4</td>
<td>1D</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<pre><code>data2 = {'ID':['L1', 'L2', 'L3', 'L1', 'L2', 'L3'], 'Text':['1A','1B','1C','2A','2B','1C'], 'Num':[1, 2, 3, 11,2,123]}
df2 = pd.DataFrame(data2)
print(df2)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Text</th>
<th>Num</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1</td>
<td>1A</td>
<td>1</td>
</tr>
<tr>
<td>L2</td>
<td>1B</td>
<td>2</td>
</tr>
<tr>
<td>L3</td>
<td>1C</td>
<td>3</td>
</tr>
<tr>
<td>L1</td>
<td>2A</td>
<td>11</td>
</tr>
<tr>
<td>L2</td>
<td>2B</td>
<td>2</td>
</tr>
<tr>
<td>L3</td>
<td>1C</td>
<td>13</td>
</tr>
</tbody>
</table>
</div>
<p>I want the out put to looks like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Text</th>
<th>Num</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1</td>
<td>2A</td>
<td>11</td>
</tr>
<tr>
<td>L2</td>
<td>2B</td>
<td>2</td>
</tr>
<tr>
<td>L3</td>
<td>1C</td>
<td>123</td>
</tr>
</tbody>
</table>
</div>",19738251.0,19738251.0,2022-08-11 13:58:33,2022-08-11 13:58:33,Compare pandas two different pandas dataframe to extract the difference,<python><pandas><data-science><analysis>,2,0,N/A,CC BY-SA 4.0
73258583,1,73343634.0,2022-08-06 09:20:53,0,126,"<p>I have a large dataset to run a specific Graph Data Science algorithm on.</p>
<p>The functional requirement is that the algorithm <strong>will be run often</strong> and that the dataset <strong>changes in real-time</strong>.</p>
<p>As I understand, in order to run an algorithm I have to project the persistent graph into memory first.</p>
<p>But, GDS only provides a projection of the whole dataset once (as a (filtered) snapshot), therefore, on each change to my dataset (i.e. a new relationship edge added between two nodes), I have to rerun the projection again, which seems <strong>quite an ineffective</strong> thing to do.</p>
<p>Is there a generic way to circumvent this and keep the Projection properly in sync with the persistent graph?</p>
",2123547.0,-1.0,N/A,2022-08-13 10:53:06,Keep a projected graph in synch with persisted graph in Neo4j GDS,<neo4j><graph-data-science>,1,1,N/A,CC BY-SA 4.0
73275061,1,73275135.0,2022-08-08 08:41:17,1,35,"<p>I have a dataframe</p>
<pre><code>&gt; df = C1. C2.  C3
&gt;      a.  1.   2
&gt;      a.  3.   5
&gt;      b.  6.   7 
&gt;      c.  0.   1 
&gt;      b.  2.   3
&gt;      a.  3.   1
</code></pre>
<p>I want to randomly select a value from C1 and take all its rows.
So if I select 'a' I will have:</p>
<pre><code>df = C1. C2.  C3
     a.  1.   2
     a.  3.   5
     a.  3.   1
</code></pre>
<p>How can I do it?
Thanks</p>
",6057371.0,-1.0,N/A,2022-08-08 08:47:27,Pandas select rows based on randomly selected group from a specific column,<python><pandas><dataframe><data-science>,2,0,N/A,CC BY-SA 4.0
73280535,1,73280587.0,2022-08-08 15:36:26,1,230,"<p>Really struggling to get this solution. Suppose I have the dataframe below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>SEX</th>
<th>ITEM</th>
<th>Some other column</th>
</tr>
</thead>
<tbody>
<tr>
<td>M</td>
<td>Socks</td>
<td>233</td>
</tr>
<tr>
<td>M</td>
<td>Socks</td>
<td>1</td>
</tr>
<tr>
<td>M</td>
<td>Hat</td>
<td>2</td>
</tr>
<tr>
<td>F</td>
<td>Socks</td>
<td>3</td>
</tr>
<tr>
<td>F</td>
<td>Hat</td>
<td>3</td>
</tr>
<tr>
<td>F</td>
<td>Hat</td>
<td>6</td>
</tr>
<tr>
<td>F</td>
<td>Hat</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>I would like to find the average number of occurrences of each ITEM based on the SEX group</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>SEX</th>
<th>ITEM</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>M</td>
<td>Socks</td>
<td>0.6666</td>
</tr>
<tr>
<td>M</td>
<td>Hat</td>
<td>0.3333</td>
</tr>
<tr>
<td>F</td>
<td>Socks</td>
<td>0.25</td>
</tr>
<tr>
<td>F</td>
<td>Hat</td>
<td>0.75</td>
</tr>
</tbody>
</table>
</div>
<p>Can anyone help me with this?</p>
",8565048.0,12370687.0,2022-08-21 03:39:35,2022-08-21 03:39:35,Group dataframe by two columns and then find average count based on one of the groups,<python><pandas><dataframe><group-by><data-science>,2,2,N/A,CC BY-SA 4.0
73315604,1,73315630.0,2022-08-11 05:21:09,0,899,"<p>This Syntax is the second line after uploading a csv file by pandas library and get_dummies drop,
i want to understand this syntax better in order to make use of it
thank !</p>
<pre><code>y = df['something'].apply(lambda x: 1 if x== 'yes' else 0)
</code></pre>
",19740084.0,2901002.0,2022-08-11 05:25:15,2022-08-11 05:25:15,"Meaning of ""y = df['something'].apply(lambda x: 1 if x== 'yes' else 0)""",<python><pandas><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
73326490,1,-1.0,2022-08-11 20:20:55,-1,30,"<p>New to data science. I am using the following code to print the number of times 'i' happens in any column:</p>
<pre><code>for i in np.arange(1,11):
  print(df[df['X']==i]['X'].value_counts())
</code></pre>
<p>The output I am getting is:</p>
<pre><code>1 4
Name:X, dtype:int64
2 3
Name:X, dtype:int64
.
.
.
</code></pre>
<p>Is this a list or an array? How to check/know this?
and how to convert this whole output into a dataframe?</p>
",19744584.0,-1.0,N/A,2022-08-11 20:22:27,How to convert the following output into a dataframe?,<python-3.x><pandas><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
73327979,1,73328132.0,2022-08-11 23:58:03,0,404,"<p>I have this Dataframe: <a href=""https://i.stack.imgur.com/XFy8v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XFy8v.png"" alt=""Dataframe"" /></a></p>
<p>I would like to copy the value of the Date column to the New_Date column, but not only to the same exact row, I want to every row that has the same User_ID value.</p>
<p>So, it will be: <a href=""https://i.stack.imgur.com/DbYLS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DbYLS.jpg"" alt=""like this!"" /></a></p>
<p>I tried groupby and then copy, but groupby made all values become lists and other columns with same user_id can have different values in different rows and then it messes up many things.</p>
<p>I tried also:</p>
<pre><code>df['New_Date'] = df.apply(lambda x: x['Date'] if x['User_ID'] == x['User_ID'] else x['New_Date'], axis=1)
</code></pre>
<p>But it only copied values to the same row and left the other two empty.</p>
<p>And this:</p>
<pre><code>if (df['User_ID'] == df['User_ID']):
    df['New_Date'] = np.where(df['New_Date'] == '', df['Date'], df['New_Date'])
</code></pre>
<p>None accomplished my intention.</p>
<p>Help is appreciated, Thanks!</p>
",16431450.0,-1.0,N/A,2022-08-12 00:29:57,Pandas conditionally copy values from one column to another row,<python><pandas><conditional-statements><data-science>,2,0,N/A,CC BY-SA 4.0
73323697,1,-1.0,2022-08-11 15:58:19,-2,359,"<p>My goal is to land a job in Data Science and I would like to ask the people who already work in this field and who can give me advise which Python Framework (<strong>Flask</strong> or <strong>Django</strong>) should I master / focus on?</p>
<p>My plan is to create machine learning projects and deploy them to a server, and present them as my experience since I don't have any actual work experience in this field. But I don't want to make a mistake spending hours and hours mastering framework that no one use and then learn again.</p>
<p>Thank You.</p>
",11248638.0,-1.0,N/A,2022-08-11 16:33:05,The Best Python Framework For Data Science,<python><machine-learning><frameworks><data-science>,2,1,2022-08-11 17:00:49,CC BY-SA 4.0
73325035,1,-1.0,2022-08-11 18:00:26,0,42,"<p>I have a dataframe with the scores of a two-class classification model...</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Observation</th>
<th>Class</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>0.5013</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0.4987</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>0.5010</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>0.4990</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0.5128</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>0.4872</td>
</tr>
</tbody>
</table>
</div>
<p>I only care about the &quot;winning&quot; class (either 0 or 1) and its corresponding probability (the max. probability). What is the best way to group or modify this dataframe to only have 3 observations (in this case) with the &quot;winning&quot; class (0 or 1) and the &quot;winning&quot; probability?</p>
<p>For example, my desired output...</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Observation</th>
<th>Class</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>0.5013</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>0.5010</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0.5128</td>
</tr>
</tbody>
</table>
</div>",19301651.0,-1.0,N/A,2022-08-12 08:39:14,Most efficient way to take max of classifier scores in Python and / or PySpark,<pandas><dataframe><pyspark><data-science>,3,0,N/A,CC BY-SA 4.0
73330185,1,-1.0,2022-08-12 06:44:02,0,40,"<p>I'm trying to migrate and upgrade my graph to the latest version of Neo4j and make use of new features and GDS algorithms. The old LPA community detection query was as follows:</p>
<pre><code>CALL algo.labelPropagation.stream(
'MATCH (p:Publication) RETURN id(p) as id',

'MATCH (p1:Publication)-[r1:HAS_WORD]-&gt;(w)&lt;-[r2:HAS_WORD]-(p2:Publication)
WHERE r1.occurrence &gt; 5 AND r2.occurrence &gt; 5
RETURN id(p1) as source, id(p2) as target, count(w) as weight',

{graph:'cypher',write:false, weightProperty : &quot;weight&quot;})

yield nodeId, label

WITH
label, collect(algo.asNode(nodeId)) as nodes where size(nodes) &gt; 2
MERGE (c:PublicationLPACommunity {id : label})
FOREACH (n in nodes |
 MERGE (n)-[:IN_LPA_COMMUNITY]-&gt;(c)
)

return label, nodes
</code></pre>
<p>I've been trying to understand the documentation for projecting a graph then performing community detection - and I think I've been close - but I just don't fully understand what is happening and how to project it correctly first in order to perform the LPA.
Here is my code so far:</p>
<pre><code>CALL gds.graph.project.cypher(
  'testProjection',
  'MATCH (p:Publication) RETURN id(p) AS id',
  'MATCH (p:Publication)-[r1:HAS_WORD]-&gt;(w)&lt;-[r2:HAS_WORD]-(p2:Publication) WHERE r1.occurrence &gt; 5 AND r2.occurrence &gt; 5 RETURN id(p1) as source, id(p2) as target, count(w) as weight'
)
YIELD
  graphName AS graph, nodeCount AS nodes, relationshipCount AS rels, weightProperty AS weight
</code></pre>
<p>I think I'm mixing up elements of the projection and elements of the algorithm - I can't figure out what should happen and why. I've managed to make simple graph projections with the Publication nodes in the past - but it seems like that isn't enough information to perform the LPA algorithm.</p>
<p>Any help very much appreciated.</p>
",19480934.0,-1.0,N/A,2022-08-12 06:44:02,Difficulty updating Neo4j LPA community detection to GDS version with graph projections,<algorithm><graph><neo4j><graph-data-science>,0,6,N/A,CC BY-SA 4.0
73286502,1,73286774.0,2022-08-09 04:20:36,1,462,"<pre><code>from sklearn.cluster import DBSCAN
model = DBSCAN(eps=3.3, leaf_size=5, min_samples=3)
y_pred = model.fit_predict(df)
</code></pre>
<p>my silhouette score is</p>
<pre><code>from sklearn.metrics import silhouette_score
silhouette_score(df, y_pred)
</code></pre>
<p>output</p>
<pre><code>0.4432857434946073
</code></pre>
<p>However, my labels are as so</p>
<p>code:</p>
<pre><code>set(model.labels_)
</code></pre>
<p>output:</p>
<pre><code>{-1, 0}
</code></pre>
<p>What does cluster <code>-1</code> and <code>0</code> mean, and how do I right this?</p>
<p>note: I don't know if this is important, but</p>
<pre><code>df.head()
</code></pre>
<p>output:</p>
<pre><code>Gender      Age      education  satisfaction    salary  performance 
----------------------------------------------------------------------
0   0.0     0.446350    -1.010909   -0.891688   1.153254    -0.108350   
1   1.0     1.322365    -0.147150   -1.868426   -0.660853   -0.291719   
2   1.0     0.008343    -0.887515   -0.891688   0.246200    -0.937654   
3   0.0     -0.429664   -0.764121   1.061787    0.246200    -0.763634   
4   1.0     -1.086676   -0.887515   -1.868426   -0.660853   -0.644858   
</code></pre>
<p>As you can see, my data is multidimensional, and I can't reduce the dimension</p>
",18258698.0,-1.0,N/A,2022-08-09 05:07:25,What does it mean when cluster label is -1?,<scikit-learn><data-science><cluster-analysis><data-analysis><dbscan>,1,0,N/A,CC BY-SA 4.0
73292420,1,73293605.0,2022-08-09 13:11:22,-1,69,"<p>on this plot i want to remove lines  on the graph directly  , how is it possible?</p>
<pre><code>import pandas as pd

y = {'sample 135': [12,15,20,15,2,10,6,8], 'sample 2654' : [12,15,20,15,2,5,9,15], 'sample 5454' : [1,2,10,6,8,12,15,20], 'sample 12454' : [15,22,10,6,8,22,25,29], 'sample 54' : [18,20,10,6,8,25,55,9], 'sample 424' : [5,2,10,6,8,4,5,8], 'sample 24545' : [9,12,2,4,55,2,3,7]} 
x = [1,2,3,4,5,6,7,8]

graph = pd.DataFrame(y,x)
graph.plot(kind='line', grid=True, title=&quot;outliers dashboard&quot;,xlabel=&quot; pixels&quot; , ylabel=&quot; absorbance&quot; )


</code></pre>
<p><a href=""https://i.stack.imgur.com/sYs9E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sYs9E.png"" alt=""enter image description here"" /></a></p>
",18643672.0,18643672.0,2022-08-18 09:07:42,2022-08-18 09:07:42,plot click on it?,<python><plotly><data-science>,1,1,N/A,CC BY-SA 4.0
73311419,1,-1.0,2022-08-10 18:58:57,0,56,"<p>Recently I have been learning Tensor Flow, and I have written a few machine learning programs, however, I am wondering in what way can I test the model on a single input and receive the prediction, and not just evaluate the accuracy of the model on a lot of data as you would do using the <code>model.fit()</code> function. I am also wondering how can I then implement the model in a script, that for example gathers data and feeds it into the model automatically to obtain the predictions and then for example plots the results on a graph.</p>
<p>Thanks in advance.</p>
",17002321.0,-1.0,N/A,2022-08-10 19:47:57,How do you apply a Tensor Flow model on a single input and obtain the actual prediction and how to implement the model in a separate script,<python><tensorflow><data-science>,1,1,N/A,CC BY-SA 4.0
73336554,1,-1.0,2022-08-12 15:33:24,0,143,"<p>Using R and the Metafor package, I'd like to know how I can run many random effect models on all combinations of all of my moderators. I know I can pre-select any combination of moderators to include manually, though I'm unsure how to loop over all of my moderators so that I can run the rma function as many times as there are moderator combinations (I assume with a loop).</p>
<p>Example of rma function with all 7 moderators</p>
<pre><code>mod &lt;- rma(smd, se, mods=~ A + B + C + D + E + F + G, data=dat)
</code></pre>
<p>I can't manually keep running the function over and over adjusting moderator combinations as that would be far too time consuming. Is there such a way to automate this process? My moderators are columns (variables) in a 'moderator' data-frame.</p>
<p>I assume I can loop over my mod variables changing them on each iteration, though I'm not sure how best to implement this. I have tried using a for loop to iterate over the moderators, updating the rma function accordingly but the function doesn't seem accept this. Here is a basic example of what I tried, this is just to see if I can insert one moderator at a time into the function:</p>
<pre><code>for (i in colnames(mods1)){
  rma(smd, sei=se, mods=~ mods1[i], data=dat)
}

&gt;&gt; invalid type (list) for variable 'mods1[i]'
</code></pre>
<p>Any help is appreciated.</p>
",7216494.0,7216494.0,2022-08-12 16:41:33,2022-08-12 16:41:33,Using Metafor to run many random effects models using all combinations of moderators,<r><data-science><metafor>,0,12,N/A,CC BY-SA 4.0
73355617,1,-1.0,2022-08-14 22:58:25,0,1304,"<p>I Want to merge these two DataFrame, One of them I have just created from OnehotEncoder, length of both Dataframe is the same.</p>
<pre><code>ohe = OneHotEncoder()
df_Holiday = pd.DataFrame(ohe.fit_transform(df[['StateHoliday']]).toarray() ,
                      columns = ['public holiday', 'Easter holiday','Christmas holiday','No 
                                  Holiday'])

df = df.merge(df_Holiday, on = df.index )
</code></pre>
<p>but I am getting this on merge</p>
<p><code>ValueError: cannot insert key_0, already exists</code></p>
",19639859.0,-1.0,N/A,2022-08-14 23:13:08,"ValueError: cannot insert key_0, already exists",<python><pandas><data-science>,1,2,N/A,CC BY-SA 4.0
73357418,1,-1.0,2022-08-15 06:08:23,-3,78,"<pre><code>I have the following data:
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Drugs</th>
<th>Diseases</th>
</tr>
</thead>
<tbody>
<tr>
<td>drug1,drug2,drug3</td>
<td>dis1,dis3</td>
</tr>
<tr>
<td>drug1,drug4</td>
<td>dis1,dis2</td>
</tr>
<tr>
<td>drug1,drug2,drug4,drug7</td>
<td>dis1, dis3,dis4,dis7</td>
</tr>
<tr>
<td>drug1,drug3</td>
<td>dis1,dis3</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>describe：first column is drugs the patient take; the second column is
the disease given by doctor;</li>
<li>need：I want to find the
(diseaseX,drugY) pairs, which indicate that for diseaseX, the drugY
is used for it.
for the given example, should output (dis1,drug1), because the two always exist simultaneously. but how to find all the pairs for much more data in the similar format?</li>
</ul>
",9814450.0,12370687.0,2022-08-15 12:01:43,2022-08-15 12:01:43,how to find the most relation drug and disease given doctor orders,<python><scikit-learn><data-science><artificial-intelligence><data-mining>,1,3,2022-08-15 08:29:18,CC BY-SA 4.0
73361906,1,73365960.0,2022-08-15 13:49:19,0,98,"<p>I would like to create another dataframe with a condition from one column.
This method I am trying is working:</p>
<pre><code>   Y = X.loc[(X['ColumnnA'] == &quot;22.33.44.55&quot;)
                                                  | (X['ColumnnA'] == &quot;12.12.32.44&quot;) 
                                                  | (X['ColumnnA'] == &quot;45.142.22.22&quot;) 
                                                  | (X['ColumnnA'] == &quot;55.197.55.8&quot;) 
                                                  | (X['ColumnnA'] == &quot;44.44.211.254&quot;) 
                                                  | (X['ColumnnA'] == &quot;33.44.234.83&quot;) 
                                                  | (X['ColumnnA'] == &quot;33.33.221.240&quot;) 
                                                  | (X['ColumnnA'] == &quot;33.33.33.1&quot;) 
                                                 ] 
</code></pre>
<p>But with .loc function, I cannot use this:</p>
<pre><code>restdataframe = X[~Y]
Y=X[Y]
</code></pre>
<p>How can I use this with .loc?</p>
<p>Strange but I was using the below method last week and it was working for another dataframe, with the same columns now this runs but it provides me a wrong &quot;shape&quot;. With .loc, it gives a correct answer.  I want to understand what I am doing wrong with below code? Why it does not work properly?</p>
<pre><code>Y = (X['ColumnnA'] == &quot;22.33.44.55&quot;)
| (X['ColumnnA'] == &quot;12.12.32.44&quot;) 
| (X['ColumnnA'] == &quot;45.142.22.22&quot;) 
| (X['ColumnnA'] == &quot;55.197.55.8&quot;) 
| (X['ColumnnA'] == &quot;44.44.211.254&quot;) 
| (X['ColumnnA'] == &quot;33.44.234.83&quot;) 
| (X['ColumnnA'] == &quot;33.33.221.240&quot;) 
| (X['ColumnnA'] == &quot;33.33.33.1&quot;) 
</code></pre>
<p>Note: I run it in one line because of the invalid syntax</p>
<p>Example of X:
<a href=""https://i.stack.imgur.com/4p32I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4p32I.png"" alt=""enter image description here"" /></a></p>
",18964569.0,18964569.0,2022-08-15 20:05:12,2022-08-15 20:07:59,Filtering the dataframe with conditions in Pandas and reaching the rest of the dataframe (.loc),<python><pandas><dataframe><data-science>,1,4,N/A,CC BY-SA 4.0
73330887,1,-1.0,2022-08-12 07:51:44,0,2858,"<p>I encounter the following error during deployment, I have <code>streamlit_option_menu</code> installed and specified in my <code>requirement.txt</code> file.</p>
<pre><code>Traceback (most recent call last):

  File &quot;/home/appuser/venv/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py&quot;, line 556, in _run_script

    exec(code, module.__dict__)

  File &quot;/app/nigeria-election2023-prediction/application.py&quot;, line 4, in &lt;module&gt;

    from streamlit_option_menu import option_menu

ModuleNotFoundError: No module named 'streamlit_option_menu'

</code></pre>
",19500489.0,19290081.0,2022-08-12 13:40:30,2022-08-12 13:40:30,ModuleNotFoundError: No module named ‘streamlit_option_menu’ during streamlit deployment,<python><machine-learning><deployment><data-science><streamlit>,1,1,N/A,CC BY-SA 4.0
73331660,1,73331731.0,2022-08-12 08:57:13,2,21,"<p>im trying to save a dict of LE encoders for use in inferencing, this is the code that trains and applies the LE and then saves the LE into dict (label_object) which then will be joblib.dump(ed)()</p>
<pre><code>for col in data:
    if data[col].dtype == 'object':
        # If 2 or fewer unique categories
        if len(list(data[col].unique())) &gt;= 2:
            # Train on the training data
            le.fit(data[col])
            label_object[col] = le
            # Transform both training and testing data
            data[col] = le.transform(data[col])
            label_object[col] = le
</code></pre>
<p>When trying this it seems the classes_ of the LE get overwritten by the last LE, in this case 'day_of_incident'
<a href=""https://i.stack.imgur.com/eTkft.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eTkft.png"" alt=""enter image description here"" /></a></p>
<p>Im not sure whats causing this issues, is there an issue with the logic of the code or am I doing something wrong?</p>
",13360530.0,-1.0,N/A,2022-08-12 09:02:34,When saving a list of LabelEncoders the classes_ get overwritten by the last LabelEncoder,<python><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
73334426,1,-1.0,2022-08-12 12:46:44,0,106,"<p>The bottleneck of some code I have is:</p>
<pre><code>for _ in range(n):
    W = np.dot(A, W)
</code></pre>
<p>where n can vary, A is a fixed size MxM matrix, W is Mx1.</p>
<p>Is there a good way to optimize this?</p>
",4605668.0,4605668.0,2022-08-12 13:04:20,2022-08-12 14:13:52,How to optimize successive numpy dot products?,<python><numpy><optimization><data-science><linear-algebra>,1,0,N/A,CC BY-SA 4.0
73371556,1,73371609.0,2022-08-16 09:14:48,0,405,"<p>I would like to use the output of this ↓↓↓ dataframe column. Using the values of it, I want to filter another dataframe.</p>
<pre><code> X.ColumnA.unique()
array(['0', '222.33.222.106', '12.12.32.44', '122.122.1.1',
       '122.222.180.150', '142.222.180.142', '222.99.222.78',
       '33.33.221.240', '151.99.222.76', '222.251.222.1',
       '222.250.184.46', '22.33.44.55', ........ ]
</code></pre>
<p>I was using some of its values to filter and create another dataframe but I understood that I need all the values above↑↑. How can I pass the array to filter the dataframe?</p>
<pre><code>Y = ((df['ColumnnA'] == &quot;22.33.44.55&quot;)
| (df['ColumnnA'] == &quot;12.12.32.44&quot;) 
| (df['ColumnnA'] == &quot;45.142.22.22&quot;) 
| (df['ColumnnA'] == &quot;55.197.55.8&quot;) 
| (df['ColumnnA'] == &quot;44.44.211.254&quot;) 
| (df['ColumnnA'] == &quot;33.44.234.83&quot;) 
| (df['ColumnnA'] == &quot;33.33.221.240&quot;) 
| (df['ColumnnA'] == &quot;33.33.33.1&quot;))
restdataframe = df[~Y]
Y=df[Y]   
</code></pre>
<p>Here some of the values</p>
<pre><code>df.head(5).to_dict()
{'Column0': {0: 0.00192, 1: 0.0, 2: 0.834324, 3: 8.588816, 4: 2.908711},
 'Column1': {0: '0',
  1: '192.168.1.1',
  2: '22.22.2.15',
  3: '10.22.2.15',
  4: '10.22.22.15'},
 'ColumnA': {0: '0',
  1: '10.0.2.22',
  2: '20.55.22.22',
  3: '22.44.1.1',
  4: '44.33.1.1'},
 'Column2': {0: 'yyy', 1: 'xxx', 2: 'zzz', 3: 'xxx', 4: 'yyy'},
 'Column3': {0: '88', 1: '88', 2: '777', 3: '666', 4: '555'},
 'Column4': {0: '0', 1: '111', 2: '222', 3: '333', 4: '444'},
 'Column5': {0: 0, 1: 1, 2: 17, 3: 8, 4: 4},
 'Column6': {0: 0, 1: 1, 2: 7, 3: 4, 4: 2},
 'Column7': {0: 0, 1: 0, 2: 10, 3: 4, 4: 2},
 'Column8': {0: 0, 1: 110, 2: 5798, 3: 504, 4: 408},
 'Column9': {0: 0, 1: 110, 2: 775, 3: 264, 4: 188},
 'Column10': {0: 0, 1: 0, 2: 5023, 3: 240, 4: 220},
 'Column11': {0: 0, 1: 0, 2: 0, 3: 3, 4: 0},
 'Column12': {0: 'DDD', 1: 'EEE', 2: 'AAA', 3: 'BBB', 4: 'CCC'}}       
</code></pre>
",18964569.0,18964569.0,2022-08-16 09:33:06,2022-08-16 09:36:49,Using an array to filter dataframe in Pandas,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
73347858,1,-1.0,2022-08-13 21:55:26,-2,109,"<p>What happens if we associate multiple biases with a single neuron in the Neural Network? Will it allows approximating more complex functions?</p>
",19598005.0,-1.0,N/A,2022-08-13 22:55:12,Why is there only a single bias per neuron in the Neural Network?,<machine-learning><deep-learning><neural-network><data-science><artificial-intelligence>,1,2,2022-08-14 06:44:17,CC BY-SA 4.0
73359735,1,-1.0,2022-08-15 10:29:06,0,128,"<p>I really wanna upload a newly updated excel file which is stored on my local machine to the SFTP folder using r  script</p>
<p>here is the code using Curl and HTTP package</p>
<pre><code>ftpUpload(what = &quot;Glo_failed.xls&quot;,
to = &quot;sftp://ubandaftp:*************://ubandaftp@159.152......../
C:/Users/Guesters/Desktop/AutoRecon/failed_report.xls&quot;)
</code></pre>
<p>But I'm getting this error below</p>
<pre><code>Error in file(what, &quot;rb&quot;) : cannot open the connection
In addition: Warning message:
In file(what, &quot;rb&quot;) :
  cannot open file 'Glo_failed.xls': No such file or directory
</code></pre>
",16087142.0,-1.0,N/A,2022-08-15 10:29:06,Trying to load to sftp server in r,<r><curl><data-science><sftp>,0,5,N/A,CC BY-SA 4.0
73367630,1,-1.0,2022-08-16 00:29:24,1,290,"<p>I am training a Gaussian-Process model iteratively. In each iteration, a new sample is added to the training dataset (Pandas DataFrame), and the model is re-trained and evaluated. Each row of the dataset comprises 5 independent variables + the dependent variable. The training ends after 150 iterations (150 samples), but I want to extend this behaviour so the training can automatically stop after a number of iterations for which no meaningful information is added to the model.</p>
<p>My first approach is to compare the distribution of the last 10 samples to the previous 10. If the distributions are very similar, I assume that not meaningful knowledge has been added in the last 10 iterations, so I abort the training.</p>
<p>I thought of using Kullback-Leibler divergence, but I am not sure if this can be used for multivariate distributions. Should I use it? If so, how?</p>
<p>Additionally, is there any other better/smarter way to proceed?</p>
<p>Thanks</p>
",4703035.0,-1.0,N/A,2022-08-16 00:29:24,How to calculate the KL divergence for two multivariate pandas dataframes,<pandas><scipy><data-science><entropy><multivariate-testing>,0,0,N/A,CC BY-SA 4.0
73376951,1,-1.0,2022-08-16 16:01:40,0,282,"<p>I've a question how should I download a .csv files from Auzre data lake then make some calculation and save this in .csv again.
I know that for downloading .csv I can use: <code>data=pd.read_csv('example.csv') #example</code></p>
<p><code>new_data=data//2+data #calculation in databricks notebook</code>
and now the question is how to save <code>new_data</code> in .csv format  in Azure Data lake with the name: <code>example_calulated.csv</code></p>
",19337217.0,-1.0,N/A,2022-09-02 13:11:10,How to save 15k csv files in databricks/ Azure data lake,<python><csv><data-science><databricks><azure-databricks>,1,1,N/A,CC BY-SA 4.0
73403908,1,-1.0,2022-08-18 13:27:26,0,139,"<pre><code>def scrape_wikipedia(name_topic, verbose=True):
def link_to_wikipedia(link):
    try:
        page = api_wikipedia.page(link)
        if page.exists():
            return {'page': link, 'text': page.text, 'link': page.fullurl, 'categories': list(page.categories.keys())}
    except:
        return None
  
api_wikipedia = wikipediaapi.Wikipedia(language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)
name_of_page = api_wikipedia.page(name_topic)
if not name_of_page.exists():
    print('Page {} is not present'.format(name_of_page))
    return

links_to_page = list(name_of_page.links.keys())
procceed = tqdm(desc='Scraped links', unit='', total=len(links_to_page)) if verbose else None
origin = [{'page': name_topic, 'text': name_of_page.text, 'link': name_of_page.fullurl, 'categories': list(name_of_page.categories.keys())}]

with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    links_future = {executor.submit(link_to_wikipedia, link): link for link in links_to_page}
    for future in concurrent.futures.as_completed(links_future):
        info = future.result()
        origin.append(info) if info else None
        procceed.update(1) if verbose else None
procceed.close() if verbose else None

namespaces = ('Wikipedia', 'Special', 'Talk', 'LyricWiki', 'File', 'MediaWiki',
             'Template', 'Help', 'User', 'Category talk', 'Portal talk')
origin = pds.DataFrame(origin)
origin = origin[(len(origin['text']) &gt; 20)
                 &amp; ~(origin['page'].str.startswith(namespaces, na=True))]
origin['categories'] = origin.categories.apply(lambda a: [b[9:] for b in a])

origin['topic'] = name_topic
print('Scraped pages', len(origin))

return origin
</code></pre>
<p><code>SSLError: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /w/api.php?action=query&amp;prop=info&amp;titles=COVID+19&amp;inprop=protection%7Ctalkid%7Cwatched%7Cwatchers%7Cvisitingwatchers%7Cnotificationtimestamp%7Csubjectid%7Curl%7Creadable%7Cpreload%7Cdisplaytitle&amp;format=json&amp;redirects=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))</code></p>
",13340699.0,19290081.0,2022-08-18 23:55:57,2022-08-18 23:55:57,I am getting SSLCertVerificationError while doing web Scraping,<python><machine-learning><web-scraping><jupyter-notebook><data-science>,1,0,N/A,CC BY-SA 4.0
73364771,1,73365220.0,2022-08-15 17:59:47,0,286,"<p>I am using this example to extract an archived folder:</p>
<pre><code>import zipfile
with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:
    zip_ref.extractall()
</code></pre>
<p>The problem is that the files do not appear in the folder where the file.zip is located, but additional subfolders are created and the files are unpacked there.</p>
<p><em>I am using Visual Studio Code.</em></p>
",15144669.0,-1.0,N/A,2022-08-15 18:47:15,Problem with ZipFile. Creates an additional subfolder?,<python><data-science>,1,5,N/A,CC BY-SA 4.0
73367381,1,73367437.0,2022-08-15 23:39:45,-2,48,"<p>Remove all comma from starting and ending</p>
<p>For Example</p>
<pre><code>id name
1  ,,par, ind, gop, abc 
2  ,raj,
3  marl, govin
4  rajjs, sun,,,
</code></pre>
<p>Ans</p>
<pre><code>id name
1  par, ind, gop, abc 
2  raj,
3  marl, govin
4  rajjs, sun
</code></pre>
",18604759.0,18604759.0,2022-08-15 23:55:15,2022-08-16 12:44:06,Some cell of column is starting from comma so how can i remove it? in Dataframe,<python><pandas><dataframe><data-science>,2,1,N/A,CC BY-SA 4.0
73391902,1,-1.0,2022-08-17 16:25:19,0,38,"<p>I have a table like the one below. I want to keep cases only when the table have 3 or more values related to a person, grouped by role and question (except when the role is Leader).</p>
<p>Example: I would keep all lines about Person A, because there's at least 3 partners answering question 1, question 2 and question 3. In contrast, I need to remove the lines about questions 1 and 2 related to Person B, because just 1 partner answered Q1 and only 2 partners answered Q2. 3 partners answered Q3, so that's ok to keep.</p>
<p>I think I explained pretty badly to be honest, sorry in advance.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th><strong>Person</strong></th>
<th><strong>Question</strong></th>
<th><strong>Role</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Person A</td>
<td>Question 1</td>
<td>Leader</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 2</td>
<td>Leader</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 3</td>
<td>Leader</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 1</td>
<td>Partner</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 1</td>
<td>Partner</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 1</td>
<td>Partner</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 2</td>
<td>Partner</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 2</td>
<td>Partner</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 2</td>
<td>Partner</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 3</td>
<td>Partner</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 3</td>
<td>Partner</td>
</tr>
<tr>
<td>Person A</td>
<td>Question 3</td>
<td>Partner</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 1</td>
<td>Leader</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 2</td>
<td>Leader</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 3</td>
<td>Leader</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 1</td>
<td>Partner</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 2</td>
<td>Partner</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 2</td>
<td>Partner</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 3</td>
<td>Partner</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 3</td>
<td>Partner</td>
</tr>
<tr>
<td>Person B</td>
<td>Question 3</td>
<td>Partner</td>
</tr>
</tbody>
</table>
</div>",16769072.0,-1.0,N/A,2022-08-17 16:43:34,Remove lines with less than 3 occurrences (by groups),<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
73409402,1,73409549.0,2022-08-18 21:02:25,-4,40,"<p>I wanted a list of columns with numeric values I tried the code in the image but it didn't work, anybody knows why? is there another way to do it? thankyou</p>
<p><a href=""https://i.stack.imgur.com/vAurV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vAurV.jpg"" alt=""enter image description here"" /></a></p>
",5130807.0,5130807.0,2022-08-18 21:06:37,2022-08-18 21:18:55,"List name of columns with numeric values, it didn't work",<python><pandas><dataframe><numpy><data-science>,1,5,2022-08-19 02:25:11,CC BY-SA 4.0
73419965,1,-1.0,2022-08-19 16:53:21,1,63,"<p>I would like to change the structure of the dataset.
(I did not come up with code for this yet...)</p>
<p>This is data that I have↓↓</p>
<p>My data below background:</p>
<ul>
<li>I set pandas dataframe into series(list of lists), but I guess I can use pandas dataframes as well (Sorry, I am a beginner, so I don't know...)</li>
<li><strong>Columns_A</strong>'s data is equal to <strong>Columns_B</strong>'s data (Columns_A=Columns_B)</li>
<li>Each <strong>Columns_C</strong>&amp;<strong>Columns_D</strong> has the same value repeated</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Columns_A</th>
<th>Columns_B</th>
<th>Columns_C</th>
<th>Columns_D</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>1</td>
<td>A</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>1</td>
<td>A</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>B</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>B</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>3</td>
<td>C</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>3</td>
<td>C</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>3</td>
<td>C</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>3</td>
<td>C</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>11</td>
<td>D</td>
<td>Z</td>
<td>Q</td>
</tr>
<tr>
<td>12</td>
<td>E</td>
<td>Z</td>
<td>Q</td>
</tr>
<tr>
<td>12</td>
<td>E</td>
<td>Z</td>
<td>Q</td>
</tr>
<tr>
<td>12</td>
<td>E</td>
<td>Z</td>
<td>Q</td>
</tr>
<tr>
<td>13</td>
<td>F</td>
<td>Z</td>
<td>Q</td>
</tr>
<tr>
<td>13</td>
<td>F</td>
<td>Z</td>
<td>Q</td>
</tr>
</tbody>
</table>
</div>
<p>This is what I would like to make based on original data↓↓</p>
<p>What I would like to do:</p>
<ul>
<li>Update new columns (Create additional columns for original <strong>Columns_A</strong> and <strong>Columns_B</strong>, original <strong>Columns_C</strong> and  <strong>Columns_D</strong> can remain the same)</li>
<li>Assign Columns_A&amp;Columns_B's values into different columns</li>
<li>based on the longst value;
→repeat pasting Columns_C&amp;Columns_D values
→add blank if the column doesn't have any more values</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Columns_A_1</th>
<th>Columns_B_1</th>
<th>Columns_A_2</th>
<th>Columns_B_2</th>
<th>Columns_A_3</th>
<th>Columns_B_3</th>
<th>Columns_C</th>
<th>Columns_D</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>2</td>
<td>B</td>
<td>3</td>
<td>C</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>1</td>
<td>A</td>
<td>2</td>
<td>B</td>
<td>3</td>
<td>C</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>1</td>
<td>A</td>
<td>Blacnk</td>
<td>Blacnk</td>
<td>3</td>
<td>C</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>Blacnk</td>
<td>Blacnk</td>
<td>Blacnk</td>
<td>Blacnk</td>
<td>3</td>
<td>C</td>
<td>X</td>
<td>Y</td>
</tr>
<tr>
<td>11</td>
<td>D</td>
<td>12</td>
<td>E</td>
<td>13</td>
<td>F</td>
<td>Z</td>
<td>Q</td>
</tr>
<tr>
<td>Blank</td>
<td>Blank</td>
<td>12</td>
<td>E</td>
<td>13</td>
<td>F</td>
<td>Z</td>
<td>Q</td>
</tr>
<tr>
<td>Blank</td>
<td>Blank</td>
<td>12</td>
<td>E</td>
<td>Blank</td>
<td>Blank</td>
<td>Z</td>
<td>Q</td>
</tr>
</tbody>
</table>
</div>
<p>I am thinking if I can use &quot;loop &amp; condition&quot; approach, since the data is list of lists. (Sorry I put the example data as dataset just to make it easier to read)</p>
<p>Please let me know if my explanation does not make sense.
Any advice would be appreciated.</p>
",19803657.0,2947378.0,2022-08-19 16:56:23,2022-08-19 17:13:20,Restructure the dataframe - dividing one column into multiple and align between rows,<python><pandas><dataframe><data-science><data-manipulation>,1,0,N/A,CC BY-SA 4.0
73368934,1,-1.0,2022-08-16 04:55:33,0,75,"<p>the data annotation company I am working at is using MS-Paint to make red bounding boxes around the objects in the images.
The problem is it takes a lot of time to open each image individually on MS-Paint and then creating a boxes around the objects. Is there a tool that can open multiple images at once like Lightroom so that I can just open the images and navigate through them while creating red bounding boxes.</p>
<p>There are a lot of annotation tools out there but I was not able to find a suitable solution. If someone has good experience on data annotation please help</p>
<p>Steps that I am doing right now:</p>
<ol>
<li>Open a single image on Paint</li>
<li>Identify the object in the image</li>
<li>Change the <strong>file name</strong> to the name of the object that I identified</li>
<li>Save the file</li>
</ol>
",16627317.0,-1.0,N/A,2023-03-28 13:20:37,Data annotation Tool for creating Red bounding boxes,<annotations><data-science><data-mining><data-annotations>,1,0,N/A,CC BY-SA 4.0
73398349,1,-1.0,2022-08-18 06:27:19,0,36,"<p>I have 2 CSV files, working with VS Code. And I am trying to extract the 2nd index values to a new column.</p>
<p>For 1st CSV file</p>
<pre><code>csv1['Location'] = Albuquerque, NM

df['Location'].apply(lambda x: x.split(',')[0]) produces Albuquerque
df['Location'].apply(lambda x: x.split(',')[1]) produces NM

</code></pre>
<p>For 2nd CSV file</p>
<pre><code>csv2['Location'] = Bromley, England
df['Location'].apply(lambda x: x.split(',')[0]) produces Bromley
df['Location'].apply(lambda x: x.split(',')[1]) produces an out of range error. 
</code></pre>
<p>Both produce &lt;class 'pandas.core.series.Series'&gt; when using type() method.</p>
<p>Does anyone know why this is happening? as much as I know both strings are of the same index format that is why [0] produces the 1st index values for both CSV files and [1] should work for both files.</p>
",8093360.0,8093360.0,2022-08-18 06:54:01,2022-08-18 06:54:01,Indexing error on same type of string with same index length after using split method,<python><csv><dataset><data-science>,0,3,N/A,CC BY-SA 4.0
73425988,1,-1.0,2022-08-20 10:39:33,-1,36,"<p>I have the code below to create an encoding. But I feel there's a better way to perform this encoding. Can I get any suggestions preferably using scikit-learn or pandas.</p>
<pre><code>&quot;&quot;&quot;
total_features = [&quot;male&quot;,&quot;female&quot;, &quot;tall&quot;, &quot;short&quot;]
features = [&quot;male&quot;, &quot;tall&quot;]
&quot;&quot;&quot;
d = dict()
for i in total_features:
    d[i] = 0
for i in features:
    d[i] = 1
final_input = list(d.values())
</code></pre>
",18567384.0,-1.0,N/A,2022-08-20 13:43:44,What's the right way to create one hot encoding?,<python><pandas><scikit-learn><data-science>,1,8,N/A,CC BY-SA 4.0
73433862,1,73434136.0,2022-08-21 11:12:22,0,47,"<pre><code>df.value_counts(subset='DstAddr', ascending=False)
</code></pre>
<p><a href=""https://i.stack.imgur.com/1ZklB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1ZklB.png"" alt=""enter image description here"" /></a></p>
<pre><code>df.head()
</code></pre>
<p><a href=""https://i.stack.imgur.com/8gR3O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8gR3O.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to find a way to show for each unique IP:
How many in the sum of total bytes does it have?
For example, if I want to find all dataset:</p>
<pre><code>df['TotBytes'].sum()
</code></pre>
<p>But I want to find for each destination unique IP to see how much KB it is used for that IP.</p>
",18964569.0,-1.0,N/A,2022-08-21 11:49:30,Total bytes for each IPs in the dataset using pandas,<python><pandas><networking><data-science>,1,4,N/A,CC BY-SA 4.0
73442155,1,-1.0,2022-08-22 08:26:07,0,281,"<p>I want to do research on certain Instagram accounts to get the information I want quickly and always get the latest data. how to connect instagram API to view specific account to get fast real time dataset in python</p>
",16702442.0,-1.0,N/A,2022-08-22 08:26:07,how to connect instagram API to view specific account use python to get fast real time,<dataframe><dataset><data-science><instagram><exploratory-data-analysis>,0,2,N/A,CC BY-SA 4.0
73413270,1,-1.0,2022-08-19 07:30:55,-1,274,"<p>I got a question listed below about the confidence interval for <strong>rolling a die 1000 times</strong>. I'm assuming that the question is using Binomial Distribution but not sure if I'm correct. I guess in the solution, the probability 0.94 comes from 1-0.06. But I'm not sure if we need the probability in this interval, except it is only used for the Z-score, 1.88. Could I assume this question like this?</p>
<p><strong>Question:</strong>
Assume that we are okay with <strong>accidentally rejecting H0​ 6% of the time</strong>, assuming H0​ is true.
If we rolled the die (6-sided) 1000 times, <strong>what is the range of times we'd expect to see a 1 rolled</strong>? (H0​ is the die is fair.)</p>
<p><strong>Answer:</strong>
The interval is (144.50135805579743, 188.8319752775359), with probability = 0.94, mu = 166.67, sigma = 11.785113019775793</p>
",18085731.0,18085731.0,2022-08-20 11:09:03,2022-08-20 13:24:05,"If we rolled the die (6-sided) 1000 time, what is the range of times we'd expect to see a 1 rolled?",<statistics><data-science>,1,0,N/A,CC BY-SA 4.0
73415109,1,73425329.0,2022-08-19 10:06:08,2,132,"<p>I have a dataframe <code>book_matrix</code> with users as rows, books as columns, and ratings as values. When I use <code>corrwith()</code> to compute the correlation between 'The Lord of the Rings' and 'The Silmarillion' the result is <code>1.0</code>, but the values are clearly different.</p>
<p><a href=""https://i.stack.imgur.com/VIxJ8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VIxJ8.png"" alt=""enter image description here"" /></a></p>
<p>The non-null values [10, 3] and [10, 9] have correlation <code>1.0</code>. I would expect them to be exactly the same when the correlation is equal to one. How can this happen?</p>
",1459339.0,12370687.0,2022-08-20 09:07:44,2022-08-20 09:07:44,Why is the correlation one when values differ?,<pandas><dataframe><data-science><correlation><pearson-correlation>,1,4,N/A,CC BY-SA 4.0
73440559,1,-1.0,2022-08-22 05:31:01,0,69,"<p>This is the code , which I tried out:</p>
<pre><code>new_data_2[new_data_2$X1998 &gt; 0.8 , 4]  = &quot;A&quot;
new_data_2[[new_data_2$X1998 &gt; 0.6, 4] &amp; new_data_2[new_data_2$X1998 &lt; 0.8, 4]] = &quot;B&quot;
new_data_2[[new_data_2$X1998 &gt; 0.4, 4] &amp;&amp; new_data_2[new_data_2$X1998 &lt; 0.6, 4]] = &quot;C&quot;
new_data_2[[new_data_2$X1998 &gt; 0.2 , 4] &amp;&amp; new_data_2[new_data_2$X1998 &lt;= 0.4, 4]] = &quot;D&quot;
</code></pre>
<p>But I get the following error</p>
<pre><code>Error: unexpected '&amp;' in &quot;new_data_2[[new_data_2$X1998 &gt; 0.6, 4] &amp;&quot;
</code></pre>
",19772396.0,2974951.0,2022-08-22 05:33:22,2022-08-22 05:33:22,How do we convert decimal numbers into a categorical variables,<r><data-science><data-wrangling>,0,3,N/A,CC BY-SA 4.0
73443407,1,-1.0,2022-08-22 10:08:15,3,3724,"<p>I have a problem with this code:</p>
<pre><code>    from sklearn import svm
    model_SVC = SVC()
    model_SVC.fit(X_scaled_df_train, y_train)
    svm_prediction = model_SVC.predict(X_scaled_df_test)
</code></pre>
<p>The error message is</p>
<blockquote>
<p>NameError<br />
Traceback (most recent call
last) ~\AppData\Local\Temp/ipykernel_14392/1339209891.py in 
----&gt; 1 svm_prediction = model_SVC.predict(X_scaled_df_test)</p>
<p>NameError: name 'model_SVC' is not defined</p>
</blockquote>
<p>Any ideas?</p>
",19663729.0,12439119.0,2022-12-24 19:11:12,2022-12-24 19:11:12,SVM problem - name 'model_SVC' is not defined,<python><machine-learning><scikit-learn><data-science><svm>,2,1,N/A,CC BY-SA 4.0
73450834,1,73450942.0,2022-08-22 20:43:29,-1,35,"<p>I am trying to plot some 336 data points and am encountering an issue with my use of pythons plt.hist() function. I would like to use more than eight bins for my data, but when I do a lot of whitespace is introduced. For example, here is a plot with bins = 8</p>
<p><a href=""https://i.stack.imgur.com/q9Azv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q9Azv.png"" alt=""
"" /></a>
and with bins = 24
<a href=""https://i.stack.imgur.com/i9H4N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i9H4N.png"" alt=""https://i.stack.imgur.com/i9H4N.png"" /></a></p>
<p>Does anyone know why this is and how I can best represent my data with more bins? Many thanks, ~S.</p>
<p>Sample code:</p>
<pre><code>tumbles = np.array(df['Tumbles'])

fig, axs = plt.subplots(1, 1,
                        tight_layout = True)

N, bins, patches = axs.hist(tumbles, bins = 24, edgecolor= &quot;black&quot;)

axs.grid(b = True, color ='grey',
        linestyle ='-.', linewidth = 0.5,
        alpha = 0.6)
    
plt.xlabel(&quot;Time (s)&quot;, size = 14)
plt.ylabel(&quot;Frequency&quot;, size = 14)
plt.title('Histogram ofTimes', size = 18)

plt.show()
</code></pre>
",16168300.0,16168300.0,2022-08-22 21:05:45,2022-08-23 00:57:24,Number of columns does not match number of bins,<python><statistics><data-science>,2,3,N/A,CC BY-SA 4.0
73424300,1,73424427.0,2022-08-20 05:26:26,1,651,"<pre><code>df.describe()
</code></pre>
<p><a href=""https://i.stack.imgur.com/0uNUl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0uNUl.png"" alt=""enter image description here"" /></a></p>
<p>I have a network dataset. To be able to understand the dataset better, I want to find a way to convert the numbers to MB or KB for packets and bytes.</p>
<pre><code>df.head()
</code></pre>
<p><a href=""https://i.stack.imgur.com/MCtkC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MCtkC.png"" alt=""enter image description here"" /></a></p>
",18964569.0,-1.0,N/A,2022-08-20 06:44:17,How to convert to MB or KB the pandas describe table?,<python><pandas><networking><data-science><byte>,1,0,N/A,CC BY-SA 4.0
73430640,1,-1.0,2022-08-20 23:02:35,0,499,"<p>Generally if one dataset is given we use</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
y_pred = lr.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
</code></pre>
<p>If we are doing validation on the training dataset</p>
<pre><code>X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)
</code></pre>
<p>If both Train and Test datasets are given in separate datasets, where do I use Test dataset in the code?</p>
",18163581.0,12370687.0,2022-08-23 21:30:00,2022-08-23 21:30:00,"How to do model evaluation with train, validation, test?",<machine-learning><scikit-learn><data-science><classification><cross-validation>,1,3,N/A,CC BY-SA 4.0
73442045,1,73442709.0,2022-08-22 08:15:21,2,176,"<p>I use the face dataset for the siamese network. In this dataset, we have 1000 unique labels(labels are names of the folders), and in each folder, we have 20 images all images in this dataset are 20000.this error is because of this line:</p>
<pre><code>idxB = np.random.choice(idx[label])
</code></pre>
<p>so I want to make positive and negative images, but when I do that, I get:</p>
<blockquote>
<p>IndexError: list index out of range error.</p>
</blockquote>
<p>Code is coming in below:</p>
<pre><code>pair_images = []
pair_labels = []
new_labels = []

for k in labels:
    new_labels.append(int(k))

numClasses  = len(np.unique(new_labels))

new_labels = np.array(new_labels)

idx = [np.where(new_labels == i)[0] for i in range(0,numClasses)]

print (len(idx))

for i,idxA in enumerate (range(len(images))):


    # print(i)
    # Make Posetive Images
    currentImage = images[idxA]
    label = new_labels[idxA]

    idxB = np.random.choice(idx[label])
    print (idxB)
    # posImage = images[idxB]
</code></pre>
<p>output:</p>
<pre><code>0
1
2
3
4
....
....
....
....
11713
11718
11709
11700
11700
11710
11717
11717
11707
Traceback (most recent call last):
    File &quot;/Users/admin/Documents/Ostad/Ostad Ghasemi/Courses/Advabced           Tensorflow/Home Works/Week-4/E-1-Face Verification/Utilities.py&quot;, line 73, in &lt;module&gt;
  make_pairs(all_image, all_label)       
  File &quot;/Users/admin/Documents/Ostad/Ostad Ghasemi/Courses/Advabced Tensorflow/Home      Works/Week-4/E-1-Face Verification/Utilities.py&quot;, line 37, in make_pairs
idxB = np.random.choice(idx[label])
IndexError: list index out of range
</code></pre>
<p>May I know how can I fix this error?</p>
",6740001.0,12370687.0,2022-08-22 10:25:21,2022-08-22 10:25:21,I want make positive image and label pairs but I take this error IndexError: list index out of range,<python><list><machine-learning><data-science><siamese-network>,1,0,N/A,CC BY-SA 4.0
73451207,1,-1.0,2022-08-22 21:33:21,2,2439,"<p>I am trying to apply a box-cox transformation to a single column but I am unable to do that. Can somebody help me with this issue?</p>
<pre><code>from sklearn.datasets import fetch_california_housing
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.preprocessing import PowerTransformer

california_housing = fetch_california_housing(as_frame=True).frame
california_housing

power = PowerTransformer(method='box-cox', standardize=True)
california_housing['MedHouseVal']=power.fit_transform(california_housing['MedHouseVal'])
</code></pre>
",16875907.0,-1.0,N/A,2022-08-22 23:09:23,how to perform box-cox transformation to single column in python,<python><pandas><dataframe><linear-regression><data-science-experience>,1,0,N/A,CC BY-SA 4.0
73431793,1,73431872.0,2022-08-21 05:09:54,1,334,"<pre><code>dups_df = df.pivot_table(columns=['DstAddr'], aggfunc='size')
print (dups_df )
</code></pre>
<p>I am using this code block to show the duplicates but I would like to see the output in order(most used one) and maybe with a better visualization. How can I do this?</p>
<p><a href=""https://i.stack.imgur.com/jbQKP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jbQKP.png"" alt=""enter image description here"" /></a></p>
",18964569.0,-1.0,N/A,2022-08-21 05:34:34,Better way to show duplicates in Pandas,<python><pandas><matplotlib><data-science><data-visualization>,3,0,N/A,CC BY-SA 4.0
73457346,1,73457652.0,2022-08-23 10:46:57,0,39,"<p>I have the following dataframe</p>
<pre><code>id | status
____________
1  | reserved
2  | signed
1  | waiting
1  | signed
3  | waiting
2  | sold
3  | reserved
1  | sold
</code></pre>
<p>I want to chech a hypothesis that statuses <code>reserved</code>, <code>waiting</code>, <code>signed</code> always lead to status <code>sold</code>. I only need to check the following order, some statuses may be omitted like for <code>id == 2</code> in dataframe.</p>
<p>I wonder if there's a way to look for next row values in grouped by id dataframe</p>
<p>Expected output is dataframe or list of <code>id</code>s that follow the above rule. For the dataframe above it would be this:</p>
<pre><code>id
__
1
2
</code></pre>
<p>My attemp was to get all unique <code>id</code> with those statuses and then for each <code>id</code> found list of it's statuses. Then I thought to filter it somehow but there are a lot of combinations.</p>
<pre><code>df = df[df.status.isin(['reserved', 'waiting', 'signed', 'sold'])]

df1 = df.groupby('flat_id').['status'].unique()

df1.where('status'== [''reserved', 'waiting', 'signed', 'sold'']
or ['reserved', 'waiting', 'sold'] ... )
</code></pre>
",9137206.0,9137206.0,2022-08-23 11:11:55,2022-08-23 11:11:55,Check if specific values in column follow each other for each id,<python><pandas><dataframe><group-by><data-science>,1,3,N/A,CC BY-SA 4.0
73460398,1,73460608.0,2022-08-23 14:18:35,-2,43,"<p>I have data that looks like this:</p>
<p><img src=""https://i.stack.imgur.com/N3ukc.png"" alt=""data"" /></p>
<p>Now I want to make a string that has a format like this:</p>
<p>Hello {first_name} {last_name}, this is the list of fruit that you want</p>
<ol>
<li>{fruit_0} with {colour_0} colour</li>
<li>{fruit_1} with {colour_1} colour</li>
<li>{fruit_2} with {colour_2} colour</li>
</ol>
<p>So the expected output would be (example first row):</p>
<pre><code>Hello Maria White, this is the list of fruit that you want
1. apple with pink colour
</code></pre>
<p>But when I code it turns out:</p>
<pre><code>Hello Maria White, this is the list of fruit that you want
1. apple with pink colour
2. - with - colour
3. - with - colour
</code></pre>
<p>This the code that I used:</p>
<pre><code>import pandas as pd

  df = pd.read_excel(&quot;fruit.xlsx&quot;)
  for x in x.dfitertuples():
    first_name = x.first_name
    last_name = x.last_name
    fruit_0 = x.fruit_0
    fruit_1 = x.fruit_1
    fruit_2 = x.fruit_2
    colour_0 = x.colour_0
    colour_1 = x.colour_1
    colour_2 = x.colour_2
    
    message = f&quot;&quot;&quot;
  Hello {first_name} {last_name}, this is the list of fruit that you want
1. {fruit_0} with {colour_0} colour
2. {fruit_1} with {colour_1} colour
3. {fruit_2} with {colour_2} colour
&quot;&quot;&quot;
print(message)
</code></pre>
<p>Thank you</p>
",19827985.0,14311263.0,2022-08-23 14:37:44,2022-08-23 14:37:44,Iterate through the dataframe and skip when it meets specific values,<python><data-science>,1,1,N/A,CC BY-SA 4.0
73461982,1,-1.0,2022-08-23 16:12:35,1,222,"<p>I am new to Deep Learning.</p>
<p>I have 3 keras model. I am trying something like this:</p>
<pre><code>weights = [] # or some data type

for i in range(3):
    weights[i] = model[i].get_weights()
</code></pre>
<p>I want to store the weights of all models and then use model.set_weights() at some later point in time</p>
",13193056.0,13193056.0,2022-08-23 16:16:07,2022-08-23 16:46:59,Store weights of multiple Keras models in one variable/array,<tensorflow><neural-network><data-science><tf.keras><keras-layer>,1,0,N/A,CC BY-SA 4.0
73460795,1,-1.0,2022-08-23 14:46:52,0,70,"<blockquote>
<p>For below I am using a Haberman's Dataset &amp; LINK FOR DATASET - <a href=""https://www.kaggle.com/gilsousa/habermans-survival-data-set/version/1"" rel=""nofollow noreferrer"">https://www.kaggle.com/gilsousa/habermans-survival-data-set/version/1</a></p>
</blockquote>
<pre><code>df_1 = df.loc[df[&quot;survival_status&quot;] == &quot;1&quot;]; #here ,I have put this &quot;1&quot; is from dataset,1 means survive ,means 1 is a dependent variable
df_2 = df.loc[df[&quot;survival_status&quot;] == &quot;2&quot;]; #here ,I have put this &quot;2&quot; is from dataset,2 means not survive, means 2 is a dependent variable
counts,bin_edges=np.histogram(df_1[&quot;age&quot;],bins=10,density=None)
pdf=counts/(sum(counts))
print(pdf)
print(bin_edges)
cdf=np.cumsum(pdf)
plt.plot(bin_edges[1:],pdf)
plt.plot(bin_edges[1:],cdf)
plt.show()
</code></pre>
<p>I am getting below error</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-147-3367cd025c24&gt; in &lt;module&gt;
      1 df_1 = df.loc[df[&quot;survival_status&quot;] == &quot;1&quot;];
      2 df_2 = df.loc[df[&quot;survival_status&quot;] == &quot;2&quot;];
----&gt; 3 counts,bin_edges=np.histogram(df_1[&quot;age&quot;],bins=10,density=None)
      4 pdf=counts/(sum(counts))
      5 print(pdf)

&lt;__array_function__ internals&gt; in histogram(*args, **kwargs)

2 frames
/usr/local/lib/python3.7/dist-packages/numpy/lib/histograms.py in histogram(a, bins, range, normed, weights, density)
    791     a, weights = _ravel_and_check_weights(a, weights)
    792 
--&gt; 793     bin_edges, uniform_bins = _get_bin_edges(a, bins, range, weights)
    794 
    795     # Histogram is an integer or a float array depending on the weights.

/usr/local/lib/python3.7/dist-packages/numpy/lib/histograms.py in _get_bin_edges(a, bins, range, weights)
    424             raise ValueError('`bins` must be positive, when an integer')
    425 
--&gt; 426         first_edge, last_edge = _get_outer_edges(a, range)
    427 
    428     elif np.ndim(bins) == 1:

/usr/local/lib/python3.7/dist-packages/numpy/lib/histograms.py in _get_outer_edges(a, range)
    320     else:
    321         first_edge, last_edge = a.min(), a.max()
--&gt; 322         if not (np.isfinite(first_edge) and np.isfinite(last_edge)):
    323             raise V`enter code here`alueError(
    324                 &quot;autodetected range of [{}, {}] is not finite&quot;.format(first_edge, last_edge))

TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>
<p>I am trying to create a PDF &amp; CDF and I am getting this above TypeError.</p>
",19829785.0,19829785.0,2022-08-23 15:01:00,2022-08-23 15:50:07,Why am I getting TypeError while creating a CDF?,<python><data-science><typeerror><data-analysis><eda>,1,0,N/A,CC BY-SA 4.0
73464839,1,-1.0,2022-08-23 20:42:00,-1,312,"<p><em>Edit: please share comments as I'm learning to post good questions</em></p>
<p>I'm trying to train this dataset with <code>IsolationForest()</code>, I need to train this dataset, and use it in another dataset with altered qualities to predict the quality values and fetch all wines with quality 8 and 9.</p>
<p>However I'm having some problems with it. Because the accuracy score is <code>0.0</code> from the classification report:</p>
<pre><code>print(classification_report(y_test, prediction))

              precision    recall  f1-score   support

          -1       0.00      0.00      0.00       0.0
           1       0.00      0.00      0.00       0.0
           3       0.00      0.00      0.00     866.0
           4       0.00      0.00      0.00     829.0
           5       0.00      0.00      0.00     841.0
           6       0.00      0.00      0.00     861.0
           7       0.00      0.00      0.00     822.0
           8       0.00      0.00      0.00     886.0
           9       0.00      0.00      0.00     851.0

    accuracy                           0.00    5956.0
   macro avg       0.00      0.00      0.00    5956.0
weighted avg       0.00      0.00      0.00    5956.0
</code></pre>
<p>I don't know if it's a hyperparameter issue, or if I'm clearing the wrong data or putting wrong parameters, I already tried to use with SMOTE and without SMOTE, I wanted to reach an accuracy of 90% at least.</p>
<p>I'll leave the shared drive link public for dataset verification::</p>
<p><a href=""https://drive.google.com/drive/folders/18_sOSIZZw9DCW7ftEKuOG4aIzGXoasFe?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/18_sOSIZZw9DCW7ftEKuOG4aIzGXoasFe?usp=sharing</a></p>
<p>Here's my code:</p>
<pre><code>from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import IsolationForest
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report,confusion_matrix

df = pd.read_csv('wines.csv')

df.head(5)

ordinalEncoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-99).fit(df[['color']])
df[['color']] = ordinalEncoder.transform(df[['color']])

df.info()

df['color'] = df['color'].astype(int)

df.head(3)

stm = SMOTE(k_neighbors=4)
x_smote = df.drop('quality',axis=1)
y_smote = df['quality']
x_smote,y_smote = stm.fit_resample(x_smote,y_smote)

print(x_smote.shape,y_smote.shape)

x_smote.columns

scaler = StandardScaler()
X = scaler.fit_transform(x_smote)
y = y_smote

X.shape, y.shape

x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

from sklearn.ensemble import IsolationForest
from sklearn.metrics import hamming_loss

iforest = IsolationForest(n_estimators=200, max_samples=0.1, contamination=0.10, max_features=1.0, bootstrap=False, n_jobs=-1, 
                            random_state=None, verbose=0, warm_start=False)

iforest_fit = iforest.fit(x_train,y_train)

prediction = iforest_fit.predict(x_test)

print (prediction.shape, y_test.shape)

y.value_counts()

prediction

print(confusion_matrix(y_test, prediction))
hamming_loss(y_test, prediction)

from sklearn.metrics import classification_report
print(classification_report(y_test, prediction))
</code></pre>
",19170581.0,19170581.0,2022-08-24 20:44:01,2022-08-24 20:44:01,Isolation Forest getting accuracy score 0.0,<machine-learning><scikit-learn><data-science><multiclass-classification><isolation-forest>,1,0,2022-08-24 01:16:59,CC BY-SA 4.0
73460536,1,73461175.0,2022-08-23 14:29:24,2,185,"<p>Shape of the original dataset is 82580×30 with multiple string columns. Example dataset:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer

df = pd.DataFrame({'Nationality': {0: 'DEU', 1: 'PRT', 2: 'PRT', 3: 'PRT', 4: 'FRA', 5: 'DEU', 6: 'CHE', 7: 'DEU', 8: 'GBR', 9: 'AUT', 10: 'PRT', 11: 'FRA', 12: 'OTR', 13: 'GBR', 14: 'ESP', 15: 'PRT', 16: 'OTR', 17: 'PRT', 18: 'ESP', 19: 'AUT'},
                   'Age': {0: 27.0, 1: 45.46, 2: 45.46, 3: 58.0, 4: 57.0, 5: 27.0, 6: 49.0, 7: 62.0, 8: 44.0, 9: 61.0, 10: 54.0, 11: 53.0, 12: 50.0, 13: 30.0, 14: 51.0, 15: 45.46, 16: 40.0, 17: 49.0, 18: 49.0, 19: 14.0},
                   'DaysSinceCreation': {0: 370, 1: 213, 2: 206, 3: 1018, 4: 835, 5: 52, 6: 597, 7: 217, 8: 999, 9: 1004, 10: 402, 11: 879, 12: 393, 13: 923, 14: 249, 15: 52, 16: 159, 17: 929, 18: 49, 19: 131},
                   'BookingsCheckedIn': {0: 1, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 1, 7: 2, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 0, 16: 0, 17: 1, 18: 1, 19: 0}})


# Encoding Variables
transformer = make_column_transformer((OneHotEncoder(sparse=False), ['Nationality']), remainder='passthrough')
    
transformed = transformer.fit_transform(df)
transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names_out())
    
# Concat the two tables
transformed_df.reset_index(drop=True, inplace=True)
df.reset_index(drop=True, inplace=True)
df = pd.concat([transformed_df, df], axis=1)
    
# Remove old columns
df.drop(['Nationality'], axis = 1, inplace = True)
print('The shape after encoding: {}'.format(df.shape))
print(df.columns.unique())

The shape after encoding: (20, 14)
Index(['onehotencoder__Nationality_AUT', 'onehotencoder__Nationality_CHE',
       'onehotencoder__Nationality_DEU', 'onehotencoder__Nationality_ESP',
       'onehotencoder__Nationality_FRA', 'onehotencoder__Nationality_GBR',
       'onehotencoder__Nationality_OTR', 'onehotencoder__Nationality_PRT',
       'remainder__Age', 'remainder__DaysSinceCreation',
       'remainder__BookingsCheckedIn', 'Age', 'DaysSinceCreation',
       'BookingsCheckedIn'],
      dtype='object')
</code></pre>
<p>After modeling, trying to test on a completely different test set:</p>
<pre><code>df = pd.DataFrame({'Nationality': {0: 'CAN', 1: 'DEU', 2: 'PRT', 3: 'PRT', 4: 'FRA'},
                   'Age': {0: 27.0, 1: 29.0, 2: 24.0, 3: 24.0, 4: 46.0},
                   'DaysSinceCreation': {0: 222, 1: 988, 2: 212, 3: 685, 4: 1052},
                   'BookingsCheckedIn': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}})

# Encoding Variables
transformer = make_column_transformer(
    (OneHotEncoder(sparse=False), ['Nationality']),
    remainder='passthrough')

transformed = transformer.fit_transform(df)
transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names_out())

# Concat the two tables
transformed_df.reset_index(drop=True, inplace=True)
df.reset_index(drop=True, inplace=True)
df = pd.concat([transformed_df, df], axis=1)

# Remove old columns
df.drop(['Nationality'], axis = 1, inplace = True)
print('The shape after encoding: {}'.format(df.shape))
print(df.columns.unique())

The shape after encoding: (5, 10)
Index(['onehotencoder__Nationality_CAN', 'onehotencoder__Nationality_DEU',
       'onehotencoder__Nationality_FRA', 'onehotencoder__Nationality_PRT',
       'remainder__Age', 'remainder__DaysSinceCreation',
       'remainder__BookingsCheckedIn', 'Age', 'DaysSinceCreation',
       'BookingsCheckedIn'],
      dtype='object')
</code></pre>
<p>As can be seen, testing dataset has some features that were not present in the original training set and many features of training set are not present in test set. If I only use .values of X_train, y_train, X_test, y_test, I can run from logistic regression to Neural Net with &gt;99% accuracy, but that feels like cheating and is not working out with Decision Trees. How do we deal with this?</p>
",14890608.0,12370687.0,2022-08-23 21:20:09,2022-08-23 21:20:09,How to keep the number and names of columns in training and test dataset equal after one hot encoding?,<pandas><dataframe><machine-learning><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
73466418,1,-1.0,2022-08-24 01:08:59,-1,86,"<p>I have generated some predicted values using a custom linear regression function, <code>reg()</code>, and iterating it over multiple variables with a for-loop. They are saved in <code>forage.pred</code>, a vector of more than 5000 values.</p>
<p>I am now trying to convert this into a dataframe, but I don't know how to.</p>
<p>Several loop tutorials that I found on the Internet use dummy example with one vector and index based on a single vector, so they don't apply to my situation.</p>
<pre class=""lang-r prettyprint-override""><code>forage
# A tibble: 5,421 × 24
#    year standid trt    plot sedge legume woody forbs
#   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
# 1  1986       2 CCSP      1    30   14.4     0   30 
# 2  1986       7 CCSP      1     0    0      20    0 
# 3  1986      12 CCSP      1    18    0       0  158.
# 4  1987       2 CCSP      1     2    0       0   32 
# 5  1987       7 CCSP      1     0   40       0   80 
# 6  1987      12 CCSP      1     0    2       0   52 
# 7  1988       2 CCSP      1     0    2     424    2 
# 8  1988       7 CCSP      1   104    0       0    0 
# 9  1988      12 CCSP      1     4    4      64    2 
# 10  1989       2 CCSP      1     0    0       0    2 
# … with 5,411 more rows, and 16 more variables:
#   panicum &lt;dbl&gt;, grass &lt;dbl&gt;, litter &lt;dbl&gt;,
#   burned &lt;dbl&gt;, nonwoody &lt;dbl&gt;, pp_grow &lt;dbl&gt;,
#   pp_watyear &lt;dbl&gt;, pp_spring &lt;dbl&gt;, pp_summer &lt;dbl&gt;,
#   pp_winter &lt;dbl&gt;, pp_grow.s50 &lt;dbl&gt;,
#   pp_grow.s30 &lt;dbl&gt;, pp_grow.s10 &lt;dbl&gt;,
#   pp_grow.a10 &lt;dbl&gt;, pp_grow.a30 &lt;dbl&gt;, …
# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names

yvar &lt;- forage[, 5:10]
new.pp &lt;- forage[, 19:24]

# Fit linear model and create predicted values
reg &lt;- function(i, j) {
  lm.forage &lt;- lm(i ~ year + pp_grow,
                 data = forage)
  forage.pred &lt;- (predict(lm.forage,
                          newdata = list(pp_grow = j)))
  }

# Loop this over multiple variables
for (i in yvar) {
  for (j in new.pp) {
    reg(i, j)
  }
}
</code></pre>
",19560668.0,13968222.0,2022-08-24 11:52:58,2022-08-24 12:42:15,How to export output from a loop into a dataframe?,<r><dataframe><loops><data-science><data-management>,1,4,N/A,CC BY-SA 4.0
73468144,1,-1.0,2022-08-24 06:05:06,-3,134,"<p>Given a dataset with 100 observations and 3 features plus one label(regression). we train the model with 100 * 4(3 features + 1 label) data. Now can we predict the features when the label is given as input.
For eg:</p>
<pre><code>f1 f2 f3 Label
2  2  3  12.5
3  6  5  3.8
6  5  4  9.2
..........
..........
..........
..........
</code></pre>
<p>now the question is to predict f1, f2 and f3 when label is given ( if label=6.7 then predict f1,f2,f3).</p>
<p>It would be of great help if any suggestion or resources is provided.</p>
",10278110.0,-1.0,N/A,2022-08-24 07:50:36,Can we predict features from labels in machine learning,<python><machine-learning><deep-learning><data-science><artificial-intelligence>,1,2,2022-08-24 10:11:15,CC BY-SA 4.0
73470973,1,-1.0,2022-08-24 09:49:23,0,42,"<p>There are two dataframes(df and dfi) and I'm trying to locate  those user_id s which are same in both dataframes and want to create new column in df named new_country in which  I will save the countries whose user_id is same in both.</p>
<pre><code>import pandas as pd


df =fb_comments_count
dfi=fb_active_users
df.sort_values(['created_at'])
df = df.loc[(df['created_at']&lt;'2020/01/31') &amp; (df['created_at']&gt;'2019/12/01' )]

n=0

for x in df['user_id'].unique():
    
    df.at[n,'sum']=df.loc[df['user_id']==x ,'number_of_comments'].sum()   #this line works 
    
    n=n+1

df =df.drop_duplicates(subset = ['user_id'])
df = df.dropna()
df.sort_values('user_id')
m=0
dfi=dfi.drop(columns=['name','status'])
for y in df['user_id']:
    
    df.at[m,'new_country']=dfi.loc[dfi['user_id']==y ,'country']   #but this line doesn't and
    #print(dfi.loc[dfi['user_id']==y])                             # shows
    #print(y)                                                      #&quot;Incompatible indexer with 
                                                                   #Series&quot;
    m=m+1
   
df    
</code></pre>
<p>it throws the error</p>
<pre><code>Incompatible indexer with Series
</code></pre>
<p>But some strange index appears when I try to  print</p>
<pre><code>print(dfi.loc[dfi['user_id']==y])
</code></pre>
<p>the code is given below</p>
<pre><code>
import pandas as pd


df =fb_comments_count
dfi=fb_active_users
df.sort_values(['created_at'])
df = df.loc[(df['created_at']&lt;'2020/01/31') &amp; (df['created_at']&gt;'2019/12/01' )]

n=0

for x in df['user_id'].unique():
    
    df.at[n,'sum']=df.loc[df['user_id']==x ,'number_of_comments'].sum()
    
    n=n+1

df =df.drop_duplicates(subset = ['user_id'])
df = df.dropna()
df.sort_values('user_id')
m=0
dfi=dfi.drop(columns=['name','status'])
for y in df['user_id']:
    
    #df.at[m,'new_country']=dfi.loc[dfi['user_id']==y ,'country']
    print(dfi.loc[dfi['user_id']==y])
    #print(y)
    m=m+1
   
df 

</code></pre>
<p>sample dataframe</p>
<pre><code>fb_comments_count

user_id created_at  number_of_comments
18  2019-12-29 00:00:00 1
25  2019-12-21 00:00:00 1
78  2020-01-04 00:00:00 1
37  2020-02-01 00:00:00 1
41  2019-12-23 00:00:00 1

</code></pre>
<pre><code>fb_active_users

user_id name       status       country
33  Amanda Leon     open        Australia
27  Jessica Farrell open        Luxembourg
18  Wanda Ramirez   open        USA
50  Samuel Miller   closed      Brazil
16  Jacob York      open        Australia
</code></pre>
",19763856.0,19763856.0,2022-08-31 13:34:08,2022-08-31 13:34:08,How to take specific values from one dataframe and put in another,<python><pandas><dataframe><data-science><data-analysis>,0,3,N/A,CC BY-SA 4.0
73475802,1,73475855.0,2022-08-24 15:22:56,1,100,"<pre><code>library(data.table)
DT &lt;- data.table(
      N = 1:16,
  x = c(11,11,11,11,11,11,11,11,21,21,21,21,21,21,21,21), 
  y = c(1,2,3,4,4,4,4,4,1,2,3,4,4,4,4,4), 
  z = c(53,71,27,64,43,62,61,85,44,56,23,37,31,48,80,38)
)
</code></pre>
<p>I want to get a column with values as average of z with respect to y for each x values
like</p>
<pre><code>N   x   y   z   Roll Mean 
1   11  1   53  NA
2   11  2   71  53
3   11  3   27  62
4   11  4   64  50.33333333
5   11  4   43  53.75
6   11  4   62  51.25
7   11  4   61  49
8   11  4   85  57.5
9   21  1   44  NA
10  21  2   56  44
11  21  3   23  50
12  21  4   37  41
13  21  4   31  40
14  21  4   48  36.75
15  21  4   80  34.75
16  21  4   38  49
</code></pre>
<p>for example when N=2 x=11, y=2 the roll mean = average of previous 1 z term = mean(53)
when N=3, x=11, y=3 the roll mean = average of previous 1 and 2 z term = mean(53&amp;71 )
when N=4 x=11, y=4 the roll mean = average of previous 1, 2 and 3rd z terms = mean(53,71,27 )
then N = 5 to 8 I have to get averages of previous 4 values.
I wrote the code</p>
<pre><code>DT[, RollingAvg := frollapply(z,4, mean), .(x)] 
</code></pre>
<p>gives the output</p>
<pre><code>    N  x y  z RollingAvg
1:  1 11 1 53         NA
2:  2 11 1 71         NA
3:  3 11 1 27         NA
4:  4 11 1 64      53.75
5:  5 11 1 43      51.25
6:  6 11 1 62      49.00
7:  7 11 1 61      57.50
8:  8 11 1 85      62.75
9:  9 21 1 44         NA
10: 10 21 1 56         NA
11: 11 21 1 23         NA
12: 12 21 1 37      40.00
13: 13 21 1 31      36.75
14: 14 21 1 48      34.75
15: 15 21 1 80      49.00
16: 16 21 1 38      49.25
</code></pre>
<p>How can I get the correct out put</p>
",16384908.0,3732271.0,2022-08-24 18:14:53,2022-08-26 18:02:23,Computation of average of a column on rolling in R data.table,<r><data.table><data-science>,2,2,N/A,CC BY-SA 4.0
73479303,1,73479573.0,2022-08-24 20:47:50,2,50,"<p>I have dataframe like this:</p>
<pre><code>id |    date   | status
________________________
...     ...      ...
1  |2020-01-01 | reserve
1  |2020-01-02 | sold
2  |2020-01-01 | free
3  |2020-01-03 | reserve
3  |2020-01-25 | signed
3  |2020-01-30 | sold
...     ...      ...
10 |2020-01-02 | signed
10 |2020-02-15 | sold 
...     ...      ....
</code></pre>
<p>I want to find indices of all rows with status <code>sold</code> and then for all rows back 29 days of these (rows with status <code>sold</code>) assign <code>1</code> and <code>0</code> in else case.</p>
<p>The desired dataframe is this</p>
<pre><code>id |    date   | status  | label
_________________________________
...     ...      ...        ...
1  |2020-01-01 | reserve | 1 
1  |2020-01-02 | sold    | 1
2  |2019-12-02 | free    | 0    # no sold status for 2
3  |2020-01-03 | reserve | 1
3  |2020-01-25 | signed  | 1
3  |2020-01-30 | sold    | 1
...     ...      ...        ...
10 |2020-01-02 | signed  | 0
10 |2020-02-15 | sold    | 1    # more than 29 days from 2020-02-15
...     ...      ....       ...
</code></pre>
<p>I attemped to use <code>apply()</code>, but I found out I can't call function like that</p>
<pre><code>def make_labels(df):    
    
    def get_indices(df):
        return list(df[df['date'] &gt;= df.iloc[-1]['date'] - timedelta(days=29)].index)

    df.sort_values(['id', 'date'], inplace=True)
    zero_labels = pd.Series(0, index = df.index, name='sold_labels')    
    one_lables = df.groupby('id')['status'].apply(lambda s: get_indices if s.iloc[-1] == 'sold').sum()
    zero_labels.loc[one_lables] = 1
    
    return zero_labels

df['label'] = make_labels(df)
</code></pre>
<p>dataframe constructor of the input:</p>
<pre><code>d = {'id': [1, 1, 2, 3, 3, 3, 10, 10], 
     'date': ['2020-01-01', '2020-01-02', '2020-01-01', '2020-01-03', '2020-01-25', '2020-01-30', '2020-01-02', '2020-02-15'],
     'status': ['reserve', 'sold', 'free', 'reserve', 'signed', 'sold', 'signed', 'sold']
    }
df = pd.DataFrame(data=d)
</code></pre>
",9137206.0,9137206.0,2022-08-24 21:04:23,2022-08-24 21:18:17,Find list of indices before specific value in column for each unique id,<python><pandas><dataframe><data-science><apply>,1,0,N/A,CC BY-SA 4.0
73485985,1,-1.0,2022-08-25 10:32:38,1,144,"<p>I have a dataset on property.
It has rental values and deposit amount, number of bedrooms, area, etc.
At least 1/3rd of the rental column values are just Zero.
There is no value in it.
I have to perform clustering.</p>
<p>However, the rent values are highly skewed.
Can I ignore 1/3rd of the rows while performing clustering or should I impute values?
What is the right method to impute values.</p>
",10550324.0,12370687.0,2022-08-25 12:25:00,2022-08-25 12:25:00,Clustering with 1/3rd of the values as Zero,<machine-learning><scikit-learn><statistics><data-science><cluster-analysis>,1,0,2022-08-25 12:08:45,CC BY-SA 4.0
73496091,1,73496781.0,2022-08-26 04:27:30,0,124,"<p>I'm developing a regression model. But I ran into a problem when preparing the data. 17 out of 20 signs are categorical, and there are a lot of categories in each of them. Using one-hot-encoding, my data table is transformed into a 10000x6000 table. How should I prepare this type of data?
I used PCA, trying to reduce the dimension, but even 70% of the variance is in 2500 features. That's why I joined.
Unfortunately, I can't attach the dataset, as it is confidential
How do I prepare the data to achieve the best results in the learning process?</p>
",13397366.0,12370687.0,2022-08-26 07:24:06,2022-09-14 17:25:51,A huge number of discrete features,<dataframe><machine-learning><scikit-learn><data-science><categorical-data>,2,1,N/A,CC BY-SA 4.0
73499815,1,-1.0,2022-08-26 10:46:02,2,171,"<p><strong>I have the following dataframe that shows the duration of a job taken by an employee as shown:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Date</th>
<th style=""text-align: center;"">ID number</th>
<th style=""text-align: center;"">Hour</th>
<th style=""text-align: center;"">Job duration</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">12</td>
<td style=""text-align: center;"">240</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">13</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">14</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">15</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">16</td>
<td style=""text-align: center;"">70</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">17</td>
<td style=""text-align: center;"">0</td>
</tr>
</tbody>
</table>
</div>
<p><strong>I've iterated through the dataframe to &quot;spread&quot; the minutes along the hour using the following code:</strong></p>
<pre><code>for i in df.index:
    if df[&quot;Job duration&quot;][i] &gt; 60:
        x = df[&quot;Job duration&quot;][i] - 60
        df[&quot;Job duration&quot;][i] = 60
        df[&quot;Job duration&quot;][i+1] = df[&quot;Job duration&quot;][i+1] + x
</code></pre>
<p><strong>This code works in a small dataset as shown below. In a large dataset however, this doesn't work and will take a long time computationally.</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Date</th>
<th style=""text-align: center;"">ID number</th>
<th style=""text-align: center;"">Hour</th>
<th style=""text-align: center;"">Job duration</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">12</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">13</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">14</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">15</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">16</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">14/07/2022</td>
<td style=""text-align: center;"">1123</td>
<td style=""text-align: center;"">17</td>
<td style=""text-align: center;"">10</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Is there a method of using the lambda function in python to iterate through the rows of the &quot;Job duration&quot; column to speed the process? Thanks in advance.</strong></p>
",19393711.0,-1.0,N/A,2022-09-06 15:17:57,Is there a way to iterate through the rows using the lambda function to add values from the a row to the following row?,<python><pandas><lambda><data-science>,1,3,N/A,CC BY-SA 4.0
73472891,1,73474039.0,2022-08-24 12:09:12,2,79,"<p>I have defined the standard deviation limits for different columns in a dataframe. I did this by defining a function. The output is a dictionary, which always consists of two values. Now how can I explicitly use a certain value in the dictionary which is in a dictionary?</p>
<p>here is my Code:</p>
<pre><code>def Sigma_limit(df, col, x):
    plus_std_dev  = ((df[col].mean()) + (x*df[col].std()))
    minus_std_dev = ((df[col].mean()) - (x*df[col].std()))
    return {'+3Sigma_limited': plus_std_dev,
            '-3Sigma_limited': minus_std_dev}

Sigma_limits = {'A2_3Sigma': Sigma_limit(A_df, 'A2_column', 3),
                'A3_3Sigma': Sigma_limit(A_df, 'A3_column', 3),
                'A4_3Sigma': Sigma_limit(A_df, 'A4_column', 3),
                'A5_3Sigma': Sigma_limit(A_df, 'A5_column', 3)}
</code></pre>
<p><code>enter code here</code></p>
<p>so I have a dictionary with 4x2 dictionaries each and now I would like to use them all separately</p>
",18352742.0,12370687.0,2022-08-24 13:28:11,2022-08-24 13:35:13,How can I work with the dictionary in the dictionary?,<pandas><dataframe><dictionary><statistics><data-science>,2,3,N/A,CC BY-SA 4.0
73515277,1,-1.0,2022-08-28 00:41:23,0,52,"<p>In pandas, if I use series.apply() to apply a function with an inner function definition, for example:</p>
<pre><code>def square_times_two(x):
  def square(y):
    return y ** 2
  return square(x) * 2

data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}
df = pd.DataFrame.from_dict(data)

df[&quot;col_3&quot;] = df.col_1.apply(square_times_two)
</code></pre>
<p>is the inner function redefined for each row? would there be a performance impact to having many inner functions in a function applied to a large series?</p>
",19860962.0,-1.0,N/A,2022-08-28 03:16:19,do inner functions have a substantial impact when used in series.apply() in Pandas,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
73515466,1,-1.0,2022-08-28 01:42:54,2,436,"<p>So I've created a way to programmatically define the source, target and value lists for the sankey diagrams in plotly starting with a list of dictionaries. So if you were looking for a way to do that here it is.</p>
<p>However, I'm stuck on figuring out a way to define the labels using a similar method.</p>
<p>Any help appreciated.</p>
<pre><code>my_data = [
{'src':'wages','dst':'budget', 'value':1500},
{'src':'other','dst':'budget', 'value':250},
{'src':'budget','dst':'taxes', 'value':450},
{'src':'budget','dst':'housing', 'value':420},
{'src':'budget','dst':'food', 'value':400},
{'src':'budget','dst':'transportation', 'value':295},
{'src':'budget','dst':'savings', 'value':25},
{'src':'budget','dst':'other necessities', 'value':160},
]

i = 0 
node_names = []
my_data2 = []
for row in my_data:
    key_src = row['src']
    if (key_src not in node_names):
        node_names.append(key_src)
        i = i + 1
    row['src_id'] = i
    my_data2.append(row)

for row in my_data:
    key_dst = row['dst']
    if (key_dst not in node_names):
        node_names.append(key_dst)
        i = i + 1
    row['dst_id'] = i
    my_data2.append(row)
    
del node_names 

my_data2 = [dict(t) for t in {tuple(d.items()) for d in my_data2}] # Remove duplicates 


source = []
target = []
value = []

for row in my_data2:
    source.append(row['src_id'])
    target.append(row['dst_id'])
    value.append(row['value'])
    

print(source)
print(target)
print(value)


import plotly.graph_objects as go

link = dict(source = source, target = target, value = value)
data = go.Sankey(link = link)


# data
label = [&quot;ZERO&quot;, &quot;ONE&quot;, &quot;TWO&quot;, &quot;THREE&quot;, &quot;FOUR&quot;, &quot;FIVE&quot;]
# data to dict, dict to sankey
link = dict(source = source, target = target, value = value)
node = dict(label = label, pad=50, thickness=5)
data = go.Sankey(link = link, node=node)
# plot
fig = go.Figure(data)
fig.show()
</code></pre>
",13187246.0,-1.0,N/A,2022-08-28 07:00:28,How to programmatically define sankey labels in Plotly,<python><python-3.x><plotly><plotly-dash><graph-data-science>,1,0,N/A,CC BY-SA 4.0
73477853,1,-1.0,2022-08-24 18:12:40,1,108,"<pre><code> DT &lt;- data.table(
 N = 1:16,
 x = c(11,11,11,11,11,11,11,11,21,21,21,21,21,21,21,21), 
 y = rep(1:4,4,4,4,1:4,4,4,4), 
 z = c(53,71,27,64,43,62,61,85,44,56,23,37,31,48,80,38),
 min = c(2.74,2.77,2.23,2.98,2.25,2.48,2.46,2.22,3.07,
        3.08,3.81,3.31,3.32,3.75,3.28,3.04))
</code></pre>
<p>I want to get data.table
with filtration using upper.limit and lowerlimit</p>
<pre><code>   N    x   y   z   min     lower.limit upper.limit
   1    11  1   53  2.74        2.6     2.88
   2    11  2   71  2.77        2.63    2.91
   3    11  3   27  2.23        2.12    2.34
   4    11  4   64  2.98        2.83    3.13
   5    11  4   43  2.25        2.14    2.36
   6    11  4   62  2.48        2.36    2.6
   7    11  4   61  2.46        2.34    2.58
   8    11  4   85  2.22        2.11    2.33
   9    21  1   44  3.07        2.92    3.22
  10    21  2   56  3.08        2.93    3.23
  11    21  3   23  3.81        3.62    4
  12    21  4   37  3.31        3.14    3.48
  13    21  4   31  3.32        3.15    3.49
  14    21  4   48  3.75        3.56    3.94
  15    21  4   80  3.28        3.12    3.44
  16    21  4   38  3.04        2.89    3.19
</code></pre>
<p>I have to filter the data.table based on a min filter
for example
when N=2, if &quot;min&quot; of that row is greater than or equal to upper.limit and less than or equal to the lower.limit of  previous row that row should be eliminated.
when N=3, if &quot;min&quot; of that row is greater than or equal to upper.limit and less than or equal to the lower.limit of any of the previous 2 rows those rows should be eliminated.
when N=4, if &quot;min&quot; of that row is greater than or equal to upper.limit and less than or equal to the lower.limit of previous 3 rows the row should be eliminated.
when N=5,6,7,8, if &quot;min&quot; of that row is greater than or equal to upper.limit and less than or equal to the lower.limit of previous 4 rows the rows should be eliminated. then I have to find rolling mean of the data set</p>
<pre><code>library(zoo)
library(data.table)
DT[, RollingAvg := shift(rollapply(z, 4, mean, 
      partial = TRUE, align = 'right')), by = x]
</code></pre>
<p>How can I apply this min filter along with this rolling function for unique values of x here. can we apply the min filter using the rollapply function?</p>
<p>I usually do like this</p>
<pre><code>N   x   y   z   min l.min   up.limit    N_2 x_2 y_2 z_2 min_2   status
2   11  2   71  2.77    2.63    2.91    1   11  1   53  2.74    TRUE
3   11  3   27  2.23    2.12    2.34    1   11  1   53  2.74    FALSE
3   11  3   27  2.23    2.12    2.34    2   11  2   71  2.77    FALSE
4   11  4   64  2.98    2.83    3.13    1   11  1   53  2.74    FALSE
4   11  4   64  2.98    2.83    3.13    2   11  2   71  2.77    FALSE
4   11  4   64  2.98    2.83    3.13    3   11  3   27  2.23    FALSE
5   11  4   43  2.25    2.14    2.36    1   11  1   53  2.74    FALSE
5   11  4   43  2.25    2.14    2.36    2   11  2   71  2.77    FALSE
5   11  4   43  2.25    2.14    2.36    3   11  3   27  2.23    TRUE
5   11  4   43  2.25    2.14    2.36    4   11  4   64  2.98    FALSE
6   11  4   62  2.48    2.36    2.6     2   11  2   71  2.77    FALSE
6   11  4   62  2.48    2.36    2.6     3   11  3   27  2.23    FALSE
6   11  4   62  2.48    2.36    2.6     4   11  4   64  2.98    FALSE
6   11  4   62  2.48    2.36    2.6     5   11  4   43  2.25    FALSE
7   11  4   61  2.46    2.34    2.58    3   11  3   27  2.23    FALSE
7   11  4   61  2.46    2.34    2.58    4   11  4   64  2.98    FALSE
7   11  4   61  2.46    2.34    2.58    5   11  4   43  2.25    FALSE
7   11  4   61  2.46    2.34    2.58    6   11  4   62  2.48    TRUE
8   11  4   85  2.22    2.11    2.33    4   11  4   64  2.98    FALSE
8   11  4   85  2.22    2.11    2.33    5   11  4   43  2.25    TRUE
8   11  4   85  2.22    2.11    2.33    6   11  4   62  2.48    FALSE
8   11  4   85  2.22    2.11    2.33    7   11  4   61  2.46    FALSE
</code></pre>
<p>then sort out all l.limit &lt;= min_2 &amp; up.limit &gt;= min_2 true(this shows in status column)
so we will get</p>
<pre><code>N   x   y   z   min l.min   up.limit    N_2 x_2 y_2 z_2 min_2   status
2   11  2   71  2.77    2.63    2.91    1   11  1   53  2.74    TRUE
5   11  4   43  2.25    2.14    2.36    3   11  3   27  2.23    TRUE
7   11  4   61  2.46    2.34    2.58    6   11  4   62  2.48    TRUE
8   11  4   85  2.22    2.11    2.33    5   11  4   43  2.25    TRUE
</code></pre>
<p>then find aggregate average of z_2 by the column N gives the expected output<br />
as</p>
<pre><code>N   x   y   z   min l.min   up.limit    Roll_avg
2   11  2   71  2.77    2.63    2.91    53
5   11  4   43  2.25    2.14    2.36    27
7   11  4   61  2.46    2.34    2.58    62
8   11  4   85  2.22    2.11    2.33    43
</code></pre>
<p>this should be repeated all unique value of &quot;x&quot;
This computation is too costly. How can I do this using rollapply function in R? or is there any less costly method to do this in R?</p>
",16384908.0,16384908.0,2022-08-25 02:06:52,2022-08-25 15:37:26,Applying filter using rollapply in R,<r><data.table><data-science><zoo><rollapply>,1,8,N/A,CC BY-SA 4.0
73485731,1,-1.0,2022-08-25 10:11:38,2,126,"<pre><code>cols = list(ds.columns.values)
ds = ds[cols[1:3] + cols[5:6] + [cols[9]]]
print(ds)
</code></pre>
<p>Why did we convert into list in this line <code>cols = list(ds.columns.values)</code>?</p>
",19836753.0,12370687.0,2022-08-25 12:48:57,2022-08-25 14:03:45,I would like to know when we want to print only specific columns in pandas how to implement that,<python><pandas><list><dataframe><data-science>,4,0,N/A,CC BY-SA 4.0
73509591,1,73510566.0,2022-08-27 08:54:46,1,201,"<p>I have one excel with large data. I want to split this excel into multiple excel with equal distribution of rows.</p>
<p>My current code is working partially as it is distributing required number of rows and creating multiple excel. but at the same time it is keep creating more excel by considering the rows number.</p>
<p>In <strong>n_partitions</strong> if I put number 5 then it is creating excel with 5 rows in two excel and after that it keeps creating three more blank excel.
I want my code to stop creating more excel after all the rows gets distributed.</p>
<p>Below is my sample excel with expected result and sample code.</p>
<p><a href=""https://i.stack.imgur.com/7nzO2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7nzO2.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/VwyzN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VwyzN.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ZWXhR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZWXhR.png"" alt=""enter image description here"" /></a></p>
<p>Code I am currently using is.</p>
<pre><code>import pandas as pd

df = pd.read_excel(&quot;C:/Zen/TestZenAmp.xlsx&quot;)

n_partitions = 5

for i in range(n_partitions):
    sub_df = df.iloc[(i*n_partitions):((i+1)*n_partitions)]
    sub_df.to_excel(f&quot;C:/Zen/-{i}.xlsx&quot;, sheet_name=&quot;a&quot;)
</code></pre>
",13727197.0,-1.0,N/A,2022-12-23 06:49:57,How to split one excel into multiple excel with common number of rows distribution across all the new excel using Python?,<python><excel><pandas><dataframe><data-science>,2,0,N/A,CC BY-SA 4.0
73518003,1,-1.0,2022-08-28 10:45:05,-1,230,"<p>Last time when I was training a dnn model I noticed that When I try to train my model with tensor (dtype = float64) it always gives error but when I train the model with numpy array with same specs(shape, values, dtype) as tensor it shows no error. Why is it so</p>
<p><a href=""https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l02c01_celsius_to_fahrenheit.ipynb"" rel=""nofollow noreferrer"">Code</a></p>
<p>For feature and labels as tensor replace numpy.arrys in 2nd script with:</p>
<pre><code>celsius_q    = tf.Variable([-40, -10,  0,  8, 15, 22,  38],  tf.float64)
fahrenheit_a = tf.Variable([-40,  14, 32, 46, 59, 72, 100],  tf.float64)
</code></pre>
<p>When using feature and label as tensor it shows this error:</p>
<pre><code>Error: ValueError: Failed to find data adapter that can handle input:
&lt;class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'&gt;,
&lt;class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'&gt;
</code></pre>
",19863110.0,4685471.0,2022-08-28 22:58:38,2022-11-06 10:11:35,"Why model.fit() method of keras do not accept any tensor as feature or label argument, on the other hand it accepts numpy arrays",<numpy><tensorflow><machine-learning><deep-learning><data-science>,1,2,N/A,CC BY-SA 4.0
73521979,1,73537351.0,2022-08-28 20:18:39,-1,587,"<p>The problem I am facing is that I have a dataframe - sector_features_ which looks like this:
<img src=""https://i.stack.imgur.com/wRXca.png"" alt=""1"" /></p>
<p>After running tsne on it I then have a 2d df which I plot with a scatter graph. The problem is that I don't know how to color the scatter points with the original label information contained in the index that seen in picture 1, which is a tuple which contains the {country} and {year} the observation belongs to. I would ideally like to color according to country only or year only to see how this changes the visualisation.</p>
<p>The data frame containing the reduced data (tsne) looks like
<img src=""https://i.stack.imgur.com/ALIFR.png"" alt=""2"" /></p>
<p>I am using matplotlib and seaborn, but have seen some solutions using altair and I am not sure how to proceed.</p>
<p>the imports are:</p>
<pre><code>import pandas as pd
import numpy as np
import random as rd
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing 
from sklearn.manifold import TSNE 
import matplotlib.pyplot as plt 
import seaborn as sns
</code></pre>
",12575943.0,12258459.0,2022-08-30 04:58:25,2022-08-30 05:02:53,"How to colour a scatter plot of a 2d data frame (reduced using tsne/umap) according to label information in index ({country,year}) of different frame",<python><matplotlib><scikit-learn><data-science><data-visualization>,1,2,N/A,CC BY-SA 4.0
73516000,1,73521141.0,2022-08-28 04:27:54,0,487,"<p>For distance, I want to accomplish conversion like below.</p>
<pre><code>┌────────────┐
│ col        │
│ ---        │
│ list[str]  │
╞════════════╡
│ [&quot;a&quot;]      │
├╌╌╌╌╌╌╌╌╌╌╌╌┤
│ [&quot;a&quot;, &quot;b&quot;] │
├╌╌╌╌╌╌╌╌╌╌╌╌┤
│ [&quot;c&quot;]      │
└────────────┘
↓
↓
↓
┌────────────┬─────────────────┐
│ col        ┆ col_cum         │
│ ---        ┆ ---             │
│ list[str]  ┆ list[str]       │
╞════════════╪═════════════════╡
│ [&quot;a&quot;]      ┆ [&quot;a&quot;]           │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ [&quot;a&quot;, &quot;b&quot;] ┆ [&quot;a&quot;, &quot;b&quot;]      │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ [&quot;c&quot;]      ┆ [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] │
└────────────┴─────────────────┘
</code></pre>
<p>I've tried <code>polars.Expr.cumulative_eval()</code>, but not work.
From the <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.Expr.cumulative_eval.html?highlight=cumulative_eval"" rel=""nofollow noreferrer"">Offical API</a> example. I can access the first element and last element in every iteration. But I want here is the result of the previous iteration i think.
Could I get some help?</p>
",17123304.0,17123304.0,2022-08-28 06:27:03,2022-08-28 18:18:15,Is there a way to cumulatively and distinctively expand list in polars,<python><dataframe><data-science><python-polars><rust-polars>,2,0,N/A,CC BY-SA 4.0
73517600,1,-1.0,2022-08-28 09:45:04,-1,139,"<p>Given a generated doc2vec vector on some document. is it possible to reverse the vector back to the original document?
If so, does there exist any hash algorithm that would make the vector irreversible but still comparable to other vectors of the same type (using cosine/Euclidean distance)?</p>
",19862963.0,19862963.0,2022-08-28 10:15:39,2022-08-29 01:53:30,Reverse TF-IDF vector (vec2text),<hash><data-science><tf-idf><doc2vec><lsh>,1,0,N/A,CC BY-SA 4.0
73529539,1,73529635.0,2022-08-29 13:26:10,2,118,"<p>I have a dataset in this form:</p>
<pre><code>Name  Batch  DXYR  Emp  Lateral  GDX  MMT  CN
Joe    2       0          2       2    2    0  
Alan   0       1          1       2    0    0
Josh   1       1          2       1    1    2 
Max    0       1          0       0    0    2
</code></pre>
<p>These columns can have only three distinct values ie. 0, 1 and 2..
So, I need percent of value counts for each column in pandas dataframe..</p>
<p>I have simply make a loop like:</p>
<pre><code>for i in df.columns:
  (df[i].value_counts()/df[i].count())*100
</code></pre>
<p>I am getting the output like:</p>
<pre><code>0    90.608831
1     0.391169
2    9.6787899
Name: Batch, dtype: float64

0    95.545455
1     2.235422
2    2.6243553
Name: MX, dtype: float64

and so on...
</code></pre>
<p>These outputs are correct but I need it in pandas dataframe like this:</p>
<pre><code>                 Batch  DXYR  Emp  Lateral  GDX     MMT    CN
Count_0_percent  98.32  52.5   22   54.5     44.2   53.4  76.01  
Count_1_percent  0.44   34.5   43   43.5     44.5   46.5  22.44
Count_2_percent  1.3    64.3   44   2.87     12.6   1.88  2.567
</code></pre>
<p>Can someone please suggest me how to get it</p>
",18009048.0,-1.0,N/A,2022-08-29 13:45:32,Convert value counts of multiple columns to pandas dataframe,<pandas><data-science>,1,0,N/A,CC BY-SA 4.0
73531855,1,-1.0,2022-08-29 16:30:27,0,59,"<pre><code>def card (Name,F_Name,Roll_No,Program):    

    x= f'''
    University Name
    
    
    Name: {Name}
    Father Name: {F_Name}
    Roll No: {Roll_No}
    Program: {Program}

    '''
    return x

data = [{'Name':'Student1','F_Name':&quot;student's 1 father&quot;,'Roll_No':'123','Program':'BSCS'},{'Name':'student2','F_Name:':&quot;student's 2 father&quot;,'Roll_No':'456','Program':'BSCS'},{'Name':'student3','F_Name':&quot;student's 3 father&quot;,'Roll_No':'789','Program':'BSCS'}]

id_card = list(map(card,**data))

print (id_card)
</code></pre>
<p>I want to run this function using the map function and iterate the dictionary value in their respective positions in the user defined function.</p>
",14152873.0,1491895.0,2022-08-29 17:01:23,2022-08-29 17:05:08,python map function with dictionaries in list,<python><python-3.x><data-science>,1,1,N/A,CC BY-SA 4.0
73488991,1,73489091.0,2022-08-25 14:06:08,2,62,"<p>For-loop iterating but store with only last value of the list</p>
<p>Here is the code:</p>
<pre><code>models = [KNN,NB,LR,SVM]
result = pd.DataFrame()
for i in models:
    i.fit(x_train, y_train)
    ypred = i.predict(x_test)
    model_valuation = result.append({'Model': i, 
                                  'Accuracy': accuracy_score(y_test, ypred),
                                    'Recall': recall_score(y_test, ypred),
                                 'Precision': precision_score(y_test, ypred),
                               'Specificity': recall_score(y_test, ypred, pos_label=0),
                                  'F1 score': f1_score(y_test, ypred)}, ignore_index=True)
</code></pre>
<p>It appends only the SVM which is the last value in the list models.</p>
",19845860.0,12370687.0,2022-08-26 01:06:43,2022-08-26 01:06:43,For loop iterating with only last value of the list,<pandas><dataframe><machine-learning><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
73491390,1,-1.0,2022-08-25 17:14:11,0,26,"<pre><code>[10 15  18  11]
[15 17  24  16]
[13 13  20  14]
[12 20  10  25]
[12 11  14  11]
</code></pre>
<p>I have this data, and I'm trying to scale it using sklearn.preprocessing.StandardScaler:</p>
<pre><code>from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled=scaler.fit_transform(df)
scaled_df = pd.DataFrame(scaled, columns = df.columns)
scaled_df.head()
</code></pre>
<p>This outputs:</p>
<pre><code>array([[-1.32680694, -0.06401844,  0.16552118, -0.85248268],
       [ 1.73505523,  0.57616596,  1.40693001,  0.11624764],
       [ 0.20412415, -0.70420284,  0.57932412, -0.27124449],
       [-0.30618622,  1.53644256, -1.4896906 ,  1.85996222],
       [-0.30618622, -1.34438724, -0.66208471, -0.85248268]])
</code></pre>
<p>I know this is wrong since the cov matrix shows variance as 1.25, when by definition it should be 1. Also, the original data is correctly saved in the 'df' variable. If I standarize the data manually I get the correct result, so I really don't know what's going on with the scaler function.</p>
",16874390.0,-1.0,N/A,2022-08-26 03:47:55,sklearn StandardScaler outputting wrong matrix,<python><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
73503944,1,73504364.0,2022-08-26 16:21:13,0,37,"<p>I'm adding dummy rows to some data so I am able to Join it with an existing dataset. I need data every 30 minutes but the dataset has every 1 hour. I have created a dummy row for every other row and imputed its value using <code>.ffill()</code> but have no idea of to fill the Pandas time series in the same manner but add 30 minutes to each value.</p>
<p>I want something along this line that replaces <code>Nan</code> values and indexes 30 minutes from the last non-Nan.</p>
<p>Input DF</p>
<pre><code>0        2011-11-11 00:00:00
1        Nan
2        2011-11-11 01:00:00
3        Nan
4        2011-11-11 02:00:00
</code></pre>
<p>Code</p>
<pre><code>df['time'] = df['time'].ffill() + 000-00-00 00:30:00
</code></pre>
<p>Output</p>
<pre><code>0        2011-11-11 00:00:00
1        2011-11-11 00:30:00
2        2011-11-11 01:00:00
3        2011-11-11 01:30:00
4        2011-11-11 02:00:00
</code></pre>
",17865922.0,14627505.0,2022-08-26 20:57:04,2022-08-26 20:57:04,Infilling every other Row in a Pandas Dataframe the the value from the last row + 30 minutes,<python><pandas><datetime><data-science>,1,0,N/A,CC BY-SA 4.0
73534134,1,73534422.0,2022-08-29 20:05:21,0,52,"<p>Let's say we have a team variable, but we also have a time period 1 and a time period 2 variable, and a numeric grade 1-10. I want to mutate and add a variable that calculates the difference from time period 1 to time period 2.</p>
<p>How do I do this?</p>
<p>Visually the table looks like this:
<a href=""https://i.stack.imgur.com/LMQeM.png"" rel=""nofollow noreferrer"">img</a></p>
",18290295.0,18290295.0,2022-08-29 20:06:54,2022-08-29 21:15:08,How to calculate difference in values grouped by 2 separate variables in R,<r><statistics><data-science><analytics>,2,1,N/A,CC BY-SA 4.0
73542558,1,73543294.0,2022-08-30 12:35:56,1,2302,"<p>I got this error message :</p>
<pre><code>There is no procedure with the name `gds.graph.create.cypher` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.
</code></pre>
<p>I think that the version of neo4j destktop(4.4.5) I'm using isn't compatible with graph data science play graoud(2.0.3). But I don't know what I'm supposed to do , thank you in advance for any suggestion.</p>
",19853959.0,-1.0,N/A,2022-08-30 13:29:01,procedure `gds.graph.create.cypher` not found,<neo4j><graph-data-science>,1,0,N/A,CC BY-SA 4.0
73551183,1,73551264.0,2022-08-31 05:23:03,1,39,"<p>Just want to help with data science to generate some synthetic data since we don't have enough labelled data. I want to cut the rows around the random position of the <code>y</code> column around 0s, don't cut 1 sequence.</p>
<p>After cutting, want to <strong>shuffle</strong> those slices and generate a new DataFrame.</p>
<p>It's better to have some parameters that adjust the maximum, and minimum sequence to cut, the number of cuts, and something like that.</p>
<p>The raw data</p>
<pre><code>ts   v1    y
0    100   1
1    120   1
2    80    1
3    5     0
4    2     0
5    100   1
6    200   1
7    1234  1
8    12    0
9    40    0
10   200   1
11   300   1
12   0.5   0
...
</code></pre>
<p>Some possible cuts</p>
<pre><code>ts   v1    y
0    100   1
1    120   1
2    80    1
3    5     0
--------------
4    2     0
--------------
5    100   1
6    200   1
7    1234  1
-------------
8    12    0
9    40    0
10   200   1
11   300   1
-------------
12   0.5   0
...
</code></pre>
<pre><code>ts   v1    y
0    100   1
1    120   1
2    80    1
3    5     0
4    2     0
-------------
5    100   1
6    200   1
7    1234  1
8    12    0
9    40    0
10   200   1
11   300   1
------------
12   0.5   0
...
</code></pre>
<p><strong>This is NOT CORRECT</strong></p>
<pre><code>ts   v1    y
0    100   1
1    120   1
------------
2    80    1
3    5     0
4    2     0
5    100   1
6    200   1
7    1234  1
8    12    0
9    40    0
10   200   1
11   300   1
12   0.5   0
...
</code></pre>
",6498757.0,-1.0,N/A,2022-08-31 05:34:44,Pandas: I want slice the data and shuffle them to genereate some synthetic data,<pandas><dataframe><math><data-science>,1,0,N/A,CC BY-SA 4.0
73556668,1,76899196.0,2022-08-31 13:30:07,1,73,"<p>I have highly imbalanced data so for binary classification I convert probabilities for 1-class with threshold = 0.06.</p>
<p>I want to show probabilities to management so I need to adjust then on condition that 0.06 is my new 50% boundary.</p>
<p>So I want my low probability, like <code>0.045, 0.067, 0.01</code> values to be recalculated to be higher percentage.</p>
<p>I guess I should multuply it, but I don't know how to find the value.</p>
<p>data for reference</p>
<pre><code>  id     probability
_____________________
168835    0.529622
168836    0.870282
168837    0.988074
180922    0.457827
78352     0.272279
            ...   
320739    0.003046
329237    0.692332
329238    0.926343
329239    0.994264
320741    0.002714
</code></pre>
",9137206.0,-1.0,N/A,2023-08-14 13:11:08,Recalculate the values of the binary classification probabilities based on the threshold,<python><machine-learning><regression><data-science><classification>,1,1,N/A,CC BY-SA 4.0
73562112,1,73562172.0,2022-08-31 21:32:41,0,31,"<p>I am trying to figure out a way to find the first and last time stamp for each asset within a dataframe for each day. For example, I have this data frame:</p>
<pre><code>import pandas as pd
data = {
    'Date':['2022-01-01','2022-01-01','2022-01-01','2022-01-01','2022-01-01','2022-01-01',
           '2022-01-01' ,'2022-01-02','2022-01-02','2022-01-02','2022-01-02','2022-01-02','2022-01-02',
            '2022-01-02','2022-01-02','2022-01-03','2022-01-03','2022-01-03','2022-01-03','2022-01-03',
            '2022-01-03','2022-01-03','2022-01-03'],
    'Time':['12:01','12:05','14:07','11:01','13:06','17:12','15:15',
           '9:02','8:06','14:06','19:19','10:00','13:01','17:00','10:15',
           '8:00','9:00','7:15','16:04','15:02','17:10','12:06','15:00'],
    'Asset':[111,111,111,222,222,222,222,
             111,111,111,111,111,222,222,222,
            333,333,111,111,111,111,333,111]
}
df = pd.DataFrame(data)
df
</code></pre>
<p>Which looks like:</p>
<pre><code>    Date    Time    Asset
0   2022-01-01  12:01   111
1   2022-01-01  12:05   111
2   2022-01-01  14:07   111
3   2022-01-01  11:01   222
4   2022-01-01  13:06   222
5   2022-01-01  17:12   222
6   2022-01-01  15:15   222
7   2022-01-02  9:02    111
8   2022-01-02  8:06    111
9   2022-01-02  14:06   111
10  2022-01-02  19:19   111
11  2022-01-02  10:00   111
12  2022-01-02  13:01   222
13  2022-01-02  17:00   222
14  2022-01-02  10:15   222
15  2022-01-03  8:00    333
16  2022-01-03  9:00    333
17  2022-01-03  7:15    111
18  2022-01-03  16:04   111
19  2022-01-03  15:02   111
20  2022-01-03  17:10   111
21  2022-01-03  12:06   333
22  2022-01-03  15:00   111
</code></pre>
<p>I would like to group this data by day and remove all duplicates for each asset in each day, only keeping the first and last timestamp for each value within each day. My ideal outcome would look like this:</p>
<pre><code>data1 = {
    'Date':['2022-01-01','2022-01-01','2022-01-01','2022-01-01',
           '2022-01-02','2022-01-02','2022-01-02','2022-01-02',
           '2022-01-03','2022-01-03','2022-01-03','2022-01-03',],
    'Time':['12:01','14:07','11:01','17:12',
           '8:06','19:19','10:15','17:00',
           '8:00','12:06','7:15','17:10'],
    'Asset':[111,111,222,222,
            111,111,222,222,
            333,333,111,111]
}
df1 = pd.DataFrame(data1)
df1
</code></pre>
<p>Looking like:</p>
<pre><code>Date    Time    Asset
0   2022-01-01  12:01   111
1   2022-01-01  14:07   111
2   2022-01-01  11:01   222
3   2022-01-01  17:12   222
4   2022-01-02  8:06    111
5   2022-01-02  19:19   111
6   2022-01-02  10:15   222
7   2022-01-02  17:00   222
8   2022-01-03  8:00    333
9   2022-01-03  12:06   333
10  2022-01-03  7:15    111
11  2022-01-03  17:10   111
</code></pre>
<p>Ideally, I would like to solve this problem in Python, however if there is an easier solution in R or SQL I am able to use those. Any help would be appreciated! Thanks in advance!</p>
",19890634.0,-1.0,N/A,2022-09-01 00:11:39,How to find first and last time for every day for each value,<python><python-3.x><pandas><time-series><data-science>,2,0,N/A,CC BY-SA 4.0
73533732,1,-1.0,2022-08-29 19:24:21,-1,72,"<p>Is it possible to assign a variable to an return result of an function ?</p>
<p>First I want to acquire the website from email like for e.g xxxxx@hotmail.com will return only hotmail.com. Then if that website is equal to 'hotmail.com' return 'Yes' if not equal return 'No'.</p>
<pre><code>def mail(var):

x = return var.split('@')[1]

if x == 'hotmail.com':
    return 'Yes'
else:
    return 'No'
</code></pre>
<p>I know it's not the right code but I hope You get the idea. Thanks for your help !</p>
",19873532.0,-1.0,N/A,2022-08-29 21:05:10,Is it possible to assign an variable to return result?,<python><data-science>,3,3,N/A,CC BY-SA 4.0
73546441,1,-1.0,2022-08-30 17:35:23,-1,103,"<p>DataFrame:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">name</th>
<th style=""text-align: left;"">editor</th>
<th style=""text-align: left;"">delegator</th>
<th style=""text-align: left;"">writer</th>
<th style=""text-align: left;"">owner</th>
<th style=""text-align: left;"">financer</th>
<th style=""text-align: left;"">toppers</th>
<th style=""text-align: left;"">best_point_of_contact</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">newpaper1</td>
<td style=""text-align: left;"">A</td>
<td style=""text-align: left;"">A</td>
<td style=""text-align: left;"">B</td>
<td style=""text-align: left;"">A,B</td>
<td style=""text-align: left;"">B</td>
<td style=""text-align: left;"">A,B,C</td>
<td style=""text-align: left;"">A</td>
</tr>
</tbody>
</table>
</div>
<p>Target Column : best_point_of_contact ( the user who should be contacted for updates )</p>
<p>Other Columns : Provide information that which user is performing which task ( for example A and B are the owners, and B is the writer )</p>
<p>Question : How should I refactor this dataset in order to create a model to determine the best point of contact given the other features?</p>
<p>Thanks!</p>
",13810872.0,13810872.0,2022-08-30 17:43:59,2022-08-30 22:38:07,How to create a model for this type of dataset,<pandas><machine-learning><deep-learning><data-science><artificial-intelligence>,1,0,2022-09-01 11:18:51,CC BY-SA 4.0
73558002,1,73559144.0,2022-08-31 15:07:18,0,28,"<p>I would like to ask how can I join the dataframe as shown in (exiting dataframe) to group values based on date&amp;time and take the means of the values. what I meant is that if col B have 2 values in the same minute , it will take average of that value and do same for rest of the columns. What I want to achieve is to have one value each minutes as shown in (preprocessed dataframe)</p>
<p>Thank you</p>
<p><img src=""https://i.stack.imgur.com/ImT4X.png"" alt=""Existing Dataframe"" /></p>
<p><img src=""https://i.stack.imgur.com/Vjtfn.png"" alt=""Preprocessed Dataframe"" /></p>
",-1.0,3494754.0,2022-08-31 16:23:31,2022-08-31 16:33:21,How to combine column in dataframe based on date&time and take the mean of the value,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
73572613,1,-1.0,2022-09-01 16:19:55,0,209,"<p>There are three types of flowers in the attached dataset(iris dataset) and I want to find out the range of sepal_length for all flower types. How can I do it?</p>
<p><strong>Dataset</strong></p>
<p><img src=""https://i.stack.imgur.com/ZOLdA.png"" alt=""enter image description here"" /></p>
<p>Dataset has 5 columns namely SEPAL_LENGHT,SEPAL_WIDTH,PETAL_LENGTH,PETAL_WIDTH &amp; SPECIES.
In the SPECIES columns, three types of flowers are mentioned namely <code>['setosa', 'versicolor', 'virginica']</code>.</p>
<p>I want to find out the range of sepal_length and all other lengths(the rest of 3 columns) for all three species of flowers. Please tell me how can I do it.</p>
<p>I wrote below code for it but it doesn't work</p>
<pre class=""lang-py prettyprint-override""><code>def rang(species,length):
    
    for i in (df.species.unique):
        
        if df.species=='setosa':
            
            mini=min(length)
            maxi=max(length)
            ran=max(length)-min(length)
            
            return(mini,maxi,ran)
        
        else:
            pass
</code></pre>
<p>when I run the above written code an error pops up :</p>
<pre><code>TypeError: 'method' object is not iterable
</code></pre>
",19476450.0,8618242.0,2022-09-02 10:58:12,2022-09-02 10:58:12,How to find out range (min & max) value in sepal length column for each flower types,<python-3.x><jupyter-notebook><data-science><iris-dataset>,0,4,N/A,CC BY-SA 4.0
73525464,1,-1.0,2022-08-29 07:32:15,0,564,"<p>This is my code:</p>
<pre><code>import pandas as pd
df=pd.read_csv('C:\\Users\\HP\\OneDrive\\Documents\\final_annotations.csv')

p=df.groupby(['img_name'])
print(p)
</code></pre>
<p>i am getting an output like this i am not getting table:</p>
<pre><code>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000028A9E35DB70&gt;
</code></pre>
",19836753.0,18470692.0,2022-08-29 07:34:48,2022-08-29 07:39:17,Why am i getting error when using groupbyfunction()?,<python><pandas><data-science>,1,2,N/A,CC BY-SA 4.0
73527518,1,73527567.0,2022-08-29 10:35:18,-1,41,"<p>i created an api using django
for visualisation i used plotly
How i can modify my code to see on my fig , outliers in one color : red?</p>
<pre><code> plt.plot(df_abs.loc[outlier].iloc[1:], markerfacecolor='red')
</code></pre>
",19868832.0,19868832.0,2022-08-30 08:31:12,2022-08-30 08:31:12,"Why i cannot see on my fig , a specific color for outliers?",<python><plotly><data-science>,1,0,N/A,CC BY-SA 4.0
73538425,1,-1.0,2022-08-30 07:05:34,3,216,"<p>I am trying to find clusters in some data with high noise (see plot below).</p>
<p><a href=""https://i.stack.imgur.com/iFux4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iFux4.png"" alt=""enter image description here"" /></a></p>
<p>I tried using DBSCAN which sort of worked, but it required quite a bit of manually tuning the input parameters to find the clusters properly. Are there any other good clustering algorithms for dealing with this kind of data?</p>
<p>Some considerations:</p>
<ul>
<li><p>I am using Julia to do my data processing.</p>
</li>
<li><p>The data has periodic boundary conditions in both directions.</p>
</li>
<li><p>The number of clusters is known a priori.</p>
</li>
<li><p>I am planning to process many datasets in this way, so it should run relatively fast and not require <em>too</em> much manual fiddling.</p>
</li>
</ul>
<p>Thanks!</p>
",19442059.0,-1.0,N/A,2022-08-30 11:06:56,Looking for a clustering algorithm for highly noisy data,<julia><data-science><cluster-analysis>,2,3,N/A,CC BY-SA 4.0
73549651,1,73549702.0,2022-08-31 00:14:40,0,73,"<p>I am working on a problem where I have to take user input which is an integer indicating the number of months I have to look back at. For example if I want to look at the data 3 months back I must take input from user as 3.
Based on this integer user input I have to filter my dataset.
For example today's date is 8/30/2022 so 3 months back will be 5/30/2022.
Now I want to filter my dataframe to include only those rows for this date which is 3 months back i.e. 5/30/2022</p>
<p>I tried using datetime and relativetime libraries but nothing seems to work for me.</p>
<p>Below is an example of my dataframe:</p>
<p><a href=""https://i.stack.imgur.com/JA1X3.png"" rel=""nofollow noreferrer"">id      text1       text2    date
1       Ram         patient  5/30/2022 10:22:00
2       John        patient  5/30/2022 11:45:08
3       Rich        child    5/28/2022 10:45:13</a></p>
<p>so I want output to be rows corresponding to 1 and 2</p>
<p><a href=""https://i.stack.imgur.com/LYESW.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",19883183.0,19883183.0,2022-08-31 00:24:06,2022-08-31 00:45:54,Take user input for number of months back and filter data based on that date,<python><pandas><date><data-science>,2,0,N/A,CC BY-SA 4.0
73574484,1,73574562.0,2022-09-01 19:20:37,0,383,"<p>help me here, estimating the root of the equation.
Here is a code:</p>
<pre class=""lang-py prettyprint-override""><code>def binary_search(f,domain, MAX = 1000):
    start, end = domain
    if start &gt;= end:
        raise ValueError(&quot;Domain is empty&quot;)
    mid = (start + end) / 2
    fmid = f(mid)
    step = 0
    while abs(fmid) &gt; 0.0001 and step &lt; MAX:
        if fmid &lt; 0:
            start = mid
        else:
            end = mid
        mid = (start + end) / 2
        fmid = f(mid)
        step += 1
    return round(mid, 2)
</code></pre>
<p>Here are inputs:</p>
<pre><code>import numpy as np

f = lambda x:(np.sin(x)**2)*(x**2)-2
domain = (0,2)
x=binary_search(f,domain)
x
</code></pre>
<p>The problem with this code is not consistent.
When the domain is (0, 2) it gives the expected answer, which is 1.43.
However, in the domain (2, 3) and (-2, -1) it gives wrong answers which are 2.0 and -2.0 respectively. But I noticed that when I change the if statement under ‘while loop’ from &quot;f mid &lt; 0” to “fmid &gt; 0”, the domains (2, 3) and (-2, -1) give correct answers, which are 2,56 and -1,43 respectively. But now, the domain (0, 2) gives the wrong answer. How can I fix this?</p>
",19898286.0,7318120.0,2022-09-01 21:28:17,2022-09-01 21:28:17,Finding the root of an equation using binary search python,<python><machine-learning><data-science-experience>,1,0,N/A,CC BY-SA 4.0
73580695,1,-1.0,2022-09-02 09:44:37,0,95,"<p>I'm pretty new to graph databases and neo4j in general but trying to solve a problem using this approach.</p>
<p>I have two datasets, one with <code>addresses</code> and <code>DOC_ID</code> where many addresses are related to document through <code>is_in</code> relationship. Many addresses can be in each <code>DOC_ID</code>.</p>
<p>Second dataset has home <code>owners</code> and <code>DOC_ID</code> where again <code>owners</code> have <code>is_in</code> relationship with each doc id.</p>
<p>Dataset 1 looks like below.</p>
<pre><code>      address               DOC_ID        
1    123 Rock Rd            0134
2.   456 John Drive         2157
3.   789 York St.           9871
4.   927 Farm Ct.           9871
...
</code></pre>
<p>Similarly for Dataset 2:</p>
<pre><code>      owner               DOC_ID        
1    John D.                0134
2.   Sarah Cote             2157
3.   Jack Ma                9871
...
</code></pre>
<p>Query I used to load these data in:</p>
<pre><code># Dataset 1 with addresses
LOAD CSV WITH HEADERS FROM 'file:///addresses.csv' AS row
WITH row WHERE row.address IS NOT NULL
MERGE (l:location {address: row.address, doc_id:row.DOC_ID})
MERGE (d:doc {doc_id:row.DOC_ID})
MERGE (l)-[r:is_in]-&gt;(d)

# Dataset 2 with home owners
LOAD CSV WITH HEADERS FROM 'file:///owners.csv' AS row
WITH row WHERE row.owner IS NOT NULL
MERGE (n:home_owner {name: row.owner, doc_id:row.DOC_ID})
MERGE (d:doc {doc_id:row.DOC_ID})
MERGE (n)-[r:is_in]-&gt;(d)
</code></pre>
<p>The addresses dataset contains address of the owner as well as other irrelevant addresses. Also the same owners may appear under different names but I know they should have the same address. What I'm ultimately trying to do is uncover <code>owner</code> entities operating under different names.</p>
<p>I can reason certain things, like an address will appear relatively few times in the dataset compared with the address of the owner, which should appear in most of the documents associated with that owner. So I wrote this query below that would help sort out some owner addresses. But I am getting 0 results from this query.</p>
<pre><code>MATCH(n:home_owner-[f:is_in]-&gt;(d:doc)&lt;-[r:is_in]-(l:location)
WITH l, n, count(r) as rct
WHERE rct &gt; 20
RETURN l, n
</code></pre>
<p>I am wondering if this is a problem with my queries, or if my logic is incorrect. Or is there a better way to solve this problem in neo4j? Any help/insights would be greatly appreciated.</p>
",16799744.0,16799744.0,2022-09-02 14:15:31,2022-09-02 14:15:31,Cypher query to reveal same entity through relationships,<neo4j><graph-databases><graph-data-science>,1,1,N/A,CC BY-SA 4.0
73592674,1,-1.0,2022-09-03 13:39:11,-1,50,"<p><a href=""https://i.stack.imgur.com/GEVsz.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>How do I convert this to a panadas dataframe. I get the error code:(ValueError: Must pass 2-d input. shape=(1, 37, 5)) when I used pd.DataFrame() command.</p>
",17529189.0,-1.0,N/A,2022-09-04 08:31:43,How do i convert a .text table to pandas dataframe,<pandas><dataframe><jupyter-notebook><data-science>,1,2,N/A,CC BY-SA 4.0
73595548,1,73596135.0,2022-09-03 21:37:21,0,131,"<p><strong>Guide</strong>:
<a href=""https://theoehrly.github.io/Fast-F1/examples_gallery/plot_qualifying_results.html#sphx-glr-examples-gallery-plot-qualifying-results-py"" rel=""nofollow noreferrer"">https://theoehrly.github.io/Fast-F1/examples_gallery/plot_qualifying_results.html#sphx-glr-examples-gallery-plot-qualifying-results-py</a></p>
<p>I am having trouble displaying the correct value or formatted form as a matplotlib label.</p>
<p><strong>Issue:</strong> Bar Graph labels displaying as an unwanted, or badly formatted values.</p>
<p><a href=""https://i.stack.imgur.com/zqjnD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zqjnD.png"" alt=""enter image description here"" /></a></p>
<p>(Is this TimeDelta[ns] in an integer under scientific notation? The dtype is timedelta64[ns])</p>
<p><strong>Expected Values:</strong> The amount of time each driver is from the leader (s.ms) (HAM=0.038). Note: order is the same</p>
<p><a href=""https://i.stack.imgur.com/Jvo8t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jvo8t.png"" alt=""enter image description here"" /></a></p>
<p>print(times)</p>
<p><strong>Code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/python3-64

#required packages
#pip3 install fastf1
#pip3 install pandas
#pip3 install matplotlib
#pip3 install numpy

import matplotlib.pyplot as plt
import matplotlib.patches as pat
import fastf1 as ff1
import fastf1.plotting as ff1p
ff1p.setup_mpl(mpl_timedelta_support=True, color_scheme=None, misc_mpl_mods=False)
from fastf1.core import Laps
import pandas as pd
import numpy as np
from timple.timedelta import strftimedelta as td
import os
l=str.lower

def data_cache():
    cache='/ff1_temp' #temp cache
    while(True):
        warn=input(l(f'!WARNING! A data cache will be made at {cache}\n'
                     f'Formula 1 Race Data will be downloaded to {cache}\n'
                     f'Would you like to continue? [y/n]\n'))
        if(warn=='n'):
            print('Quitting!\n')
            exit(0)
        elif(warn=='y'):
            print(f'cache location: {cache}\n')
            if not os.path.exists(cache): # os.path.exists(cache)
                os.mkdir(cache) # os.mkdir(cache)
            ff1.Cache.enable_cache(cache) # Fast F1 Cache API
            break
        else:
            print('Plese Enter [y/n]\n')
            continue
    
def data_load():
    data=ff1.get_session(2021,'Netherlands','Q') #Y,L,S = Year, Location, Session
    data.load(laps=True,telemetry=False,weather=False,messages=False)
    return(data)

def data_graph():
    data=data_load()
    drivers=pd.unique(data.laps['DriverNumber'])
    fll=list()
    for row in drivers: #get fastest laps for session from each driver
       fld=data.laps.pick_driver(row).pick_fastest()
       fll.append(fld)
    fl=Laps(fll).sort_values(by='LapTime').reset_index(drop=True)
    flf=fl.pick_fastest() 
    fl['LapTimeDelta']=fl['LapTime']-flf['LapTime'] #determine the TimeDelta from leader
    tc=list()
    for index, lap in fl.iterlaps(): #team colours
        color=ff1p.team_color(lap['Team'])
        tc.append(color)
    return(fl,tc,flf)

def data_plot():
    fl,tc,flf=data_graph()
    fig,ax=plt.subplots()
    times=fl['LapTimeDelta']
    fli=fl.index
    #             y    x
    bars=ax.barh(fli,times, color=tc,edgecolor='grey')

    print(times) #expected values

    ax.set_yticks(fl.index)
    ax.set_yticklabels(fl['Driver'])
    ax.set_xlabel('Time Difference (ms)')

    #should be x axis?
    ax.bar_label(bars) #(times)

    ax.invert_yaxis()
    lt=td(flf['LapTime'], '%m:%s.%ms')
    plt.suptitle(f'2021 Dutch GP Qualifying\n'
        f&quot;Fastest  at {lt} ({flf['Driver']})&quot;)
    plt.show()

if(__name__==&quot;__main__&quot;):
    data_cache()
    data_plot()
    exit(0)
</code></pre>
<p>results of print(bars)</p>
<p><a href=""https://i.stack.imgur.com/A7XeA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A7XeA.png"" alt=""print(bars)"" /></a></p>
<p>results of print(type(times)) and print(type(bars))</p>
<p><a href=""https://i.stack.imgur.com/kSqnz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kSqnz.png"" alt=""print(type(times)) &amp;&amp; print(type(bars))"" /></a></p>
<p><strong>What has been Attempted:</strong></p>
<pre class=""lang-py prettyprint-override""><code>def data_plot():
    ax.bar_label(times)

Traceback (most recent call last):
  File &quot;\python\datacollection\fp1.ff1.graph.py&quot;, line 144, in &lt;module&gt;
    data_plot()
  File &quot;\python\datacollection\fp1.ff1.graph.py&quot;, line 132, in data_plot
    ax.bar_label(times)
  File &quot;\Python\Python310\lib\site-packages\matplotlib\axes\_axes.py&quot;, line 2609, in bar_label
    bars = container.patches
  File &quot;\Python\Python310\lib\site-packages\pandas\core\generic.py&quot;, line 5575, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Lap' object has no attribute 'patches'

---

def data_plot_label(fli,times):
    for i in range(len(fli)):
        plt.text(i,times[i],times[i],ha='center',bbox=dict(alpha=0.8))

def data_plot():
    data_plot_label(fli,times)
</code></pre>
<p><a href=""https://i.stack.imgur.com/PXzcy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PXzcy.png"" alt=""failed attempt"" /></a></p>
<p><strong>Close:</strong></p>
<p>I'm still pretty green with this stuff,</p>
<ol>
<li>Am I going about this correctly?</li>
<li>What are my options regarding labelling and matplotlib?</li>
<li>How do I set the correct formatted value for this label?</li>
</ol>
<p>I find the graph is harder to understand without the actual values on it. It has less depth.</p>
<p>Relevant Docs:</p>
<ul>
<li><a href=""https://theoehrly.github.io/Fast-F1/"" rel=""nofollow noreferrer"">https://theoehrly.github.io/Fast-F1/</a></li>
<li><a href=""https://pandas.pydata.org/docs/reference/index.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/index.html</a></li>
<li><a href=""https://matplotlib.org/stable/api/index"" rel=""nofollow noreferrer"">https://matplotlib.org/stable/api/index</a></li>
</ul>
",18257801.0,18257801.0,2022-09-04 00:47:00,2022-09-04 00:47:00,Struggling to displaying the right (formatted) value for a matplotlib labels,<python-3.x><pandas><matplotlib><data-science>,1,0,N/A,CC BY-SA 4.0
73611094,1,-1.0,2022-09-05 14:42:46,-1,45,"<p>This is my first time posting a question here and I'm kinda a newbie in R. So forgive me my errors posting it here.</p>
<p>Basicaly I have multiple excel files with meteorological data that is organized like this:</p>
<p><a href=""https://i.stack.imgur.com/03cfY.png"" rel=""nofollow noreferrer"">C1_Maio.csv</a>
OBS: I have the files C1_Maio, C1_Junho, C1_Julho (names in portuguese = May, June and July)</p>
<p>The all the files contains three different temperatures measurements and one for humidity (C1_Maio -&gt; T1, T2, T3 and H)</p>
<p>I have 4 other excel files C2_Maio; C3_Maio; C4_Maio; C5_Maio following the same logic.</p>
<p>I'm trying to join all these files in sequence like this:<a href=""https://i.stack.imgur.com/hs3nR.png"" rel=""nofollow noreferrer"">2</a></p>
<p>So, to summarize, I need to join all C1_Maio, C1_Junho and C1_Julho in one continuous excel file. I tried to write a code (again a newbie here) and I don't know what im missing... When I merge them something is going on that it is messing with the data inside the csv file. Here is the code bellow with the descriptions:</p>
<pre><code>    # Packages

#install.packages(openxlsx)
#install.packages(tidyverse)

# Libraries

library(&quot;openxlsx&quot;)
library(&quot;tidyverse&quot;)

# Insertion of data from the Maio de Cadiretas sensors

C_1_Maio &lt;- read.csv(&quot;D:/Carreira Acadêmica/Doutorado - UB/Projetos/HoliSoils/Areas de Estudo/Cadiretes/Dados/Sensores/2022-05-11/C-1.csv&quot;, header = FALSE, sep = &quot;;&quot;)


# Insertion of data from the Junho de Cadiretas sensors

C_1_Junho &lt;- read.csv(&quot;D:/Carreira Acadêmica/Doutorado - UB/Projetos/HoliSoils/Areas de Estudo/Cadiretes/Dados/Sensores/2022-06-29/C-1.csv&quot;, header = FALSE, sep = &quot;;&quot;)


# Joining dataframes from different Cadiretas dates

C1_cad &lt;- rbind(C_1_Maio, C_1_Junho)

#Here I need to find a way to join these two excel files based on datetime column. That is, these two files have a column that R calls it V2 that has the following format: 2021.07.02 07:45

#In this way, it would specify that where the observations of the May file end, those of June will begin.

#The problem is that these files are automatically generated by a program, so in the June file there may be observations that were measured in May, and it seems to me that due to errors in the program the measurements may be different. As in the following generic example:

#EX:
#                 A          B        C   D           E       F            G     H     I
#C1_Maio:   14   2021.07.02 11:00     4  23,1875     23,375   23,1875     461    202    0
#C1_Junho:  50   2021.07.02 11:00     4  24,7500     22,375   22,1975     461    202    0

#In short, I needed the data to follow the order established by the date and time of observation and if there are duplicate data for the same date and time, they should appear repeated without one replacing the other.



# Extracting the joined data to Excel in separate columns Cadiretes

cad_T1_C1 &lt;- C1_cad[, c(&quot;V2&quot;,&quot;V4&quot;), drop = FALSE]

cad_T2_C1 &lt;- C1_cad[, c(&quot;V2&quot;,&quot;V5&quot;), drop = FALSE]

cad_T3_C1 &lt;- C1_cad[, c(&quot;V2&quot;,&quot;V6&quot;), drop = FALSE]

cad_H_C1 &lt;- C1_cad[, c(&quot;V2&quot;,&quot;V7&quot;), drop = FALSE]



C1_TM_Cadiretes &lt;- list('T1' = cad_T1_C1, 'T2' = cad_T2_C1, 'T3' = cad_T3_C1, 'Moisture' = cad_H_C1)
write.xlsx(C1_TM_Cadiretes,&quot;C:\\Users\\Lenovo\\Desktop\\Holisoils data\\C1_TM_Cadiretes.xlsx&quot;)


# In this way what you would need is the Excel file generated to contain the data of &quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot; and &quot;Humidity&quot; in separate sheets in the same CSV or Excel file
</code></pre>
",17253396.0,-1.0,N/A,2022-09-07 05:58:09,How to join multiple meteorological data excel files in one single dataframe in R?,<r><data-science>,1,3,N/A,CC BY-SA 4.0
73608360,1,-1.0,2022-09-05 11:05:16,-1,568,"<p>The following is the code I have written to find the null values for each row in the dataset. I am unable to find a way to get the sum of null values for a particular type of row in the first column of the dataset. The first column is &quot;name&quot;.</p>
<p>I am asking for help to find the sum of null values for each kind of name.
Thank you</p>
<pre><code>for i in range(len(requiredCSV.index)):
    print(&quot;Nulls in row &quot;, i , &quot;:&quot; ,  requiredCSV.iloc[i].isnull().sum())
    
</code></pre>
",11913914.0,-1.0,N/A,2022-09-05 13:29:52,How to count NaN or missing values in Pandas DataFrame at a particular row level?,<python><pandas><dataframe><jupyter-notebook><data-science>,1,0,N/A,CC BY-SA 4.0
73615372,1,73615586.0,2022-09-05 23:15:04,0,68,"<p>In a data, frame match two-column and if any value from the second column is available in the first column, remove value from the second columns</p>
<pre><code>col1 col2
1   
2     1
3     9
4
5     1
6     2
</code></pre>
<p>Output</p>
<pre><code>col1 col2
1
2
3    9
4
5
6
</code></pre>
<p>Here, 1 and 2 from col2 are available in col1.
So, this repeated data should be removed</p>
",18591660.0,18591660.0,2022-09-05 23:56:19,2022-09-06 05:03:24,"data frame, match two columns and remove repeated value from second columns if that exist in first columns, row number or id is not matter",<python><pandas><dataframe><data-science>,2,2,N/A,CC BY-SA 4.0
73620626,1,73620750.0,2022-09-06 10:39:04,0,28,"<p>I'm new to R and I'm stuck. I'm working on a health dataset with each row as one patient's information.</p>
<p>I have a variable called diag_codes. It has the patient's medical condition in the form of a diagnostic code/number. I want to group the individual condition codes into broader categories (heart disease, resp disease, liver disease) and make that a new variable.</p>
<p>E.g. I know that 1,2,3,4,84 are all respiratory diseases. I also know that 5, 6, 7, 32, 56 are all cardiovascular diseases. I want to create a new variable called diagnosis.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">diag_code</th>
<th style=""text-align: left;"">diagnosis</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">&quot;resp disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">&quot;resp disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">56</td>
<td style=""text-align: left;"">&quot;CVD disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: left;"">&quot;resp disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: left;"">&quot;resp disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">84</td>
<td style=""text-align: left;"">&quot;resp disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td style=""text-align: left;"">&quot;CVD disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">6</td>
<td style=""text-align: left;"">&quot;CVD disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">7</td>
<td style=""text-align: left;"">&quot;CVD disease&quot;</td>
</tr>
<tr>
<td style=""text-align: left;"">32</td>
<td style=""text-align: left;"">&quot;CVD disease&quot;</td>
</tr>
</tbody>
</table>
</div>
<p>I have tried to use case_when() and mutate(), or ifelse() and mutate(), but they usually involve a single true or false condition.</p>
<p>I want to be able to do something like this (I know this is incorrect):</p>
<pre><code>data &lt;- data %&gt;%
mutate(diagnosis = case_when(diag_code==c(1,2,3,5,84)) ~ &quot;Resp disease&quot;,
                   case_when(diag_code==c(5,6,7,32,56)) ~ &quot;CVD disease&quot;, 
                   TRUE ~ &quot;Unknown)
</code></pre>
",19761440.0,19761440.0,2022-09-06 10:40:52,2022-09-06 10:49:23,How to group individual values from an existing variable into a new variable in R,<r><data-science>,1,1,N/A,CC BY-SA 4.0
73621159,1,-1.0,2022-09-06 11:19:39,0,96,"<p>I'm doing a Social Network Analysis of <a href=""https://github.com/iainrj/SimpsonsNetwork"" rel=""nofollow noreferrer"">this dataset</a> with NetworkX and I want to make a degree and closeness centrality analysis.</p>
<p>The graph I obtain is undirected (graph.is_directed() returns false), and I have node 1 with degree 593 but it has 0 as target and weight on the edges csv. The graph is undirected so I expect node 1 to be the central node but it's not and I don't get why (the dataset is based on the animated series The Simpson so I bet I know who is the most central character).</p>
<p>I'm afraid the analysis ends up unreliable this way.</p>
<p>---edit</p>
<p>This is the code where I import and create the graph.</p>
<pre><code>dfN=pd.read_csv('gdrive/My Drive/SNA/simpsonsNodes.csv')
dfE=pd.read_csv('gdrive/My Drive/SNA/simpsonsEdges.csv')
df = pd.merge(left=dfN, right=dfE, left_on=&quot;Id&quot;, right_on='Source', how='outer').drop(['Id', 'Type'], axis=1)
df.columns = ['Name', 'Source', 'Target', 'Weight']
df = df.fillna(0)
df = df.astype({'Source':'int', 'Target':'int', 'Weight':'int'})
df

graph = nx.from_pandas_edgelist(df, 'Source', 'Target', edge_attr='Weight', create_using=nx.Graph() )
print(graph.is_directed())
</code></pre>
<p>I dropped two columns: id because is redundant and Type because it's &quot;Undirected&quot; for every row so I don't really need it.</p>
<p>I used df = df.fillna(0) because node with id = 1 had source, target and weight as NaN so I converted it and used df.loc[0,&quot;Source&quot;]=1 to insert 1 as its source.</p>
",14467038.0,14467038.0,2022-09-06 20:26:40,2022-09-06 22:20:31,Centrality Analysis of undirected graph,<graph><data-science><networkx><social-networking><sna>,1,9,N/A,CC BY-SA 4.0
73627993,1,73635767.0,2022-09-06 21:40:23,2,233,"<p>I am trying to create a graph using altair in Python and I am getting the error code <strong>&quot;AttributeError: 'tuple' object has no attribute 'configure_title&quot;</strong>
I have tried but search for similar problems but it seems like there not many resources.</p>
<p>I have created a graph similar to what I want to accomplish but for some reason I am getting this problem.</p>
<p>This is my data</p>
<pre><code>chicago = {'Year': [2022, 2023, 2024, 2025, 2026, 2022, 2023, 2024, 2025, 2026],
        'Level': ['Low', 'Low', 'Low', 'Low', 'Low', 'Medium','Medium','Medium','Medium','Medium'],
        'Leads': [1795, 3590, 5388, 7184, 8980, 2154,4308,6462,8616,10770],
        'Gross Profit': [131475, 262950, 394425, 525900, 657375, 94845,189690,284535,379380,474225]}

df3 = pd.DataFrame(chicago)
df3
</code></pre>
<p>My Attempt:</p>
<pre><code>base = alt.Chart(df3).encode(
x=alt.X('Year:O', title= 'Year',axis=alt.Axis(labelAngle=325))
)
#Add line
line = base.mark_line(color='#55F546').encode(
y=alt.Y('Gross Profit',title='Gross Profit', axis=alt.Axis(grid=True)),
strokeDash ='Level',
)
#Add background
bar = base.mark_bar(color='red').encode(
y='Leads',
color = alt.Color('Level', scale=alt.Scale(scheme = 'set1')),
)

#configure graph (size &amp; colors)
de = (bar + line ).resolve_scale(y='independent').properties(title= '$100 Vs $120'),
d= de.configure_title(fontSize=14).configure(background='#888888')
d.configure_axisLeft(
labelColor='white',
titleColor='white',
labelFontSize=15,
titleFontSize=15
).configure_axisRight(
labelColor='#55F546',
titleColor='#55F546',
labelFontSize=15,
titleFontSize=15
).configure_axisBottom(
labelColor='white',
titleColor='white',
labelFontSize=13,
titleFontSize=13
).configure_legend(
labelColor='white',
titleColor='white',
labelFontSize=16,
titleFontSize=16
)
</code></pre>
",15188629.0,-1.0,N/A,2022-09-07 12:45:01,AttributeError: 'tuple' object has no attribute python-altair,<python><data-science><altair>,2,0,N/A,CC BY-SA 4.0
73634601,1,-1.0,2022-09-07 11:20:16,0,99,"<p>We have requirement where we need aggregate the data over two different keys. Input record for the job is as below.</p>
<pre><code>{
   .....
   .....
   &quot;key1&quot; : &quot;093570lells99345dfklsfkd&quot;,
   &quot;key2&quot; : &quot;8587656783487535nxdghljd&quot;,
   .....
   .....
}
</code></pre>
<p>The records are stored in couchbase. Now Flink data pipeline would look like.</p>
<pre><code>env.addSource(readFromCouchBase...)
  .name(&quot;couchbase-flinkjob&quot;)
  .assignTimestampsAndWatermarks(
    new TimestampExtractorAndWatermarkEmitter(60 * 1000, false))
  .keyBy[(String)](key1)
  .timeWindow(Time.seconds(10))
  .aggregate(new CountGroupFunctionWithEventTimeProcessing, new CountGroupWindowFunction)
  .addSink(new IdempotentPostgresSqlSinkFunction).name(&quot;postgres-sink&quot;)
</code></pre>
<p>Now we need grouping to happen on key2 as well. For this do we need to create new pipeline or is there a way such that we can modify the current pipeline to facilitate groupings to happen separately on key1 and key2? Efficient way would be to use the current pipeline itself to get the required groupings - as this would avoid reading from the couchbase twice.</p>
",3553913.0,1606632.0,2022-09-07 11:24:50,2022-09-07 20:01:02,How to aggregate on different keys in single Flink pipeline,<data-science><apache-flink>,1,0,N/A,CC BY-SA 4.0
73634847,1,73634922.0,2022-09-07 11:38:56,2,38,"<p>I've got a list of over 500k people. The data looks like the first table. I'd like to use the admission date from the first table and if the admission date of the same person in the second table is within 30 days of their admission date in the first table then I'd like to store that overlapping record in the third table. The example of what I'd like is below. Is there a faster way to do this than using iterrows using the person_ids and dates from the first table and checking every row in the second table?</p>
<pre><code>Table 1
| person_id | admission_date | value |
|      1234 |     2017-01-31 |     6 |
|      5678 |     2018-03-20 |    12 |
|      9101 |     2017-02-22 |    11 |
|      1234 |     2020-10-31 |    19 |
|      5678 |     2019-06-16 |    21 |
|      9101 |     2021-12-14 |     8 |

Table 2
| person_id | admission_date | value |
|      1234 |     2015-01-31 |    10 |
|      1234 |     2017-02-12 |   152 |
|      5678 |     2017-01-31 |    10 |
|      5678 |     2018-04-10 |    10 |
|      9101 |     2017-02-25 |    99 |
|      9101 |     2017-03-01 |    10 |
|      1234 |     2012-12-31 |    10 |
|      5678 |     2019-07-10 |    11 |
|      9101 |     2017-01-31 |    10 |


Table 3
| person_id | admission_date | value |
|      1234 |     2017-02-12 |   152 |
|      5678 |     2018-04-10 |    10 |
|      9101 |     2017-02-25 |    99 |
|      9101 |     2017-03-01 |    10 |
|      5678 |     2019-07-10 |    11 |

</code></pre>
",8095201.0,-1.0,N/A,2022-09-07 12:01:04,Pandas selecting rows in a second dataframe based on overlapping dates in the first dataframe?,<python><pandas><data-science>,2,0,N/A,CC BY-SA 4.0
73638367,1,-1.0,2022-09-07 15:48:18,0,31,"<p>I have the following code trying to calculate a beta (<em>if priceA goes up 1, then priceY goes up X</em>) and theta (<em>each month that passes priceY goes down Z</em>).</p>
<pre><code>lmS9 = lm(priceY ~ months_past + priceA, data = S9)
summary(lmS9)
</code></pre>
<p>It prints the following:</p>
<pre><code>              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.172e+03  3.126e+02   3.750 0.000854 ***
months_past -5.620e+01  2.019e+01  -2.783 0.009703 ** 
priceA       1.959e-01  9.945e-03  19.701  &lt; 2e-16 ***
</code></pre>
<p>Both are significant input variables, but I'm not sure if I'm interpreting it correctly.</p>
<p>In the example above <code>months_past</code> would be theta and <code>priceA</code> would be a form of beta. Is that correct? How can I create a coefficient that says if <code>priceA</code> goes up 1%, the <code>priceY</code> goes up Z?</p>
",1052827.0,-1.0,N/A,2022-09-07 15:48:18,How to calculate a financial beta and theta (time decay) in R linear regression?,<r><statistics><data-science><linear-regression><finance>,0,2,N/A,CC BY-SA 4.0
73640609,1,-1.0,2022-09-07 19:29:36,1,59,"<p>Say I have the following DataFrame <code>df</code>:</p>
<pre><code>time    person    attributes
----------------------------
 1        1           a
 2        2           b
 3        1           c
 4        3           d
 5        2           e
 6        1           f
 7        3           g
...      ...         ...
</code></pre>
<p>I want to write a function <code>get_latest()</code> that, when given a <code>request_time</code> and a list of persons <code>ids</code>, it will return a DataFrame containing the latest entry (row) for each person, up to the <code>request_time</code>.</p>
<p>So for instance, if I called <code>get_latest(request_time = 4.5, ids = [1, 2])</code>, then I want it to return</p>
<pre><code>time    person    attributes
----------------------------
 2        2           b
 3        1           c
</code></pre>
<p>since those are the latest entries for persons <code>1</code> and <code>2</code> up to the time <code>4.5</code>.</p>
<p>I've thought about doing a truncation of the DataFrame and then doing search from there by going up the DataFrame, but that's an okay efficiency of O(n), and I was wondering if there are functions or logic that make this a faster computation.</p>
<p>EDIT: I made this example DataFrame on the fly but it is perhaps important that I point out that the times are Python <em>datetimes</em>.</p>
",10356663.0,10356663.0,2022-09-07 19:52:36,2022-09-07 20:01:57,"How do I get the latest entries in a DataFrame up to a certain time, for a given list of column values?",<python><pandas><dataframe><data-science>,2,0,N/A,CC BY-SA 4.0
73646475,1,-1.0,2022-09-08 09:10:46,0,58,"<p>I have created a graph but the numbers are not correct when I compare the dataset to the actual graph. The y left axis which is &quot;Gross Profit&quot; does not match the dataset. I have tried to search for similar cases but did not find anything. The y right axis seems to match the information I have in my dataset</p>
<p>data:</p>
<pre><code>chica = {'Year': [2022, 2023, 2024, 2025, 2026, 
                  2022, 2023, 2024, 2025, 2026,
                  2022, 2023, 2024, 2025, 2026],
        'Investment': ['Low', 'Low', 'Low', 'Low', 'Low', 
                       'Medium','Medium','Medium','Medium','Medium', 
                       'High','High','High','High','High'],
        'Customers': [902, 1804, 2706, 3608, 4510, 
                      1077, 2154, 3231,4308,5385, 
                      1346, 2692, 4038, 5388, 6730],
        'Gross Profit': [131475, 262950, 394425, 525900, 657375, 
                         94845, 189690, 284535, 379380, 474225,
                         38140, 76280, 114420, 152560, 190700]}

df3 = pd.DataFrame(chica)
df3
</code></pre>
<p>attempt:</p>
<pre class=""lang-py prettyprint-override""><code>base = alt.Chart(df3).encode(
x=alt.X('Year:O', title= 'Year',axis=alt.Axis(labelAngle=325))
)
#Add line
line = base.mark_line(color='#55F546').encode(
y=alt.Y('Customers',title='Customers', axis=alt.Axis(grid=True),),
strokeDash ='Investment',
)
#Add background
bar = base.mark_bar(color='Investment').encode(
y='Gross Profit',
color = alt.Color('Investment', scale=alt.Scale(scheme = 'set1')),
)
#configure graph (size &amp; colors)
de = (bar + line ).resolve_scale(y='independent').properties(title= 'Low, Medium and High Investment')
d= de.configure_title(fontSize=14).configure(background='white')
d.configure_axisLeft(
labelColor='red',
titleColor='red',
labelFontSize=15,
titleFontSize=15
).configure_axisRight(
labelColor='blue',
titleColor='blue',
labelFontSize=15,
titleFontSize=15
).configure_axisBottom(
labelColor='black',
titleColor='black',
labelFontSize=13,
titleFontSize=13
).configure_legend(
labelColor='black',
titleColor='black',
labelFontSize=16,
titleFontSize=16
).properties(
    width=500,
    height=350
)
</code></pre>
",15188629.0,2166823.0,2022-09-12 16:04:34,2022-09-12 16:04:34,Altair wrong results,<python><data-science><altair>,1,1,N/A,CC BY-SA 4.0
73612523,1,-1.0,2022-09-05 16:51:11,-1,93,"<p>I am a pretty amateur data science student and I am working on a project where I compared two servers in a team based game but my two datasets are formatted differently from one another. One column for instance would be first blood, where one set of data stores this information as &quot;blue_team_first_blood&quot; and is stored as True or False where as the other stores it as just &quot;first blood&quot; and stores integers, (1 for blue team, 2 for red team, 0 for no one if applicable)</p>
<p>I feel like I can code around these difference but whats the best practice? should I take the extra step to make sure both data sets are formatted correctly or does it matter at all?</p>
",14943524.0,-1.0,N/A,2023-02-18 19:20:14,About Data Cleaning,<python><sql><pandas><data-science><data-analysis>,2,1,N/A,CC BY-SA 4.0
73636723,1,-1.0,2022-09-07 13:52:42,1,79,"<p>I have a very simple custom loss function that basically does mae*=2 if the predictions are smaller than true value else returns mae. Now I am training my model in an sklearn pipeline and I want to deepcopy the pipeline along with the model and custom objects as following:</p>
<pre><code>def custom_loss(y_true, y_pred):
        mae = tf.keras.losses.MeanAbsoluteError()
        penalty = 2
        # penalize the loss heavily if the prediction is smaller than true
        loss = tf.where(
            condition=tf.greater(y_true, y_pred),
            x=mae(y_true, y_pred) * penalty,
            y=mae(y_true, y_pred)
        )
        return loss
</code></pre>
<pre><code>regr = deepcopy(regr)
temp = RegressionRecords([], regr, r2_score(np.array(predict_df[&quot;true_data&quot;]), np.array(predict_df[&quot;predictions&quot;])), predict_df, None)
</code></pre>
<pre><code>class PredictionTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, estimator):
        self.estimator = estimator # Keras model passed in as estimator
    @property
    def history(self):
        return self.estimator.history

    @property
    def model(self):
        return self.estimator.model

    def fit(self, X, y):
        self.estimator.train(X, y)

    def predict(self, X):
        return self.estimator.transform(X)

</code></pre>
<p>But I get the following error:</p>
<pre><code>File &quot;/usr/lib/python3.8/copy.py&quot;, line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 270, in _reconstruct
    state = deepcopy(state, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 205, in _deepcopy_list
    append(deepcopy(a, memo))
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 210, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 210, in &lt;listcomp&gt;
    y = [deepcopy(a, memo) for a in x]
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 270, in _reconstruct
    state = deepcopy(state, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 270, in _reconstruct
    state = deepcopy(state, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 146, in deepcopy
    y = copier(x, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File &quot;/usr/lib/python3.8/copy.py&quot;, line 153, in deepcopy
    y = copier(memo)
  File &quot;/usr/local/lib/python3.8/dist-packages/keras/engine/training.py&quot;, line 337, in __deepcopy__
    new = pickle_utils.deserialize_model_from_bytecode(
  File &quot;/usr/local/lib/python3.8/dist-packages/keras/saving/pickle_utils.py&quot;, line 48, in deserialize_model_from_bytecode
    model = save_module.load_model(temp_dir)
  File &quot;/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/load.py&quot;, line 994, in revive_custom_object
    raise ValueError(
ValueError: Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`
</code></pre>
<p>I need to use sklearn pipeline because I am doing some other operations in the pipeline before running the final step which is the Keras model. I know what the error means but I can't find an easy way to fix it.
Can anybody please help?</p>
",16037571.0,-1.0,N/A,2022-09-07 13:52:42,Deepcopy keras model with custom loss function (custom objects),<python><tensorflow><keras><deep-learning><data-science>,0,0,N/A,CC BY-SA 4.0
73662972,1,-1.0,2022-09-09 13:32:00,0,342,"<p>I created a model in R for my df as</p>
<pre><code>fit &lt;- randomForest(y ~ x1 + 
                     x2 + x3 + 
                     x4+ x5+x6+ x7+x8, 
                     data = data_train,ntree=35,
                     keep.forest=FALSE, importance=TRUE)
</code></pre>
<p>gives result as</p>
<pre><code> Call:
  randomForest(formula = y ~ x1 + x2 
 +      x3 + x4 + x5 + 
   x6 +      x7 + x8, data = 
data_train, ntree = 35,      keep.forest = FALSE, importance = TRUE) 
           Type of random forest: regression
                 Number of trees: 35
   No. of variables tried at each split: 2

        Mean of squared residuals: 2901510
                % Var explained: 53.45
</code></pre>
<p>but while I am predicting using</p>
<pre><code>p &lt;- predict(data_test, fit, type='prob')
</code></pre>
<p>Shows error</p>
<pre><code>Error in ets(object, lambda = lambda, biasadj = biasadj, 
allow.multiplicative.trend = allow.multiplicative.trend,  : 
 y should be a univariate time series
</code></pre>
<p>When I am using</p>
<pre><code>predict(fit, newdata= data_test)
</code></pre>
<p>Showing error</p>
<pre><code>Error in predict.randomForest(fit, newdata = data_test) : 
 No forest component in the object
</code></pre>
<p>how can I solve this.. I am new in using RandomForest in R</p>
",16384908.0,404970.0,2022-09-10 05:32:27,2022-09-10 05:32:27,Prediction using Random Forest in R,<r><machine-learning><data-science><random-forest>,1,3,N/A,CC BY-SA 4.0
73668529,1,-1.0,2022-09-10 00:29:21,-1,33,"<p>I have this college project with a good focus on the frontend, but I'm struggling with a SQL query (PostgreSQL) that needs to be executed at one of the backend endpoints.</p>
<p>The table I'm speaking of is the following:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>todo_id</th>
<th>column_id</th>
<th>time_in_status</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>259190</td>
<td>3</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>259190</td>
<td>10300</td>
<td>30</td>
</tr>
<tr>
<td>2</td>
<td>259190</td>
<td>10001</td>
<td>60</td>
</tr>
<tr>
<td>3</td>
<td>259190</td>
<td>10600</td>
<td>90</td>
</tr>
<tr>
<td>4</td>
<td>259190</td>
<td>6</td>
<td>30</td>
</tr>
</tbody>
</table>
</div>
<p>A good way to simplify what it is, is saying it's a to-do organizer by vertical columns where each column would be represented by its <code>column_id</code>, and each row is task column change event.</p>
<p>With all that said what I need to get the job done is to generate a view (or another suggested better way) from this table that will show how long each task spent on each <code>column_id</code>. Also for a certain <code>todo_id</code>, <code>column_id</code> is not unique, so that could be multiple events on column 10300 and the table below would group by it and sum them</p>
<p>For example, the table above would output a view like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>todo_id</th>
<th>time_in_column_3</th>
<th>time_in_column_10300</th>
<th>time_in_column_10001</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>259190</td>
<td>0</td>
<td>30</td>
<td>60</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>",10762158.0,-1.0,N/A,2022-09-10 02:05:43,SQL Group by joining with time difference,<sql><postgresql><data-science>,1,0,N/A,CC BY-SA 4.0
73536690,1,-1.0,2022-08-30 03:01:51,-1,19,"<p>I created a python class called 'loan' which has a method that produces a pandas data frame of principal and interest payment along with current balance. I now want to create another method which produces the same data frame with additional columns in the same class without copying and pasting the same code from the other method. How can I do that?</p>
",19834580.0,-1.0,N/A,2022-08-30 03:08:38,How do I modify a pandas dataframe I created in one method of a class in another method of the same class?,<python><pandas><dataframe><data-science><data-analysis>,1,2,N/A,CC BY-SA 4.0
73569197,1,-1.0,2022-09-01 12:15:43,1,204,"<p>I want to recreate <code>catboost.utils.select_threshold</code>(<a href=""https://catboost.ai/en/docs/concepts/python-reference_utils_get_roc_curve"" rel=""nofollow noreferrer"">desc</a>) method for <code>CalibratedClassifierCV</code> model.</p>
<p>In Catboost I can select desired fpr value, <strong>to return the boundary at which the given FPR value is reached</strong>.</p>
<p>My goal is to the same logic after computing fpr, tpr and boundaries from <code>sklearn.metrics.roc_curve</code></p>
<p>I have the following code</p>
<pre><code>prob_pred = model.predict_proba(X[features_list])[:, 1]
            
fpr, tpr, thresholds = metrics.roc_curve(X['target'], prob_pred)

optimal_idx = np.argmax(tpr - fpr) # here I need to use FPR=0.1
boundary = thresholds[optimal_idx]
 
binary_pred = [1 if i &gt;= boundary else 0 for i in prob_pred]
</code></pre>
<p>I guess it should be simple formula but I am not sure how to place 0.1 value here to adjust threshold.</p>
",9137206.0,9137206.0,2022-09-01 13:11:58,2022-09-02 10:39:52,Select threshold for binary classification using desired fpr value,<python><scikit-learn><data-science><classification><catboost>,1,0,N/A,CC BY-SA 4.0
73575439,1,-1.0,2022-09-01 21:09:41,0,193,"<p>I am trying to clean my data, so I want to handle outliers.</p>
<p>I was able to get the index of each outlier value, but my data contained too many outliers, so I don't want to remove them or replace them with one value &quot;the mean for instance&quot;.</p>
<p>I come up with this function, but it seems to have a problem that I can't discover:</p>
<pre class=""lang-py prettyprint-override""><code>def Handle_outliers(df,feature):
    R = df[feature].drop(Outliers(df,feature)).unique()
    df[feature] = df[feature].replace(
        Outliers(df,feature),
        pd.Series( np.random.choice(R, size=len(Outliers(df,feature))) )
        )

    return df[feature]
</code></pre>
<ul>
<li><code>R</code> is a list that should contain all the feature values except for the outliers !!</li>
<li>and then I want to replace each index that contains an outlier with a random choice from <code>R</code>.</li>
<li><code>Outlier</code> is a function that detects the outliers:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>def Outliers(df,feature):
    Q3=df[feature].quantile(0.72)
    Q1=df[feature].quantile(0.28)
    IQR= Q3-Q1
    Lower=Q1-(1.5*IQR)
    Upper=Q3+(1.5*IQR)
    OutLiers_list=df.index[(df[feature]&gt; Upper) | (df[feature] &lt; Lower)]
    return OutLiers_list
</code></pre>
<p>Would anyone please tell me how to fix this issue?</p>
",14769444.0,14627505.0,2022-09-01 21:20:31,2022-09-01 21:28:45,replace outliers with multiple values,<python><pandas><numpy><replace><data-science>,1,0,N/A,CC BY-SA 4.0
73652259,1,-1.0,2022-09-08 16:20:15,0,24,"<p>I am trying to sort values descendingly for:-</p>
<pre><code>FeatureImportances=pd.DataFrame(Tclf.feature_importances_,index=Name_features).sort_values(by='',axis = 0, ascending = False)
</code></pre>
<p>whenever I try to assign a feature&quot;column&quot; Name to the attribute 'by', it gives me an error
So, what to assign for the attribute 'by'!!</p>
",14769444.0,10836309.0,2022-09-08 17:11:19,2022-09-08 17:11:19,Sort_values for know created DF,<python><sorting><data-science><decision-tree>,0,2,N/A,CC BY-SA 4.0
73676886,1,-1.0,2022-09-11 04:47:09,2,103,"<p>I have following data frame of datetime stamp and values.</p>
<pre><code>date        Value                
2022-07-19 44.43000000
2022-07-20 43.43000000
2022-07-21 42.43000000
2022-07-22 41.43000000
2022-07-25 41.43000000

... ...
2022-09-02  86.40000000
2022-09-06  85.13000000
2022-09-07  86.86000000
2022-09-08  88.44000000
2022-09-09  89.44000000
</code></pre>
<p>What would be efficient way to code in python to get this outlier?</p>
",19968062.0,19968062.0,2022-09-13 18:57:04,2022-09-13 18:57:04,Find outlies in timeseries date,<python><pandas><dataframe><data-science><outliers>,1,4,N/A,CC BY-SA 4.0
73661570,1,-1.0,2022-09-09 11:38:17,0,482,"<p>I want to replace nan in html to blank space but when I'm using s.replace I'm getting</p>
<blockquote>
<p>AttributeError: 'Styler' object has no attribute 'replace'</p>
</blockquote>
<p>This is what I was trying</p>
<pre><code>s=b.style.apply(lambda x: [&quot;background-color: green&quot;]*n if x['MessageType']== '[43503]' else [&quot;background-color: white&quot;]*n, axis = 1)

s= s.replace(r'^\s*$', np.nan, regex=True)
s.to_html(&quot;SORTED.html&quot;)
</code></pre>
",14876672.0,10315163.0,2022-09-09 11:41:57,2022-09-09 11:41:57,How to make styler object to dataframe,<python><pandas><csv><data-science>,0,2,N/A,CC BY-SA 4.0
73671880,1,-1.0,2022-09-10 12:39:42,0,36,"<p>I want to do a correlation analysis between 2 dataframes in python using pandas.</p>
<p>The data in one df is recorded in 3 minute sections and the data in the other is recorded in 30 second sections. However to do this I first need to clean the data which results in some three minute values consisting of less than 6 values or not existing at all. And than create an average value to compare it with the other df.</p>
<p>How can I average the 30 seconds of data into three minutes and compare it to the other df?</p>
<pre class=""lang-none prettyprint-override""><code>          Day      Time Senderrate     S-Unit Retransmissions Receiverrate     R-Unit
0  18.08.2019  11:42:05       1087  Mbits/sec               0         1087  Mbits/sec
1  18.08.2019  11:45:06       1141  Mbits/sec               0         1141  Mbits/sec
2  18.08.2019  11:48:06       1049  Mbits/sec               0         1049  Mbits/sec
...
</code></pre>
<p>And the other:</p>
<pre class=""lang-none prettyprint-override""><code>                     Time  Alarm Sichtweite  Error Code  Sichtweite (m) 1min average  ...  Rain (mm) cumulated Snow (mm) cummulated  Temp (C°)  Luminance (cd/m²)
0     2019-06-12 00:00:20                 0           0                        20000  ...                 0.29                    0       23.6                  4
1     2019-06-12 00:01:20                 0           0                        20000  ...                 0.29                    0       23.6                  4
3     2019-06-12 00:01:50                 0           0                        20000  ...                 0.29                    0       23.6                  4
...
</code></pre>
<p>So far I came across no solution, so any help would be greatly appreciated</p>
",19964476.0,14909621.0,2022-09-10 13:48:18,2022-09-10 13:48:18,How to average the data from 6 or less rows in a dataframe into one row?,<python><pandas><dataframe><data-science>,0,2,N/A,CC BY-SA 4.0
73672354,1,-1.0,2022-09-10 13:54:33,0,251,"<p>I have a dataset with 150+ features, and I want to separate them as text, categories and numerics. The categorical and text variables are having the Object data type. How do we distinguish between a categorical and text variable? Is there any threshold value for the categorical variable?</p>
",16779098.0,16779098.0,2022-09-11 01:28:50,2022-09-14 14:38:13,identifying categorical variables in a dataset,<python-3.x><data-science><data-cleaning><feature-selection><feature-engineering>,1,1,N/A,CC BY-SA 4.0
73676022,1,73678848.0,2022-09-11 00:21:15,0,89,"<p>I have on specific issue that I cannot find a solution for, and I have looked at many of the articles here and other locations.  I have successfully parsed a set of json files into a dataframe for the purpose of subsequent data analysis and cleanup tasks- the problem I am running into is that the parsing did not work for deeply nested json elements.</p>
<p>Here is the beginning of what each file generally looks like:</p>
<pre><code>{
  &quot;job_id&quot;: &quot;0a0440fc-a651-4738-933e-51b5d1654831&quot;,
  &quot;provider&quot;: &quot;provider&quot;,
  &quot;provider_version&quot;: &quot;2.0.7&quot;,
  &quot;timestamp&quot;: &quot;2022-08-18T14:03:43.054532&quot;,
  &quot;language&quot;: &quot;en-US&quot;,
  &quot;transcription&quot;: {
    &quot;confidence&quot;: {
      &quot;overall&quot;: 0.64,
      &quot;channels&quot;: [
        0.64
      ]
    },
    &quot;segments&quot;: [
      {
        &quot;text&quot;: &quot;welcome to provider to continue in english please press one but i contin into mercado&quot;,
        &quot;formatted&quot;: &quot;Welcome to provider, to continue in English, please press one but I contin into mercado.&quot;,
        &quot;confidence&quot;: 0.4252,
        &quot;channel&quot;: 0,
        &quot;raw_data&quot;: [
          {
            &quot;word&quot;: &quot;welcome&quot;,
            &quot;confidence&quot;: 0.4252,
            &quot;start_ms&quot;: 400,
            &quot;end_ms&quot;: 1120
           } # and each word receives this set of data including the &quot;word&quot;, &quot;confidence&quot;, &quot;start_ms&quot; (of the word) and &quot;end_ms&quot; of the word
          ],
        &quot;start_ms&quot;: 400, # this is the start time of the full 
       utterance
        &quot;end_ms&quot;: 6920 # this is the end time of the full utterance
      }, # and this is where the new json element starts  with &quot;text&quot; and &quot;formatted&quot; 
</code></pre>
<p>For reference, we have a few root elements that I do not care about, I only care about the root element &quot;job_id&quot;.  I also only want the nested elements with &quot;transcription&quot;:</p>
<p><code>transcriptions--&gt;segments--&gt;text</code><br />
<code>transcriptions--&gt;segments--&gt;formatted</code></p>
<p>I do NOT want the sisters of &quot;text&quot; and &quot;formatted&quot; which include &quot;confidence&quot;, &quot;channel&quot; and &quot;raw_data&quot; (this one being another list of dictionaries).</p>
<p>My code for parsing each file is below:</p>
<pre><code>json_dir = 'provider_care_json'
json_pattern = os.path.join(json_dir, '*.json')
file_list = glob.glob(json_pattern)
dfs = []
for file in file_list:
    with open(file) as f:
        json_data = pd.json_normalize(json.loads(f.read()))
        json_data['job_id'] = file.rsplit(&quot;/&quot;, 1)[-1] 
    dfs.append(json_data)
vsdf = pd.concat(dfs)

</code></pre>
<p>So far so good - I end up with this structure:</p>
<pre><code>vsdf.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 50 entries, 0 to 0
Data columns (total 8 columns):
 #   Column                             Non-Null Count  Dtype  
---  ------                             --------------  -----  
 0   job_id                             50 non-null     object 
 1   provider                           50 non-null     object 
 2   provider_version                   50 non-null     object 
 3   timestamp                          50 non-null     object 
 4   language                           50 non-null     object 
 5   transcription.confidence.overall   50 non-null     float64
 6   transcription.confidence.channels  50 non-null     object 
 7   transcription.segments             50 non-null     object 
dtypes: float64(1), object(7)
memory usage: 3.5+ KB
</code></pre>
<p>I drop the columns I do not need and end up with a dataframe that only has three columns, including my job_id (which is my document number):</p>
<pre><code> #   Column                  Non-Null Count  Dtype 
---  ------                  --------------  ----- 
 0   job_id                  100 non-null    object
 1   timestamp               100 non-null    object
 2   transcription.segments  100 non-null    object
dtypes: object(3)
</code></pre>
<p>The issue now is that the &quot;transcription.segments&quot; still has structure where it is a list with multiple embedded dictionaries, including the &quot;confidence&quot;, &quot;channel and &quot;raw_data&quot; dictionary sets.</p>
<p>Ultimately, aside from the &quot;job_id&quot; (for the document reference), the &quot;text&quot; and &quot;formatted&quot; elements are really the only two elements that I care about.</p>
<p>I have tried parsing the json individually for those two elements, but I lose the connection to the <code>job_id</code>.  What would be the best way to further split the <code>transcription.segments</code> into something like <code>transcription.segments.text</code> and <code>transcription.segments.formatted</code>  columns?</p>
",16961408.0,16961408.0,2022-09-11 00:40:36,2022-09-11 11:22:55,JSON data parsing into pandas dataframe,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
73676566,1,73676691.0,2022-09-11 03:15:42,1,134,"<p>We need to return the list of dates for n consecutive business days ( Friday to Monday is 1 business day ). where values are not changed. Do not assume that dates column have every single dates.
Data frame structure would be as given below</p>
<pre><code>date        Value                
2022-07-19 44.43000000
2022-07-20 44.43000000
2022-07-21 44.43000000
2022-07-22 44.43000000
2022-07-25 44.43000000

... ...
2022-09-02  86.40000000
2022-09-06  85.13000000
2022-09-07  86.86000000
2022-09-08  88.44000000
2022-09-09  89.44000000
</code></pre>
<p>If we assume n is 5. We need to return list of 5 consecutive dates. For above examples answer would be</p>
<pre class=""lang-none prettyprint-override""><code>[2022-07-22,2022-07-20,2022-07-21,2022-07-22,2022-07-25]
</code></pre>
<p>I tried below code to get consecutive dates present in data frame but I am unable to get consecutive business days.</p>
<pre><code>for k, v in px_dirty.groupby((px_dirty['value'].shift() != px_dirty['value']).cumsum()):

if len(v) == 5:
    print(f'[group {k}]')
    print(v)
</code></pre>
<p>I am not able to figure out how to get consecutive business days.</p>
",19968062.0,19968062.0,2022-09-11 03:41:28,2022-09-11 03:59:30,Find n consecutive business date where values are equal in Panadas data frames,<python><pandas><dataframe><group-by><data-science>,2,2,N/A,CC BY-SA 4.0
73576037,1,-1.0,2022-09-01 22:33:55,1,98,"<p>I have a method which I want to map a date column to a new column season but it only maps spring. I have defined the season using a dictionary where keys are season names and values as date ranges. I do not know why it is only returning only one season as I have defined the dates for each season. Here is the code for the function</p>
<pre><code>def do_season_on_date(date):
    year = str(date.year)
    seasons = {'spring': pd.date_range(start='01/09/'+year, end='30/11/'+year),
               'summer': pd.date_range(start='01/12/'+year, end='28/02/'+year),
               'autumn': pd.date_range(start='01/03/'+year, end='31/05/'+year)}
    if date in seasons['spring']:
      return 'spring'
    elif date in seasons['summer']:
      return 'summer'
    elif date in seasons['autumn']:
      return 'autumn'
    else:
     return 'winter'
</code></pre>
<p>Here is the output</p>
<pre><code> date       ndvi        seasons
2000-02-29  0.331070    spring
2000-03-31  0.326608    spring
2000-04-30  0.300348    spring
2000-05-31  0.251368    spring
2000-06-30  0.216910    spring
2020-07-31  0.205169    spring
2020-08-31  0.198418    spring
2020-09-30  0.192516    spring
2020-10-31  0.201836    spring
2020-11-30  0.210474    spring
</code></pre>
<p>This how I map date to seasons</p>
<pre><code>df_monthly['seasons'] = df_monthly.date.map(do_season_on_date)
</code></pre>
",15869559.0,-1.0,2022-09-02 06:45:10,2022-09-02 06:45:10,Function returns only one season from my do_season_on_date method,<python><pandas><dataframe><data-science>,2,4,N/A,CC BY-SA 4.0
73635643,1,73635977.0,2022-09-07 12:35:51,0,62,"<p>The problem occurs when having a set of X values and Y values (41 to be exact) and I want an error bar, this works, but it creates a lot of lines instead only one.
Here is my code:</p>
<pre><code>y_values = [
    np.mean(image_data['some_parameter'][x]) 
    for x in image_data['some_parameter']
]

yerr = [
    statistics.stdev(image_data['some_parameter'][x]) 
    for x in image_data['some_parameter']
]


x_values = list(image_data['some_parameter'].keys())

plt.errorbar(x_values, y_values, yerr=yerr, capsize=5, ecolor=&quot;red&quot;)
plt.show()

</code></pre>
<p>In the end, the result looks something like this:</p>
<p><a href=""https://i.stack.imgur.com/Itrx5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Itrx5.png"" alt=""the plot that comes out"" /></a></p>
",18921083.0,-1.0,N/A,2022-09-07 17:16:04,Multiple Lines in errorbar Matplotlib,<python><matplotlib><graph><data-science>,1,0,2022-09-07 17:09:26,CC BY-SA 4.0
73679809,1,-1.0,2022-09-11 13:50:47,0,48,"<p>I am trying to implement something in Python and I am stuck.</p>
<p>I have 2 dataframes, <code>df1</code> and <code>df2</code>. I have to create a new column in <code>df1</code> using the following logic.</p>
<pre><code>col_list = ['colA', 'colB', 'colC']
df1['label_id'] = df1[['colA', 'colB', 'colC']].apply(
            lambda x: df2[x[0], x[1], x[2]], axis=1)
</code></pre>
<p>Here, <code>x[0]</code> corresponds to <code>colA</code>, <code>x[1]</code> corresponds to <code>colB</code>, <code>x[2]</code> corresponds to <code>colC</code>. As you can see, this is hardcoding. The number and names of columns in <code>col_list</code> will vary, and accordingly I should have <code>x[0], x[1], x[2], x[3]</code>... and so on.</p>
<p>So how do I make this configurable in implementation?</p>
<p>Appreciate any help. Thanks in advance.</p>
",19969602.0,19969602.0,2022-09-12 05:10:00,2022-09-12 05:10:00,Applying lambda to variable number of columns,<python><pandas><dataframe><for-loop><data-science>,0,5,N/A,CC BY-SA 4.0
73679893,1,-1.0,2022-09-11 14:02:52,0,210,"<p>If we have a input of a dictionary with format {key: [list]} like below</p>
<pre><code>List1: [value01, value02, value 03]
List2: [value02, value04, value 05]
List3: [value04, value05, value 07]
</code></pre>
<ul>
<li>The values are strings</li>
</ul>
<p>Is there a way where we can group/cluster the keys (list names ) based on the similarity between it's values(lists) in python?</p>
<p>Thanks in advance!</p>
",13012941.0,-1.0,N/A,2022-09-12 23:38:49,Clustering lists based on values,<python><python-3.x><data-science><hierarchical-clustering><dbscan>,1,2,N/A,CC BY-SA 4.0
73686750,1,-1.0,2022-09-12 08:55:09,0,173,"<p>epoch = 0
#putting the contours of the broken rice grain filled in white on the black background and saving it in the given address</p>
<pre><code>for j in broken_cnt:
for i in range(0, len(j)):
bg_copy = bg.copy()
cnt_scaled = scale_contour(j[i], 10)
 cv2.fillPoly(bg_copy, pts =[cnt_scaled], color=(255,255,255)) #filling in the contours with white
cv2.imwrite(r'C:\Users\mrsks\Downloads\Akaike Technologies_ Computer Vision Assignment\data\train\broken_train\broken_%04d.png'%(epoch+1), bg_copy)
epoch+=1
</code></pre>
<blockquote>
</blockquote>
<p>error: OpenCV(4.6.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\drawing.cpp:2396: error: (-215:Assertion failed) p.checkVector(2, CV_32S) &gt;= 0 in function 'cv::fillPoly'</p>
",19930473.0,-1.0,N/A,2022-09-12 08:55:09,OpenCV(4.6.0) cv2.fillPoly throws an error -,<machine-learning><computer-vision><data-science>,0,2,N/A,CC BY-SA 4.0
73686905,1,73687860.0,2022-09-12 09:08:57,1,72,"<p>I am trying to calculate the difference in the &quot;time&quot; column between each pair of elements having the same value in the &quot;class&quot; column.
This is an example of an input:</p>
<pre><code>  class   name                time
0     A    Bob 2022-09-05 07:22:15
1     A    Sam 2022-09-04 17:18:29
2     B    Bob 2022-09-04 03:29:06
3     B    Sue 2022-09-04 01:28:34
4     A  Carol 2022-09-04 10:40:23
</code></pre>
<p>And this is an output:</p>
<pre><code>class  name1  name2        timeDiff
0     A    Bob  Carol 0 days 20:41:52
1     A    Bob    Sam 0 days 14:03:46
2     A  Carol    Sam 0 days 06:38:06
3     B    Bob    Sue 0 days 02:00:32
</code></pre>
<p>I wrote this code to solve this problem:</p>
<pre><code>from itertools import combinations


df2 = pd.DataFrame(columns=['class', 'name1', 'name2', 'timeDiff'])

for c in df['class'].unique():
    df_class = df[df['class'] == c]
    groups = df_class.groupby(['name'])['time']

    if len(df_class) &gt; 1:
        out = (pd
               .concat({f'{k1} {k2}': pd.Series(data=np.abs(np.diff([g2.values[0],g1.values[0]])).astype('timedelta64[s]'), index=[f'{k1} {k2}'], name='timeDiff')
                        for (k1, g1), (k2, g2) in combinations(groups, 2)},
                       names=['name']
                      )
               .reset_index()
              )
        new = out[&quot;name&quot;].str.split(&quot; &quot;, n = -1, expand = True)

        out[&quot;name1&quot;]= new[0].astype(str)
        out[&quot;name2&quot;]= new[1].astype(str)
        out[&quot;class&quot;] = c

        del out['level_1'], out['name']

        df2 = df2.append(out, ignore_index=True)
</code></pre>
<p>I didn't come up with a solution without going through all the class values in a loop. However, this is very time-consuming if the input table is large. Does anyone have any solutions without using a loop?</p>
",19956286.0,-1.0,N/A,2022-09-12 10:29:18,Calculate a difference in times between each pair of values in class using Pandas,<python><pandas><datetime><data-science>,2,2,N/A,CC BY-SA 4.0
73701361,1,-1.0,2022-09-13 10:24:50,-1,701,"<p>I want to use BeautifulSoup to get the text from an HTML string. While <code>get_text()</code>'s separator argument is nice, I would like to use different separators for different tags (or not use any at all for others).</p>
<p>As an example, consider the HTML:</p>
<pre><code>&lt;p&gt;This is some paragraph text. With a &lt;a href=&quot;example.com&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;div&gt;This is another paragraph.&lt;/div&gt;
</code></pre>
<p>Dummy code:</p>
<pre><code>from bs4 import BeautifulSoup

string = '&lt;p&gt;This is some paragraph text. With a &lt;a href=&quot;example.com&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;\n&lt;div&gt;This is another paragraph.&lt;/div&gt;'
soup = BeautifulSoup(string)
text = soup.get_text('\n', strip=True)
print(text)
</code></pre>
<p>Using <code>get_text('\n')</code> outputs</p>
<pre><code>This is some paragraph text. With a 
link
.
This is another paragraph.
</code></pre>
<p>But the desired output would be</p>
<pre><code>This is some paragraph text. With a link.
This is another paragraph.
</code></pre>
<p>Is there a way to use <code>get_text()</code> and use the '\n' string as a separator for most tags and no separators for &quot;inline&quot; tags like <code>&lt;a&gt;</code> or <code>&lt;b&gt;</code>?</p>
<p>Note that the HTML I am parsing is not consistent so I can't use a function that corrects this behavior afterwards.</p>
<p>EDIT:
The reason for using a separator as an argument in <code>get_text()</code> is that the input is not guaranteed to have a newline between the two paragraphs.</p>
<p>If the example HTML was</p>
<pre><code>&lt;p&gt;This is some paragraph text. With a &lt;a href=&quot;example.com&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;&lt;div&gt;This is another paragraph.&lt;/div&gt;
</code></pre>
<p>the output still has to have the contents of <code>&lt;p&gt;</code> tags separated somehow.</p>
<p>EDIT 2:
Added different tags to the examples.</p>
",10403061.0,10403061.0,2022-09-13 10:51:59,2022-09-13 11:02:23,Use BeautifulSoup to get text with different delimiters for each tag,<python><beautifulsoup><data-science><html-parsing><text-processing>,2,0,N/A,CC BY-SA 4.0
73703355,1,73703720.0,2022-09-13 12:49:21,0,258,"<p>I understand how to encode labeled data into numerical data, using any of several techniques, including One-hot Encoding, Label Encoding, Ordinal Encoding, etc. I am wondering how to convert the numerical data back into labeled data. Here's a simple example.</p>
<pre><code>import pandas as pd
import numpy as np

# Load Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_moons
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier# Step1: Create data set


# Define the headers since the data does not have any
headers = [&quot;symboling&quot;, &quot;normalized_losses&quot;, &quot;make&quot;, &quot;fuel_type&quot;, &quot;aspiration&quot;,
           &quot;num_doors&quot;, &quot;body_style&quot;, &quot;drive_wheels&quot;, &quot;engine_location&quot;,
           &quot;wheel_base&quot;, &quot;length&quot;, &quot;width&quot;, &quot;height&quot;, &quot;curb_weight&quot;,
           &quot;engine_type&quot;, &quot;num_cylinders&quot;, &quot;engine_size&quot;, &quot;fuel_system&quot;,
           &quot;bore&quot;, &quot;stroke&quot;, &quot;compression_ratio&quot;, &quot;horsepower&quot;, &quot;peak_rpm&quot;,
           &quot;city_mpg&quot;, &quot;highway_mpg&quot;, &quot;price&quot;]

# Read in the CSV file and convert &quot;?&quot; to NaN
df = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data&quot;,
                  header=None, names=headers, na_values=&quot;?&quot; )
df.head()

df.columns

df_fin = pd.DataFrame({col: df[col].astype('category').cat.codes for col in df}, index=df.index)
df_fin


X = df_fin[['symboling', 'normalized_losses', 'make', 'fuel_type', 'aspiration',
       'num_doors', 'body_style', 'drive_wheels', 'engine_location',
       'wheel_base', 'length', 'width', 'height', 'curb_weight', 'engine_type',
       'num_cylinders', 'engine_size', 'fuel_system', 'bore', 'stroke',
       'compression_ratio', 'horsepower', 'peak_rpm']]
y = df_fin['city_mpg']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Fit a Decision Tree model
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy_score(y_test, y_pred)
</code></pre>
<p>Now, how can I make a prediction of the target variable (dependent variable) based on the independent variables???</p>
<p>Something like this should work, I think, but it doesn't...</p>
<pre><code>clf.predict([[2,164,'audi','gas','std','four','sedan','fwd','front',99.8,176.6,66.2,54.3,2337,'ohc','four',109,'mpfi',3.19,3.4,10,102,5500,24,30,13950,]])
</code></pre>
<p>If we leave numerics as numerics, and put quotes around labels, I would like to predict the dependent variable, but I can't, because of the labeled data. If the data was all numerics, and this was a regression problem, it would work!! My question is, how can we convert categorical codes back into numerical labeled data, and make a prediction??</p>
",5212614.0,-1.0,N/A,2022-09-13 13:43:07,How can we convert numerical data to labeled data and make a prediction?,<python><python-3.x><machine-learning><data-science><artificial-intelligence>,1,1,N/A,CC BY-SA 4.0
73737340,1,-1.0,2022-09-15 20:55:35,-2,208,"<p>The link I am using is <a href=""https://www.influenster.com/reviews/loreal-paris-elvive-extraordinary-oil-deep-nourishing-shampoo-and-conditioner-set-126-fl-oz"" rel=""nofollow noreferrer"">https://www.influenster.com/reviews/loreal-paris-elvive-extraordinary-oil-deep-nourishing-shampoo-and-conditioner-set-126-fl-oz</a>. Please guide me on how I can get the stars as there is no aria label or numerical value to scrape.</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
import configparser
from datetime import datetime

parser = configparser.RawConfigParser()
parser.read('config.ini')

url= parser['PROPERTIES']['URL']
END_DATE = datetime.strptime(parser['DATE']['END'], '%Y-%m-%d')
START_DATE=datetime.strptime(parser['DATE']['START'],'%Y-%m-%d')
# Setting up driver options
options = webdriver.ChromeOptions()
# Setting up Path to chromedriver executable file
CHROMEDRIVER_PATH =r'C:\Users\HP\Desktop\INTERNSHIP\influenster\chromedriver.exe'
# Adding options
options.add_experimental_option(&quot;excludeSwitches&quot;, [&quot;enable-automation&quot;])
options.add_experimental_option(&quot;useAutomationExtension&quot;, False)
# Setting up chrome service
service = ChromeService(executable_path=CHROMEDRIVER_PATH)
# Establishing Chrom web driver using set services and options
driver = webdriver.Chrome(service=service, options=options)
wait = WebDriverWait(driver, 20)
driver.get(url)
# The 2 lines below is what I actually added here + necessary imports
# and `wait` object initialization   
wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, &quot;button#onetrust-accept-btn-handler&quot;))).click()
reviews = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, &quot;.conversations-left .item&quot;)))
count=0
item_list = []

for review in reviews:
    item={  
         #stars     
        'username': review.find_element_by_xpath(&quot;.//a[contains(@class,'name')]&quot;).text,
        'userurl':review.find_element_by_xpath(&quot;.//a[contains(@class,'name')]&quot;).get_attribute(&quot;href&quot;),
        'title': 'NA',
        # 'review_text':review.find_element_by_xpath(&quot;.//div[contains(@class,'review-text')]&quot;).text,
        #  'permalink': 'NA',
        #  'date':curr_date,
        #  'subproduct name': 'NA',
        #  'subproduct link': 'NA',
}
    item_list.append(item)
print(item_list)
</code></pre>
",-1.0,-1.0,2022-09-15 21:05:10,2022-09-15 21:55:54,I am performing web scraping on influenster.com but I am not able to scrape the star rating,<python><selenium><web-scraping><data-science><scrape>,2,4,N/A,CC BY-SA 4.0
73743471,1,-1.0,2022-09-16 10:31:40,0,133,"<p>I am working with a pandas dataframe and performing the plots and tables as the following.
Here is a sample of the data;</p>
<p>companyID,county,city,companytype,turnover_class,customertype,sni,emp_class,yearmonth,productcode,sales,month,day,weekday,year,subproduct
0,127,S                                         ,180,81,11,B5,20150,14,2015-04-01,Övrigt,205940,4,1,Wednesday,2015,Kolhydrater
1,127,S                                         ,180,81,11,B5,20150,14,2015-05-01,Övrigt,1088628,5,1,Friday,2015,Kolhydrater
2,127,S                                         ,180,81,11,B5,20150,14,2015-06-01,Övrigt,264358,6,1,Monday,2015,Kolhydrater
3,127,S                                         ,180,81,11,B5,20150,14,2015-06-01,Övrigt,521313,6,1,Monday,2015,Godis
4,127,S                                         ,180,81,11,B5,20150,14,2015-06-01,Övrigt,795825,6,1,Monday,2015,Godis
5,127,S                                         ,180,81,11,B5,20150,14,2015-07-01,Övrigt,107325,7,1,Wednesday,2015,Kolhydrater
6,127,S                                         ,180,81,11,B5,20150,14,2015-07-01,Övrigt,576303,7,1,Wednesday,2015,Godis
7,127,S                                         ,180,81,11,B5,20150,14,2015-08-01,Övrigt,1137793,8,1,Saturday,2015,Kolhydrater
8,127,S                                         ,180,81,11,B5,20150,14,2015-08-01,Övrigt,13308,8,1,Saturday,2015,Godis
9,127,S                                         ,180,81,11,B5,20150,14,2015-08-01,Övrigt,80975,8,1,Saturday,2015,Godis
10,127,S                                         ,180,81,11,B5,20150,14,2015-09-01,Övrigt,662355,9,1,Tuesday,2015,Godis</p>
<pre><code>sns.lineplot(data=df_replace_code3,x='subproduct',y='sales',hue='productcode',markers=True,style='productcode',dashes=False)
</code></pre>
<p><a href=""https://i.stack.imgur.com/jQEz8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jQEz8.png"" alt=""sns.lineplot"" /></a></p>
<pre><code>product_sales = pd.pivot_table(data=df_replace_code3, index=['productcode', 'subproduct'], values='sales', 
           aggfunc='sum').reset_index().sort_values(['productcode', 'sales'], ascending=False)
</code></pre>
<p>product_sales</p>
<p><a href=""https://i.stack.imgur.com/twP8D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/twP8D.png"" alt=""pivot table"" /></a></p>
<p>and</p>
<pre><code>from pickle import TRUE


plt.rcParams['figure.figsize'] = (15,22)

ax = plt.axes()
ax.set_facecolor('#2F2F2F')

sns.barplot(y='subproduct', x='sales', data=df_replace_code3, hue='productcode')

bbox_args = dict(boxstyle = 'round', fc = '1')
for p in ax.patches:
    width = p.get_width()
    plt.text(p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), 
             ha = 'center', 
             va = 'center', 
             color = 'black', 
             bbox = bbox_args, 
             fontsize = 13)

plt.title('Sales by product &amp; sub product', fontsize = 18)
plt.ylabel(None)
plt.tick_params(left=False, bottom=False, labelbottom=False)
plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/Qr4Kp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qr4Kp.png"" alt=""sns barplot"" /></a></p>
<p>My question is that how interpret the plots and why the numbers in table is different with the numbers in barplot and lineplot?</p>
",8194692.0,8194692.0,2022-09-16 11:42:11,2022-09-16 11:42:11,"Interpretation of python sns.lineplot, pd.pivot_table and sns.barplot for a certain dataframe",<python><pandas><seaborn><data-science><exploratory-data-analysis>,0,5,2022-09-16 11:01:05,CC BY-SA 4.0
73743488,1,73775880.0,2022-09-16 10:33:26,1,121,"<p>this is my dataframe :</p>
<pre><code>artists &lt;- c(&quot;Black Waitress&quot;, &quot;Black Pumas&quot;)
tee_x&lt;- c(20, 0)
tee_y &lt;- c(3, 18)
tee_z &lt;- c (30,0)
tee_t &lt;- c(0,35)
data2 &lt;- data.frame(artists, tee_x, tee_y,tee_t)
</code></pre>
<p>And this is what I am trying to create :</p>
<pre><code>fig &lt;- plot_ly(data=data2, x = ~artists, y = ~tee_x, type = 'bar', name = 'tee_x')
fig &lt;- fig %&gt;% add_trace(y = ~tee_y, name = 'tee_y')
fig &lt;- fig %&gt;% add_trace(y = ~tee_t, name = 'tee_t')
fig &lt;- fig %&gt;% layout(yaxis = list(title = 'Count'), barmode = 'group',
                      updatemenus = list(
                          list(
                            y = 0.8,
                            buttons = list(
                              list(method = &quot;restyle&quot;,
                                   args = list(&quot;x&quot;, list(data2[c(1),(2:4)])),
                                   label = &quot;Black Waitress&quot;),

                              list(method = &quot;restyle&quot;,
                                   args = list(&quot;x&quot;, list(data2[c(2),(2:4)])),
                                   label = &quot;Black Pumas&quot;)))
                            ))

fig
</code></pre>
<p>I am trying to create a grouper barplot in plotly which shows, for each artist the number of tees they sold and their type. I am also trying to create buttons so that you can look at individual artists instead of both of them. However it is not working and I have no clue how to solve the problem.</p>
<p>Thank you</p>
<p>EDIT :</p>
<p>I have been also trying this way</p>
<pre><code>    product &lt;- c(&quot;tee_X&quot;,&quot;tee_y&quot;,&quot;tee_t&quot;)
artists &lt;- c(&quot;Black Waitress&quot;, &quot;Black Pumas&quot;)
Black_Waitress&lt;- c(20, 0, 0)
Black_Pumas &lt;- c(3, 18, 0)
tee_z &lt;- c (30,0)
tee_t &lt;- c(0,35)
data2 &lt;- data.frame(product, Black_Waitress, Black_Pumas)
show_vec = c()

for (i in 1:length(artists)){
  show_vec = c(show_vec,FALSE)
}
get_menu_list &lt;- function(artists){
  n_names = length(artists)
  buttons = vector(&quot;list&quot;,n_names)
  
  for(i in seq_along(buttons)){
    show_vec[i] = TRUE
    buttons[i] = list(list(method = &quot;restyle&quot;,
                           args = list(&quot;visible&quot;, show_vec),
                           label = artists[i]))
    print(list(show_vec))
    show_vec[i] = FALSE
  }
  
  return_list = list(
    list(
      type = 'dropdown',
      active = 0,
      buttons = buttons
    )
  )
  
  return(return_list)
}
print(get_menu_list(artists)) 
fig &lt;- plot_ly(data=data2, x = ~product, y = ~Black_Waitress, type = 'bar')
fig &lt;- fig %&gt;% add_trace(y = ~Black_Pumas)
fig &lt;- fig %&gt;% layout(showlegend = F,yaxis = list(title = 'Count'), barmode = 'group',
                      updatemenus = get_menu_list(artists))
fig
</code></pre>
<p>However the problem is that when I choose an artist in the dropdown menu I want to be shown ONLY his/her products (in other words I would like to get rid of the 0 values dynamically) Is this possible?</p>
",14729063.0,14729063.0,2022-09-19 10:26:26,2022-09-20 17:08:42,Plotly grouped barchart : how to create buttons to display different x values R,<r><plotly><data-science><data-analysis>,1,2,N/A,CC BY-SA 4.0
73708880,1,73718918.0,2022-09-13 20:29:29,0,793,"<p>I am unable to understand if is there any order in soccer position or is just random because I have to answer whether it is ordinal data or not</p>
",13862628.0,-1.0,N/A,2022-09-15 01:00:48,"Soccer positions (i.e. Defender, Midfielder, Forward) is there any order",<statistics><data-science><data-analysis>,1,3,N/A,CC BY-SA 4.0
73717846,1,-1.0,2022-09-14 13:37:55,0,36,"<p>My question is based on this post: <a href=""https://stackoverflow.com/questions/43545879/bar-chart-with-multiple-labels"">Bar Chart with multiple labels</a></p>
<p>Where the top answers shows us how to make this: <a href=""https://i.stack.imgur.com/Hd0eL.png"" rel=""nofollow noreferrer"">Image from post</a></p>
<p>However, their labels are &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot; for each. I wish to make varying labels.</p>
<p>I.e My bar charts are all in twos so follow an &quot;A&quot;,&quot;B&quot; format. However, Each label will be completely different. they are as follows. The first two are &quot;project 7&quot;, &quot;project 8&quot;. Then the next two are &quot;low salary 5 years&quot;, &quot;high salary 5 years&quot;.</p>
<p>How would one do this?</p>
",18620925.0,-1.0,N/A,2022-09-14 13:37:55,Python Multiple Bar Charts with Different names for each bar,<python><python-3.x><matplotlib><data-science>,0,2,N/A,CC BY-SA 4.0
73747170,1,-1.0,2022-09-16 15:32:55,-1,37,"<p>I am using BFS Algorithm to find the shortest path between the points which covers all the points and generates the shortest path. I am giving input(nearest neighbors) manually, and finding difficulty in automating the input to BFS Algorithm. Also please suggest if you know any algorithm which does a better job to generate the shortest path covering all the points.</p>
<p><strong>Example:</strong> Points - <code>[R59C36,R59C39,R59C52,R60C1,R60C20,R60C34,R62C2,R62C7,R63C8,R65C9,R66C11,R66C6,R67C11]</code></p>
<p><strong>Input</strong> – Giving nearest neighbors to each point</p>
<pre><code>graph = {
    'R59C36':['R59C39','R59C52','R60C34','R60C20','R60C1'],
    'R59C39':['R59C52','R60C34','R60C20','R60C1'],
    'R60C1':['R60C20','R62C2','R62C7'],
    'R60C20':['R60C34','R62C2','R62C7'],
    'R60C34':['R62C2','R62C7'],
    'R59C52':['R60C34'],
    'R62C2':['R62C7','R63C8'],
    'R62C7':['R63C8'],
    'R63C8':['R65C9'],
    'R65C9':['R66C6','R66C11'],
    'R66C6':['R66C11','R67C11'],
    'R66C11':['R67C11'],
    'R67C11':[]
}
</code></pre>
<p><strong>Output</strong> - <code>R59C36, R59C39, R59C52, R60C34, R60C20, R60C1, R62C2, R62C7, R63C8, R65C9, R66C6, R66C11, R67C11</code></p>
",20013591.0,10559142.0,2022-09-16 15:47:02,2022-09-16 17:17:42,Input Automation to Shortest Path Algorithm (BFS Algorithm),<python><algorithm><machine-learning><data-science><breadth-first-search>,1,1,N/A,CC BY-SA 4.0
73750909,1,73751303.0,2022-09-16 22:46:03,2,110,"<p>I have a df with two columns IP address and user id. I am attempting to determine the distinct values across both the user id and the IP addresses. I know how to get a distinct list of users per IP and vis versa, but can't figure out if I wanted them smashed into a single row and how I'd do it.</p>
<p>what's the recommended way to get this result I'm looking for?</p>
<p><strong>Example Data:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ip_address</th>
<th>user_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>127.0.0.1</td>
<td>111</td>
</tr>
<tr>
<td>192.168.1.1</td>
<td>111</td>
</tr>
<tr>
<td>127.0.0.1</td>
<td>444</td>
</tr>
<tr>
<td>10.10.0.1</td>
<td>555</td>
</tr>
<tr>
<td>8.8.8.8</td>
<td>666</td>
</tr>
<tr>
<td>8.8.8.8</td>
<td>888</td>
</tr>
<tr>
<td>8.8.8.8</td>
<td>999</td>
</tr>
<tr>
<td>10.0.0.1</td>
<td>777</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Final Format:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ip_address</th>
<th>user_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>127.0.0.1, 192.168.1.1</td>
<td>111, 444, 555</td>
</tr>
<tr>
<td>8.8.8.8</td>
<td>666, 888, 999</td>
</tr>
<tr>
<td>10.0.0.1</td>
<td>777</td>
</tr>
</tbody>
</table>
</div>",1683592.0,-1.0,N/A,2022-09-17 00:14:37,Find all duplicate values across two columns and make a single distinct row,<python><pandas><dataframe><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
73708204,1,73755311.0,2022-09-13 19:12:29,0,1062,"<p>I'm trying to download a folder from a public AWS S3 bucket using the Python library <code>cloudpathlib</code>. My code looks like this:</p>
<pre><code>from cloudpathlib import CloudPath
path = r&quot;C:\some\path\to\folder&quot;
url = &quot;s3://some-example-bucket/folder/&quot;

cloud_path = CloudPath(url)
cloud_path.download_to(path)
</code></pre>
<p>Really straight forward. To my knowledge, this should work, because the bucket is public:</p>
<p><a href=""https://i.stack.imgur.com/NabgT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NabgT.png"" alt=""enter image description here"" /></a></p>
<p>Here is the bucket policy (nabbed from the AWS S3 tutorial):</p>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;PublicRead&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: &quot;*&quot;,
            &quot;Action&quot;: [
                &quot;s3:GetObject&quot;,
                &quot;s3:GetObjectVersion&quot;
            ],
            &quot;Resource&quot;: &quot;arn:aws:s3:::cozy-auto-texture-sd-repo/*&quot;
        }
    ]
}
</code></pre>
<p>When I run the Python code to install the folder <code>stable-diffusion-v1-4</code> the following error message appears:</p>
<pre><code>  File &quot;E:\Cozy-Auto-Texture-Files\venv\lib\site-packages\botocore\auth.py&quot;, line 418, in add_auth
    raise NoCredentialsError()
botocore.exceptions.NoCredentialsError: Unable to locate credentials
</code></pre>
<p>My question is why is this happening? My bucket shouldn't require credentials since it's public. Is there something I'm missing with the buckets permissions or is it a Python code thing?</p>
<p>Thank you for reading and I appreciate the help!</p>
<p>Edit:</p>
<p>I've tried again with this method:</p>
<pre><code>BUCKET_NAME = 'cozy-auto-texture-sd-repo'  # replace with your bucket name
KEY = 'stable-diffusion-v1-4'

s3 = boto3.resource('s3')

try:
    s3.Bucket(BUCKET_NAME).download_file(KEY, sd_path)
except botocore.exceptions.ClientError as e:
    if e.response['Error']['Code'] == &quot;404&quot;:
        print(&quot;The object does not exist.&quot;)
    else:
        raise

</code></pre>
<p>However the same error message <code>botocore.exceptions.NoCredentialsError: Unable to locate credentials</code> appears, leading me to believe there is something wrong with my S3 bucket setup.</p>
",15739035.0,15739035.0,2022-09-13 20:33:13,2023-03-10 07:05:50,How do I download a folder from AWS S3 with CloudPathLib from a public bucket?,<python><amazon-s3><data-science><boto3><pathlib>,3,5,N/A,CC BY-SA 4.0
73724397,1,73724411.0,2022-09-15 00:29:42,0,40,"<p>I compiled a list of the top artists for every year across 14 years and I want to gather the top 7 for the 14 years combined so my idea was to gather them all in a dataframe then gather the most repeated artists for these years, but it didn't work out.</p>
<pre><code>#Collecting the top 7 artists across the 14 years
artists = []
year = 2020
while year &gt;= 2006:
    TAChart = billboard.ChartData('Top-Artists', year = year)
    artists.append(str(TAChart))
    year -= 1

len(artists)
Artists = pd.DataFrame(artists)
n = 7
Artists.value_counts().index.tolist()[:n]
</code></pre>
",17283521.0,-1.0,N/A,2022-10-23 00:17:45,How to get the most repated elements in a dataframe/array,<python><python-3.x><pandas><dataframe><data-science>,2,1,N/A,CC BY-SA 4.0
73763397,1,-1.0,2022-09-18 14:03:44,0,46,"<p>I have several dictionaries</p>
<pre><code>dict_1 = {('ABD12-GOU14', '4W', 'ASS 4W LINE 4', 80): [4, 5],
('ABD13-GOU14', '10W', 'ASS 4W LINE 5', 43): [2, 5],
('ABD14-GOU14', '11W', 'ASS 4W LINE 6', 90): [3, 5]}

dict_2 = {('ABD12-GOU14', '7W', 'ASS 4W LINE 4', 20): [5, 5],
('ABD13-GOU14', '2W', 'ASS 4W LINE 5', 31): [3, 5],
('ABD14-GOU14', '9W', 'ASS 4W LINE 5', 75): [2, 5]}

dict_3 = {('ABD12-GOU14', '23W', 'ASS 4W LINE 4', 20): [6, 5],
('ABD13-GOU14', '26W', 'ASS 4W LINE 5', 31): [2, 5],
('ABD14-GOU14', '6W', 'ASS 4W LINE 5', 75): [4, 5]}
</code></pre>
<p>Note: ('ABD12-GOU14', '23W', 'ASS 4W LINE 4', 20) =&gt; (keys1, keys2, keys3, keys4)</p>
<p>How can I sum dictionaries by matching tuples? I want it to look like this</p>
<pre><code>{('ABD12-GOU14', 'ASS 4W LINE 4'):[15,5],
('ABD13-GOU14','ASS 4W LINE 5'):[7,5],
('ABD14-GOU14','ASS 4W LINE 5'):[6,5],
('ABD14-GOU14','ASS 4W LINE 6'):[3,5],
}
</code></pre>
",20026319.0,15497141.0,2022-09-19 17:27:40,2022-09-19 17:27:40,How to merge and sum value in dictionary based on key 1 and key 3 on tuple using Python,<python-3.x><database><dictionary><data-science><data-analysis>,1,2,N/A,CC BY-SA 4.0
73707311,1,-1.0,2022-09-13 17:49:17,0,34,"<p>My target feature in the dataset contains various classes, and I want to split them into only two classes.
For clarification: my target--&gt; <code>DF['normal'].unique()</code> gives <code>array(['normal', 'neptune', 'warezclient', 'ipsweep', 'portsweep','teardrop', 'nmap', 'satan', 'smurf', 'pod', 'back', 'guess_passwd', 'ftp_write', 'multihop', 'rootkit','buffer_overflow', 'imap', 'warezmaster', 'phf', 'land','loadmodule', 'spy', 'perl'],</code>
I want to make them only two classes <code>['normal', 'Attack']</code>
How to do so?</p>
",14769444.0,-1.0,N/A,2022-09-13 17:57:58,how to categorize object feature into two classes in pandas,<python><pandas><numpy><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
73714373,1,-1.0,2022-09-14 09:15:15,0,10,"<p>I have a table which e.g. looks like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Food</th>
<th>Compound</th>
<th>Content in grams</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kiwi</td>
<td>Fat</td>
<td>5</td>
</tr>
<tr>
<td>Kiwi</td>
<td>Protein</td>
<td>3</td>
</tr>
<tr>
<td>Apple</td>
<td>Protein</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>I want to pivot it with pandas, so that it looks like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Food</th>
<th>Fat</th>
<th>Protein</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kiwi</td>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>Apple</td>
<td>2</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>Thanks in Advance!</p>
",15241939.0,15241939.0,2022-09-14 09:16:10,2022-09-14 09:16:10,"Pandas Pivot Tables is empty, want to fill with values from other column",<python><pandas><dataframe><data-science>,0,2,2022-09-14 09:15:38,CC BY-SA 4.0
73719439,1,73719965.0,2022-09-14 15:23:00,0,26,"<p>I'm looking to do hazard analysis but before I do that I want to clean my dataset so I have only the data from right before a &quot;death&quot;, if you will. I'm studying countries and since countries don't &quot;die&quot; per say I need to basically find the point where an event occurs, coded as a '1' in an indicator column, and then generate a column that has 0s everywhere except for every time except for n-periods before my indicator column hits '1'.</p>
<p>For example, if my data were the first row, I would be looking to find a way to generate the second row.</p>
<pre><code>number_of_years = 5
year = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
indicator = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 0)
lag_column = c(0, 1, 1, 1, 1, 1, 0, 0, 0, 0) #I need to make this, the 5 years before the event occurs
</code></pre>
<p>Thank you!</p>
",13298650.0,-1.0,N/A,2022-09-14 16:00:36,Create a sequence before an indicator variable,<r><data-science><hazard>,1,0,N/A,CC BY-SA 4.0
73748745,1,73748833.0,2022-09-16 18:05:13,-1,87,"<p>I am trying to extract the month from the the Order_date column by using postgres SQL , and the format is required to come in the name of the month format ;for example, December.</p>
<p>When I applied the extract function , It gave me an error message saying that the date setting has to be changed.</p>
<p>Please advise how to extract the month from the mentioned column ?</p>
<p>The data has been enclosed</p>
<pre><code>SELECT EXTRACT(MONTH FROM order_date::date)
FROm think_sales;
</code></pre>
<p>The error message :</p>
<blockquote>
<p>[22008] ERROR: date/time field value out of range: &quot;25/06/2021&quot; Hint: Perhaps you need a different &quot;datestyle&quot; setting.</p>
</blockquote>
<p>Data :</p>
<p><a href=""https://i.stack.imgur.com/nVDsM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nVDsM.png"" alt=""enter image description here"" /></a></p>
",19960525.0,5193536.0,2022-09-16 18:08:03,2022-09-16 18:17:28,How do I extract the month from the Date column in SQL-postgres,<sql><postgresql><data-science><data-analysis>,3,1,N/A,CC BY-SA 4.0
73769647,1,-1.0,2022-09-19 07:09:07,-1,44,"<p>I have several dictionary</p>
<pre><code>{
('ABD12-GOU10', 'ASS 4W LINE 3'):[15,5],
('ABD13-GOU11', 'ASS 4W LINE 1'):[7,5],
('ABD13-GOU11', 'ASS 4W LINE 1'):[22,5],
('ABD14-GOU13', 'ASS 4W LINE 1'):[12,5],
('ABD14-GOU19', 'ASS 4W LINE 2'):[4,5],
('ABD14-GOU10', 'ASS 4W LINE 3'):[2,5],
('ABD14-GOU13', 'ASS 4W LINE 4'):[18,5],
('ABD14-GOU12', 'ASS 4W LINE 5'):[9,5],
('ABD14-GOU11', 'ASS 4W LINE 6'):[3,5],
}
</code></pre>
<p>How to sorting by key 2 ex: ASS 4W LINE 1 and sorting by key 1 ex: ABD12-GOu10. I want like this:</p>
<pre><code>{
('ABD13-GOU11', 'ASS 4W LINE 1'):[7,5],
('ABD13-GOU11', 'ASS 4W LINE 1'):[22,5],
('ABD14-GOU13', 'ASS 4W LINE 1'):[12,5],
('ABD14-GOU19', 'ASS 4W LINE 2'):[4,5],
('ABD12-GOU10', 'ASS 4W LINE 3'):[15,5],
('ABD14-GOU10', 'ASS 4W LINE 3'):[2,5],
('ABD14-GOU13', 'ASS 4W LINE 4'):[18,5],
('ABD14-GOU12', 'ASS 4W LINE 5'):[9,5],
('ABD14-GOU11', 'ASS 4W LINE 6'):[3,5],
}
</code></pre>
",20026319.0,20026319.0,2022-09-19 08:13:14,2022-09-19 14:25:46,How to sorting (ASCENDING) by key using Python dictionary,<sorting><dictionary><tuples><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
73753960,1,73754324.0,2022-09-17 10:08:36,0,34,"<p>I have below two doubts</p>
<p>1)I am trying to build a wordcloud and for doing that am defining a variable a=[ ] but it throws error but if I define it as a='' it works well. Please tell me what is the difference between them?</p>
<ol start=""2"">
<li>I am using below two for loops but both of them show difference output whereas I expect them to show the same? What is the difference between them.</li>
</ol>
<p>a)</p>
<pre><code>allwords=[]
for i in data['Url']:
    allwords+= ' '.join(i)
</code></pre>
<p>b) <code>all_words = ' '.join([text for text in data['Url']])</code></p>
",19476450.0,5660284.0,2022-09-17 10:22:47,2022-09-17 11:06:13,Difference between defining a variable as a=[a] and a= ' ' in python,<python-3.x><data-science><word-cloud>,1,0,N/A,CC BY-SA 4.0
73755863,1,-1.0,2022-09-17 14:51:41,-1,137,"<p>I am trying to modify the code so that if the change in loss is less than  1%, it exits the iterations.</p>
<pre><code>class MyLinReg(object):
    
    def __init__(self, activation_function):
        self.activation_function = activation_function
        
    def fit(self, X, y, alpha = 0.001, epochs = 10):
        self.theta = np.random.rand(X.shape[1] + 1)
        self.errors =[]
        n = X.shape[0]
        
        for _ in range(epochs):
            errors = 0
            sum_1 = 0
            sum_2 = 0
            for xi, yi in zip(X, y):
                sum_1 += (self.predict(xi) - yi)*xi
                sum_2 += (self.predict(xi) - yi)
                errors += ((self.predict(xi) - yi)**2)
            self.theta[:-1] -= 2*alpha*sum_1/n
            self.theta[-1] -= 2*alpha*sum_2/n
            self.errors.append(errors/n)
            if (((self.errors[-1] - self.errors[-2])/self.errors[-1]) &lt; 0.01):
                break
        return self
    
    def predict(self, X):
        weighted_sum = np.dot(X, self.theta[:-1]) + self.theta[-1]
        return self.activation_function(weighted_sum)
</code></pre>
",19266212.0,4685471.0,2022-09-17 20:24:58,2022-09-17 20:24:58,"""IndexError: list index out of range."" Need help fixing this error",<python><machine-learning><data-science><linear-regression>,1,2,N/A,CC BY-SA 4.0
73759136,1,73864023.0,2022-09-17 23:18:29,1,355,"<p>I can easily train and test a classifier using the code below.</p>
<pre><code>import pandas as pd
import numpy as np

# Load Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_moons
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier# Step1: Create data set


# Define the headers since the data does not have any
headers = [&quot;symboling&quot;, &quot;normalized_losses&quot;, &quot;make&quot;, &quot;fuel_type&quot;, &quot;aspiration&quot;,
           &quot;num_doors&quot;, &quot;body_style&quot;, &quot;drive_wheels&quot;, &quot;engine_location&quot;,
           &quot;wheel_base&quot;, &quot;length&quot;, &quot;width&quot;, &quot;height&quot;, &quot;curb_weight&quot;,
           &quot;engine_type&quot;, &quot;num_cylinders&quot;, &quot;engine_size&quot;, &quot;fuel_system&quot;,
           &quot;bore&quot;, &quot;stroke&quot;, &quot;compression_ratio&quot;, &quot;horsepower&quot;, &quot;peak_rpm&quot;,
           &quot;city_mpg&quot;, &quot;highway_mpg&quot;, &quot;price&quot;]

# Read in the CSV file and convert &quot;?&quot; to NaN
df = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data&quot;,
                  header=None, names=headers, na_values=&quot;?&quot; )
df.head()

df.columns

df_fin = pd.DataFrame({col: df[col].astype('category').cat.codes for col in df}, index=df.index)
df_fin


X = df_fin[['symboling', 'normalized_losses', 'make', 'fuel_type', 'aspiration',
       'num_doors', 'body_style', 'drive_wheels', 'engine_location',
       'wheel_base', 'length', 'width', 'height', 'curb_weight', 'engine_type',
       'num_cylinders', 'engine_size', 'fuel_system', 'bore', 'stroke',
       'compression_ratio', 'horsepower', 'peak_rpm']]
y = df_fin['city_mpg']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Fit a Decision Tree model
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy_score(y_test, y_pred)
</code></pre>
<p>Now, how can I make a prediction of the target variable (dependent variable) based on the independent variables?</p>
<p>Something like this should work, I think, but it doesn't...</p>
<pre><code>clf.predict([[2,164,'audi','gas','std','four','sedan','fwd','front',99.8,176.6,66.2,54.3,2337,'ohc','four',109,'mpfi',3.19,3.4,10,102,5500,24,30,13950,]])
</code></pre>
<p>If we leave numerics as numerics, and put quotes around labels, I would like to predict the dependent variable, but I can't, because of the labeled data. If the data was all numerics, and this was a regression problem, it would work!! My question is...how can we feed in numbers and labels, like a real person would understand, rather than using the the numerics that the labels are converted into. I've gotta believe, labels are converted into numerics (one hot encoding, catagorical codes, or whatever) before the training and testing is done, right.</p>
<p>Here is the error message that I'm getting.</p>
<pre><code>clf.predict([[2,164,'audi','gas','std','four','sedan','fwd','front',99.8,176.6,66.2,54.3,2337,'ohc','four',109,'mpfi',3.19,3.4,10,102,5500,24,30,13950,]])
C:\Users\ryans\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names
  warnings.warn(

Traceback (most recent call last):

  Input In [20] in &lt;cell line: 1&gt;
    clf.predict([[2,164,'audi','gas','std','four','sedan','fwd','front',99.8,176.6,66.2,54.3,2337,'ohc','four',109,'mpfi',3.19,3.4,10,102,5500,24,30,13950,]])

  File ~\anaconda3\lib\site-packages\sklearn\tree\_classes.py:505 in predict
    X = self._validate_X_predict(X, check_input)

  File ~\anaconda3\lib\site-packages\sklearn\tree\_classes.py:471 in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse=&quot;csr&quot;, reset=False)

  File ~\anaconda3\lib\site-packages\sklearn\base.py:577 in _validate_data
    X = check_array(X, input_name=&quot;X&quot;, **check_params)

  File ~\anaconda3\lib\site-packages\sklearn\utils\validation.py:856 in check_array
    array = np.asarray(array, order=order, dtype=dtype)

ValueError: could not convert string to float: 'audi'
</code></pre>
",5212614.0,11989081.0,2022-09-18 04:56:24,2022-09-27 07:43:55,How can we make a prediction using Scikit-Learn Classifiers?,<python><machine-learning><scikit-learn><data-science><classification>,1,1,N/A,CC BY-SA 4.0
73776248,1,73776524.0,2022-09-19 16:03:41,-1,41,"<p>What's the neatest way to create a new column based on the values from another column being contained in a list of lists with some extra conditions as well?</p>
<p>So the dataframe and the nested list are:</p>
<pre><code>df = pd.DataFrame({ &quot;col&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;D&quot;, &quot;G&quot;, &quot;C&quot;, nan, &quot;H&quot;]})

categ = [[&quot;A&quot;, &quot;D&quot;], [&quot;Missing&quot;, &quot;C&quot;], [&quot;Other&quot;]]
</code></pre>
<p>In my case I would also like np.nan to be considered as &quot;Missing&quot; and if the column value is not present in the lists then it should be considered as &quot;Other&quot;.</p>
<p>So the resulting df should like this:</p>
<pre><code>   col        NewCol
0    A        [A, D]
1    B       [Other]
2    D        [A, D]
3    G       [Other]
4    C  [Missing, C]
5  NaN  [Missing, C]
6    H       [Other]
 
</code></pre>
",10918384.0,10918384.0,2022-09-19 16:38:11,2022-09-19 16:51:34,Pandas: How do I create a new column given the column values exist in a list of lists?,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
73776655,1,-1.0,2022-09-19 16:37:40,0,158,"<p>I have a pandas DataFrame that looks like this:</p>
<p><a href=""https://i.stack.imgur.com/z3hjo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z3hjo.png"" alt=""enter image description here"" /></a></p>
<p>One instance can have a value of 1 in more than one category. I was wondering how to visualize the categories that overlap the most with each other. I thought of maybe Venn's diagrams, but I don't know ow to generate them from my position.</p>
<p>Thanks in advance :)</p>
",14014925.0,-1.0,N/A,2022-09-19 20:00:16,Visualize overlapping categories in one-hot encoding Pandas DataFrame,<python><pandas><deep-learning><data-science>,1,0,N/A,CC BY-SA 4.0
73768196,1,-1.0,2022-09-19 03:32:01,-1,128,"<p>I have several dictionary and i want to remove several key 2 and key 4</p>
<pre><code>dict_1 = {('ABD12-GOU14', '4W', 'ASS 4W LINE 4', 80): [4, 5],
('ABD13-GOU14', '10W', 'ASS 4W LINE 5', 43): [2, 5],
('ABD14-GOU14', '11W', 'ASS 4W LINE 6', 90): [3, 5]}
</code></pre>
<p>i want like this</p>
<pre><code>dict_1 = {('ABD12-GOU14', 'ASS 4W LINE 4'): [4, 5],
('ABD13-GOU14', 'ASS 4W LINE 5'): [2, 5],
('ABD14-GOU14', 'ASS 4W LINE 6'): [3, 5]}
</code></pre>
",20026319.0,-1.0,N/A,2022-09-20 02:03:03,How to delete keys dictionary in tuple using Python,<dictionary><tuples><data-science><data-analysis>,2,0,N/A,CC BY-SA 4.0
73771761,1,-1.0,2022-09-19 10:07:30,0,139,"<p>I have a folder with subfolders in each subfolder there is an XML file with the same name. I need to make a loop that goes over these subfolders, finds files with the same name, combines them, and saves them in a different folder.</p>
",19135855.0,-1.0,N/A,2022-09-19 10:07:30,How to merge multiple xml files with same name but in different subfolders using Python?,<python><xml><dataframe><data-science>,0,3,N/A,CC BY-SA 4.0
73774553,1,73774880.0,2022-09-19 13:56:11,-1,45,"<p>For examples this does not work:</p>
<pre><code>mtcars %&gt;%
filter(cyl == 8) %&gt;%
select(mpg) &lt;- 1
</code></pre>
<p>I would like to replace all the values I selected with 1</p>
<p>I want to replace everything and not only selected values, so I am not sure how to use <code>replace</code></p>
",13034641.0,13034641.0,2022-09-19 14:02:17,2022-09-19 17:26:20,How do I overwrite values I select from a dataframe?,<r><dplyr><data-science>,1,5,N/A,CC BY-SA 4.0
73789655,1,-1.0,2022-09-20 16:09:21,0,238,"<p>I have a couple of questions about the group by function.
<strong>1.</strong> I would like to group by pandas data frame by single column without aggregation.
<strong>2.</strong> After group by, I would like to split the dataset into several datasets by the month date.
So, I wasn't able to do so, I am requesting help. I would appreciate it if you guys can help me.
I have provided the code, expected results, and dataset below.</p>
<p>Original dataframe</p>
<pre><code>data = {'month': ['2022-01-01', '2022-02-01', '2022-03-01', '2022-01-01', '2022-02-01', '2022-03-01', '2022-01-01', '2022-02-01', '2022-03-01',], 
'Name': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'], 
'num': [1234, 1234, 1234, 456, 456, 456, 456, 100, 200,],
}

df = pd.DataFrame(data)
df
</code></pre>
<p><a href=""https://i.stack.imgur.com/HJRbn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HJRbn.png"" alt=""enter image description here"" /></a></p>
<p><strong>Expected Result for question #1</strong></p>
<p><a href=""https://i.stack.imgur.com/w6fcd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w6fcd.png"" alt=""enter image description here"" /></a></p>
<p>And I want to split the dataset into different datasets like this
<strong>Expected Result for question #2</strong>
<a href=""https://i.stack.imgur.com/uXsoX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uXsoX.png"" alt=""enter image description here"" /></a></p>
<p>Thank You</p>
",16875907.0,-1.0,N/A,2022-09-20 16:50:03,How can we perform group by single column without aggregation in pandas?,<python><pandas><dataframe><group-by><data-science-experience>,2,1,N/A,CC BY-SA 4.0
73796359,1,-1.0,2022-09-21 06:42:27,1,860,"<p>I'm getting error while implementing model.fit(dis=0). Please help me on this. How to get rid of this error? I'm doing time series and forecasting for Electricity Board in USA PJMW.</p>
<pre><code>ar_preds = []

history = [x for x in train_series]

for t in range(len(test_series)):

 model = ARIMA(history, order=(1,0,0))
model_fit = model.fit(disp=0)
output = model_fit.forecast()
ar_preds.append(output[0][0])
history.append(test_series[t])

ar_pred_series = pd.Series(ar_preds, index=test_series.index)

error = mean_squared_error(test_series, ar_pred_series)

print('MSE: %.3f' % error)

plt.plot(test_series, label='Observed Values')
plt.plot(ar_pred_series, color='blue', label='Forecasted Values')
plt.legend()
plt.title('AR(1) Model Forecasts')
plt.ylabel('Energy Consumption (MW)')
plt.show()



TypeError                                 Traceback (most recent call last)
Input In [64], in &lt;cell line: 4&gt;()
      4 for t in range(len(test_series)):
      5     model = ARIMA(history, order=(1,0,0))
----&gt; 6     model_fit = model.fit(disp=0)
      7     output = model_fit.forecast()
      8     ar_preds.append(output[0][0])

TypeError: fit() got an unexpected keyword argument 'disp'
</code></pre>
",19930473.0,-1.0,N/A,2022-09-21 06:42:27,Stats Model - TypeError: fit() got an unexpected keyword argument 'disp',<python><data-science><forecasting><arima><sarimax>,0,3,N/A,CC BY-SA 4.0
73760966,1,73761546.0,2022-09-18 07:50:20,1,618,"<p>My question is a little bit theoretical.</p>
<p>I have a dataset with 100+ columns, Every EDA method that I use results in a messed-up plot, How can I get more interpretable plots and tables with such data?</p>
",15602341.0,-1.0,N/A,2022-09-19 15:22:23,Exploratory Data Analysis on Datasets with too much variables,<pandas><data-science><eda>,2,0,N/A,CC BY-SA 4.0
73788546,1,73788601.0,2022-09-20 14:40:33,0,136,"<pre><code>frames = [df1, df2]
result = pd.concat(frames)
result.sample(n=5)
</code></pre>
<p>Two datasets have 4 columns and I would like to show them in one output, 8 columns together.
The way I do is just filling with NaN. I just want to combine two separate tables.</p>
<pre><code>df1
Column1         Column2 Column3 Column4
151.99.51.78    56.809  54.306  2.503
99              17.727  0.000   17.727
131.253.33.203  11.136  0.211   10.925
04.79.197.203   9.298   1.013   8.285

df2
Column5         Column6 Column7 Column8
13.11.51.78     54.809  54.306  4.503
93              15.727  3.000   16.727
144.44.33.203   16.136  122.211 17.925
04.74.447.2443  8.298   23.013  77.285
</code></pre>
<p>Expected:</p>
<pre><code> Column1        Column2 Column3 Column4   Column5       Column6 Column7 Column8
151.99.51.78    56.809  54.306  2.503   13.11.51.78     54.809  54.306  4.503
99              17.727  0.000   17.727    93            15.727  3.000   16.727
131.253.33.203  11.136  0.211   10.925  144.44.33.203   16.136  122.211 17.925
04.79.197.203   9.298   1.013   8.285   04.74.447.2443  8.298   23.013  77.285
</code></pre>
",18964569.0,-1.0,N/A,2022-09-20 15:27:55,How to merge two dataframes without filling with NaN or zeros,<python><pandas><dataframe><data-science>,2,1,N/A,CC BY-SA 4.0
73830999,1,-1.0,2022-09-23 17:13:01,-1,51,"<p>Using Numpy, how can i produce an array that looks like this:
<code>array(['1940', '1950', '1960', '1970', '1980', '1990', '2000', '2010', '2020'], dtype='datetime64[Y]')</code></p>
",5347405.0,20623798.0,2022-09-23 17:20:39,2022-09-23 17:25:39,Use arange to generate an array containing all the decades from 1940 to 2020,<python><datetime><data-science>,1,3,N/A,CC BY-SA 4.0
73838068,1,73838144.0,2022-09-24 14:40:38,0,209,"<p>I am a newbie and please help me. Thanks in advance. I am trying to find full width at half maxima of a derivative curve and I am getting errors.</p>
<pre><code>def cauchy(x, l, k, x1):
    return l / (1+np.exp(-k*(x-x1)))

def fwhm(x, y, k=10):
    half_max = amax(y)/2
    s = splrep(x, y - half_max, k=k)
    roots = sproot(s)
    return abs(roots[1] - roots[0])

distance = [1000*0.001, 2000*0.001, 3000*0.001, 4000*0.001,5000*0.001,6000*0.001,7000*0.001,8000*0.001,
           9000*0.001, 11000*0.001, 12000*0.001, 13000*0.001, 14000*0.001, 15000*0.001, 16000*0.001,
           17000*0.001, 18000*0.001, 19000*0.001, 21000*0.001, 22000*0.001, 23000*0.001, 24000*0.001, 25000*0.001, 26000*0.001,
           27000*0.001, 28000*0.001, 29000*0.001, 30000*0.001, 31000*0.001, 32000*0.001, 33000*0.001,
           34000*0.001, 35000*0.001]
amplitude= [26, 31, 29, 26, 27, 24, 24, 28, 24, 24, 28, 31, 24, 26, 55, 30, 73, 101, 168, 219, 448, 833, 1280, 1397, 1181, 1311,
            1715, 1975, 2003, 2034, 2178, 2180, 2182]
plt.plot(distance,amplitude, 'o')
popt, pcov = curve_fit(cauchy, distance, amplitude,maxfev=100, bounds=((-10, -10, -10), (3000, 3000, 3000)),p0=[2500, 1, 30])
plt.plot(distance, cauchy(distance, *popt), 'r', label='cauchy fit')
l,k,x,x1 = sp.symbols('l k x x1')
expr_diff1 = Derivative(l / (1+sp.exp(-k*(x-x1))), x)
print(&quot;Derivative of the function : {}&quot;.format(expr_diff1.doit()))
var1=[]
for i in distance:
    xnew = ((4.06252582e-01*2.22741362e+03*exp(-4.06252582e-01*(i - 2.57502110e+01))/(1 + exp(-4.06252582e-01*(i - 2.57502110e+01)))**2))*10
    var1.append(xnew)
print(var1)
plt.plot(distance, var1)
fwhm(distance, var1)
</code></pre>
",19109547.0,-1.0,N/A,2022-09-26 11:10:59,Getting error TypeError: unsupported operand type(s) for 'list' and 'Float' when I am trying to find full width at half maxima of a derivative curve,<python><database><data-science><curve-fitting>,1,0,N/A,CC BY-SA 4.0
73805083,1,-1.0,2022-09-21 17:56:02,0,410,"<p>The title may not be clear but I will try to explain my problem as clearly as possible. I have dummy data i.e.</p>
<pre><code>data = {'month': ['2022-01-01', '2022-02-01', '2022-03-01', '2022-01-01', '2022-02-01', '2022-03-01', '2022-01-01', '2022-02-01', '2022-03-01','2022-01-01',], 
'Name': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'D', ],
'Age': [23, 24, 34, 45, 56, 46, 40, 30, 20, 50,],
'Experience': [1, 2, 4, 6, 7, 7, 5, 10, 9, 8], 
'salary': [50, 60, 70, 80, 80, 90, 55, 75, 100, 95,],
}

df = pd.DataFrame(data)
df
</code></pre>
<p><a href=""https://i.stack.imgur.com/PEaer.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PEaer.png"" alt=""enter image description here"" /></a></p>
<p>In machine learning, we will split the data sets into 60-40/70-30/80-20, and so on. Before splitting data, we dropped unnecessary data for the training and separate input and output. Like below:</p>
<pre><code>labels=df[['month', 'Name']]
y=df[['salary']]
X=df.drop(['month', 'Name', 'salary'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)
</code></pre>
<p>And after splitting, we can assume that this is our training data.
<a href=""https://i.stack.imgur.com/PRPIh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PRPIh.png"" alt=""enter image description here"" /></a></p>
<p>And this is our test data.
<a href=""https://i.stack.imgur.com/IUjDM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IUjDM.png"" alt=""enter image description here"" /></a></p>
<p>So, I was wondering how can we add back <code>month, name, and salary</code> column back to train data and test data to make sure that row belongs to a particular <code>month and Name</code>?</p>
<p>Expected results for test data
<a href=""https://i.stack.imgur.com/ONsgO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ONsgO.png"" alt=""enter image description here"" /></a></p>
",16875907.0,-1.0,N/A,2022-09-21 17:56:02,How to match labels for train and test in machine learning using python?,<python><pandas><dataframe><scikit-learn><data-science-experience>,0,4,N/A,CC BY-SA 4.0
73826830,1,73827353.0,2022-09-23 11:11:46,0,49,"<p>Hello Everyone I am doing a web scraping of a website which has multiple pages(doing for 9 pages) and writing data in a csv file. every page has 24 rows of data which comes in total of 216 rows data for 9 pages but I am getting only 24 rows of data which I think is page no 9 data and python just re-writing the data again &amp; again for every page in same rows instead of appending it.so please help me to figure out how I can make python to append each page data in ex. Here is my code:</p>
<pre><code>import requests
from bs4 import BeautifulSoup
from csv import writer
for page in range(1,10):
    url = 'https://www.flipkart.com/searchq=laptops&amp;otracker=search&amp;otracker1=search&amp;marketplace=FLIPKART&amp;as-show=on&amp;as=off&amp;page={page}'.format(page =page)
    req = requests.get(url)
    soup = BeautifulSoup(req.content, 'html.parser')
    links = soup.find_all('div', class_= '_2kHMtA')
    with open('Flipkart.csv', 'w', encoding = 'utf8', newline= '') as f:
        thewriter = writer(f)
        header = ('Title', 'Specification', 'price', 'Rating Out of 5')
        thewriter.writerow(header)
        for link in links:
            title = link.find('div', class_= '_4rR01T').text
            Specification = link.find('ul', class_='_1xgFaf').text
            price = link.find('div', class_ = '_30jeq3 _1_WHN1').text
            Rating = link.find('span', class_='_1lRcqv')
            if Rating:
                Rating = Rating.text
            else:
                Rating = 'N/A'
            info = [title, Specification, price,Rating]
            thewriter.writerow(info)
</code></pre>
",20053292.0,-1.0,N/A,2022-09-23 11:58:20,Rewriting Rows instead of adding to new one,<python><excel><web-scraping><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
73851229,1,-1.0,2022-09-26 08:09:14,2,904,"<p>I am trying to plot the below line chart in plotly, but my x axis is a quarter such as</p>
<pre><code>df['quarter'] = pd.PeriodIndex(df.date, freq='Q')
quarter
2017Q1
2017Q2
...

fig = px.line(qtrly_comp, x=&quot;quarter&quot;, y=&quot;counts&quot;,template=template_style,markers = True)


fig.show()
</code></pre>
<p>I am getting error that reads - <strong>TypeError: Object of type Period is not JSON` serializable</strong></p>
<p>datatype of the column is <code>period[Q-DEC]</code></p>
<p>Is there anyway I can get plotly to read the x axis ? Thanks!</p>
",8797830.0,8797830.0,2022-09-26 19:56:06,2022-12-16 14:09:20,Plotting Quarterly Data - Plotly,<python><python-3.x><plotly><data-science><plotly-python>,1,1,N/A,CC BY-SA 4.0
73773467,1,73773953.0,2022-09-19 12:38:21,0,548,"<p>I have a large dataset (hundreds of millions of rows) that I need to heavily process using spark with Databricks. This dataset has tens of columns, typically an integer, float, or array of integers.</p>
<p>My question is: does it make any difference if I drop some columns that are not needed before processing the data? In terms of memory and/or processing speed?</p>
",7192989.0,-1.0,N/A,2022-09-19 13:15:06,Does dropping columns that are not used in computation affect performance in spark?,<apache-spark><pyspark><data-science><distributed-computing><data-processing>,1,0,N/A,CC BY-SA 4.0
73864454,1,-1.0,2022-09-27 08:23:20,-4,633,"<pre><code>    Name
4   A-------
5   ---
6   Father Name
7   ------
8   Gender
9   Country of
10  M
11  Oman
12  Identity Number -n?
13  Date of Birth
14  ------------9
15  28.10.1995
16  ----
17  Date of Issue
18  Date of Expiry
</code></pre>
",19769279.0,10836309.0,2022-09-27 08:28:21,2022-09-27 09:58:18,"how to extract specific data from csv file like""Name"", ""Address""?",<python><pandas><dataframe><csv><data-science>,1,4,N/A,CC BY-SA 4.0
73868794,1,73868919.0,2022-09-27 13:51:44,5,491,"<p>I find it hard to explain with words what I want to achieve, so please don't judge me for showing a simple example instead. I have a table that looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>main_col</th>
<th>some_metadata</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>this</td>
<td>True</td>
<td>10</td>
</tr>
<tr>
<td>this</td>
<td>False</td>
<td>3</td>
</tr>
<tr>
<td>that</td>
<td>True</td>
<td>50</td>
</tr>
<tr>
<td>that</td>
<td>False</td>
<td>10</td>
</tr>
<tr>
<td>other</td>
<td>True</td>
<td>20</td>
</tr>
<tr>
<td>other</td>
<td>False</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
<p>I want to normalize this data separately for each case of <code>main_col</code>. For example, if we're to choose min-max normalization and scale it to range [0; 100], I want the output to look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>main_col</th>
<th>some_metadata</th>
<th>value (normalized)</th>
</tr>
</thead>
<tbody>
<tr>
<td>this</td>
<td>True</td>
<td>100</td>
</tr>
<tr>
<td>this</td>
<td>False</td>
<td>30</td>
</tr>
<tr>
<td>that</td>
<td>True</td>
<td>100</td>
</tr>
<tr>
<td>that</td>
<td>False</td>
<td>20</td>
</tr>
<tr>
<td>other</td>
<td>True</td>
<td>100</td>
</tr>
<tr>
<td>other</td>
<td>False</td>
<td>25</td>
</tr>
</tbody>
</table>
</div>
<p>Where for each case of <code>main_col</code>, the highest value is scaled to 100 and another value is scaled in respective proportion.</p>
",12308302.0,-1.0,N/A,2022-09-27 14:06:51,Pandas: normalize values by group,<python><pandas><dataframe><data-science><data-wrangling>,2,0,N/A,CC BY-SA 4.0
73892926,1,-1.0,2022-09-29 09:02:08,9,6025,"<p>I am trying to a clear picture of how they are interconnected and if the use of one always require the use of the other. If you could give a non-technical definition or explanation of each of them, I would appreciate it.
Please do not paste a technical definition of the two. I am not a software engineer or data analyst or data engineer.</p>
",18196726.0,-1.0,N/A,2023-06-09 18:09:22,What is the difference between Databricks and Spark?,<database><apache-spark><data-science><azure-databricks>,1,0,N/A,CC BY-SA 4.0
73897472,1,73897511.0,2022-09-29 14:42:49,0,53,"<p>I have been trying for the past two days to sort this out and i can't figure it out.</p>
<p>I have a list of dictionaries that needs to be printed out as a table, the employees need to be ranked by the number of properties sold, highest to lowest.</p>
<pre><code>import tabulate

data_list = [
         {
          'Name': 'John Employee',
          'Employee ID': 12345,
          'Properties Sold': 5,
          'Commission': 2500,
          'Bonus to Commission': 0,
          'Total Commission': 2500
         },
         {
          'Name': 'Allie Employee',
          'Employee ID': 54321,
          'Properties Sold': 3,
          'Commission': 1500,
          'Bonus to Commission': 0,
          'Total Commission': 1500
         },
         {
          'Name': 'James Employee',
          'Employee ID': 23154,
          'Properties Sold': 7,
          'Commission': 3500,
          'Bonus to Commission': 525,
          'Total Commission': 4025
          }
         ]

header = data_list[0].keys()

rows = [x.values() for x in data_list]

print(tabulate.tabulate(rows, header))
</code></pre>
<p>Output:</p>
<pre><code>Name              Employee ID    Properties Sold    Commission    Bonus to Commission    Total Commission
--------------  -------------  -----------------  ------------  ---------------------  ------------------
John Employee           12345                  5          2500                      0                2500
Allie Employee          54321                  3          1500                      0                1500
James Employee          23154                  7          3500                    525                4025
</code></pre>
<p>Output needed:</p>
<pre><code>Name              Employee ID    Properties Sold    Commission    Bonus to Commission    Total Commission
--------------  -------------  -----------------  ------------  ---------------------  ------------------
James Employee          23154                  7          3500                    525                4025
John Employee           12345                  5          2500                      0                2500
Allie Employee          54321                  3          1500                      0                1500
</code></pre>
",15776014.0,-1.0,N/A,2022-09-29 14:48:13,How to rank a list of dictionaries by the highest value of a certain key within the dictionary?,<python><data-science>,2,1,N/A,CC BY-SA 4.0
73857858,1,73858208.0,2022-09-26 17:35:04,0,41,"<p>I want to scrap Name, Phone Number and email from the webpage but it seems like the entire detail is in dictionary which is inside a dictionary someone please correct me I am confused how I am going to extract these values in there particular column. Here is the code</p>
<pre><code>import requests
from bs4 import BeautifulSoup
from csv import writer

url ='https://mainapi.dadeschools.net/api/v1/employees?limit=10&amp;skip=0&amp;sortDesc=false'
R = requests.get(url)

soup = BeautifulSoup(R.text, 'html.parser')
print(soup)
with open('school.csv', 'a', encoding='utf8', newline ='') as f:
thewriter = writer(f)
header = ['Name', 'Location', 'Phone Number', 'Email' ]
thewriter.writerow(header)
thewriter.writerow(soup)
</code></pre>
",20053292.0,14460824.0,2022-09-26 18:07:18,2022-09-26 18:19:44,Want to extract values which is in a webpage as dictionary format using python,<python><json><dictionary><web-scraping><data-science>,1,1,N/A,CC BY-SA 4.0
73867969,1,73869440.0,2022-09-27 12:56:05,1,168,"<p>DataFrame head sample:
<a href=""https://i.stack.imgur.com/cjFx2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cjFx2.png"" alt=""enter image description here"" /></a>
Where I need to create a new feature of the name category which will contain sarcastic category data like data from the last six columns should combine to make the new column and the values in that column should be labeled as per the column's name of the last five columns.</p>
",12799346.0,-1.0,N/A,2022-09-27 14:37:48,How to create a target from multiple columns to train a classification model?,<python><pandas><data-science><data-preprocessing>,1,2,N/A,CC BY-SA 4.0
73870639,1,73870934.0,2022-09-27 16:09:15,-3,85,"<p>Currently my data frame looks something like this</p>
<pre><code>A B C D
1 2 2 3
4 5 5 6
</code></pre>
<p>I want some thing like this</p>
<pre><code>Insert_date  Bulk_load
09-09-2022   [[A, B,C,D],
             [1,2,2,3],
             [4,5,5,6]]
10-09-2022   [[Q,Z,R,F], (This is an example for dataframe with new values.)
             [1,2,2,3],
             [4,5,5,6]]
</code></pre>
<p>Has any one tried to implement this before?</p>
",20102565.0,20102565.0,2022-09-27 17:31:25,2022-09-27 18:03:34,Convert Entire Dataframe to a column and insert into a table,<python><pandas><dataframe><data-science><data-scrubbing>,1,5,N/A,CC BY-SA 4.0
73881550,1,-1.0,2022-09-28 12:35:28,1,17,"<p>I'm looking for a term or technique that might store schema changes over time.  What did we do before Big Data?  Is there something like a &quot;Schema Ledger&quot; that can capture things like &quot;<em>this field was renamed to that field?</em>&quot; Or &quot;<em>this SKU was moved to a different department on July 4th, 1976?</em>&quot;</p>
",16060149.0,-1.0,N/A,2022-09-28 12:35:28,Is there another method of handling Time Travel in Data Science in cases where the underlying system does not have that feature?,<data-science><database-schema>,0,0,N/A,CC BY-SA 4.0
73896387,1,-1.0,2022-09-29 13:28:46,0,603,"<p>Hi I am trying to read a csv file where the floating numbers have comma as decimal separator. When I read the file with Spark, it will just ignore the comma sign and concatenate everything together and I end up with something like:</p>
<p>77563215,23 becomes 7756321523.00.</p>
<p>How can I make sure that while reading it through spark, I get the same number as my original csv file?</p>
",18196726.0,-1.0,N/A,2022-10-17 14:46:22,Numbers with comma as decimal separator in Databricks,<database><apache-spark><apache-spark-sql><data-science><databricks>,1,0,N/A,CC BY-SA 4.0
73910367,1,73910815.0,2022-09-30 14:36:33,0,49,"<p>Hello I am trying to write a chatbot which takes the follow input : name and last name
The idea is that when the user click the &quot;Send&quot; button the input is stored in a &quot;name&quot; variable. The text box is cleared and when the user click the button again the new input should go into another variable &quot;lastname&quot;.
How can I achieve that? I would like to print &quot;Hello name + lastname&quot; inside the chatbot's GUI</p>
<p>This is what I have so far :</p>
<pre><code>from tkinter import *  #GUI library a
import sys

root = Tk() #create a tkinter object which represents the parent window

root.title(&quot;Chat Bot&quot;)
root.geometry(&quot;400x500&quot;)
root.resizable(width=FALSE, height=FALSE)

chatWindow = Text(root, bd=1, bg=&quot;black&quot;,  width=&quot;50&quot;, height=&quot;8&quot;, font=(&quot;Arial&quot;, 12), foreground=&quot;#00ffff&quot;) #create a window for the conversation and place it on the parent window
chatWindow.place(x=6,y=6, height=385, width=370)

messageWindow = Text(root, bd=0, bg=&quot;black&quot;,width=&quot;30&quot;, height=&quot;4&quot;, font=(&quot;Arial&quot;, 23), foreground=&quot;#00ffff&quot;) #create the text area where the message will be entered and place it on the parent window
messageWindow.place(x=128, y=400, height=88, width=260)

def get():
    name = messageWindow.get('1.0', END) #&quot;1.0&quot; means that the input should be read from line one, character zero (ie: the very first character). END is an imported constant which is set to the string &quot;end&quot;. The END part means to read until the end of the text box is reached.
    messageWindow.delete('1.0', END)

Button= Button(root, text=&quot;Send&quot;, command = get, width=&quot;12&quot;, height=5,
                    bd=0, bg=&quot;#0080ff&quot;, activebackground=&quot;#00bfff&quot;,foreground='#ffffff',font=(&quot;Arial&quot;, 12))
Button.place(x=6, y=400, height=88) #create button to send the message and place it in the parent window


#Whenever I am calling print) I am actually calling sys.stdout.write() so with this function redirector which redirect the print() to the GUI
def redirector(inputStr):
    chatWindow.insert(INSERT, inputStr)

sys.stdout.write = redirector #whenever sys.stdout.write is called, redirector is called.

print(&quot;Hello, I am your awesome assistant today. \n&quot;)
print(&quot;Please enter your name and last name. \n&quot;)
root.mainloop()
</code></pre>
",14729063.0,-1.0,N/A,2022-09-30 15:15:46,Everytime button is clicked store value in a different variable python tkinter,<python><tkinter><data-science>,1,0,N/A,CC BY-SA 4.0
73802995,1,-1.0,2022-09-21 14:59:18,1,250,"<p>How do I detect multivariate outliers within large data with more than 50 variables. Do i need to plot all of the variables or do i have to group them based independent and dependent variables or do i need an algorithm for this?</p>
",20053157.0,20053157.0,2022-09-21 15:00:09,2022-09-21 18:54:39,How to detect multivariate outliers within large dataset?,<data-science><large-data><outliers>,1,1,N/A,CC BY-SA 4.0
73803236,1,73803408.0,2022-09-21 15:16:49,0,23,"<p>Hello I am new to the field of Data science &amp; currently I am doing project of web scraping. I comes to a problem where I getting error of 'none type object has no attribute text'.I just print out the funtion output without the '.text' getting none value as output anyone please check the code and see where I am going wrong I already put tag and class accordingly.</p>
<pre><code>import requests
from bs4 import BeautifulSoup
from csv import writer
url = 'https://www.99acres.com/search/property/buy/pune? city=19&amp;preference=S&amp;area_unit=1&amp;res_com=R'
R = requests.get(url)
soup = BeautifulSoup(R.content, 'html.parser')
lists = soup.find_all('table', class_='srpTuple__tableFSL')

for list in lists:
   title = list.find('h2', class_='srpTuple__tupleTitleOverflow').text
   Location = list.find('a', class_='srpTuple__dFlex').text
   price = list.find('td', class_ = 'srpTuple__col title_semiBold ')
   info = [title,Location,price]
   print(info)
</code></pre>
<p>Here is the Output :</p>
<pre><code>['2 BHK Apartment in Moshi', 'Flower City', None]
['3 BHK Apartment in Kondhwa', 'Mannat Towers', None]
</code></pre>
",20053292.0,-1.0,N/A,2022-09-21 15:29:04,Getting None value after I put '.text' in 'find_all' funtion even though I set the tag and class correctly,<python><web-scraping><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
73886664,1,-1.0,2022-09-28 19:07:12,0,59,"<p>I tried to analize this code <a href=""https://www.kaggle.com/code/livandiogenskiy/start-on-titanic"" rel=""nofollow noreferrer"">Titanic Problem link</a></p>
<p>And when i launch it on Jupyter notebook it cout me this:</p>
<p><a href=""https://i.stack.imgur.com/305l1.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/305l1.png</a> --- code is 10-th Karnel on first  link ↑</p>
<p><a href=""https://i.stack.imgur.com/axVML.png"" rel=""nofollow noreferrer"">enter image description here</a> --- also picture of the code</p>
",19607175.0,19607175.0,2022-09-28 19:10:48,2022-09-28 19:10:48,"in pandas._libs.index.IndexEngine.get_loc(), in pandas._libs.index.IndexEngine.get_loc() , in pandas._libs.index.IndexEngine.get_loc()",<python><jupyter-notebook><data-science><kaggle>,0,2,N/A,CC BY-SA 4.0
71156615,1,-1.0,2022-02-17 10:44:11,0,609,"<p>I want to use breusch_pagan test in statsmodels, but i facea strange error:</p>
<pre><code>print(het_breuschpagan(resid=lr.resid,exog_het=df['iq']))
  File &quot;C:\Users\aleks\PycharmProjects\statistics\venv\lib\site-packages\statsmodels\stats\diagnostic.py&quot;, line 810, in het_breuschpagan
    nobs, nvars = x.shape
ValueError: not enough values to unpack (expected 2, got 1)
</code></pre>
<p>linear model:</p>
<pre><code>lr = sm.OLS.from_formula('lw80~age80+iq+school80+expr80',df).fit()
</code></pre>
<p>Dataset:</p>
<pre><code>  rns rns80  mrt mrt80 smsa  ...  expr80  tenure  tenure80     lw   lw80
0  no    no   no   yes  yes  ...  10.635       0         2  5.900  6.645
1  no    no   no   yes  yes  ...  11.367       2        16  5.438  6.694
2  no    no   no   yes  yes  ...  11.035       1         9  5.710  6.715
3  no    no   no   yes  yes  ...  13.089       1         7  5.481  6.477
4  no    no  yes   yes  yes  ...  14.402       3         5  5.927  6.332
</code></pre>
<p>I would like to test wether there is a heteroskadicity assuming that the variable leading to heteroskadicity is iq and only iq, but I don't know what parameters to use so that there are no errors</p>
",17417282.0,-1.0,N/A,2022-08-09 20:07:57,How to perform breusch-pagan test in statsmodels?,<python><statistics><data-science><data-analysis><statsmodels>,1,2,N/A,CC BY-SA 4.0
71156668,1,71156697.0,2022-02-17 10:47:36,0,109,"<p>I replicated a Pandas series with the following code:</p>
<pre class=""lang-py prettyprint-override""><code>data = np.array([1, 2, 3, 4, 5, np.nan, np.nan, np.nan, 9,10,11,12,13,14])
  
ser = pd.Series(data)
print(ser)
</code></pre>
<p>I would like to select only the columns before the NaN values so that I only get the values 1,2,3,4,5. How should I do that?</p>
",12966950.0,-1.0,N/A,2022-02-17 11:06:13,How to delete rows in Pandas after a certain value?,<python><pandas><data-science><series>,2,0,N/A,CC BY-SA 4.0
73855669,1,73857906.0,2022-09-26 14:30:22,1,244,"<p>In Neo4j, I want to create a query that returns nodes grouped on common property without altering my graph.
For example, if I have the following graph where each person has a property corresponding to his department in the company:
<a href=""https://i.stack.imgur.com/mG4Zw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mG4Zw.png"" alt=""enter image description here"" /></a></p>
<p>I want my query to return a graph like the following, where the nodes are replaced by the property and where the weight of the edges corresponds to the number of relations:</p>
<p><a href=""https://i.stack.imgur.com/cJx3Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cJx3Y.png"" alt=""enter image description here"" /></a></p>
<p>Do you have any idea how to proceed? I looked at APOC nodes collapse without success.</p>
<p>Thank you</p>
",11509861.0,-1.0,N/A,2022-09-26 17:39:08,Neo4j group nodes with property,<graph><neo4j><graph-databases><neo4j-apoc><graph-data-science>,3,0,N/A,CC BY-SA 4.0
73862661,1,-1.0,2022-09-27 05:13:31,1,51,"<pre><code>A  B
t  f
t  t
f  f
t  t
</code></pre>
<p>How to convert this kind of data in numeric form in python for multiple columns?</p>
",20081065.0,9473764.0,2022-09-27 05:18:20,2022-09-27 11:15:23,How can I convert t and f values in numeric in python Dataframe?,<python><python-3.x><pandas><data-science><one-hot-encoding>,3,0,N/A,CC BY-SA 4.0
73883591,1,73884142.0,2022-09-28 14:51:59,-1,25,"<p>I am trying to train and fit a classifier, and then use it to make a prediction, based on a combination of numeric data and labeled data.</p>
<p>I am trying to predict the <code>price</code> of a vehicle, based on these prediction variables.</p>
<pre><code>prediction_values = [2, 164, 'audi', 'gas', 'std', 'four', 'sedan', 'fwd', 'front', 99.8, 176.6, 66.2, 54.3, 2337, 'ohc', 'four', 109, 'mpfi', 3.19, 3.4, 10, 102, 5500, 30]
</code></pre>
<p>Here is my code.</p>
<pre><code>import pandas as pd
import numpy as np

# Load Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_moons
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier# Step1: Create data set


# Define the headers since the data does not have any
headers = [&quot;symboling&quot;, &quot;normalized_losses&quot;, &quot;make&quot;, &quot;fuel_type&quot;, &quot;aspiration&quot;,
           &quot;num_doors&quot;, &quot;body_style&quot;, &quot;drive_wheels&quot;, &quot;engine_location&quot;,
           &quot;wheel_base&quot;, &quot;length&quot;, &quot;width&quot;, &quot;height&quot;, &quot;curb_weight&quot;,
           &quot;engine_type&quot;, &quot;num_cylinders&quot;, &quot;engine_size&quot;, &quot;fuel_system&quot;,
           &quot;bore&quot;, &quot;stroke&quot;, &quot;compression_ratio&quot;, &quot;horsepower&quot;, &quot;peak_rpm&quot;,
           &quot;city_mpg&quot;, &quot;highway_mpg&quot;, &quot;price&quot;]

# Read in the CSV file and convert &quot;?&quot; to NaN
df = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data&quot;,
                  header=None, names=headers, na_values=&quot;?&quot; )
df.head()

df.columns

df_fin = pd.DataFrame({col: df[col].astype('category').cat.codes for col in df}, index=df.index)
df_fin


X = df_fin[[&quot;symboling&quot;, &quot;normalized_losses&quot;, &quot;make&quot;, &quot;fuel_type&quot;, &quot;aspiration&quot;,
           &quot;num_doors&quot;, &quot;body_style&quot;, &quot;drive_wheels&quot;, &quot;engine_location&quot;,
           &quot;wheel_base&quot;, &quot;length&quot;, &quot;width&quot;, &quot;height&quot;, &quot;curb_weight&quot;,
           &quot;engine_type&quot;, &quot;num_cylinders&quot;, &quot;engine_size&quot;, &quot;fuel_system&quot;,
           &quot;bore&quot;, &quot;stroke&quot;, &quot;compression_ratio&quot;, &quot;horsepower&quot;, &quot;peak_rpm&quot;,
           &quot;city_mpg&quot;, &quot;highway_mpg&quot;]]

y = df_fin[&quot;price&quot;]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit a Decision Tree model
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy_score(y_test, y_pred)


# create a map of your columns values with the corresponding categorical values
col_dictionary = {}
for col in df:
    dictionary = dict(enumerate(df[col].astype('category').cat.categories))
    col_dictionary[col] = {v: k for k, v in dictionary.items()}


# then use this map to convert the array you want to predict
prediction_values = [2, 164, 'audi', 'gas', 'std', 'four', 'sedan', 'fwd', 'front', 99.8, 176.6, 66.2, 54.3, 2337, 'ohc', 'four', 109, 'mpfi', 3.19, 3.4, 10, 102, 5500, 30]
to_predict = []
for (column, value) in zip(X.columns, prediction_values):
    to_predict.append(col_dictionary[column][value])
to_predict_df = pd.DataFrame([to_predict], columns=X.columns)
clf.predict([to_predict_df.iloc[0].values])
</code></pre>
<p>When I run the code, I get this error.</p>
<pre><code>The above exception was the direct cause of the following exception:

Traceback (most recent call last):

  Input In [101] in &lt;cell line: 5&gt;
    to_predict_df = pd.DataFrame([to_predict], columns=X.columns)

  File ~\AppData\Roaming\Python\Python39\site-packages\pandas\core\frame.py:570 in __init__
    arrays, columns = to_arrays(data, columns, dtype=dtype)

  File ~\AppData\Roaming\Python\Python39\site-packages\pandas\core\internals\construction.py:528 in to_arrays
    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)

  File ~\AppData\Roaming\Python\Python39\site-packages\pandas\core\internals\construction.py:571 in _list_to_arrays
    raise ValueError(e) from e

ValueError: 25 columns passed, passed data had 24 columns
</code></pre>
",5212614.0,-1.0,N/A,2022-09-28 15:32:19,How can we use a Classifier to Make a Prediction based on Numeric Data and Labeled Data?,<python><python-3.x><machine-learning><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
73900116,1,-1.0,2022-09-29 18:26:20,0,88,"<p>Is it possible to understand the new dataset features obtained after applying PCA transformation to it?
clarification:
A dataset contains only numerical input variables resulting from a PCA transformation,
Features V1, V2, … V28 are the principal components obtained with PCA
it there any possibility of understanding the Features V1, V2, … V28 !!!</p>
<p>the dataset link:<a href=""https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud</a></p>
",14769444.0,-1.0,N/A,2022-09-29 18:26:20,Understanding transformed data (PCA transformation ),<python><data-science><data-analysis><pca>,0,4,N/A,CC BY-SA 4.0
71153067,1,-1.0,2022-02-17 05:49:11,-1,166,"<p>why i get this error while auto tuning my xgboost algorithm,in amazon sagemaker&quot;Error for HyperParameterTuning job sagemaker-xgboost-220217-0532: Failed. Reason: All training jobs failed. Please take a look at the training jobs failures to get more details.&quot;</p>
",18230273.0,-1.0,N/A,2022-02-22 00:15:23,HyperParameterTuning job failed,<amazon-web-services><machine-learning><data-science><amazon-sagemaker>,1,0,N/A,CC BY-SA 4.0
71155959,1,71161289.0,2022-02-17 10:01:04,0,690,"<p><strong>Background</strong>: In my projects I'm using GIT and <a href=""https://dvc.org/"" rel=""nofollow noreferrer"">DVC</a> to keep track of versions:</p>
<ul>
<li>GIT - only for source codes</li>
<li>DVC - for dataset, model objects and outputs</li>
</ul>
<p>I'm testing different approaches in separate branches, i.e:</p>
<ul>
<li>random_forest</li>
<li>neural_network_1</li>
<li>...</li>
</ul>
<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:</p>
<ul>
<li>ID</li>
<li>Prediction</li>
</ul>
<p><strong>Question</strong>: What is the best way to merge those prediction files into single big file?</p>
<p>I would like to obtain a file with structure:</p>
<ul>
<li>ID</li>
<li>Prediction_random_forest</li>
<li>Prediction_neural_network_1</li>
<li>Prediction_...</li>
</ul>
<p>My main issue is how to access files with predictions which are in different branches?</p>
",269758.0,298182.0,2022-02-17 21:13:50,2022-02-17 21:13:50,How to merge data (CSV) files from multiple branches (Git and DVC)?,<git><data-science><dvc>,1,5,N/A,CC BY-SA 4.0
71155567,1,-1.0,2022-02-17 09:35:04,0,252,"<p><strong>How can I do sentence to sentence mapping?</strong></p>
<p>example: If you have an input text &quot;The price of orange has increased&quot; and output text &quot;Increase the production of orange&quot;</p>
<p>So should I convert into vector then use any algorithm or cosine similarity</p>
",18232108.0,-1.0,N/A,2022-04-06 07:13:42,How to map input sentence to output sentence in NLP,<python><machine-learning><nlp><data-science><word-embedding>,1,1,N/A,CC BY-SA 4.0
71163113,1,71163610.0,2022-02-17 17:54:21,0,36,"<p>I have a dataframe that is 49x10, and I need to find the percent change for each index item to a specific column in that row, for each index item.</p>
<p>Here is a sample of the dataframe being 5x10.</p>
<pre><code>        -15         -10         -5          -2          -1          0           1           2           5           10          15
symbol                                          
ADS     80.098282   77.709549   72.130524   69.178314   72.947937   74.548950   75.364349   77.907074   79.691963   73.998260   67.965523
BEPC    49.635834   48.256989   45.957493   45.627144   47.968739   49.610779   48.056187   49.659355   56.013733   56.412094   59.579567
CBSH    70.400002   69.800003   70.533333   66.476189   65.250000   66.910004   66.180000   68.809998   68.089996   68.839996   68.110001
CIG     2.014711    2.097131    2.225340    2.371864    2.280286    2.089544    2.070374    2.127884    2.127884    2.204564    2.377095
CPE     12.200000   13.100000   11.800000   11.100000   10.400000   10.690000   10.050000   10.040000   8.340000    6.920000    6.630000
</code></pre>
<p>Essentially, I for column 0, I am trying to find the percent change centered around that date.
which I have done with:</p>
<pre><code>for col in df.columns:
    df[col] = (df[col] - df['0'])/df['0']*100
</code></pre>
<p>however, that returns a dataframe that looks like this:</p>
<pre><code>        -15         -10         -5          -2          -1          0   1   2   5   10  15
symbol                                          
ADS     7.443876    4.239629    -3.244079   -7.204174   -2.147600   0.0 inf inf inf inf inf
BEPC    0.050503    -2.728823   -7.363896   -8.029777   -3.309846   0.0 inf inf inf inf inf
CBSH    5.215958    4.319234    5.415228    -0.648356   -2.480950   0.0 inf inf inf inf inf
CIG     -3.581304   0.363103    6.498839    13.511108   9.128440    0.0 inf inf inf inf inf
CPE     14.125353   22.544442   10.383542   3.835368    -2.712815   0.0 inf inf inf inf inf
</code></pre>
<p>The script is modifying <code>df['0']</code> but I'm not sure how to find a workaround for it. The expected outcome would be percent change from <code>df['0']</code>, as what happened before it iterated through the 0 placement.</p>
<p>Any help would be appreciated.</p>
",14847960.0,14847960.0,2022-02-17 18:03:17,2022-02-17 18:34:18,how do I find the percent change from column X in dataframe?,<python><pandas><numpy><data-science>,2,0,N/A,CC BY-SA 4.0
71170441,1,71170476.0,2022-02-18 08:29:56,0,220,"<p>I am trying to get the next month first date based on billDate in a dataframe.</p>
<p>I did this:</p>
<pre><code>import pandas as pd
import datetime
from datetime import timedelta
dt = pd.to_datetime('15/4/2019', errors='coerce')
print(dt)
print((dt.replace(day=1) + datetime.timedelta(days=32)).replace(day=1))
</code></pre>
<p>It is working perfectly, and the output is :</p>
<pre><code>2019-04-15 00:00:00
2019-05-01 00:00:00
</code></pre>
<p>Now, I am applying same logic in my dataframe in the below code</p>
<pre><code>df[comNewColName] = (pd.to_datetime(df['billDate'], errors='coerce').replace(day=1) + datetime.timedelta(days=32)).replace(day=1)
</code></pre>
<p>But I am getting error like this:</p>
<pre><code>---&gt; 69                 df[comNewColName] = (pd.to_datetime(df['billDate'], errors='coerce').replace(day=1) + datetime.timedelta(days=32)).replace(day=1)
     70                 '''print(df[['billDate']])'''
     71                 '''df = df.assign(Product=lambda x: (x['Field_1'] * x['Field_2'] * x['Field_3']))'''
</code></pre>
<blockquote>
<p>TypeError: replace() got an unexpected keyword argument 'day'</p>
</blockquote>
",3655069.0,472495.0,2022-02-23 22:00:37,2022-02-23 22:00:37,Pandas dataframe timedelta is giving exceptions,<python><pandas><data-science>,1,2,N/A,CC BY-SA 4.0
71160647,1,71160851.0,2022-02-17 15:11:05,0,106,"<p>What is the best/fastest way to do a selective lookup/cross-reference/overlay/partial join between two Pandas DataFrames? I'm not sure of the right terminology to use....</p>
<p>Given:</p>
<ol>
<li>A primary table filled with numerical values and some arbitrary lookup/reference strings, indexed by date/month</li>
<li>A secondary table, with a consistent index to the first. Columns of this dataframe match with the arbitrary lookup/reference strings in the primary table which can change over time, and serve as the lookup column to replace values in the primary table.</li>
</ol>
<p>The indices of the two tables don't necessarily need to be exactly the same set (or even contiguous), but if a lookup is present in <strong>Table 1</strong> it obviously needs a corresponding row in <strong>Table 2</strong>.</p>
<p>i.e. I'd like to find every lookup/placeholder in <strong>Table 1</strong> and replace with the correct lookup value in <strong>Table 2</strong> from the correct month.</p>
<p>Example below:</p>
<p><strong>Table 1</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>dte</th>
<th>value1</th>
<th>value2</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2022-02</td>
<td>3</td>
<td>LOOKUP1</td>
</tr>
<tr>
<td>2022-03</td>
<td>LOOKUP3</td>
<td>4</td>
</tr>
<tr>
<td>2022-04</td>
<td>5</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 2</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>dte</th>
<th>LOOKUP1</th>
<th>LOOKUP2</th>
<th>LOOKUP3</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-12</td>
<td>101</td>
<td>105</td>
<td>109</td>
</tr>
<tr>
<td>2022-02</td>
<td>102</td>
<td>106</td>
<td>110</td>
</tr>
<tr>
<td>2022-03</td>
<td>103</td>
<td>107</td>
<td>111</td>
</tr>
<tr>
<td>2022-10</td>
<td>104</td>
<td>108</td>
<td>112</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Result</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>dte</th>
<th>value1</th>
<th>value2</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2022-02</td>
<td>3</td>
<td>102</td>
</tr>
<tr>
<td>2022-03</td>
<td>111</td>
<td>4</td>
</tr>
<tr>
<td>2022-04</td>
<td>5</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
<p>I need to do a pile of these in my use case, so was hoping for something at least reasonably fast. I'm not an expert with Pandas, wondering if there is a smart way to do this. I could loop through every row/column in <strong>Table 1</strong>, look for values that match column names in <strong>Table 2</strong> and then do a lookup in <strong>Table 2</strong> replacing value in <strong>Table 1</strong>. There seems to often be a smart way to do this kind of thing in Pandas where it automatically paralellizes for you. Any help would be appreciated!</p>
",1089153.0,-1.0,N/A,2022-02-17 16:14:07,Pandas DataFrame Cross-Reference/Selective Join/Overlay?,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
73913138,1,-1.0,2022-09-30 19:16:45,0,45,"<p>i have a pivot table</p>
<p><a href=""https://i.stack.imgur.com/v7brP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v7brP.png"" alt=""enter image description here"" /></a></p>
<p>and i want to make an image of this pivot table in this manner</p>
<p><a href=""https://i.stack.imgur.com/MJB5d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MJB5d.png"" alt=""enter image description here"" /></a></p>
<p>the colors are for each label</p>
<p>how do i create the latter image? I am working on jupyter notebook.</p>
",16502243.0,-1.0,N/A,2022-09-30 19:16:45,make an image of pivot table,<python><data-science><visualization>,0,2,N/A,CC BY-SA 4.0
71179543,1,71180317.0,2022-02-18 20:34:05,1,49,"<p>So, I have a data frame like this (the important column is the third one):</p>
<pre><code>   |  ABC  |  DEF  |  fruit |
----------------------------
1  |  12   |  LO   | banana
2  |  45   |  KA   | orange
3  |  65   |  JU   | banana
4  |  25   |  UY   | grape
5  |  23   |  TE   | apple
6  |  28   |  YT   | orange
7  |  78   |  TR   | melon
</code></pre>
<p>I want to keep the rows that have the 5 most occurring  fruits and drop the rest, so I made a variable to hold those fruits to keep in a list, like this:</p>
<pre><code>fruits = df['fruit'].value_counts()
fruits_to_keep = fruits[:5].reset_index()
fruits_to_keep.drop(['fruit'], inplace=True, axis=1)
fruits_to_keep = fruits_to_keep.to_numpy()
fruits_to_keep = fruits_to_keep.tolist()
fruits_to_keep

[['banana'],['orange'],[apple],[melon],[grape]]
</code></pre>
<p>I have the feeling that I made unnecessary steps, but anyway, the problem arises when I try to select the rows containing those fruits_to_keep</p>
<pre><code>df = df.set_index('fruit')
df = df.loc[fruits_to_keep,:]
</code></pre>
<p>Then I get the Key Error saying that &quot;None of [Index([('banana',), \n   ('orange',), \n  ('apple',)...... dtype='object', name='fruit')] are in the [index]&quot;</p>
<p>I also tried:</p>
<pre><code>df[df.fruit in fruits_to_keep]
</code></pre>
<p>But then I get the following error:
('Lengths must match to compare', (43987,), (1,))</p>
<p>Obs.: I actually have 43k rows, many 'fruits' that I don't want on the dataframe and 30k+ rows with the 5 most occurring 'fruits'</p>
<p>Thanks in advance!</p>
",8162202.0,-1.0,N/A,2022-02-18 21:57:06,Pandas - Keeping rows with determined values and droping the rest,<python><pandas><dataframe><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
71176019,1,-1.0,2022-02-18 15:33:36,0,96,"<p>I am a beginner in data science, and programming I am trying to do a program but I don't understand the question.</p>
<p>the question - you should write a function that takes train and test data as NumPy ndarrays, and a k-value as an integer, and returns the class-values of the test data.</p>
<p>I understood that I need to write a function that takes test, train,k-values. but what does returning the classvalue of test data means?</p>
<p><a href=""https://i.stack.imgur.com/RMasc.png"" rel=""nofollow noreferrer"">Screenshot</a></p>
<p>Thanks</p>
",18245183.0,901925.0,2022-02-18 18:02:07,2022-02-18 18:02:07,What is Classvalue in KNN?,<python><pandas><numpy><data-science><knn>,0,6,N/A,CC BY-SA 4.0
71178313,1,-1.0,2022-02-18 18:33:00,0,481,"<p>I've recently been trying to implement the default reg:squarederror loss function for xgboost regression, to enable me to later change it to an asymmetrical function on the basis of this function. However, I've not been able to get the same results with my custom version compared to the default implementation.</p>
<p>Here's the code I've been trying:</p>
<pre><code>import xgboost as xgb
import numpy as np
import pandas as pd

a = np.array([1,2,3,4,5,6])
b = np.array([2,3,4,5,6,7])

a = pd.DataFrame(data=a)
b = pd.DataFrame(data=b)
model = xgb.XGBRegressor(random_state=0, objective='reg:squarederror')
model.fit(a, b)
print(model.predict(a))

def squared_error(predt: np.ndarray, dtrain: xgb.DMatrix):
    y = dtrain.get_label()
    grad = predt - y
    hess = np.ones(predt.shape)
    return grad, hess


dtrain = xgb.DMatrix(a.values, label=b.values)
dtest = xgb.DMatrix(a.values)

model2 = xgb.train({'seed': 0}, dtrain=dtrain, obj=squared_error)
print(model2.predict(dtest))
</code></pre>
<p>The problem is that the two models don't give the same results.
Any ideas what's wrong with my code?</p>
<p>I've also tried the same with reg:squaredlogerror and the given example (<a href=""https://xgboost.readthedocs.io/en/stable/tutorials/custom_metric_obj.html"" rel=""nofollow noreferrer"">https://xgboost.readthedocs.io/en/stable/tutorials/custom_metric_obj.html</a>), which gave the same result for both models. This leads me to believe that there is a problem in my code.</p>
<p>I'd appreciate any help in finding my mistake.</p>
<p>-Timo</p>
",13063671.0,-1.0,N/A,2022-11-04 09:02:46,XGBoost custom squarederror loss function not working similar to default implementation,<python><machine-learning><regression><data-science><xgboost>,1,0,N/A,CC BY-SA 4.0
71180077,1,-1.0,2022-02-18 21:31:45,0,189,"<p>If I'm working with a table like so as a Spark dataframe:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>received</th>
<th>userId</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01-07  06:23:02</td>
<td>se23289</td>
</tr>
<tr>
<td>2022-01-03  22:21:33</td>
<td>se23289</td>
</tr>
<tr>
<td>2022-01-16  18:01:45</td>
<td>se12355</td>
</tr>
<tr>
<td>2022-01-11  02:35:23</td>
<td>se23289</td>
</tr>
<tr>
<td>2022-01-13  05:24:21</td>
<td>se12355</td>
</tr>
</tbody>
</table>
</div>
<p>How would I go about dropping duplicates or repeated row occurrences of userId based upon the first or earliest date they are seen in the table? Been struggling with this for a bit.</p>
<p>New to spark. I understand i can do drop.where(), but that requires a specific comparison. Where this is not that simple. Any tips much appreciated.</p>
",7128040.0,16930243.0,2022-02-18 21:36:24,2022-02-20 05:33:37,Dropping duplicate spark data frame rows based on the first occurrence of schema values,<python><pyspark><jupyter-notebook><data-science>,3,0,N/A,CC BY-SA 4.0
71188568,1,71211926.0,2022-02-19 19:44:15,0,194,"<p>I created a machine learning model to predict the daily rate for short term rentals. I have about two thousand rows of csv data about short term rentals with a large amount of features.</p>
<p>However, the only features that impact the predicted daily rate are the # of bedrooms and the property type (condo, house, etc.). This made sense to me at first, until I saw that removing those features from the model decreased the R2. It also seems strange that a property with a pool as an amenity would not charge more. I have tried changing the estimators and some feature engineering to increase the R2 to no avail.</p>
<p>Can someone explain why my model would have lower R2 for features that don't affect the predictions? What other options do I have to increase my accuracy?</p>
",3944602.0,-1.0,N/A,2022-02-21 19:22:56,Why do features increase R2 but not affect predictions?,<machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
71154201,1,-1.0,2022-02-17 07:48:15,0,72,"<p>i was make a program in python for desktop application. and create a login form for employee and after login i was create a schedule call a api for check employee is login or not and open a pop up message for Acknowledgement for login employee.</p>
<p>my problem is when schedule start so i can so stop the code</p>
<pre><code>import PySimpleGUI as sg
import requests
from psgtray import SystemTray
import json
import schedule
from schedule import *
import gtts
import threading
import time


def make_sch(user_id: object, emp_name: object):
    schedule.every(4).seconds.do(func_msg, user_id, emp_name)
    while True:
        schedule.run_pending()
        time.sleep(4)


def func_msg(user_id: object, emp_name: object):
    response_msg = '{ &quot;msg&quot;: &quot;Data Found&quot;,&quot;status&quot;: 1,&quot;Data&quot;: &quot;hello team&quot;,&quot;id&quot;: 2}'
    response_msg_json: object = json.loads(response_msg)
    print(response_msg_json)
    if str(response_msg_json['status']) == '1' and str(response_msg_json['id']) != '':
        sg.theme('DarkTeal9')  # Add a touch of color
        layout = [[sg.Text(response_msg_json['Data'])],
                  [sg.Text('Enter your Acknowledge'), sg.Multiline(size=(50, 5), key='textbox')],
                  [sg.Button('Enter'), sg.Button('Cancel')],
                  [sg.Text(size=(40, 1), key='ACK_OUTPUT')]]

        # Create the Window
        window = sg.Window('Team Alert', layout)
        # Event Loop to process &quot;events&quot; and get the &quot;values&quot; of the inputs
        while True:
            event, values = window.read()
            if event == sg.WIN_CLOSED or event == 'Cancel':  # if user closes window or clicks cancel
                break
            elif event == 'Enter' and values['textbox'] == '':
                window['ACK_OUTPUT'].update('Please enter your Acknowledgement', text_color='red')
            else:
                window['ACK_OUTPUT'].update('', text_color='red')
                response_update = '{&quot;msg&quot;: &quot;Data Found&quot;,&quot;status&quot;: 1}'
                response_update_json: object = json.loads(response_update)
                sg.Popup('Thanks For Your Acknowledgement!!')
                window.close()

        window.close()


def main():
    menu = ['',
            ['Show Window', 'Hide Window', '---', '!Disabled Item', 'Exit']]
    tooltip = 'Team Alert'
    sg.theme('DarkTeal9')
    layout = [[sg.Text(&quot;* Enter Your Id&quot;)],
              [sg.Input(key='emp_id')],
              [sg.Text(size=(40, 1))],
              [sg.Text(&quot;* Enter Your Password&quot;)],
              [sg.Input(key='emp_pass')],
              [sg.Text(size=(40, 1), key='-OUTPUT-')],
              [sg.Button('Login')]]

    window = sg.Window('Team Alert', layout, finalize=True, enable_close_attempted_event=True)
    tray = SystemTray(menu, single_click_events=True, window=window, tooltip=tooltip)
    tray.show_message('Team Alert', 'Team Alert Started!')
    sg.cprint(sg.get_versions())
    while True:
        event, values = window.read()
        if values['emp_id'] and values['emp_pass']:
            response = '{ &quot;msg&quot;: &quot;Sucessfully login&quot;, &quot;status&quot;: 1}'
            login_data_json: object = json.loads(response)
            if str(login_data_json['status']) == '1':
                window.hide()
                tray.show_icon()
                window['-OUTPUT-'].update(login_data_json['msg'], text_color='yellow')
                emp_id = values['emp_id']
                emp_name = 'emp'
                make_sch(emp_id, emp_name)
            else:
                window['-OUTPUT-'].update(login_data_json['msg'], text_color='red')
            if event == 'Login':
                sg.Popup(login_data_json['msg'])
        else:
            window['-OUTPUT-'].update('** Username and Password is required', text_color='red')
        if event == tray.key:
            sg.cprint(f'Team Alert Event = ', values[event], c='white on red')
            event = values[event]
        if event in (sg.WIN_CLOSED, 'Exit'):
            sg.Popup(&quot;You are Logout!!&quot;)
            break

        sg.cprint(event, values)
        tray.show_message(title=event, message=values)

        if event in ('Show Window', sg.EVENT_SYSTEM_TRAY_ICON_DOUBLE_CLICKED):
            window.un_hide()
            window.bring_to_front()
        elif event in ('Hide Window', sg.WIN_CLOSE_ATTEMPTED_EVENT):
            window.hide()
            tray.show_icon()
        elif event == 'Hide Icon':
            tray.hide_icon()
        elif event == 'Show Icon':
            tray.show_icon()
        elif event == 'Change Tooltip':
            tray.set_tooltip(values['-IN-'])

    tray.close()
    window.close()

    print('this is event', event)


if __name__ == '__main__':
    main()

</code></pre>
",17258799.0,17258799.0,2022-02-24 08:25:45,2022-02-24 08:25:45,with python schedule tray can't open and can't perform any action,<python><data-science>,1,6,N/A,CC BY-SA 4.0
71178927,1,71179464.0,2022-02-18 19:31:28,0,515,"<p>I am attempting to relearn data-science in rust.</p>
<p>I have a <code>Vec&lt;String&gt;</code> that includes a delimiter &quot;|&quot; and a new line &quot;!end&quot;.</p>
<p>What I'd like to end up with is <code>Vec&lt;Vec&lt;String&gt;&gt;</code> that can be put into a 2D ND array.</p>
<p>I have this python Code:</p>
<pre><code>file = open('somefile.dat')
lst = []
for line in file:
    lst += [line.split('|')]
    
df = pd.DataFrame(lst)
SAMV2FinalDataFrame = pd.DataFrame(lst,columns=column_names)
</code></pre>
<p>And i've recreated it here in rust:</p>
<pre><code>

fn lines_from_file(filename: impl AsRef&lt;Path&gt;) -&gt; Vec&lt;String&gt; {
    let file = File::open(filename).expect(&quot;no such file&quot;);
    let buf = BufReader::new(file);
    buf.lines()
        .map(|l| l.expect(&quot;Could not parse line&quot;))
        .collect()
}

fn main() {
    let lines = lines_from_file(&quot;.dat&quot;);
    let mut new_arr = vec![];
//Here i get a lines immitable borrow
    for line in lines{
        new_arr.push([*line.split(&quot;!end&quot;)]);
    }

// here i get expeected closure found str
let x = lines.split(&quot;!end&quot;);



let array = Array::from(lines)

</code></pre>
<p>what i have: ['1','1','1','end!','2','2','2','!end']
What i need: [['1','1','1'],['2','2','2']]</p>
<p>Edit: also why when i turbo fish does it make it disappear on Stack Overflow?</p>
",12957674.0,3005167.0,2022-02-18 19:33:21,2022-02-18 20:49:05,Splitting a Vec of strings into Vec<Vec<String>>,<rust><data-science><rust-ndarray>,1,8,N/A,CC BY-SA 4.0
71190539,1,71191061.0,2022-02-20 01:01:21,0,71,"<p>I  have a set of data where I want to do, If the latest <strong>st_1</strong> or <strong>st_2</strong> are greater than earlier <strong>st_1</strong> or  <strong>st_2</strong> put True or False respectively in another column. How can I do that on the basis of <strong>date</strong> and <strong>id</strong>?</p>
<pre><code>id  date                        st_1    st_2
1   2022-02-28 00:00:00+00:00   60.0    6.0
2   2021-10-31 00:00:00+00:00   70.0    0.0
2   2021-12-31 00:00:00+00:00   70.0    4.0
3   2021-10-31 00:00:00+00:00   60.0    0.0
4   2021-06-30 00:00:00+00:00   63.3    2.66
4   2021-08-31 00:00:00+00:00   60.0    3.0
4   2022-02-28 00:00:00+00:00   70.0    2.0
5   2021-06-30 00:00:00+00:00   70.0    3.0
4   2022-02-28 00:00:00+00:00   70.0    2.0
5   2021-06-30 00:00:00+00:00   70.0    3.0
5   2021-08-31 00:00:00+00:00   80.0    2.0
5   2021-10-31 00:00:00+00:00   70.0    3.5
</code></pre>
<p>My expected outcome:</p>
<pre><code>id  date                        st_1    st_2  outcome
1   2022-02-28 00:00:00+00:00   60.0    6.0   false
2   2021-10-31 00:00:00+00:00   70.0    0.0   false
2   2021-12-31 00:00:00+00:00   70.0    4.0   true
3   2021-10-31 00:00:00+00:00   60.0    0.0   false
4   2021-06-30 00:00:00+00:00   63.3    2.66  false
4   2021-08-31 00:00:00+00:00   60.0    3.0   true
4   2022-02-28 00:00:00+00:00   70.0    2.0   true
5   2021-06-30 00:00:00+00:00   70.0    3.0   false 
5   2021-08-31 00:00:00+00:00   80.0    2.0   true
5   2021-10-31 00:00:00+00:00   70.0    3.5   true
</code></pre>
",15280062.0,15280062.0,2022-02-20 08:13:20,2022-02-20 16:43:23,Python Data Grouping and compare on the basis of date,<python><pandas><time-series><data-science><data-analysis>,3,3,N/A,CC BY-SA 4.0
71190766,1,-1.0,2022-02-20 01:49:25,1,59,"<p>I'm looking to build a regression model where I have time based variables that may or may not exist for each data sample.</p>
<p>For instance, let's say we wanted to build a regression model where we could predict how long a new car will last. One of the values is when the car gets its first servicing. However, there are some samples where the car never gets serviced at all. In these situations, how can I account for this when building the model? Can I even use a linear regression model or will I have to choose a different regression model?</p>
<p>When I think about it, this is basically the equivalent of having 2 fields: one for whether the car was serviced and if that is true, a second field for when. But I'm not sure how to build a regression that has data that is intentionally missing.</p>
",6281978.0,-1.0,N/A,2022-02-20 02:08:24,Best regression model where some fields may be intentionally blank for some samples,<regression><data-science>,1,0,N/A,CC BY-SA 4.0
71189765,1,71190015.0,2022-02-19 22:32:58,0,2334,"<p>I am sure this is not hard, but I can't figure it out!</p>
<p>I want to create a dataframe that starts at 1 for the first row and ends at 100,000 in increments of 1, 2, 4, 5, or whatever. I could do this in my sleep in Excel, but is there a slick way to do this without importing a .csv or .txt file?</p>
<p>I have needed to do this in variations many times and just settled on importing a .csv, but I am tired of that.</p>
<p><a href=""https://i.stack.imgur.com/aWxJ5.png"" rel=""nofollow noreferrer"">Example in Excel</a></p>
",16616191.0,-1.0,N/A,2022-02-19 23:14:56,"How Do I Create a Dataframe from 1 to 100,000?",<python><dataframe><numbers><data-science><increment>,1,3,N/A,CC BY-SA 4.0
71189888,1,71193561.0,2022-02-19 22:52:50,2,136,"<p>I am currently attempting to estimate the parameters of a logistic regression model &quot;by hand&quot; on the <code>iris</code> dataset via minimisation of cross-entropy. Please note, when I say <code>iris</code> dataset, it has been changed such that there are only two classes - Setosa and Other. It was also normalised via the <code>scale</code> function:</p>
<pre><code>library(dplyr)
library(optimx)
iriso &lt;- iris %&gt;%
  mutate(Species = ifelse(Species == &quot;setosa&quot;, &quot;setosa&quot;, &quot;other&quot;)) %&gt;%
  mutate(Species_n = ifelse(Species == &quot;setosa&quot;, 1, 0)) %&gt;%
  as.data.frame()

iriso[,1:4] &lt;- scale(iriso[,1:4])
</code></pre>
<p>Based on my understanding: should everything be correct, when the optimisation is completed we should obtain the same set of parameters each time - no matter the starting point for the optimisation algorithm. However, the parameter estimates following optimisation are bouncing around:</p>
<p>Some functions being defined:</p>
<pre><code>X &lt;- model.matrix(~.,data=iriso[,1:4])
Y &lt;- model.matrix(~0+Species_n,data=iriso)

e &lt;- exp(1)
sigmoid &lt;- function(y){
  1/(1 + e^-y)
}

#w is an array of weights. x is matrix of observations
logistique &lt;- function(w, x){
  sigmoid(
    y = w[1]*x[,1] + w[2]*x[,2] + w[3]*x[,3] + w[4]*x[,4] + w[5]*x[,5]
    )
}

#y is obsrved values
entropie &lt;- function(w, y, x){
  prob_pred &lt;- logistique(w = w, x = x)

  -sum(
    y*log(prob_pred) + (1-y)*log(1-prob_pred)
  ) 
}

</code></pre>
<p>Optimisation step:</p>
<pre><code>for(i in 1:5){
  w0 &lt;- rnorm(n = 5) #set of initial parameters
  optimx(par = w0, fn = entropie,  
         method = &quot;Nelder-Mead&quot;,
         y = iriso$Species_n, x = X) %&gt;%
    print()
}
</code></pre>
<p>I can not seem to understand why I am not obtaining consistent answers. Is there something wrong with the code above? Is there a concept I am not aware of? Something I missed?</p>
<p>Thanks.</p>
",9771792.0,-1.0,N/A,2022-02-20 10:38:11,Parameter estimation in logistic model by negative log-likelihood minimization - R,<r><optimization><data-science><logistic-regression>,1,0,N/A,CC BY-SA 4.0
71212373,1,71214020.0,2022-02-21 20:02:16,2,480,"<p>BLUF: For a specific epsilon (or for HDBSCAN's 'favorite' epsilon), I can extract the mapping of my data in that epsilon's partition. But how can I see my data's full tree membership?</p>
<p>I've gotten a ton out of the terrific tutorial <a href=""https://hdbscan.readthedocs.io/en/latest/advanced_hdbscan.html"" rel=""nofollow noreferrer"">here</a>. In scikit learn's HDBSCAN, I can use <code>clusterer.labels</code> to see the best epsilon's partition labels. And I can use <code>clusterer.single_linkage_tree_.get_clusters(0.023, min_cluster_size=2)</code> to see the an arbitrary epsilon's partition labels. I can even plot the entire dendogram using <code>clusterer.condensed_tree_.plot()</code>. But how do I see the dendogram's labels for individual datapoints?</p>
<p>For Example: It's nice that my pets' names are {Spot, Felix, Nemo, Fido, Tigger}. Or the species are {Dog, Cat, Guppy, Dog, Cat}. But I'd like one output that tells me:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Spot</td>
<td>Dog</td>
<td>Mammal</td>
<td>Animal</td>
</tr>
<tr>
<td>Felix</td>
<td>Cat</td>
<td>Mammal</td>
<td>Animal</td>
</tr>
<tr>
<td>Nemo</td>
<td>Guppy</td>
<td>Fish</td>
<td>Animal</td>
</tr>
<tr>
<td>Fido</td>
<td>Dog</td>
<td>Mammal</td>
<td>Animal</td>
</tr>
<tr>
<td>Tigger</td>
<td>Cat</td>
<td>Mammal</td>
<td>Animal</td>
</tr>
</tbody>
</table>
</div>
<p>With <em>this</em> sort of output, I could see precisely <em>how</em> related Spot and Felix are, instead of &quot;Do they have the same species? Y/N?&quot; &quot;Do they have the same kingdom? Y/N?&quot;</p>
",14444137.0,4685471.0,2022-02-22 15:41:51,2022-02-23 04:58:33,Scikit HDBSCAN *tree* labeling (not single-slice labeling),<scikit-learn><data-science><cluster-analysis><hierarchical-clustering><hdbscan>,1,0,N/A,CC BY-SA 4.0
71194541,1,-1.0,2022-02-20 12:47:12,0,47,"<p>I'm working on a data set that shows mortality rate for certain diseases and other info in hospitals in various states, and here it is.
<a href=""https://drive.google.com/open?id=1FTZJQLdw0PKw2bQ7XvxWnOITU7-yOCXC"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=1FTZJQLdw0PKw2bQ7XvxWnOITU7-yOCXC</a></p>
<p>I'm trying to write a function called rankall() that takes TWO (2) arguments: (a) the disease (output) which might be one of three: heart attack, heart failure, pneumonia; and (b) a hospital ranking (num). The function reads the dataset and returns a TWO(2)-column data frame containing the hospital in EACH state that has the ranking specified in num. For example the function call</p>
<pre><code>rankall(“heart attack”, “best”)
</code></pre>
<p>would return a data frame containing the names of the hospitals that are the best in their respective states for THIRTY(30)-day heart attack death rates. The function should return a value for EVERY state (some may be NA). The FIRST (1st) column in the data frame is named hospital, which contains the hospital name, and the SECOND (2nd) column is named state, which contains the TWO(2)-character abbreviation for the state name. The function should use the following template.</p>
<p>i've written the function and it works perfectly fine if the output argument is heart attack or heart failure, but when the output is pneumonia it gives wrong values.
and here is my code:</p>
<pre><code> rankall &lt;- function(outcome, num = &quot;best&quot;){
  outcome1 &lt;- read.csv(&quot;outcome-of-care-measures.csv&quot;)
  if (!outcome %in% c(&quot;heart attack&quot;, &quot;heart failure&quot;, &quot;pneumonia&quot;)){
    stop(&quot;invalid outcome&quot;)
  }
  if (outcome == &quot;heart attack&quot;){
    column &lt;- 11
  }
  else if (outcome == &quot;heart failure&quot;){
    column &lt;- 17
  }
  else{
    column &lt;- 23
  }
  vec &lt;- unique(outcome1[,7])
  x &lt;- vector()
  for (i in vec){
    outcome2 &lt;- subset(outcome1, State == i)
    outcome2[,column] &lt;- as.numeric(outcome2[,column])
    outcome3 &lt;- outcome2[order(outcome2[,column], outcome2[,2]),]
    outcome3 &lt;- outcome3[(!is.na(outcome3[,column])),]
    if (num == &quot;best&quot;){
      num &lt;- 1
      }
    else if (num == &quot;worst&quot;){
      num &lt;- nrow(outcome3)
    }
    ans &lt;- outcome3[num,2]
    x &lt;- c(x, ans)
  }
  df &lt;- data.frame(hospitals =x, state = vec)
  final &lt;- df[order(df[,2]),]
  final
}
</code></pre>
",17074045.0,8245406.0,2022-02-20 13:26:06,2022-02-22 13:55:42,a function that exhibits wrong values only when a specific value for an argument is inserted,<r><function><data-science>,2,0,N/A,CC BY-SA 4.0
71213488,1,71214001.0,2022-02-21 21:57:38,0,136,"<p>I have a dataframe listing consumer complaints, I am analyzing the following columns: 'Company', with repeating values, and 'Grade' given by customer, being an integer from 1 to 5.</p>
<p>I looks like this:</p>
<pre><code>    | Company   |  Grade  |
-------------------------------
0   | Company1  |    5    |
1   | Company64 |    2    |
2   | Company1  |    1    |
3   | Company6  |    3    |
...
</code></pre>
<p>I want to analyze the relation in between the number of complaints for each company and the mean() of the grade given by customers for each company.</p>
<p>I'm not sure about the best way to do this, so far I created another df (df2) to hold those values:</p>
<pre><code>df2 = pd.DataFrame(columns= ('Complaints','Grade'))
df2['Complaints'] = df2['Company'].value_counts()
df2

          |  Complaints   |   Grade   |
--------------------------------------
Company1  |     5549      |    NaN
Company23 |     5403      |    NaN
Company8  |     3883      |    NaN
Company30 |     2493      |    NaN
</code></pre>
<p>I'm not sure how to insert the values of the <code>df.Grade.mean()</code> for each company on the <code>df2</code></p>
<p>Something is telling me that I can use multi-indexing for that, by grouping the companies on the first <code>df</code>, but I'm unsure how to proceed with that.</p>
<p>Afterwards I will plot this <code>df2</code> with seaborn in a way too clearly see the relation in between those 2 variables, so if there is a graph that would shortcut this, please let me know.</p>
",8162202.0,13302.0,2022-02-23 19:14:17,2022-02-23 19:14:17,Relating value_counts() of a variable with mean() of another variable,<python><pandas><dataframe><pandas-groupby><data-science>,2,0,N/A,CC BY-SA 4.0
71156801,1,-1.0,2022-02-17 10:55:11,1,269,"<p>I have a month column which has values such as 202212, 202103, 201901. The entry 202212 means, December 2022.
I want to transform all the month column in the dataset to date for example 202212 should become 01-12-2022, 202103 become 01-03-2021 an so on.</p>
",9951314.0,9951314.0,2022-02-17 11:00:43,2022-02-17 11:12:42,Date transformation in time series,<python><machine-learning><time-series><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
71160212,1,-1.0,2022-02-17 14:41:24,0,93,"<p>I gets the expression and new column name as parameters.</p>
<pre><code>aggExpression = &quot;(billDate.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)&quot;
comNewColName = &quot;nextMonthFirstOfBillDate&quot;
</code></pre>
<p>This is just calculate next month first date of billDate.</p>
<pre><code>df = df.assign(comNewColName=lambda x: (pd.to_datetime(x['billDate']).replace(day=1) + datetime.timedelta(days=32)).replace(day=1)))
</code></pre>
<p>What I am trying to do: next month first date of billDate and rename the column as &quot;nextMonthFirstOfBillDate&quot;.</p>
",3655069.0,472495.0,2022-02-23 22:00:57,2022-02-23 22:00:57,How to apply an expression on pandas dataframe,<python><pandas><data-science>,0,3,N/A,CC BY-SA 4.0
71191987,1,-1.0,2022-02-20 06:53:35,0,1648,"<p>I get the below output on running <code>df['CodingHours'].describe()</code>.</p>
<ol>
<li>Is there a way to round this up to a few decimal places?</li>
<li>Also, why is the standard deviation not exactly 1, can someone help me understand this?</li>
</ol>
<pre><code>count    1.000000e+02
mean    -2.220446e-17
std      1.005038e+00
min     -2.056291e+00
25%     -9.160188e-01
50%      2.242536e-01
75%      6.043443e-01
max      1.364526e+00
Name: CodingHours, dtype: float64
</code></pre>
",5810845.0,-1.0,N/A,2022-02-20 07:57:40,How to limit the floating point decimal while using describe function on dataframe?,<python><pandas><dataframe><jupyter-notebook><data-science>,1,2,N/A,CC BY-SA 4.0
71205924,1,-1.0,2022-02-21 11:55:48,0,12,"<p>I have the following dataframe:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>userid</th>
<th>month</th>
</tr>
</thead>
<tbody>
<tr>
<td>user1</td>
<td>jan</td>
</tr>
<tr>
<td>user2</td>
<td>jan</td>
</tr>
<tr>
<td>user3</td>
<td>jan</td>
</tr>
<tr>
<td>user1</td>
<td>feb</td>
</tr>
<tr>
<td>user3</td>
<td>feb</td>
</tr>
<tr>
<td>user1</td>
<td>march</td>
</tr>
</tbody>
</table>
</div>
<p>if user appears more than 2 months, I will group them as active, else no active. The desired output is:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>userid</th>
<th>month</th>
<th>active</th>
</tr>
</thead>
<tbody>
<tr>
<td>user1</td>
<td>jan,feb,march</td>
<td>true</td>
</tr>
<tr>
<td>user2</td>
<td>jan</td>
<td>false</td>
</tr>
<tr>
<td>user3</td>
<td>jan,feb</td>
<td>false</td>
</tr>
</tbody>
</table>
</div>
<p>how can i do it with pandas? pardon me if i do not have a starting code, as i am totally unsure. dont mind helping a newbie here.</p>
",14404904.0,-1.0,N/A,2022-02-21 11:57:47,How to group base on occurrences across months?,<pandas><data-science>,1,0,N/A,CC BY-SA 4.0
71221354,1,-1.0,2022-02-22 12:31:11,0,322,"<pre><code>def split_data(data):
    print(&quot;Splitting data...&quot;)
    features = data[:,1]
    labels = data[:,0]
    training_data, test_data, training_labels, test_labels = train_test_split( features, 
    labels, test_size=0.3, random_state=0)
    print(labels)
    print(&quot;flag 3: splitted data&quot;)
    return training_data, test_data, training_labels, test_labels
</code></pre>
<p>And here we are getting error as :
 in split_data(data)
1 def split_data(data):
2     print(&quot;Splitting data...&quot;)
----&gt; 3     features = data[:,1]
4     labels = data[:,0]
5     training_data, test_data, training_labels, test_labels = train_test_split( features, labels, test_size=0.3, random_state=0)</p>
<p>TypeError: 'NoneType' object is not subscriptable</p>
<p>Here type of data is array, we have tried data as dataframe also but it didn't work,What should we do?</p>
",18278176.0,404970.0,2022-02-22 12:52:14,2022-02-22 12:52:14,We are getting error as : TypeError: 'NoneType' object is not subscriptable what should we do,<python><dataframe><nlp><data-science>,0,2,N/A,CC BY-SA 4.0
71207186,1,71207910.0,2022-02-21 13:31:04,2,528,"<p>I'm new to the numpy in general so this is an easy question however i'm clueless as how to solve it.
<br/>
i'm trying to implement K nearest neighbor algorithm for classification of a Data set</p>
<p>there are to arrays named <em><strong>new_points</strong></em> and <em><strong>point</strong></em> that respectively have the shape of <strong>(30,4)
and (120,4)</strong> (with 4 being the total number of the properties of each element) <br/>
so i'm trying to calculate the distance between each new point and all old points using <a href=""https://numpy.org/doc/stable/user/basics.broadcasting.html"" rel=""nofollow noreferrer"">numpy.broadcasting</a></p>
<pre><code>def calc_no_loop(new_points, points):
  return np.sum((new_points-points)**2,axis=1)
#doesn't work here is log 
</code></pre>
<blockquote>
<p>ValueError: operands could not be broadcast together with shapes (30,4) (120,4)</p>
</blockquote>
<p>however as per rules of broadcasting two array of shapes (30,4) and (120,4) are incompatible
so i would appreciate any insight on how to slove this (using .reshape prehaps - not sure)
<br/></p>
<p><em>please note: that i'have already implemented the same function using one and two loops but can't implement it without one</em></p>
<pre><code>def calc_two_loops(new_points, points):
m, n = len(new_points), len(points)
d = np.zeros((m, n))
for i in range(m):
    for j in range(n):
        
        d[i, j] = np.sum((new_points[i] - points[j])**2)
return d


def calc_one_loop(new_points, points):
m, n = len(new_points), len(points)    
d = np.zeros((m, n))
print(d)
for i in range(m):
    d[i] = np.sum((new_points[i] - points)**2)
return d
</code></pre>
",13527652.0,13527652.0,2022-02-21 13:48:44,2022-02-21 14:36:58,Subtracting Two dimensional arrays using numpy broadcasting,<python><arrays><numpy><data-science>,2,0,N/A,CC BY-SA 4.0
71209468,1,-1.0,2022-02-21 16:05:02,0,246,"<p>I implement <strong>Crank-Nicolson 2D finite-difference method</strong>.</p>
<p>I get a matrix A which is banded with 1 band above and below the main diagonal, but also contains 2 additional bands , further apart from the main diagonal, so it is NOT penta-diagonal.</p>
<p>A picture showing the structure is below. My matrix is the RHS one. The LHS is easy, it's the penta-diagonal one.
<a href=""https://i.stack.imgur.com/EF85y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EF85y.png"" alt=""enter image description here"" /></a></p>
<p>I couldn't find up until now a way to solve Ax = b with A being the RHS matrix from the photo in python.
I could barely find a name for it, in these lecture notes <a href=""https://ocw.mit.edu/ans7870/2/2.086/F12/MIT2_086F12_notes_unit5.pdf"" rel=""nofollow noreferrer"">https://ocw.mit.edu/ans7870/2/2.086/F12/MIT2_086F12_notes_unit5.pdf</a> it is called an 'outrigger' matrix (page 403).</p>
<p>At the moment I am using <code>spsolve</code> from from <code>scipy.sparse.linalg</code>, into which I feed two arguments, namely <code>sparse.csc_matrix(A)</code> and <code>sparse.csc_array(b)</code>, where <code>A</code> and <code>b</code> have been defined initially as <code>A = sparse.dok_matrix((size, size), dtype=np.complex64)</code> and <code>b = sparse.dok_array((size, 1), dtype=np.complex64)</code>, then populated with values by iterating element by element through them.
It is extremely slow and I was wondering maybe someone more experienced knows a way to exploit the structure appearing in <code>A</code>.</p>
<p>Thank you!</p>
",12248220.0,-1.0,N/A,2022-02-22 06:58:48,solve Ax=b for outrigger A matrix python,<python><arrays><data-structures><data-science><linear-algebra>,1,0,N/A,CC BY-SA 4.0
71229653,1,-1.0,2022-02-22 23:39:01,-2,981,"<p>Hello stackoverflow community!</p>
<p>I recently found out that Bigquery ML does not support random forest classification models. To overcome that, I figured that I might be able to build a model with sklearn package and then use the same hyperparameters on bigqueryml. Is using the boosted tree model in bigquery the best option in this case?</p>
",17499842.0,-1.0,N/A,2022-09-08 10:26:41,BigQueryML: Random Forest Classification,<python><google-bigquery><data-science><random-forest>,2,0,N/A,CC BY-SA 4.0
71213550,1,71213625.0,2022-02-21 22:02:55,1,132,"<p>I have 2 pandas data-frames, with 2 columns namely index and date. Some of the dates are missing from the first data-frame, and those values can be obtained from the second data-frame corresponding to the index. I tried using pd.concat, pd.merge and pd.join etc but those doesn't seem to give me the results that I want. Here is the table.</p>
<p>df1 = <a href=""https://i.stack.imgur.com/KlNiy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KlNiy.png"" alt=""data-frame 1"" /></a></p>
<p>df2 = <a href=""https://i.stack.imgur.com/HC5Pi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HC5Pi.png"" alt=""data-frame 2"" /></a></p>
",9992250.0,-1.0,N/A,2022-02-21 22:44:43,"Merging two datasets based on the 2 columns, or finding the missing values in first dataframe and filling that from the other",<python><python-3.x><pandas><dataframe><data-science>,3,0,N/A,CC BY-SA 4.0
71223167,1,71223225.0,2022-02-22 14:33:48,2,309,"<p>I have a dataframe something like this.
<a href=""https://i.stack.imgur.com/vAYt9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vAYt9.jpg"" alt=""enter image description here"" /></a></p>
<p>I want to calculate the variance and standard deviation of co2_emission for each food_category by grouping and aggregating. And it has to be in this format</p>
<pre><code>print(food_consumption.____(____)['co2_emission'].agg([____]))
</code></pre>
<p>This is I have done so far</p>
<pre><code>print(food_consumption. .....(....)['co2_emission'].agg([np.var(food_consumption['co2_emission'], ddof=1),np.sqrt(np.var(food_consumption['co2_emission'], ddof=1))]))
</code></pre>
<p>I have to select the each category of the column named food_category. how to do that?</p>
",9604231.0,-1.0,N/A,2022-02-22 14:37:22,How to select every category of a column in a data frame?,<python><pandas><dataframe><numpy><data-science>,1,1,N/A,CC BY-SA 4.0
71241614,1,-1.0,2022-02-23 17:32:22,1,132,"<p>I have the following graph:</p>
<p><a href=""https://i.stack.imgur.com/Z0CUO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z0CUO.png"" alt=""incorrect_graph"" /></a></p>
<p>I'm looking for a Graph Data Science algorithm that can find a path inside the graph in a way, that given the start-point all end-points can be reached.
Like here:</p>
<p><a href=""https://i.stack.imgur.com/5aN26.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5aN26.png"" alt=""correct_graph"" /></a></p>
<p>So far I've tried Dijkstra Single Source &amp; Depth First Search algorithms &amp; Nth degree relationships with pure cypher but to no avail.</p>
<p>I have several similar graphs and therefore I am rather looking for an algorithmic solution but I am open to any suggestions.</p>
<p>cypher code:</p>
<pre><code>:begin
CREATE CONSTRAINT ON (node:Node1) ASSERT (node.node1) IS UNIQUE;
CREATE CONSTRAINT ON (node:Node2) ASSERT (node.node2) IS UNIQUE;
:commit
CALL db.awaitIndexes(300);
:begin
UNWIND [{node1:&quot;D100660078&quot;, properties:{CASE_ID:9}}, {node1:&quot;D100660074&quot;, properties:{CASE_ID:9}}, {node1:&quot;D100660080&quot;, properties:{CASE_ID:9}}, {node1:&quot;200030914&quot;, properties:{CASE_ID:9}}, {node1:&quot;D100660082&quot;, properties:{CASE_ID:9}}, {node1:&quot;2100020143&quot;, properties:{CASE_ID:9}}, {node1:&quot;D100660076&quot;, properties:{CASE_ID:9}}, {node1:&quot;1600020093&quot;, properties:{CASE_ID:9, start_marker:1}}] AS row
CREATE (n:Node1{node1: row.node1}) SET n += row.properties;
UNWIND [{node2:&quot;200030914&quot;, properties:{CASE_ID:9}}, {node2:&quot;2100020143&quot;, properties:{CASE_ID:9}}] AS row
CREATE (n:Node2{node2: row.node2}) SET n += row.properties;
:commit
:begin
UNWIND [{start: {node2:&quot;2100020143&quot;}, end: {node1:&quot;2100020143&quot;}, properties:{}}, {start: {node2:&quot;200030914&quot;}, end: {node1:&quot;200030914&quot;}, properties:{}}, {start: {node2:&quot;2100020143&quot;}, end: {node1:&quot;D100660076&quot;}, properties:{}}, {start: {node2:&quot;2100020143&quot;}, end: {node1:&quot;D100660080&quot;}, properties:{}}, {start: {node2:&quot;2100020143&quot;}, end: {node1:&quot;D100660078&quot;}, properties:{}}, {start: {node2:&quot;2100020143&quot;}, end: {node1:&quot;D100660074&quot;}, properties:{}}, {start: {node2:&quot;2100020143&quot;}, end: {node1:&quot;D100660082&quot;}, properties:{}}, {start: {node2:&quot;200030914&quot;}, end: {node1:&quot;2100020143&quot;}, properties:{}}, {start: {node2:&quot;200030914&quot;}, end: {node1:&quot;1600020093&quot;}, properties:{}}] AS row
MATCH (start:Node2{node2: row.start.node2})
MATCH (end:Node1{node1: row.end.node1})
CREATE (start)-[r:GOES]-&gt;(end) SET r += row.properties;
:commit
</code></pre>
",2915383.0,2915383.0,2022-02-23 17:45:15,2022-02-28 11:18:53,Looking for Path Finding Algorithm with Neo4j,<algorithm><neo4j><cypher><graph-data-science>,1,1,N/A,CC BY-SA 4.0
71229858,1,71235668.0,2022-02-23 00:07:35,1,339,"<p>I have two arrays (vel_y,vel_z) representing velocities in the y and z directions, respectively, that are both shaped as (512,512) that I am attempting to represent using a quiver plot.
Here is how I plotted the quiver:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0,512,num=512)
[X,Y] = np.meshgrid(x,x)
fig, ax = plt.subplots(figsize=(7,7))
ax.quiver(X,Y,vel_y,vel_z,np.arctan2(vel_z,vel_y),pivot='mid',units='x',scale=2)
</code></pre>
<p>The arctan2() call is so that different orientations are colored differently as in <a href=""https://stackoverflow.com/questions/40026718/different-colours-for-arrows-in-quiver-plot"">this answer</a>.
Obviously plotting all 512<sup>2</sup> of these arrows makes for a jumbled and difficult to parse plot, which you can see here:
<a href=""https://i.stack.imgur.com/9dfbm.png"" rel=""nofollow noreferrer"">Quiver Plot</a>.</p>
<hr />
<p>I was wondering if there was a better way to scale/represent the arrows so that it is more readable?</p>
<p><strong>However</strong>, the main question I have is how I could 'downsample' this velocity information to go from plotting 512<sup>2</sup> arrows to 100<sup>2</sup> for example. Would this require interpolation between points where the velocity is defined?</p>
<p>Thank you!</p>
",13324882.0,-1.0,N/A,2022-02-23 10:59:08,Downsampling Velocity Information in a Quiver Plot,<python><numpy><matplotlib><plot><data-science>,1,0,N/A,CC BY-SA 4.0
71232311,1,71245485.0,2022-02-23 06:36:33,0,68,"<pre><code>import pandas as pd
test_df =pd.DataFrame({&quot;col1&quot;:[1,12,3,4],
            &quot;col2&quot;:[3,14,5,6],
             &quot;col3&quot;:[4,5,6,7]})

print(test_df)
   col1  col2  col3
0     1     3     4
1    12    14     5
2     3     5     6
3     4     6     7

def highlight(row):
    df1 = pd.DataFrame('', index=row.index, columns=row.columns)
    # set columns by condition
    df1.loc[row['col2'] == 5, 'col2'] = &quot;AA&quot;
    df1.loc[row['col3'] == 5, 'col3'] = &quot;BB&quot;
        
    return ret

df_output= test_df.apply(highlight, axis=1)


</code></pre>
<p>Error given above code: AttributeError: 'Series' object has no attribute 'columns' Request you please help me to correct highlight method code.</p>
",8241863.0,8241863.0,2022-02-26 21:05:24,2022-02-26 21:05:24,How to get all df data inside apply function of pandas,<python><pandas><data-science>,2,0,N/A,CC BY-SA 4.0
71232797,1,71232829.0,2022-02-23 07:26:24,1,205,"<p>I am doing a groupby practice. But it returning dict not dataframe. I fallowed some of the solutions from Stack Overflow even no luck.</p>
<p>My code:</p>
<pre><code>result[comNewColName] = sourceDF.groupby(context, as_index=False)[aggColumn].agg(aggOperation).reset_index()
</code></pre>
<p>and I tried:</p>
<pre><code>result[comNewColName] = sourceDF.groupby(context)[aggColumn].agg(aggOperation).reset_index()
</code></pre>
<p>and</p>
<pre><code>result[comNewColName] = sourceDF.groupby(context, as_index=False)[aggColumn].agg(aggOperation)
</code></pre>
<p>all three cases, I am getting dict only. But I should get dataframe</p>
<p>here:</p>
<pre><code>comNewColName = &quot;totalAmount&quot;
context =['clientCode']
aggColumn = 'amount'
aggOperation = 'sum'
</code></pre>
",3655069.0,472495.0,2022-02-23 22:00:09,2022-02-23 22:00:09,How to get dataframe from groupby,<python><pandas><data-science>,1,1,N/A,CC BY-SA 4.0
71242873,1,-1.0,2022-02-23 19:17:56,0,92,"<p>I mostly deal with text data. I'm not sure if I can do something like this.</p>
<pre><code>for plaint,law in zip(train_seqs1,train_seqs2):
   plaint_emb = get_tokens_embeddings(plaint).numpy().flatten()
   law_emb = get_tokens_embeddings(law).numpy().flatten()
   cosine_dis = cosine(plaint_emb,law_emb)
</code></pre>
<p>I tried getting word embeddings from bert , and used them to calculate distance between two texts. I want to know that , can I use this cosine distances to train model? If I can , how to preprocess these number inputs. I want to know the right threshold of cosine distances to predict that these two texts are relevant or not relevant to each other. (Binary classification)</p>
",14997042.0,-1.0,N/A,2022-02-23 19:17:56,Can I use cosine distance as inputs for model in text classification task?,<python><machine-learning><nlp><data-science>,0,4,N/A,CC BY-SA 4.0
71239059,1,71239265.0,2022-02-23 14:46:23,1,41,"<p>I have a large dataframe which I would like to downsample from 120hz to 40hz. It's not consistent enough though that I could just take every third row from the dataframe.</p>
<p>I have something like the following dataframe:</p>
<pre><code>df &lt;- data.frame(RelativeTime = c(0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550), 
         Marker = c(&quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;), 
         size=c(NA, -1, -1, 4, -1, -1 , 3.5, -1, -1, 4, -1, -1, NA, 4, -1, -1, 2, -1, -1, -1, -1, -1, 4.5, -1),
         trial=c(&quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial1&quot;, &quot;trial1&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot; ))
</code></pre>
<p>What I would like is for to identify the first non-0 entry in <code>size</code> for each <code>trial</code>. Then <strong>starting from there</strong> filter the data.frame so only every third row is retained. As you can see in this example as well, the starting point for each first non-0 number is variable relative to &quot;trial_onset&quot;, which is giving me issues! Furthermore, the reason I need this particular approach (instead of just filtering out non-0 values) is that it is well possible that sometimes the &quot;size&quot; value is also -1 just due to a failed data collection point. I need to retain those for further processing. So, I would like to end up with the following dataframe:</p>
<pre><code>df2 &lt;- data.frame(RelativeTime = c(0, 150, 300, 450, 0, 50, 200, 350, 500), 
         Marker = c(&quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;), 
         size=c(NA, 4, 3.5, 4, NA, 4, 2, -1, 4.5),
         trial=c(&quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;))
</code></pre>
<p>Thanks a lot for helping out!</p>
",15834079.0,15834079.0,2022-02-23 15:15:38,2022-02-23 15:22:07,Identify first occurence in dataframe in R and subsequently label every third row,<r><dataframe><dplyr><data-science>,2,0,N/A,CC BY-SA 4.0
71254440,1,-1.0,2022-02-24 15:32:10,1,244,"<p>If i apply a window to my time domain transient signal i convoulte the temporal signal spectrum with the spectrum of the window function. If i use no window i convolute the signal with the spectrum of a rectangular window.</p>
<p>What i have is a transient signal with differing temporal spectral bandpower, meaning high frequencies are at the beginning of the signal and low frequencies are at the middle and end. But this can also change. Now if i multiply my time domain signal with a hann window the amplutide of the beginning and the end will be substantially damped due to the window but if i use no window (rectangular) the spektrum will be broadend and my energies will be wrong too.</p>
<p>What i want to have is the most accurate <strong>band powers</strong>, which window is the best for this application?
<a href=""https://i.stack.imgur.com/8uueo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8uueo.png"" alt=""Window and temporal influence of the fft"" /></a></p>
",4406671.0,4406671.0,2022-02-24 17:55:57,2022-02-25 01:36:59,FFT window functions and the influence of them on the bandpower of a spectrum,<algorithm><data-science><signal-processing><fft>,2,0,N/A,CC BY-SA 4.0
71261517,1,71273119.0,2022-02-25 05:40:40,0,19,"<p>I need to identify (and label) nodes with many relationships (say, 10 on a possible scale of 1 to 60) but weak weights for the relationships (say 1 or 2 on a possible scale of 1 to 100). I could write a Cypher query to find them. I don’t need help with that. What I want to ask is, is there a GDS metric for this?</p>
",3709953.0,-1.0,N/A,2022-02-26 00:37:05,"Identifying nodes with many, but thin connections",<neo4j><graph-data-science>,1,0,N/A,CC BY-SA 4.0
71263860,1,-1.0,2022-02-25 09:40:34,0,27,"<p>Trying to match the skills as a patter and apply a groupby function however receiving as unhashable list.</p>
<pre><code>Tec_skills=['MS SQL Server', 'Oracle','Machine Learning', 'Artificial Intelligence', 'Deep Neural Networks', 
        'Convolutional Neural Network', 'Sklearn Libraries', 'Keras','Tensor flow', 'SQL', 'C#','NoSQL',' Docker',
       'Python','Shell','SQL/PLSQL','PLSQL','R','C','C++','AWS','Neural Networks,','CNN','RNN','Linear/Logistic Regression', 
        'Ensemble Trees, Gradient','Boosted trees, Bagging, Random forest','Time series','Data Visualization','Sentiment Analysis',
       'Docker &amp; Kubernetes','Classification','clustering','supervised','unsupervised']
def tech_skills(text):
    word_tokens=word_tokenize(text)
    filtered_stop_word = [word for word in word_tokens if word not in stopwords.words('english')]
    all_combinations=' '.join,everygrams(filtered_stop_word,2,3)
    #ext_skills=[]
    ext_skills=re.findall(Tec_skills,all_combinations)
    if ext_skills:
        return (ext_skills.group(0))
    return (ext_skills.group(0))

All_pdf_data_bygroup=All_pdf_data.groupby(All_pdf_data.index)


All_pdf_data_bygroup[&quot;text&quot;].apply(lambda x: ' '.join(x)).apply(lambda x:tech_skills(x))
</code></pre>
<p><code>ERROR:TypeError: unhashable type: 'list'</code></p>
<p>Please suggets how to resolve the issue .</p>
",18116476.0,8893827.0,2022-02-25 09:45:05,2022-02-25 09:45:05,Python - trying to get specific words as output however receiving error,<python-3.x><pandas><dataframe><lambda><data-science>,0,3,N/A,CC BY-SA 4.0
71268657,1,-1.0,2022-02-25 16:13:25,0,29,"<p>I have a dataframe in below format:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>daily_dates</th>
<th>registered</th>
<th>attended</th>
</tr>
</thead>
<tbody>
<tr>
<td>02/10/2022</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>02/09/2022</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>02/08/2022</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>And my <code>response</code> object is of this format:</p>
<pre><code>response = {&quot;registered&quot;:[], &quot;attended&quot;:[]}
</code></pre>
<p>I am looking to combine <code>daily_dates</code> with <code>registered</code> and <code>attended</code> column to result in below <code>response</code> object.</p>
<pre><code>response = {&quot;registered&quot;:[{&quot;02/10/2022&quot;:0},{&quot;02/09/2022&quot;:0},{&quot;02/08/2022&quot;:1}], &quot;attended&quot;:[{&quot;02/10/2022&quot;:0},{&quot;02/09/2022&quot;:0},{&quot;02/08/2022&quot;:0}]}
</code></pre>
<p>The below is my current coding approach:</p>
<pre><code>weekly['registered'] = weekly.apply(lambda row: {row['daily_dates']: row['registered']}, axis=1)
response[&quot;registered&quot;] += weekly['registered'].tolist()
</code></pre>
<p>I am looking for a much efficient and direct way of achieving this.</p>
",15318538.0,-1.0,N/A,2022-02-26 04:21:10,combine and append dataframe as list of dictionarys to a response object,<python><python-3.x><pandas><dataframe><data-science>,2,0,N/A,CC BY-SA 4.0
71278935,1,-1.0,2022-02-26 17:39:19,0,586,"<p><a href=""https://github.com/veekun/pokedex/blob/master/pokedex/data/csv/abilities.csv"" rel=""nofollow noreferrer"">Here</a> is the csv file I using.</p>
<p>Goals:</p>
<ol>
<li><p>I want to extract the rows that has values of columns &quot;generation_id&quot; or &quot;is_main_series&quot; greater than 0 and assign them a variable &quot;selection&quot;
(Just to be clear, I want rows that has at least one of two columns greater than 0.)</p>
</li>
<li><p>Then, I extract the &quot;identifier&quot; columns from the &quot;selection&quot; and assign them a variable &quot;name&quot;.</p>
</li>
<li><p>Finally, I would like to format the rows of &quot;selection&quot; for each &quot;name&quot; in this way --&gt; name(generation_id, is_main_series), name1(is_main_series), etc</p>
</li>
</ol>
<p>Error:
line 7: ValueError: Cannot index with multidimensional key</p>
<p>I'm struggling to find solution for this error because I don't think this Dataframe is multiindexed.</p>
<p>Here is the code I wrote so far:</p>
<pre><code>import pandas as pd

df = pd.read_csv('abilities.csv')
df = df.fillna(0)

def getPokedex():
    selection = df.loc[df[['generation_id', 'is_main_series']] != 0 ]
    for donor in selection:
        name = selection['identifier']
        name = name.to_string(index=False)
        for types in selection.columns:
            return f&quot;{name}(types)&quot;

print(getPokedex())
</code></pre>
<p>Would appreciate any help.</p>
<p><strong>UPDATED:</strong>
Excel File sample:</p>
<pre><code>TypeA1, TypeA2, TypeA3, TypeA4, TypeA5, TypeA6, TypeB, TypeC, TypeD, TypeE
20, 0, 10, 4, 0.5, 75, 1000, 0, 2, 14
0, 1, 1, 1, 1, 0, 23, 2, 10, 34
</code></pre>
<p>Code I wrote:</p>
<pre><code>  import pandas as pd
  import numpy as np
    
    df = pd.read_excel('sample.xlsm', header=2)
    # Replace empty cells with 0
    df = df.fillna(0)
    
    # Format rows
    df['Formatted_column'] = df['Name'] + '(' + np.where(df[['TypeA1', 'TypeA2', 'TypeA3', 'TypeA4', 'TypeA5', 'TypeA6']].astype(str) !='0', 'A', '') + ',' + np.where(df['TypeB'].astype(str) !='0', 'B', '') + np.where(df['TypeC'].astype(str) !='0', 'C', '') + np.where(df['TypeD'].astype(str) !='0', 'D', '') + np.where(df['E'].astype(str) !='0', 'E', '') + ')'
    print(', '.join(df['Formatted_column'].to_list()))
</code></pre>
<p>My desired outcome would be james(A,C), mary(C), luke(A,B,D), etc</p>
<p>But Python raises this error in line 8:</p>
<pre><code>TypeError: unsupported operand type(s) for +: 'int' and 'str'
</code></pre>
",-1.0,-1.0,2022-03-05 13:53:16,2022-03-05 13:53:16,Pandas: How to do operations using loc with multiple columns and format the results,<python><pandas><csv><data-science>,2,2,N/A,CC BY-SA 4.0
71255950,1,71256134.0,2022-02-24 17:29:19,0,35,"<p>I have a sample dataframe of a very huge dataframe as given below.</p>
<pre><code>import pandas as pd
import numpy as np

NaN = np.nan

data = {'Start_x':['Tom', NaN, NaN, NaN,NaN],
    'Start_y':[NaN, 'Nick', NaN, NaN, NaN],
    'Start_z':[NaN, NaN, 'Alison', NaN, NaN],
    'Start_a':[NaN, NaN, NaN, 'Mark',NaN],
    'Start_b':[NaN, NaN, NaN, NaN, 'Oliver'],
    'Sex': ['Male','Male','Female','Male','Male']}

df = pd.DataFrame(data)
df
</code></pre>
<p>I want the final result to look like the image given below. The 4 columns have to be merged to a single new column but the 'Sex' column should be as it is.</p>
<p><a href=""https://i.stack.imgur.com/HVnrT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HVnrT.png"" alt=""enter image description here"" /></a></p>
<p>Any help is greatly appreciated. Thank you!</p>
",16547860.0,16547860.0,2022-02-24 17:40:07,2022-02-24 18:05:54,Combine different columns into a new column in a dataframe using pandas,<python-3.x><pandas><dataframe><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
71268768,1,-1.0,2022-02-25 16:22:31,0,69,"<p>I am creating my first multivariate multistep encoder-decoder LSTM to forecast revenues.</p>
<p>As you can see, the values move towards a value and then stop at that value. The aim is to create a forecast for a longer period, but there is no deviation at all from this standard value after the first week.
What is wrong and what can I do? To me it doesn't look like it is working at all.</p>
<h2>code:</h2>
<pre><code>    class ModelTrainer:
        def __init__(self, prediction_length=30, offset=1):
            self.prediction_length = prediction_length
            self.offset = offset
            self._setup_values()
            self.use_scaling = True
            self.__prepare_data()
    
        def _setup_values(self):
            # Model configuration
            self.additional_metrics = ['accuracy']
            self.embedding_output_dims = 15
            self.max_sequence_length = 300
            self.num_distinct_words = 5000
            self.verbosity_mode = 1
    
            # DATA
            self.WINDOW_LENGTH = 70  # ! SHOULD BE ADJUSTED TO THE AMOUNT OF FORECASTING DAYS
            self.SAMPLING_RATE = 1
            self.BATCH_SIZE = 128
    
            # MODEL
            self.DROPOUT = 0.3
            self.NODES_PER_LAYER = 256
            self.NUMBER_OF_LAYERS = 3
    
            # TRAINING
            self.LEARNING_RATE = 0.001
            self.OPTIMIZER = Adam(learning_rate=self.LEARNING_RATE)
            self.VALIDATION_SPLIT = 0.20
            self.NUMBER_OF_EPOCHS = 10
            self.TEST_SIZE = 0.1
            self.RANDOM_STATE = 123
            self.LOSS_FUNCTION = MeanSquaredError()
    
        def __import_data(self):
            self.series = DataOrganizer().df
    
        def __prepare_data(self):
            self.__import_data()
            self.scaler = preprocessing.MinMaxScaler()
            data_scaled = self.scaler.fit_transform(self.series)
            self.features, self.target = self._create_feature_target_values_window(
                data_scaled)
    
        def _create_feature_target_values_window(self, data):
            self.number_of_output_columns = 4
            feature_data = data
            target_data = data[:, :self.number_of_output_columns]
            features, target = list(), list()
            in_start = 0
            for _ in range(len(data)):
                in_end = in_start + self.WINDOW_LENGTH
                out_end = in_end + self.prediction_length
                if out_end &lt;= len(data):
                    features.append(feature_data[in_start:in_end, :])
                    target.append(
                        target_data[in_end:out_end, 0:self.number_of_output_columns])
                in_start += 1
            return np.array(features), np.array(target)
    
        def __create_LSTM_model(self):
            num_feature_columns = self.features.shape[2]
            num_output_columns = self.target.shape[2]
            model = Sequential()
            model.add(LSTM(self.NODES_PER_LAYER, input_shape=(
                self.WINDOW_LENGTH, num_feature_columns)))
            model.add(Dropout(self.DROPOUT))
            model.add(RepeatVector(self.prediction_length))
            model.add(LSTM(self.NODES_PER_LAYER, return_sequences=True))
            model.add(Dropout(self.DROPOUT))
            model.add(TimeDistributed(Dense(self.NODES_PER_LAYER)))
            model.add(Dropout(self.DROPOUT))
            model.add(TimeDistributed(Dense(num_output_columns)))
            model.summary()
            return model
    
        def train_model(self, callbacks=[]):
            model = self.__create_LSTM_model()
            model.compile(loss=self.LOSS_FUNCTION,
                          optimizer=self.OPTIMIZER,
                          metrics=['accuracy', MeanAbsoluteError()]
                          )
            model.fit(
                x=self.features,
                y=self.target,
                epochs=self.NUMBER_OF_EPOCHS,
                validation_split=self.TEST_SIZE,
                shuffle=False,
                callbacks=callbacks
            )
            self.model = model
    
        def create_forecast(self):
            prediction = self.model.predict(self.features[-1:])
            # prediction = self.model.predict(self.features[-30:-29]) # Show forecast from a month old
            test_X = self.features.copy()
            test_X = test_X[:self.prediction_length,
                            :1, self.number_of_output_columns:]
            test_X = test_X.reshape(
                self.prediction_length, self.series.shape[1] - self.number_of_output_columns)
            prediction = prediction.reshape(self.prediction_length,
                                            self.number_of_output_columns)
            inv_yhat = np.concatenate((prediction, test_X), axis=1)
            inv_yhat = self.scaler.inverse_transform(inv_yhat)
            prediction_df = pd.DataFrame(
                inv_yhat, columns=self.scaler.feature_names_in_)
            first_date = self.series.last_valid_index() + timedelta(days=1)
            last_date = first_date + timedelta(days=self.prediction_length-1)
            days = pd.date_range(first_date, last_date, freq='D')
            prediction_df.set_index(days, inplace=True)
            prediction_df = prediction_df[self.series.columns[0:4]]
</code></pre>
<h2>Actual</h2>
<p><a href=""https://i.stack.imgur.com/9kVpO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9kVpO.png"" alt=""actual"" /></a></p>
<h2>Forecast:</h2>
<p>(I know the x-axis description is incorrect. Don't worry about it)
<a href=""https://i.stack.imgur.com/cHEFV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cHEFV.png"" alt=""forecast"" /></a></p>
",11489713.0,-1.0,N/A,2022-02-25 16:22:31,Multivariate Encoder-Decoder Model approaches value,<keras><time-series><data-science><lstm><encoder-decoder>,0,2,N/A,CC BY-SA 4.0
71276813,1,71277546.0,2022-02-26 12:50:28,-1,374,"<p>RidgeCV() also searches on a set of hyperparameters, and given a similar kernel in GridSearchCV() along with similar parameters would there be any difference in the results of the two?</p>
",7156020.0,-1.0,N/A,2022-02-26 14:42:43,Difference between RidgeCV() and GridSearchCV(),<machine-learning><regression><data-science><linear-regression><linear-algebra>,1,1,N/A,CC BY-SA 4.0
71280145,1,71280393.0,2022-02-26 20:43:14,1,52,"<p>I am not sure how to best describe this (I am sure there is a more proper way of describing it).</p>
<p>I have a large dataset full of house details (eg. walls, bathrooms, bedrooms, etc.) that I need to analyze and rank based on their characteristics. I have created a ranking system with &quot;4&quot; being the best and &quot;0&quot; being the worst, for example, a house with 1 bedroom may get a &quot;0&quot; for their bedroom score but a house with a 3 bathrooms may get a &quot;4&quot; for their bathroom score.</p>
<p>Once I assocaite the ranks to all the characteristics, I plan on creating a weighted average to see which houses are the best.</p>
<p>How is the best way to do this? I need to do this about 20 times (for 20 characteristics) and so far this is the only way I know how to do it-- and it is quite tedious, especially if I ever need to go back and change anything.</p>
<p>Also, would be good to better understand how the df.loc function works, I was able to do make it work but I don't quite understand it.</p>
<pre><code>    #EXAMPLE ONE, GRADING LAND USE  
    ParcelsData.loc[ParcelsData[&quot;land_use&quot;] == 'Flum/Swim Floodway (Restrected)', 'LandUseGrade'] = 0
    ParcelsData.loc[ParcelsData[&quot;land_use&quot;] == 'Single Family Residential', 'LandUseGrade'] = 4
    ParcelsData.loc[ParcelsData[&quot;land_use&quot;] == 'Wasteland, Slivers, Gullies, Rock Outcrop', 'LandUseGrade'] = 0
    ParcelsData.loc[ParcelsData[&quot;land_use&quot;] == 'Single Family Residential - Common', 'LandUseGrade'] = 4
    ParcelsData.loc[ParcelsData[&quot;land_use&quot;] == 'Multi Family', 'LandUseGrade'] = 2
    
    #EXAMPLE TWO, STORY 
    ParcelsData.loc[ParcelsData[&quot;HomeDetails_storyheight&quot;] == '1 STORY', 'StoryGrade'] = 4
    ParcelsData.loc[ParcelsData[&quot;HomeDetails_storyheight&quot;] == '1.5 STORY', 'StoryGrade'] = 2
    ParcelsData.loc[ParcelsData[&quot;HomeDetails_storyheight&quot;] == '2.0 STORY', 'StoryGrade'] = 3
    ParcelsData.loc[ParcelsData[&quot;HomeDetails_storyheight&quot;] == '2.5 STORY', 'StoryGrade'] = 2
    ParcelsData.loc[ParcelsData[&quot;HomeDetails_storyheight&quot;] == '3.0 STORY', 'StoryGrade'] = 2
    ParcelsData.loc[ParcelsData[&quot;HomeDetails_storyheight&quot;] == 'RANCH W/BSMT', 'StoryGrade'] = 4
    ParcelsData.loc[ParcelsData[&quot;HomeDetails_storyheight&quot;] == 'BI-LEVEL', 'StoryGrade'] = 1
    ParcelsData.loc[ParcelsData[&quot;HomeDetails_storyheight&quot;] == 'SPLIT LEVEL', 'StoryGrade'] = 1
    
     #EXAMPLE THREE, ACRES
    ParcelsData.loc[ParcelsData[&quot;Acres&quot;] &lt;= .1, 'AcresGrade'] = 1
    ParcelsData.loc[ParcelsData[&quot;Acres&quot;] &lt;= .2, 'AcresGrade'] = 2
    ParcelsData.loc[ParcelsData[&quot;Acres&quot;] &lt;= .3, 'AcresGrade'] = 3
    ParcelsData.loc[ParcelsData[&quot;Acres&quot;] &lt;= .4, 'AcresGrade'] = 7
    ParcelsData.loc[ParcelsData[&quot;Acres&quot;] &lt;= .5, 'AcresGrade'] = 8
    ParcelsData.loc[ParcelsData[&quot;Acres&quot;] &gt; .5, 'AcresGrade'] = 9
</code></pre>
",12915431.0,12915431.0,2022-02-26 21:10:58,2022-02-26 21:24:35,Add values to a Pandas Dataframe in order to make a ranking system with weighted averages,<python><python-3.x><pandas><data-science>,1,5,N/A,CC BY-SA 4.0
71272299,1,-1.0,2022-02-25 22:21:25,2,183,"<p>UPDATED to include a sample of the data:
<a href=""https://i.stack.imgur.com/i3ajV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i3ajV.png"" alt=""enter image description here"" /></a></p>
<p>I have a bar graph that takes the data I have, grouped by starting location (five boroughs of NYC), ad then within that, grouped by data of two different companies X and Y.</p>
<p>As of now the data shows 10 bars all separated equally (each borough and each company), and instead I want it to have 10 bars, but each borough sticks both companies together, so that I have 5 different sections, instead of 10.</p>
<p>Also, I would like to show the percentage overall within the bars if possible, but I don't know how to design it as such.</p>
<p>Lastly, what method would I use to add titles to the axis within the code?</p>
<p>I'm relatively new to Jupyter Notebooks and Data Analytics, so any help would be greatly appreciated, thank you so much!</p>
<p>(Note: It's not actually the five boroughs, but it's close enough - that's just for the purposes of the explanation, as the solution would be the same.)</p>
<p>My current code and graph:</p>
<pre><code>df_stacked = df.groupby(['Starting Location', 'Company Chosen']).count()
# df_stacked['Customer_ID'].plot(kind = 'bar', stacks = df['BREAKDOWN', SUCCESSFUL TRIP])

df_stacked.drop(df_stacked.columns.difference(['Unavailable Scooter', 'BREAKDOWN', 'Successful Trip']), 1, inplace = True)

df_stacked.plot.bar(stacked = True, figsize = (10, 5))
</code></pre>
<p><a href=""https://i.stack.imgur.com/Bqkcs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bqkcs.png"" alt=""my graph"" /></a></p>
<p>This is what I would like it to look like:
<a href=""https://i.stack.imgur.com/BxScD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BxScD.png"" alt=""enter image description here"" /></a></p>
",14050605.0,14050605.0,2022-02-27 00:08:17,2022-03-01 19:04:23,"How to make a bar graph that sticks the ""groupby""s together",<python><pandas><matplotlib><jupyter-notebook><data-science>,1,2,N/A,CC BY-SA 4.0
71280683,1,-1.0,2022-02-26 22:14:50,0,438,"<p>I am trying to add a sentence as well as a coin(like a label in this case I guess) to a DataFrame. Although I keep getting this error:</p>
<pre><code>Traceback (most recent call last):
      File &quot;c:\Users\gjohn\Documents\code\machineLearning\trading_bot\filter.py&quot;, line 132, in &lt;module&gt;
        df = df.append({'coin': coin, 'review': sentence}, ignore_index=True)
      File &quot;C:\Users\gjohn\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\series.py&quot;, line 2877, in append
        return concat(
      File &quot;C:\Users\gjohn\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\util\_decorators.py&quot;, line 311, in wrapper
        return func(*args, **kwargs)
      File &quot;C:\Users\gjohn\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\reshape\concat.py&quot;, line 294, in concat
        op = _Concatenator(
      File &quot;C:\Users\gjohn\AppData\Local\Programs\Python\Python39\lib\site-packages\pandas\core\reshape\concat.py&quot;, line 384, in __init__
        raise TypeError(msg)
 TypeError: cannot concatenate object of type '&lt;class 'dict'&gt;'; only Series and DataFrame objs are valid
</code></pre>
<p>Here is the code:</p>
<pre><code>data = pd.read_csv('C:\\Users\\gjohn\\Documents\\code\\machineLearning\\trading_bot\\testreviews.csv')
df = data['review'] # Create a dataframe of the reviews.
classes = data['class'] # Create a dataframe of the classes.
for sentence in sentences:
    coin = find_coin(common_words, sentence)
    if len(sentence) &gt; 0 and coin != None:
        df = df.append({'coin': coin, 'review': sentence}, ignore_index=True)
</code></pre>
<p>I can't find how to fix this and I really need help, it would be great if you could help me out. Thanks!</p>
<p>Also sorry for the messy code :D</p>
",15942211.0,-1.0,N/A,2022-02-26 22:36:51,Cannot concatenate object when adding to a DataFrame,<python><python-3.x><pandas><dataframe><data-science>,1,6,N/A,CC BY-SA 4.0
71258744,1,-1.0,2022-02-24 21:58:46,0,48,"<p>When I break by image dataset into train and validation using the following code:</p>
<pre><code>datagen_kwargs = dict(rescale=1./255, validation_split=.20)
valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)
valid_generator = valid_datagen.flow_from_directory(
  TRAINING_DATA_DIR,
  subset=&quot;validation&quot;,
  shuffle=True,
  target_size=IMAGE_SHAPE
)
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)
train_generator = train_datagen.flow_from_directory(
  TRAINING_DATA_DIR,
  subset=&quot;training&quot;,
  shuffle=True,
  target_size=IMAGE_SHAPE)
</code></pre>
<p>I get the following output: &quot;Found 1997 images belonging to 5 classes.
Found 7999 images belonging to 5 classes.&quot;</p>
<p>So my validation data has 1997 images and train data has 7999 images.</p>
<p>Now, I am trying to make predictions on the validation data using my trained model. I use <code>val_image_batch, val_label_batch = next(iter(valid_generator))</code> to extract the images and labels. When I print the length of <code>val_image_batch</code> or <code>val_label_batch</code> I get 32 instead of 1997- the size of my validation data.</p>
<p>I want to make predictions on the entire validation data and not just 32 images. Can someone help me figure the problem?</p>
<p>I used this <a href=""https://medium.com/analytics-vidhya/create-tensorflow-image-classification-model-with-your-own-dataset-in-google-colab-63e9d7853a3e"" rel=""nofollow noreferrer"">article</a> for help.</p>
",18086775.0,-1.0,N/A,2022-02-24 21:58:46,Why the validation data is small?,<python><tensorflow><image-processing><data-science><image-classification>,0,4,N/A,CC BY-SA 4.0
71275433,1,71275463.0,2022-02-26 09:19:09,0,30,"<p>I want to find a book with most sold score. For this, I have created a series (unique_books) with names of all the books. Next, I want to compare its values with the books column (Book Name) but it is keep generating errors. I an a newbie so this question may feel dumb to you. Please excuse my silliness and do help me sort it out. Any help would be appreciated.</p>
<pre><code>unique_books = pd.Series(unique_books) #converting ndarray to pd series
type(unique_books)
bestselling = pd.Series(unique_books, dtype=&quot;int&quot;)
type(bestselling)

counter = 0
bestselling_book = 0
for i in range(0, len(df['Book Name'])):
    
    if df[df[&quot;Book Name&quot;]].equals(unique_books[unique_books[i]]):
        counter += 1
    
    if bestselling_book &lt; counter:
        bestselling_book = counter
        
print (&quot;The best selling book is &quot; + str(bestselling_book))
</code></pre>
",6726421.0,-1.0,N/A,2022-02-26 10:17:37,How can we compare values of a data frame to a series?,<python><pandas><compare><data-science><series>,1,2,N/A,CC BY-SA 4.0
71283626,1,-1.0,2022-02-27 09:33:16,1,106,"<p>I run CondensedNearestNeighbour() undersampling method in jupyter notebook for 1 million rows, according to one variable and target. I think it takes long time. Almost two days are over but, it is still running without result.</p>
<p>I really don't understand, if it doesn't work for huge data, what does it do. I need undersampling to reduce sample number. I don't want to use random sampling. If you have any opinion, i would appreciate. My code sample is below:</p>
<pre><code>X = df1[['var1']].to_numpy()
y=df1['target'].to_numpy()

 
counter = Counter(y)
undersample = CondensedNearestNeighbour(random_state=44, n_neighbors=1)
X1, y1 = undersample1.fit_resample(X, y)
sample_counter = Counter(y1)
</code></pre>
",15050120.0,-1.0,2022-02-27 21:43:00,2022-02-27 21:43:00,Why doesn't CondensedNearestNeighbour() end up with large data?,<python><machine-learning><data-science><sampling><resampling>,0,0,N/A,CC BY-SA 4.0
71289730,1,71289750.0,2022-02-28 01:17:23,1,888,"<p>I'm using an excel file and would like to drop first two rows of headers that has 3 rows of headers.</p>
<p>Current File Example:</p>
<pre><code>    Type1, Type2, Type3, Type4
    SubType1, SubType2, SubType3, SubType4
    SubSubType1-3,,,SubSubType4
0   Blah, Blah1, Blah2, Blah4
1
2
</code></pre>
<p>After dropping two headers:</p>
<pre><code>    SubSubType1-3,,,SubSubType4
0   Blah, Blah1, Blah2, Blah4
1
2
</code></pre>
<p>I know there are several ways to drop rows using index but I could not find a way to drop first two rows of headers in multiple-header data.</p>
<p>Would appreciate any help.</p>
",-1.0,-1.0,N/A,2022-02-28 01:21:10,Pandas: How do I delete first two rows of headers?,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
71279545,1,71279673.0,2022-02-26 19:07:37,2,303,"<p>I have two Pandas DataFrames I'd like to add together, with a datetime index, and a set of common columns.</p>
<p>The datetime indices will have 95% common values, but some of the rows in <code>df2</code> may not be in <code>df1</code> and vice versa.</p>
<p>I'd like to add the two DataFrames together, and when one of the DataFrames do not have the index the other does just treat is as <code>0</code> (or take the one with a value, whichever is better).</p>
<p>The result should <strong>not</strong> drop any indices, i.e. something like an outer join, rather than an inner.</p>
<p>I have tried <code>pd.add</code>, but that appears to drop <code>NaN</code> results where both DataFrames do not have an entry.</p>
<p><code>pd.concat</code> works where they don't have common indices, but where they do I get duplicates instead of adding together. Do I have to do a second <code>groupby</code> sum step? I thought there'd be a simpler way to do this.</p>
<p>For example:</p>
<p><strong>FRAME 1</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Month</th>
<th>Val 1</th>
<th>Val 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01-01</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2022-03-01</td>
<td>5</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
<p><strong>FRAME 2</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Month</th>
<th>Val 1</th>
<th>Val 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-03-01</td>
<td>101</td>
<td>102</td>
</tr>
<tr>
<td>2022-04-01</td>
<td>103</td>
<td>104</td>
</tr>
<tr>
<td>2024-01-01</td>
<td>105</td>
<td>106</td>
</tr>
<tr>
<td>2025-01-01</td>
<td>107</td>
<td>108</td>
</tr>
</tbody>
</table>
</div>
<p><strong>DESIRED RESULT</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Month</th>
<th>Val 1</th>
<th>Val 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01-01</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2022-03-01</td>
<td>106</td>
<td>108</td>
</tr>
<tr>
<td>2022-04-01</td>
<td>103</td>
<td>104</td>
</tr>
<tr>
<td>2024-01-01</td>
<td>105</td>
<td>106</td>
</tr>
<tr>
<td>2025-01-01</td>
<td>107</td>
<td>108</td>
</tr>
</tbody>
</table>
</div>",1089153.0,-1.0,2022-02-26 20:57:52,2022-02-26 20:57:52,Adding DataFrames with partially overlapping indices,<python><pandas><dataframe><data-science>,2,0,N/A,CC BY-SA 4.0
71284572,1,-1.0,2022-02-27 12:00:27,1,180,"<p>This is my code.</p>
<pre><code>fig, ax1 = plt.subplots() 
fig.set_figheight(7)
fig.set_figwidth(12)
ax1.bar(df.index, df['occurence of defects'], color=&quot;C0&quot;)
ax1.set_ylabel(&quot;Qty&quot;, color=&quot;C0&quot;)
ax1.tick_params(axis=&quot;y&quot;, colors=&quot;C0&quot;)
ax1.set_xlabel(&quot;Defect&quot;)
ax1.set_xticklabels(df['Name of Defect'],rotation=45)
ax2 = ax1.twinx()
ax2.plot(df.index, df[&quot;cum percentage&quot;], color=&quot;C1&quot;, marker=&quot;D&quot;, ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())
ax2.tick_params(axis=&quot;y&quot;, colors=&quot;C1&quot;)
plt.show()
</code></pre>
<p>this is ss of output
<a href=""https://i.stack.imgur.com/a4bj0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a4bj0.png"" alt=""enter image description here"" /></a>
I made circles where labels are missing. How can I fix that? Even the current labels on the x-axis aren't in their supposed positions.</p>
",18013381.0,8881141.0,2022-02-27 12:07:05,2022-02-27 13:10:47,Why aren't all ticklabels shown on the x-axis?,<python><pandas><matplotlib><data-science><pareto-chart>,2,1,N/A,CC BY-SA 4.0
71301537,1,71301592.0,2022-02-28 22:00:34,0,84,"<p>I have some earthquake data. I have a Magnitude, Distance, and Percent that I care about. I want to <strong>group all of the MAGNITUDES together and sum</strong> the distances and percents for each magnitudes. Here is a part of my data:</p>
<pre><code>import pandas as pd

data = {'Distance': [1, 5, 9, 3, 5, 4, 2, 3.1],
        'Magnitude': [7.3, 7.3, 7.3, 6.0, 8.2, 6.0, 8.2, 5.7],
        'Percent': [0.1, 0.05, 0.07, 0.11, 0.2, 0.07, 0.08,0.11]
       }

df = pd.DataFrame(data)

print(df)


         Distance  Magnitude  Percent
 0       1.0        7.3     0.10
 1       5.0        7.3     0.05
 2       9.0        7.3     0.07
 3       3.0        6.0     0.11
 4       5.0        8.2     0.20
 5       4.0        6.0     0.07
 6       2.0        8.2     0.08
 7       3.1        5.7     0.11
</code></pre>
<p>My idea was this. Groupby and sum:</p>
<pre><code>df2 = df.groupby(['Distance','Magnitude','Percent'],as_index=False).agg({'Percent': 'sum'},{'Distance': 'sum'})
</code></pre>
<p>I get the same dataframe upon running my code except it is ascending by distance which is fine, but nothing groupped together or summed.</p>
<p>I want it to look like this:</p>
<pre><code>       Distance  Magnitude  Percent
0      15.0        5.7     0.22
1       7.0        6.0     0.18
2       7.0        7.3     0.28
3       3.1        8.2     0.11
</code></pre>
<p>There is only 1 value for each magnitude and the distances and percents have been summed for each magnitude.</p>
",16616191.0,-1.0,N/A,2022-02-28 22:12:20,Pandas Merging rows and summing values?,<python><pandas><dataframe><pandas-groupby><data-science>,1,1,N/A,CC BY-SA 4.0
71305033,1,-1.0,2022-03-01 07:27:48,0,254,"<p>While running the code az.plot_trace(result_final);
facing the below error
<strong>TypeError: expected dtype object, got 'numpy.dtype[float64]'</strong></p>
<p>The above exception was the direct cause of the following exception:</p>
<p>SystemError: CPUDispatcher(&lt;function histogram at 0x00000147106C8B88&gt;) returned a result with an error set</p>
<p>Can you please let me know how to solve this</p>
",18341144.0,-1.0,N/A,2022-05-14 02:56:31,In Arviz while ploting the plot_trace or plot_posterior getting the type error,<python><data-science><arviz><bambi>,1,2,N/A,CC BY-SA 4.0
71307149,1,71307267.0,2022-03-01 10:37:06,2,126,"<p>I have a problem I can't wrap my head around at the moment. I have a large dataframe that looks something like this:</p>
<pre><code>df &lt;- data.frame( 
         Marker = c(&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_tone&quot;, &quot;&quot;, &quot;&quot;, &quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_tone&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;), 
         size=c(3, NA, -1, -1, 4, -1, -1 , 3.5, -1, -1, 4, -1, -1, NA, 4, -1, -1, 2, -1, -1, -1, -1, -1, 4.5, -1))

df
</code></pre>
<p>What I want is to count the number of samples <strong>starting from the &quot;start_trial&quot; marker</strong>. I want that count to go both directions, in negative and positive values. Furthermore, I want that negative count to stop an <strong>n</strong> amount of samples (say 2) before the &quot;start_tone&quot; marker. In the positive direction, I want that count to stop <strong>n</strong> (here 2) samples before the next &quot;start_tone&quot; marker (where the negative count starts). Lastly, I want a new column that keeps track of the trials around each &quot;start_trial&quot; marker. So, it should look something like the following:</p>
<pre><code>df2 &lt;- data.frame(SampleCount = c(&quot;&quot;, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6), 
         Marker = c(&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_tone&quot;, &quot;&quot;, &quot;&quot;, &quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_tone&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;), 
         size=c(3, NA, -1, -1, 4, -1, -1 , 3.5, -1, -1, 4, -1, -1, NA, 4, -1, -1, 2, -1, -1, -1, -1, -1, 4.5, -1),
         trial=c(&quot;&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial 1&quot;, &quot;trial1&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot;, &quot;trial 2&quot; ))

df2
</code></pre>
<p>Thanks a lot for helping out!</p>
",15834079.0,15834079.0,2022-03-01 13:37:50,2022-03-02 09:53:14,Conditional row count based on two variables (R),<r><dataframe><dplyr><data-science>,1,0,N/A,CC BY-SA 4.0
71321221,1,71360206.0,2022-03-02 10:36:22,1,607,"<p>I am trying to solve the following problem:</p>
<p>I have a pandas dataframe df with multiple columns. I want to find values a and b to maximise the sum of column 'result' divided by the number of selected rows from the dataframe where a and b are used to select rows of the dataframe using the following constraints:</p>
<pre><code>df['x'] &gt;= a &amp; df['y'] &lt;= b
2.5 &lt;= a &lt;= 20
0.05 &lt;= b &lt;= 0.35
</code></pre>
<p>I tried using PuLP, but never worked with it before and therefore am stuck.
Here's a sample code for the problem and how I tried to solve it:</p>
<pre><code>import pandas as pd
from pulp import LpMaximize, LpProblem, LpStatus, lpSum, LpVariable

df = pd.DataFrame({'x': [2.94, 10.33, 8.67, 10.18, 2.82], 'y': [0.34, 0.21, 0.06, 0.24, 0.28], 'result': [-0.5, 9.55, 13.59, -0.2, 11.59]})
model = LpProblem(name='find_values', sense=LpMaximize)
a = LpVariable(name='a', lowBound=2.5, upBound=20)
b = LpVariable(name='b', lowBound=0.05, upBound=0.35)

# add constraints
model += a &lt;= 20
model += b &lt;= 0.35
# Set the objective
model += lpSum(
    df[(df['x'] &gt;= a) &amp; (df['y'] &lt;= b)]['result']) / len(
    df[(df['x'] &gt;= a) &amp; (df['y'] &lt;= b)])

print(model)
model.solve()
# Get the results
print(model.status)
print(LpStatus[model.status])
print(model.objective.value())
print(a.value())
print(b.value())
</code></pre>
<p>Once this is run, the following output is shown:</p>
<pre><code>find_values:
MAXIMIZE
6.806
SUBJECT TO
_C1: a &lt;= 20

_C2: b &lt;= 0.35

VARIABLES
2.5 &lt;= a &lt;= 20 Continuous
0.05 &lt;= b &lt;= 0.35 Continuous

1
Optimal
Objective value
None
2.5
0.05
</code></pre>
<p>For me it looks like the mistake is in the objective function, since the model has a fixed value there indicating the code is already evaluated.
However, I have no idea, how I have to formulate it to work or if this is even possible with PuLP.</p>
",18352664.0,-1.0,N/A,2022-03-05 07:13:38,How do I select dataframe rows using PuLP for linear optimisation,<python><pandas><algorithm><data-science><pulp>,1,1,N/A,CC BY-SA 4.0
71296769,1,72369192.0,2022-02-28 14:48:23,0,249,"<p>I'm trying to plot a Polynomial Plot with Matplotlib/Seaborn. I am new to Data Science and thus I'm having trouble with this bit of code:</p>
<pre><code>def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):
    width = 12
    height = 10
    plt.figure(figsize=(width, height))
    
    
    #training data 
    #testing data 
    # lr:  linear regression object 
    #poly_transform:  polynomial transformation object 
 
    xmax=max([xtrain.values.max(), xtest.values.max()])

    xmin=min([xtrain.values.min(), xtest.values.min()])

    x=np.arange(xmin, xmax, 0.1)


    plt.plot(xtrain, y_train, 'ro', label='Training Data')
    plt.plot(xtest, y_test, 'go', label='Test Data')
    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')
    plt.ylim([-10000, 60000])
    plt.ylabel('Price')
    plt.legend()
</code></pre>
<p>This is the function that plots the polynomial function. However when I call the function with:</p>
<pre><code>PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly, pr)
</code></pre>
<p>I get the following error:</p>
<pre><code>InvalidIndexError: (slice(None, None, None), None)
</code></pre>
<p>Any assistance given would be greatly appreciated.</p>
",18310392.0,7758804.0,2022-03-01 17:55:38,2022-05-24 20:37:51,Training and Testing Model PollyPlot,<python><matplotlib><data-science>,1,0,N/A,CC BY-SA 4.0
71306034,1,71306394.0,2022-03-01 09:07:36,2,1061,"<p>I have a dataframe which looks like below,</p>
<pre><code>name,value,id
meanerror,0.55,aa
meanamount,120,aa
meanerror,0.45,bb
meanamount,150,bb
meanerror,0.88,cc
meanamount,110,cc
meanerror,0.1,dd
meanamount,50,dd
</code></pre>
<p>I would like to create a matrix from this dataframe like below.</p>
<pre><code>,         meanamount,    total_y
meanerror,0-100,100-200
0.0-0.5,    1,    1,      2   
0.5-1,      0,    2,      2
total_x,    1,    3
</code></pre>
<p>what I actually need is, in the matrix, each cell should contain count of ids which has value(from value column) in the range on both x and y axis of the matrix. i.e for example the first cell should contain count of ids with meanamount in range 0-100 and meanerror in range 0.0-5.</p>
<p>I have tried pandas pivot table and crosstab but unsure how to achieve this. Can anyone help?</p>
",7975462.0,7975462.0,2022-03-01 09:30:55,2022-03-01 09:38:22,Create count matrix based on two columns and respective range values pandas,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
71315082,1,-1.0,2022-03-01 21:59:14,0,133,"<p>Is it possible to call an in-memory graph (projected graph) in a way that returns a graph (not only Table, Text &amp; Code) in <code>Neo4j</code> database?</p>
",2915383.0,-1.0,N/A,2022-03-02 10:07:44,How to view in-memory graph in Neo4j?,<neo4j><cypher><graph-data-science>,1,0,N/A,CC BY-SA 4.0
71332974,1,-1.0,2022-03-03 06:33:41,1,232,"<p>The question is about using a chat-bot framework in a research study, where one would like to measure the improvement of a rule-based decision process over time.
For example, we would like to understand how to improve the process of medical condition identification (and treatment) using the minimal set of guided questions and patient interaction.</p>
<p>Medical condition can be formulated into a work-flow rules by doctors; possible technical approach for such study would be developing an app or web site that can be accessed by patients, where they can ask free text questions that a predefined rule-based chat-bot will address. During the study there will be a doctor monitoring the collected data and improving the rules and the possible responses (and also provide new responses when the workflow has reached a dead-end), we do plan to collect the conversations and apply machine learning to generate improved work-flow tree (and questions) over time, however the plan is to do any data analysis and processing offline, there is no intention of building a full product.
This is a low budget academy study, and the PHD student has good development skills and data science knowledge (python) and will be accompanied by a fellow student that will work on the engineering side. One of the conversational-AI options recommended for data scientists was RASA.</p>
<p>I invested the last few days reading and playing with several chat-bots solutions: RASA, Botpress, also looked at Dialogflow and read tons of comparison material which makes it more challenging.</p>
<p>From the sources on the internet it seems that RASA might be a better fit for data science projects, however it would be great to get a sense of the real learning curve and how fast one can expect to have a working bot, and the especially one that has to continuously update the rules.</p>
<p>Few things to clarify, We do have data to generate the questions and in touch with doctors to improve the quality, it seems that we need a way to introduce participants with multiple choices and provide answers (not just free text), being in the research side there is also no need to align with any specific big provider (i.e. Google, Amazon or Microsoft) unless it has a benefit, the important consideration are time, money and felxability, we would like to have a working approach in few weeks (and continuously improve it) the whole experiment will run for no more than 3-4 months. We do need to be able to extract all the data. We are not sure about which channel is best for such study WhatsApp? Website? Other? and what are the involved complexities?</p>
<p>Any thoughts about the challenges and considerations about dealing with chat-bots would be valuable.</p>
",3127998.0,-1.0,N/A,2022-03-03 06:33:41,Choosing a chat-bot framework for data science research project and understanding the hidden costs of the development and rollout?,<data-science><chatbot><rasa><conversational-ai>,0,2,N/A,CC BY-SA 4.0
71323687,1,71331742.0,2022-03-02 13:38:43,1,144,"<p>I am looking for the 10 IDs that have the highest a value at each date.</p>
<p><a href=""https://i.stack.imgur.com/2sAVo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2sAVo.png"" alt=""df"" /></a></p>
<pre><code>for date in df:
    a = df.nlargest(10, ['a'])
    Top_performer.append(a[['ID','Renta','Date']])
</code></pre>
<p>as output I would like the IDs and their 'renta' for each date</p>
<p>I'm bothering you for something pretty simple I guess but I'm stuck! thanks</p>
",16046034.0,-1.0,N/A,2022-03-03 03:19:32,find nlargest ID for a value for each date,<python><pandas><data-science><finance>,1,0,N/A,CC BY-SA 4.0
71331129,1,71333300.0,2022-03-03 01:34:41,0,1124,"<p>I'm trying to assign some custom weight to my PyTorch model but it doesn't work correctly.</p>
<pre><code>class Mod(nn.Module):
    def __init__(self):
        super(Mod, self).__init__()
        
        self.linear = nn.Sequential(
            nn.Linear(1, 5)
        )
    def forward(self, x):
        x = self.linear(x)
        return x
mod = Mod()

mod.linear.weight = torch.tensor([1. ,2. ,3. ,4. ,5.], requires_grad=True)
mod.linear.bias = torch.nn.Parameter(torch.tensor(0., requires_grad=True))

print(mod.linear.weight)
&gt;&gt;&gt; tensor([1., 2., 3., 4., 5.], requires_grad=True)

output = mod(torch.ones(1))
print(output)
&gt;&gt;&gt; tensor([ 0.2657,  0.3220, -0.0726, -1.6987,  0.3945], grad_fn=&lt;AddBackward0&gt;)
</code></pre>
<p>The output is expected to be [1., 2., 3., 4., 5.] but it doesn't work as expected. What am I missing here?</p>
",16605558.0,4685471.0,2022-03-03 08:25:25,2022-03-06 19:53:32,Assign custom weight in pytorch,<machine-learning><deep-learning><pytorch><data-science><torch>,1,1,N/A,CC BY-SA 4.0
71335161,1,-1.0,2022-03-03 09:56:09,0,13,"<p>I'm new to pandas so please go easy on me,</p>
<p>I'm changing the values in a specific column of my data frame by iterating through the column and changing it. the code is shown below.</p>
<pre><code>row = 0
for values in nonNumericColumns['timestamp in UTC']:
    values = values[:9]
    nonNumericColumns.at[row, 'timestamp in UTC'] = values
    row+=1 
</code></pre>
<p>the values in the column before the change is :</p>
<pre><code>0     2022-02-19T06:33:13.582610
1     2022-02-19T06:33:27.737695
2     2022-02-19T06:33:35.754507
3     2022-02-19T06:33:45.681506
4     2022-03-03T08:41:44.297195
5     2022-03-03T08:43:04.868416
6     2022-03-03T08:50:45.769626
7     2022-03-03T08:52:10.764062
8     2022-03-03T09:24:23.985866
9     2022-03-03T09:25:20.357228
10    2022-03-03T09:30:22.536442
11    2022-03-03T09:51:31.012427
</code></pre>
<p>now the values after the slicing is done are:</p>
<pre><code>0     2022-02-1
1     2022-02-1
2     2022-02-1
3     2022-02-1
4     2022-03-0
5     2022-03-0
6     2022-03-0
7     2022-03-0
8     2022-03-0
9     2022-03-0
10    2022-03-0
11    2022-03-0
</code></pre>
<p>But the problem is that this method of iterating is making the program slow. can someone tell me how to make it efficient?</p>
<p>Any help would be appreciated.</p>
",16618594.0,-1.0,N/A,2022-03-03 09:56:09,changing values in a column of a data frame efficiently,<python><pandas><dataframe><data-science>,0,2,2022-03-03 10:01:05,CC BY-SA 4.0
71335230,1,-1.0,2022-03-03 10:00:48,1,1487,"<p>I want to append new values to an existing column with values in a dataframe.</p>
<p>Example: This is the initial dataframe</p>
<pre><code>name    gender   age  
harry     M      10    
martha    F      11    
</code></pre>
<p>After checking conditions the data frame expands and adds a new column with values:</p>
<pre><code>name    gender   age   remarks
harry     M      10    is_boy 
martha    F      11    is_girl
</code></pre>
<p>and again the process continues</p>
<p>This is the Final dataframe</p>
<pre><code>name    gender   age   remarks
harry     M      10    is_boy | iowa | 2008
martha    F      11    is_girl| florida | 2007
</code></pre>
<p>This is a bit tricky for me as I have spend a lot of time behind this.</p>
<p>Would like to know how the code will look like.</p>
",10748412.0,-1.0,N/A,2022-03-03 10:50:55,Append a value to an existing column in a dataframe after conditions,<python><pandas><data-science><data-analysis>,2,0,N/A,CC BY-SA 4.0
71336374,1,-1.0,2022-03-03 11:25:46,2,723,"<p>I am trying to render RandomForestClassifier model dashboard using ExplainerDashboard package, but it is not rendering the dashboard in notebook.</p>
<p>Here is the code</p>
<pre><code>model = RandomForestClassifier(n_estimators=50, max_depth=10).fit(X_train, y_train)
explainer = ClassifierExplainer(model, X_test, y_test) 
ExplainerDashboard(explainer).run()
</code></pre>
<p>I was getting below output</p>
<pre><code>=========================================================================

Detected RandomForestClassifier model: Changing class type to RandomForestClassifierExplainer...

Note: model_output=='probability', so assuming that raw shap output of RandomForestClassifier is in probability space...

Generating self.shap_explainer = shap.TreeExplainer(model)
Building ExplainerDashboard..

Detected notebook environment, consider setting mode='external', mode='inline' or mode='jupyterlab' to keep the notebook interactive while the dashboard is running...

Warning: calculating shap interaction values can be slow! Pass shap_interaction=False to remove interactions tab.

Generating layout...

Calculating shap values...

Calculating prediction probabilities...

Calculating metrics...

Calculating confusion matrices...

Calculating classification_dfs...

Calculating roc auc curves...

Calculating pr auc curves...

Calculating liftcurve_dfs...

Calculating shap interaction values... (this may take a while)

Reminder: TreeShap computational complexity is O(TLD^2), where T is the number of trees, L is the maximum number of leaves in any tree and D the maximal depth of any tree. So reducing these will speed up the calculation.

Calculating dependencies...

Calculating permutation importances (if slow, try setting n_jobs parameter)...

Calculating pred_percentiles...

Calculating predictions...

Calculating ShadowDecTree for each individual decision tree...

Reminder: you can store the explainer (including calculated dependencies) with explainer.dump('explainer.joblib') and reload with e.g. ClassifierExplainer.from_file('explainer.joblib')

Registering callbacks...

Starting ExplainerDashboard on http://19.221.249.249:8055

Dash is running on http://0.0.0.0:8055/


 * Serving Flask app 'explainerdashboard.dashboards' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http://19.221.249.249:8055/

=========================================================================
</code></pre>
<p>But dashboard is not rendered in notebook. I tried with InlineExplainer also, it was returning &lt;IPython.lib.display.IFrame at 0x7f4eea3e1c70&gt;</p>
<p>Can you please suggest any idea to render dashboard in databricks notebook</p>
",2304217.0,2304217.0,2022-03-03 11:32:37,2022-10-11 10:56:15,RandomForestClassifier Explainer Dashboard output in databricks notebook is not rendered,<python><machine-learning><data-science><databricks>,2,0,N/A,CC BY-SA 4.0
71338701,1,-1.0,2022-03-03 14:20:48,0,55,"<p>PySpark: Possibly a duplicate, can't find a similar question.</p>
<p>I have a table A:</p>
<pre><code>a | b | c
---------
1 | 3 | p
2 | 4 | q
3 | 4 | r
4 | 7 | s

</code></pre>
<p>And a table B:</p>
<pre><code>p | q
---------
1 | Yes
2 | No
3 | Yes
</code></pre>
<p>I want the resultant table to be joined on column a value being equal to column p value. I tried the inner join, but it returns a copy of the entire table A for each q value. I want the resultant table to be:</p>
<pre><code>a | b | c | q
--------------
1 | 3 | p | Yes
2 | 4 | q | No
3 | 4 | r | Yes
</code></pre>
<p>Please help out with how to achieve this in PySpark? And also, what do I do if I wanted this table:</p>
<pre><code>a | b | c | q
--------------
1 | 3 | p | Yes
2 | 4 | q | No
3 | 4 | r | Yes
4 | 7 | s | null
</code></pre>
",17859183.0,466862.0,2022-03-03 18:03:42,2022-03-03 18:03:42,Add another column based on equal column values,<python><pyspark><bigdata><data-science>,1,1,N/A,CC BY-SA 4.0
71310069,1,-1.0,2022-03-01 14:31:03,0,190,"<p>Looking at a fruit and veg dataset with prices and dates. However when I try to plot anything with the date there are way too many instances as the date feature does it by each week. Is there anyway to either group the dates by month or something? The date format is like 2022-02-11.</p>
",10140970.0,-1.0,N/A,2022-03-01 14:37:32,Anyway to group or sort dates by month and/or year using python?,<python><date><time-series><data-science>,1,1,N/A,CC BY-SA 4.0
71311169,1,71311603.0,2022-03-01 15:53:07,1,382,"<p>I have the following data set.</p>
<pre><code>ID     Date   
abc    2017-01-07  
abc    2017-01-08  
abc    2017-01-09  
abc    2017-12-09  
xyz    2017-01-05  
xyz    2017-01-06 
xyz    2017-04-15  
xyz    2017-04-16 
</code></pre>
<p>I am able to generate the following output</p>
<pre><code>ID     Count
abc    3
abc    1
xyz    2
xyz    2
</code></pre>
<p>using the following code mentioned in <a href=""https://stackoverflow.com/questions/52957834/count-consecutive-days-python-dataframe/71310881#71310881"">count consecutive days python dataframe</a></p>
<pre><code>d = {
    'ID': ['abc', 'abc', 'abc', 'abc', 'xyz', 'xyz', 'xyz', 'xyz'],
    'Date': ['2017-01-07','2017-01-08', '2017-01-09', '2017-12-09', '2017-01-05', '2017-01-06', '2017-04-15', '2017-04-16']
}

df = pd.DataFrame(data=d)
df['Date'] = pd.to_datetime(df['Date'])

series = df.groupby('ID').Date.diff().dt.days.ne(1).cumsum()
df.groupby(['ID', series]).size().reset_index(level=1, drop=True)
</code></pre>
<p>How can I get the following output?</p>
<pre><code>ID     Start        End
abc    2017-01-07   2017-01-09
abc    2017-12-09   2017-12-09
xyz    2017-01-05   2017-01-06
xyz    2017-04-15   2017-04-16  
</code></pre>
",2272878.0,-1.0,N/A,2022-03-01 16:23:48,Grouping by consecutive dates into date ranges using Python,<python><pandas><data-science>,2,1,N/A,CC BY-SA 4.0
71325832,1,-1.0,2022-03-02 16:11:54,0,156,"<p>I have an array of pairwise differences of features:</p>
<pre><code>diff.shape = (200, 200, 2)
</code></pre>
<p>of which I am trying to take only the columns corresponding to the 50 closest points. For each row, I have the indices of the closest 50 points stored as:</p>
<pre><code>dist_idx.shape = (200, 50).
</code></pre>
<p>How can I index the 50 closest entries (different indices per row) using fancy indexing? I have tried:</p>
<pre><code>diff[dist_idx].shape = (200, 50, 200, 2)
diff[np.arange(200), dist_idx] -&gt; IndexError: shape mismatch: indexing arrays could not be broadcast together with shapes (200,) (200,50) 
diff[np.arange(200), dist_idx[np.arange(200)]] -&gt; IndexError: shape mismatch: indexing arrays could not be broadcast together with shapes (200,) (200,50) 
</code></pre>
<p>An iterative solution that works is:</p>
<pre><code>X_diff = np.zeros((200, 50, 2))
for i in range(200):
    X_diff[i] = diff[i, dist_idx[i]]
</code></pre>
",17619929.0,-1.0,N/A,2022-03-02 19:00:00,Taking a Different Subset of Indices per Row in Numpy Using Fancy Indexing,<python><numpy><indexing><data-science>,1,0,N/A,CC BY-SA 4.0
71341766,1,-1.0,2022-03-03 18:02:06,1,273,"<p>This is how my plot actually is: <a href=""https://i.stack.imgur.com/BWVGZ.png"" rel=""nofollow noreferrer"">current output</a></p>
<p>This is the Data Frame: <a href=""https://i.stack.imgur.com/GANHH.png"" rel=""nofollow noreferrer"">DataFrame</a></p>
<p>This is my current code:</p>
<pre class=""lang-py prettyprint-override""><code>
import pandas as pd
import plotly.express as px

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

df = pd.read_csv('sales.csv')

df.columns = df.columns.str.replace(' ', '')

fig= px.bar(df, x='number', y='number', color='profit')
fig.show()
</code></pre>
<p>As you can see, the sales numbers are many times the same, so i want to plot a histogram being each x value the total profit of all sales with the same key number, so i can compare the profit of each sales key number.</p>
<p>How can i do that using Pandas and plotly express?</p>
<p>ps: I'm a real noobie with all this</p>
",16804541.0,16804541.0,2022-03-03 18:07:34,2022-03-03 19:55:43,How can i use Plotly express to make a 1D histogram without making new x values for each row of the same value?,<python><pandas><data-science><data-analysis><plotly-express>,1,0,N/A,CC BY-SA 4.0
71343264,1,-1.0,2022-03-03 20:16:03,0,77,"<p>Let's say I have a set of <em>X</em> entities, which have been clustered into <em>K</em> clusters <em>N</em> times.
For example, let's assume <em>X = 9</em>, <em>K = 3</em> and <em>N = 4</em> and:</p>
<ol>
<li><code>{a,b,c}</code>, <code>{d,e,f}</code>, <code>{g,h,i}</code></li>
<li><code>{c,d}</code>, <code>{a,b,f,i}</code>, <code>{e,g,h}</code></li>
<li><code>{d,g,i}</code>, <code>{a,f}</code>, <code>{b,c,e,h}</code></li>
<li><code>{a}</code>, <code>{b,c,d,e,f,i}</code>, <code>{g,h}</code></li>
</ol>
<p>are the 4 <em>clusterings</em> of 9 items into 3 clusters each. I am now looking for an algorithm to form  a <strong>single clustering</strong> of <em>X</em> entities into <em>k</em> clusters based on given <em>N</em> clusterings (based on how many times an entity was clustered with another one). For example let's say that in our example a correct clustering would be:</p>
<p><code>{a,b,c,d}</code>, <code>{e,f,i}</code>, <code>{g,h}</code></p>
<p>I would be grateful for any algorithms / notions that would point me to possible solutions.</p>
<hr />
<p><strong>[EDIT]</strong> I will add information about the specific problem I want to solve with this approach.</p>
<p>I have <em>N</em> test problems.
And I have an algorithm which outputs solutions in form of sequences to a given problem. This algorithm has many parameters and say there is <em>X</em> combinations of possible parameters. Now, I want to perform further experiments, but cannot afford to experiment with all <em>X</em> combinations, and want to choose <em>K</em> meaningful combinations. By meaningful I mean producing most diverse solutions.</p>
<p>My idea was: take 1 problem, generate output for all <em>X</em> combinations of parameters, then cluster the outputs into <em>K</em> clusters based on how similar they were. Then, if I was interested in 1 problem only, I would just simply take one output from each cluster and the associated parameter combinations would be the chosen <em>K</em> combinations.</p>
<p>But I want to do it for <strong>ALL</strong> <em>N</em> test problems. So I would have all <em>X</em> combinations, clustered into <em>K</em> clusters <em>N</em> times. And now I wanted to create final <em>K</em> clusters based on the previous.</p>
<p>I would not want to simply choose one clustering over another, but rather create a new one, which based on all previous ones.</p>
",13660145.0,13660145.0,2022-03-04 08:19:49,2022-03-04 08:19:49,Algorithm for data clustering based on clustered data,<algorithm><data-science><cluster-analysis><data-analysis><k-means>,1,2,N/A,CC BY-SA 4.0
71354682,1,71355452.0,2022-03-04 16:55:08,0,45,"<p>I'm trying to write a code that will allow me to create a <code>TRUE</code> or <code>FALSE</code> variable within the groups <code>name</code> depending on the value of the earliest record of the column poped of the following <code>data.frame</code>: Thank you guys so much in advance for your help!</p>
<pre><code> library(tidyverse)   
  name&lt;-c(&quot;AAA&quot;,&quot;AAA&quot;,&quot;AAA&quot;,&quot;AAA&quot;,&quot;AAA&quot;,&quot;AAA&quot;,&quot;AAA&quot;)
  poped&lt;-c(NA,1,NA,NA,1,NA,NA)
  order&lt;-c(1:7)
  tag&lt;-c(&quot;X&quot;,&quot;Y&quot;,&quot;X&quot;,&quot;X&quot;,&quot;Y&quot;,&quot;X&quot;,&quot;X&quot;)

&gt;   df
  name order tag poped
1  AAA     1   X    NA
2  AAA     2   Y     1
3  AAA     3   X    NA
4  AAA     4   X    NA
5  AAA     5   Y     1
6  AAA     6   X    NA
7  AAA     7   X    NA
</code></pre>
<p>I want to mutate a two new variable named <code>CHECK</code> and <code> POS</code></p>
<p><code>CHECK</code> will take on the values</p>
<pre><code>    1= If the closest (above) value where the tag column is Y and poped is 1
    0= If the closest (above) value where the tag column is Y and poped is 0
    2 = If the current row has tag = Y
    NA = Otherwise
</code></pre>
<p><code>POS</code> will take on the value of the closest (above) row number where the tag column is Y and poped is 1, and <code>NA</code> otherwise.</p>
<p>My desired outout will be:</p>
<pre><code>&gt;   df
  name order tag poped CHECK POS                                                            why
1  AAA     1   X    NA    NA  NA                                      There is no previous data
2  AAA     2   Y     1    NA  NA                                                current tag = Y
3  AAA     3   X    NA     1   2 the closest value above where tag=Y is in row 2 and poped is 1
4  AAA     4   X    NA     1   2 the closest value above where tag=Y is in row 2 and poped is 1
5  AAA     5   Y     1    NA  NA                                                current tag = Y
6  AAA     6   X    NA     1   5 the closest value above where tag=Y is in row 5 and poped is 1
7  AAA     7   X    NA     1   5 the closest value above where tag=Y is in row 5 and poped is 1
</code></pre>
<p>It will be so much appreciated if you guys could help me out witha solution using tidyverse but I will be open for all posible solutions thank you so much</p>
",15587184.0,-1.0,N/A,2022-03-04 17:57:49,Conditioning previous values within groups in R,<r><data-science><data-wrangling>,1,1,N/A,CC BY-SA 4.0
71301864,1,-1.0,2022-02-28 22:44:33,0,102,"<p>Currently my code creates a table with different columns (Title, Date, etc.) where the news of the Google News keywords can be found. For example, in my code you can see the keyword &quot;Mcdonalds&quot; in the first place. Let's assume that the first four rows of the table contain news about MCdonalds. Now I want there to be another column called Keyword, where next to the news about Mcdonalds there is always the keyword (Mcdonalds) from my code in every row about the news of Mcdonalds. The same should be possible for all keywords. If line 5-10 has messages about Burger King, the keyword Burger King should be in the column Keyword in line 5-10.</p>
<pre><code>import ssl
import smtplib
from GoogleNews import GoogleNews

ssl._create_default_https_context = ssl._create_unverified_context

googlenews = GoogleNews()
googlenews.set_encode('utf_8')
googlenews.set_lang('en')
googlenews.set_period('7d')

googlenews.get_news(&quot;Mcdonalds&quot;)
googlenews.get_news(&quot;Burger King&quot;)
googlenews.get_news(&quot;New York&quot;)

keys = googlenews.results()[0].keys()
Table = []

for row in googlenews.results():
    Table.append({'Keyword': row['??'], 'Title': row['title'], 'Date': row['date'], 'Link': row['link'], 'Source': row['site']})

import pandas as pd
from email.message import EmailMessage

df = pd.DataFrame(Table)
´´´
</code></pre>
",17636578.0,17636578.0,2022-03-01 08:35:28,2022-03-01 08:35:28,Python: How to add a dynamic column in table?,<python><data-science>,0,2,N/A,CC BY-SA 4.0
71320598,1,-1.0,2022-03-02 09:50:13,0,17,"<p>I am trying to do a simple thing on dataframe, but I am getting output differently than I expected.</p>
<p>These are parameters and it's values I am getting from called function:</p>
<pre><code>context        : ['documentNo']
aggOperation   : sum
aggColumn      : amount
comNewColName  : totalAmountCollected
Result type    : &lt;class 'pandas.core.frame.DataFrame'&gt;
Index(['Result Columns :\ndocumentNo', 'Result Columns :\ntotalAmountCollected'], dtype='object')
</code></pre>
<p>Result I am getting:</p>
<pre><code>      documentNo                               totalAmountCollected
0      200002692   8,260  1,132,210  80,536  40,120  14,160  1,6...
1      200002699                                  711,093 (711,093)
2      200002700                                         (968) 968 
3      200002764   5,310  62,304  61,124  14,603  75,190  62,658...
4      200002765                                    10,620 (10,620)
</code></pre>
<p>My groupby code is this:</p>
<pre><code>df = sourceDF.groupby(context)[aggColumn].agg(aggOperation).reset_index().rename(columns={'amount':comNewColName})
</code></pre>
<p>I have to get</p>
<pre><code>     documentNo                               totalAmountCollected
0      200002692                                   978979
1      200002699                                   897897
2      200002700                                   968 
3      200002764                                   877899
4      200002765                                   10,620
</code></pre>
<p>How can I correct my code?</p>
",3655069.0,472495.0,2022-03-13 22:38:06,2022-03-13 22:38:06,How to get sum column with groupby pandas,<python><pandas><dataframe><data-science>,0,3,2022-03-02 09:52:05,CC BY-SA 4.0
71350092,1,-1.0,2022-03-04 10:33:39,1,759,"<p>I want to determine the <strong>quality score</strong> of the <strong>text</strong> by giving them some score or rating (something like ' image-text is 90% bad. Texts are not readable ).</p>
<p><img src=""https://i.stack.imgur.com/CCTul.jpg"" alt=""img1"" />
<img src=""https://i.stack.imgur.com/PUKZu.jpg"" alt=""img2"" /></p>
<p>What I am doing now is I am using the <strong>Blind/referenceless image spatial quality evaluator (BRISQUE)</strong> model to assess the quality.</p>
<p>It gives scores from 0 to 100. 0 score for good quality and 100 for bad quality.</p>
<p><strong>The problem</strong> I am having with this code is that it is giving bad scores to even good quality &quot;images-texts&quot;.
Also, the score exceeds 100 sometimes but according to the <a href=""https://towardsdatascience.com/automatic-image-quality-assessment-in-python-391a6be52c11"" rel=""nofollow noreferrer"">reference</a> I am taking, the score should be between 0 to 100 only.</p>
<p>Can someone please suggest to me how can I get promising and reliable results for assessing the quality of the text-based images?</p>
<pre class=""lang-py prettyprint-override""><code>import collections
from itertools import chain
# import urllib.request as request
import pickle

import numpy as np

import scipy.signal as signal
import scipy.special as special
import scipy.optimize as optimize

# import matplotlib.pyplot as plt

import skimage.io
import skimage.transform

import cv2

from libsvm import svmutil
from os import listdir

# Calculating Local Mean
def normalize_kernel(kernel):
    return kernel / np.sum(kernel)

def gaussian_kernel2d(n, sigma):
    Y, X = np.indices((n, n)) - int(n/2)
    gaussian_kernel = 1 / (2 * np.pi * sigma ** 2) * np.exp(-(X ** 2 + Y ** 2) / (2 * sigma ** 2))
    return normalize_kernel(gaussian_kernel)

def local_mean(image, kernel):
    return signal.convolve2d(image, kernel, 'same')

# Calculating the local deviation
def local_deviation(image, local_mean, kernel):
    &quot;Vectorized approximation of local deviation&quot;
    sigma = image ** 2
    sigma = signal.convolve2d(sigma, kernel, 'same')
    return np.sqrt(np.abs(local_mean ** 2 - sigma))


#  Calculate the MSCN coefficients
def calculate_mscn_coefficients(image, kernel_size=6, sigma=7 / 6):
    C = 1 / 255
    kernel = gaussian_kernel2d(kernel_size, sigma=sigma)
    local_mean = signal.convolve2d(image, kernel, 'same')
    local_var = local_deviation(image, local_mean, kernel)

    return (image - local_mean) / (local_var + C)


# It is found that the MSCN coefficients are distributed as a Generalized Gaussian Distribution (GGD) for a broader spectrum of distorted image.
# Calculate GGD
def generalized_gaussian_dist(x, alpha, sigma):
    beta = sigma * np.sqrt(special.gamma(1 / alpha) / special.gamma(3 / alpha))

    coefficient = alpha / (2 * beta() * special.gamma(1 / alpha))
    return coefficient * np.exp(-(np.abs(x) / beta) ** alpha)


# Pairwise products of neighboring MSCN coefficients
def calculate_pair_product_coefficients(mscn_coefficients):
    return collections.OrderedDict({
        'mscn': mscn_coefficients,
        'horizontal': mscn_coefficients[:, :-1] * mscn_coefficients[:, 1:],
        'vertical': mscn_coefficients[:-1, :] * mscn_coefficients[1:, :],
        'main_diagonal': mscn_coefficients[:-1, :-1] * mscn_coefficients[1:, 1:],
        'secondary_diagonal': mscn_coefficients[1:, :-1] * mscn_coefficients[:-1, 1:]
    })


# Asymmetric Generalized Gaussian Distribution (AGGD) model
def asymmetric_generalized_gaussian(x, nu, sigma_l, sigma_r):
    def beta(sigma):
        return sigma * np.sqrt(special.gamma(1 / nu) / special.gamma(3 / nu))

    coefficient = nu / ((beta(sigma_l) + beta(sigma_r)) * special.gamma(1 / nu))
    f = lambda x, sigma: coefficient * np.exp(-(x / beta(sigma)) ** nu)

    return np.where(x &lt; 0, f(-x, sigma_l), f(x, sigma_r))



# Fitting Asymmetric Generalized Gaussian Distribution
def asymmetric_generalized_gaussian_fit(x):
    def estimate_phi(alpha):
        numerator = special.gamma(2 / alpha) ** 2
        denominator = special.gamma(1 / alpha) * special.gamma(3 / alpha)
        return numerator / denominator

    def estimate_r_hat(x):
        size = np.prod(x.shape)
        return (np.sum(np.abs(x)) / size) ** 2 / (np.sum(x ** 2) / size)

    def estimate_R_hat(r_hat, gamma):
        numerator = (gamma ** 3 + 1) * (gamma + 1)
        denominator = (gamma ** 2 + 1) ** 2
        return r_hat * numerator / denominator

    def mean_squares_sum(x, filter=lambda z: z == z):
        filtered_values = x[filter(x)]
        squares_sum = np.sum(filtered_values ** 2)
        return squares_sum / ((filtered_values.shape))

    def estimate_gamma(x):
        left_squares = mean_squares_sum(x, lambda z: z &lt; 0)
        right_squares = mean_squares_sum(x, lambda z: z &gt;= 0)

        return np.sqrt(left_squares) / np.sqrt(right_squares)

    def estimate_alpha(x):
        r_hat = estimate_r_hat(x)
        gamma = estimate_gamma(x)
        R_hat = estimate_R_hat(r_hat, gamma)

        solution = optimize.root(lambda z: estimate_phi(z) - R_hat, [0.2]).x

        return solution[0]

    def estimate_sigma(x, alpha, filter=lambda z: z &lt; 0):
        return np.sqrt(mean_squares_sum(x, filter))

    def estimate_mean(alpha, sigma_l, sigma_r):
        return (sigma_r - sigma_l) * constant * (special.gamma(2 / alpha) / special.gamma(1 / alpha))

    alpha = estimate_alpha(x)
    sigma_l = estimate_sigma(x, alpha, lambda z: z &lt; 0)
    sigma_r = estimate_sigma(x, alpha, lambda z: z &gt;= 0)

    constant = np.sqrt(special.gamma(1 / alpha) / special.gamma(3 / alpha))
    mean = estimate_mean(alpha, sigma_l, sigma_r)

    return alpha, mean, sigma_l, sigma_r


# Calculate BRISQUE features
def calculate_brisque_features(image, kernel_size=7, sigma=7 / 6):
    def calculate_features(coefficients_name, coefficients, accum=np.array([])):
        alpha, mean, sigma_l, sigma_r = asymmetric_generalized_gaussian_fit(coefficients)

        if coefficients_name == 'mscn':
            var = (sigma_l ** 2 + sigma_r ** 2) / 2
            return [alpha, var]

        return [alpha, mean, sigma_l ** 2, sigma_r ** 2]

    mscn_coefficients = calculate_mscn_coefficients(image, kernel_size, sigma)
    coefficients = calculate_pair_product_coefficients(mscn_coefficients)

    features = [calculate_features(name, coeff) for name, coeff in coefficients.items()]
    flatten_features = list(chain.from_iterable(features))
    return np.array(flatten_features, dtype=object)




# Loading image from local machine
def load_image(file):
    return cv2.imread(file)
    # return skimage.io.imread(&quot;img.png&quot;, plugin='pil')

path = &quot;C:\\Users\\Krishna\\PycharmProjects\\ImageScore\\images2\\&quot;
image_list = listdir(path)

for file in image_list:
    image = load_image(path+file)
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # image = load_image()
    # gray_image = skimage.color.rgb2gray(image)

    # _ = skimage.io.imshow(image)


    #%%time

    #  Calculate Coefficients
    mscn_coefficients = calculate_mscn_coefficients(gray_image, 7, 7/6)
    coefficients = calculate_pair_product_coefficients(mscn_coefficients)

    # Fit Coefficients to Generalized Gaussian Distributions
    brisque_features = calculate_brisque_features(gray_image, kernel_size=7, sigma=7/6)

    # Resize Image and Calculate BRISQUE Features
    downscaled_image = cv2.resize(gray_image, None, fx=1/2, fy=1/2, interpolation = cv2.INTER_CUBIC)
    downscale_brisque_features = calculate_brisque_features(downscaled_image, kernel_size=7, sigma=7/6)

    brisque_features = np.concatenate((brisque_features, downscale_brisque_features))


    #  a pretrained SVR model to calculate the quality assessment. However, in order to have good results, we need to scale the features to [-1, 1]
    def scale_features(features):
        with open('normalize.pickle', 'rb') as handle:
            scale_params = pickle.load(handle)

        min_ = np.array(scale_params['min_'])
        max_ = np.array(scale_params['max_'])

        return -1 + (2.0 / (max_ - min_) * (features - min_))


    def calculate_image_quality_score(brisque_features):
        model = svmutil.svm_load_model('brisque_svm.txt')
        scaled_brisque_features = scale_features(brisque_features)

        x, idx = svmutil.gen_svm_nodearray(
            scaled_brisque_features,
            isKernel=(model.param.kernel_type == svmutil.PRECOMPUTED))

        nr_classifier = 1
        prob_estimates = (svmutil.c_double * nr_classifier)()

        return svmutil.libsvm.svm_predict_probability(model, x, prob_estimates)

    print(calculate_image_quality_score(brisque_features))
</code></pre>
<p>Here is one output for the quality score I am getting for one of the &quot;text-based image&quot;</p>
<blockquote>
<p>156.04440687506016</p>
</blockquote>
",9945146.0,8618242.0,2022-03-04 19:17:25,2022-03-04 19:17:25,Determine the rating for text quality in an Image,<python-3.x><opencv><image-processing><data-science><image-quality>,0,2,N/A,CC BY-SA 4.0
71359323,1,71359896.0,2022-03-05 03:47:55,1,410,"<p>I have two geodataframes: One containing sightings of animals of a specific species called <code>bird_df</code> (with location as points), and the other detailing the boundaries of each municipality within my state called <code>map_df</code>.</p>
<p>I want to use the geopandas method .contains(x) to count the number of animals that were found in each municipality and add that total to the boundaries dataframe so i can generate a choropleth.</p>
<p>My pandas is a bit rusty but I've tried things like
<code>map_df[map_df[&quot;geometry&quot;].contains(bird_df[&quot;geometry&quot;]) == True]</code></p>
<p>I just don't know how to wrap my head around this problem. Some help would be appreciated.</p>
<p>Thanks</p>
",13623174.0,-1.0,N/A,2022-03-05 06:11:41,How to sum the number of entries in a dataframe that are located within a geopandas boundary,<python><pandas><data-science><geopandas>,1,0,N/A,CC BY-SA 4.0
71341856,1,71361150.0,2022-03-03 18:08:43,3,2919,"<p>I created a simple regression model in tensorflow for learning purpose and i am stuck in this problem. Not sure where i have made the mistake, please help me in solving this trivial problem. posting the code below.</p>
<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

X = np.array([2,4,6,8,1,3,5,7,9])
y = np.array([i**2 for i in X])
# we are creating a dataset for y = x^2

#converting the numpy array into tensors
X_tensor = tf.cast(tf.constant(X), dtype = tf.float32)
y_tensor = tf.cast(tf.constant(y), dtype = tf.float32)

model = tf.keras.Sequential([tf.keras.layers.Dense(1)])

model.compile(loss=tf.keras.losses.mae, optimizer=tf.keras.optimizers.SGD(), metrics = [&quot;mae&quot;])

model.fit([X_tensor],y_tensor, epochs=5) 
</code></pre>
<p>executing the above code gives the following error</p>
<pre><code>WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=(&lt;tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32&gt;,). Consider rewriting this model with the Functional API.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-10-3af7f7bccd4f&gt; in &lt;module&gt;()
----&gt; 1 model.fit([X_tensor],y_tensor, epochs=5)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1145           except Exception as e:  # pylint:disable=broad-except
   1146             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:
   1149               raise

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1021, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1010, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1000, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 859, in train_step
        y_pred = self(x, training=True)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py&quot;, line 228, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer &quot;{layer_name}&quot; '

    ValueError: Exception encountered when calling layer &quot;sequential_1&quot; (type Sequential).
    
    Input 0 of layer &quot;dense_1&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)
    
    Call arguments received:
      • inputs=('tf.Tensor(shape=(None,), dtype=float32)',)
      • training=True
      • mask=None
</code></pre>
",18008382.0,-1.0,N/A,2022-03-05 09:50:30,"Input 0 of layer ""dense_1"" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)",<python-3.x><machine-learning><data-science><tensorflow2.0>,1,0,N/A,CC BY-SA 4.0
71363525,1,-1.0,2022-03-05 15:36:34,3,2341,"<p>I am following a video lecture series on YT to learn pandas. The jupyter notebook interface is different on my pc and in the tutorial.</p>
<p>This is the interface in the tutorial:</p>
<p><a href=""https://i.stack.imgur.com/LDv7c.png"" rel=""nofollow noreferrer"">Original Interface</a></p>
<p>This is the interface that I see:</p>
<p><a href=""https://i.stack.imgur.com/fkinA.png"" rel=""nofollow noreferrer"">My interface:</a></p>
<p>As it is clearly seen, the interface in tutorial is compact compared to mine. It also has In and Out symbols before every code line which is missing from my interface. As a result I don't see output to the commands when I run the code. Plz help.</p>
",18383179.0,-1.0,N/A,2023-07-25 10:36:06,Jupyter notebook interface different,<python><pandas><jupyter-notebook><data-science><jupyter>,2,1,N/A,CC BY-SA 4.0
71361904,1,71362341.0,2022-03-05 11:48:16,1,585,"<p>I’m working with different types of financial data inputs for my models and I would like to know more about normalization of them.</p>
<p>In particular, working with some technical indicators, I’ve normalized them to have a range between 0 and 1.</p>
<p>Others were normalized to have a range between -1 and 1.</p>
<p>What is your experience with mixed normalized data?</p>
<p>Could it be acceptable to have these two ranges or is it always better to have the training dataset with a single range i.e. [0 1]?</p>
",8992125.0,3666197.0,2022-03-06 09:59:22,2022-03-07 16:27:25,Deep Learning Data Normalization,<deep-learning><data-science><artificial-intelligence><normalization><quantitative-finance>,1,0,N/A,CC BY-SA 4.0
71371687,1,-1.0,2022-03-06 15:33:08,0,899,"<p>While removing the zero and null values from pandas dataframe, the datatype of field gets changed.</p>
<pre><code>df.info()
</code></pre>
<p>Output :</p>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 10866 entries, 0 to 10865
Data columns (total 2 columns):
budget            10866 non-null int64
revenue           10866 non-null int64
dtypes: int64(2)
memory usage: 509.4+ KB
</code></pre>
<p>After running below code to remove zero and null values the datatype got changed.</p>
<pre><code>temp_list_to_check_zero_values=['budget', 'revenue']
df[temp_list_to_check_zero_values] = df[temp_list_to_check_zero_values].replace(0, np.NAN)
df.info()
</code></pre>
<p>Output :</p>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 3854 entries, 0 to 10848
Data columns (total 2 columns):
budget            3854 non-null float64
revenue           3854 non-null float64
dtypes: float64(2)
memory usage: 210.8+ KB
</code></pre>
<p>To preserve the datatype, we used applymap</p>
<pre><code>df[temp_list_to_check_zero_values] = df[temp_list_to_check_zero_values].applymap(np.int64)
</code></pre>
<p>But got error :</p>
<pre><code>ModuleNotFoundError: No module named 'pandas.core.apply'
</code></pre>
<p>Do we need to install any specific library for using applymap() ?</p>
",9079461.0,-1.0,N/A,2022-03-06 15:35:59,No module named 'pandas.core.apply',<pandas><dataframe><data-science><data-analysis><exploratory-data-analysis>,1,0,N/A,CC BY-SA 4.0
71389641,1,71398041.0,2022-03-08 03:15:22,1,103,"<p>I'm working on an application where datasets have programmatically generated names and are frequently created and destroyed by users. I want to graph these datasets within the application using D3.js.</p>
<p>My datasets are stored like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Wavelength</th>
<th>Transducer Output 1</th>
<th>Transducer Output 2</th>
<th>Transducer Output 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>19</td>
<td>21</td>
<td>23</td>
</tr>
<tr>
<td>3</td>
<td>23</td>
<td>20</td>
<td>21</td>
</tr>
<tr>
<td>5</td>
<td>33</td>
<td>23</td>
<td>19</td>
</tr>
<tr>
<td>7</td>
<td>33</td>
<td>24</td>
<td>45</td>
</tr>
<tr>
<td>etc..</td>
<td>etc..</td>
<td>etc..</td>
<td>etc..</td>
</tr>
</tbody>
</table>
</div>
<p>Where wavelength should be mapped along the x axis, and magnitude mapped along the y axis, with an individual line for each set of magnitudes.</p>
<p>I'm struggling to get my head around how one should pass such data into D3.js. Each tutorial I read uses different data formats and different code. I have read the documentation, but it hasn't helped me much in learning how to format my data for D3 either.</p>
<p>What's the correct way to map these datasets onto a graph from within a script? At the moment I'm trying to use <code>d3.csvParse(data)</code>, but am unsure where to go from there. I suspect I may be formatting my data awkwardly but am not sure.</p>
",15488999.0,15488999.0,2022-03-08 14:56:26,2022-03-08 16:45:05,Correct method for graphing programmatically created data with D3,<javascript><d3.js><data-science><dataformat><graph-data-science>,1,6,N/A,CC BY-SA 4.0
71397110,1,-1.0,2022-03-08 14:59:56,0,140,"<p>I'm trying to overlay a boxplot over a violin plot, but despite setting the &quot;fill&quot; for the boxes to be white, they're not coming up as completely white because they're not opaque. You can see in the picture below how they're allowing the underlying color to seep through. I've tried toying around with zorder a million times but to no avail. Does anyone know how I can make the white completely opaque? Here's my code, and a picture of the plot:</p>
<pre><code>fig, ax = plt.subplots(figsize=(25, 12.5))

vp = ax.violinplot(dataset[:2], widths=0.7, showextrema=False)

list_of_colors = ['tab:blue', 'tab:orange']
for color, poly in zip(list_of_colors, vp['bodies']):
    poly.set_facecolor(color)
    poly.set_edgecolor('#333333')
    poly.set_alpha(0.95)

bp = ax.boxplot(dataset[:2], sym='', widths=0.1, patch_artist=True)

for linetype in bp.values():
    for line in linetype:
        line.set_color('black')
        line.set_alpha(0.5)

for patch in bp['boxes']:
    patch.set(facecolor='white')
</code></pre>
<p><a href=""https://i.stack.imgur.com/jj2Sw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jj2Sw.png"" alt=""enter image description here"" /></a></p>
<p>Thank you so much for any help you can provide.</p>
",12577026.0,-1.0,N/A,2022-03-08 14:59:56,"Matplotlib boxplot ""fill"" is not opaque over violin plot",<python><matplotlib><jupyter-notebook><data-science>,0,5,N/A,CC BY-SA 4.0
71359837,1,71359859.0,2022-03-05 05:45:45,1,38,"<p>I'm writing a function where I'd like to add a series to a dataframe that is <code>True</code> if <code>n</code> previous rows of another column are negative.</p>
<p>I have this working for a specific number as <code>n</code>, but I don't know how to generalize it into a parameter that could be passed in.</p>
<p>For example, we have a dataframe with a column named <code>Total</code>. The following code will put <code>True</code> in rows in a column titled <code>consecutive_negative_value</code> if there are 6 consecutive rows where <code>Total</code> is less than zero. How can this be generalized to accept any number <code>n</code> instead of just checking six periods?</p>
<pre><code>    df['negative_value'] = df['Total'] &lt; 0
    
    df['consecutive_negative_value'] = ( 
        df['negative_value'] &amp;
        df['negative_value'].shift(1) &amp;
        df['negative_value'].shift(2) &amp;
        df['negative_value'].shift(3) &amp;
        df['negative_value'].shift(4) &amp;
        df['negative_value'].shift(5)
    )
</code></pre>
",1089153.0,-1.0,N/A,2022-03-05 05:50:52,Pandas - General function for checking condition on previous rows,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
71369814,1,-1.0,2022-03-06 11:23:42,0,69,"<pre class=""lang-py prettyprint-override""><code>p = [i.text.strip() for i in soup.select('p.card-text')]
j = []
for i in p:
     if p.index(i)%2 == 0:
          j.append(i) 
</code></pre>
<ul>
<li>I am doing this because I only want to extract even position element from my list p.</li>
<li>Is there any other way to do this (to get only even position element from list)?</li>
<li>If not, how can I write the code above using list comprehension? I have extracted even position element using this code. I want to know of any other method I can apply or how to write list comprehension for the above code?</li>
</ul>
",18382743.0,15332650.0,2022-03-06 11:26:54,2022-03-06 11:57:15,How can I do this with list comprehension? Or how can I extract every even position element from list and store it in a list?,<python-3.x><data-science><list-comprehension>,2,0,N/A,CC BY-SA 4.0
71381262,1,-1.0,2022-03-07 12:43:31,-1,45,"<p>I am trying to convert string to int for a column in my dataframe.</p>
<p>I have amount column contains values like this:</p>
<pre><code>123,123
(343,344)
</code></pre>
<p>I am converting this:</p>
<pre><code>123123
343344
</code></pre>
<p>For that my code I wrote:</p>
<pre><code>def strToInt(str2):
    '''print(type(str2))'''
    if type(str2) == str:
        temp = str2.replace(&quot;(&quot;, &quot;&quot;).replace(&quot;)&quot;,&quot;&quot;).replace(&quot;,&quot;,&quot;&quot;)
        '''print(&quot;GOT :&quot; + str2 + &quot;     RETURN :&quot; + str(int(temp)))'''
        if checkInt(temp):
              return int(temp)
    return None

def checkInt(s):
    if s[0] in ('-', '+'):
        return s[1:].isdigit()
    return s.isdigit()



print(df['amount'])

df['amount'] = df[['amount']].apply(lambda a: strToInt(a))
print(df['amount'])
print(df.columns)

print(df['amount'])
</code></pre>
<p>But I am getting all null values:
I check the function strToInt individually, it giving correct output.</p>
<p>But after apply I am getting all NaN values.</p>
<p>Before:</p>
<pre><code>0            45,105 
1            24,250 
2          8,35,440 
3          3,00,900 
4          1,69,920 
</code></pre>
<p>After:</p>
<pre><code>0         NaN
1         NaN
2         NaN
3         NaN
4         NaN
</code></pre>
<p>What can I try to resolve this?</p>
",3655069.0,472495.0,2022-03-13 22:36:47,2022-03-13 22:36:47,Apply a function to a column in pandas giving issues,<python><pandas><dataframe><data-science>,2,3,N/A,CC BY-SA 4.0
71407066,1,71408222.0,2022-03-09 09:30:02,0,159,"<p>I am trying to insert nested dictionary structure into postgredb. My dictionary is as follows:</p>
<pre><code>dict = {'x': {'2022-02-09': 0.8},'y':{'2022-02-14': 0.9},'z':{'2022-01-14': 0.4}}
</code></pre>
<p>and i want to insert into table like this:</p>
<pre><code>a       b     c
x 2022-02-09 0.8
y 2022-02-14 0.9
z 2022-01-14 0.4
</code></pre>
<p>My code is follows as :</p>
<pre><code>conn = pg.connect(&quot;dbname=foo user=foo_user&quot;)
cursor = conn.cursor()

for name , date in dict.items():
cursor.execute(

    &quot;&quot;&quot;
     INSERT INTO &quot;sometable&quot; (&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)
              VALUES (%s,%s,%s)

    &quot;&quot;&quot;, (name, date.keys(),date.values())

)conn.commit()
cursor.close()
conn.close()
</code></pre>
<p>when i run the code, i get <strong>psycopg2.ProgrammingError: can't adapt type 'dict_values'</strong></p>
<p>What might be the solution to import dict values into postgre by using pyscopg2 adapter ?</p>
",12906920.0,-1.0,N/A,2022-03-09 10:57:45,Insert Python Nested Dictionary using Psycopg2,<python><database><postgresql><data-science>,1,0,N/A,CC BY-SA 4.0
71404357,1,-1.0,2022-03-09 04:36:08,-1,443,"<p>I have 2 tables in my database which have time series data. <strong>table 1</strong> has the following schema</p>
<p><a href=""https://i.stack.imgur.com/v66t9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v66t9.png"" alt=""enter image description here"" /></a></p>
<p>using <strong>SQL</strong> I'm creating a <strong>3rd table</strong> using values from 1st and 2nd table. the 3rd table will have 3 columns</p>
<blockquote>
<p>key . date_b, sum</p>
</blockquote>
<p>. The row count in 3rd table should be same as in 2nd table and <strong>sum column</strong> for <strong>each key</strong> will be calculated using the logic, when a_date&gt;b_date sum the count value, otherwise keep it 0.</p>
<p><a href=""https://i.stack.imgur.com/eWeXg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eWeXg.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>There are 3 dates greater than 1st feb in table 1 so the sum is <strong>9</strong>.
while 2 dates greater than 2nd feb in table 1 so sum is 7 and so on.</p>
</blockquote>
<p>in excel i used sumifs and was able to calculate it successfully but in SQL when i'm not able to do it, if i use joins multiple records are created since keys are duplicate. what will be the sql query for creating the table 3?</p>
<p>Sample query that I have used:</p>
<pre><code>SELECT *,
CASE WHEN &quot;date_a&quot;&gt; &quot;date_b&quot; 
    THEN SUM(&quot;count&quot;)OVER(PARTITION BY &quot;key_b&quot;,&quot;date_b&quot; ) 
    ELSE 0 END AS &quot;sum&quot;
from
(
SELECT *
  FROM table 1
) A
right JOIN
(
     SELECT * 
     FROM table_b
     
 ) B
ON A.&quot;key&quot; = B.&quot;key&quot;)
C
</code></pre>
",7655239.0,10910692.0,2022-03-09 05:28:18,2022-03-09 05:28:18,SQL query to calculate sum value in SQL using 2 tables without using a join,<mysql><sql><join><time-series><data-science>,1,6,N/A,CC BY-SA 4.0
71407666,1,71407847.0,2022-03-09 10:15:10,0,46,"<p>The dataset</p>
<pre><code>ID Product
1   A
1   B
2   A 
3   A
3   C 
3   D 
4   A
4   B
5   A
5   C
5   D
.....
</code></pre>
<p>The goal is to have the most frequent combinaisons of product by ID regardless the number of string value.</p>
<p>The expected result here is :</p>
<pre><code>[A, C, D]  2
[A, B]     2
[A, C]     2
......
</code></pre>
<p>Like that but with a working value</p>
<pre><code>import itertools

(df.groupby('ID').Product.agg(lambda x: list(set(itertools.combinations(x,**?**))))
                 .explode().str.join('-').value_counts())
</code></pre>
",10987256.0,-1.0,N/A,2022-03-09 10:27:49,Table with most frequent combinations with pandas python,<python><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
71417338,1,-1.0,2022-03-09 23:54:03,0,36,"<p>I need to group this rows as one row according to &quot;Receipt No_&quot;</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Item No_</th>
<th style=""text-align: left;"">Receipt No_</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">101870</td>
<td style=""text-align: left;"">00000P2811000047666</td>
</tr>
<tr>
<td style=""text-align: left;"">102595</td>
<td style=""text-align: left;"">00000P2811000047666</td>
</tr>
<tr>
<td style=""text-align: left;"">102675</td>
<td style=""text-align: left;"">00000P2811000047666</td>
</tr>
<tr>
<td style=""text-align: left;"">101870</td>
<td style=""text-align: left;"">00000P2811000047666</td>
</tr>
<tr>
<td style=""text-align: left;"">189203</td>
<td style=""text-align: left;"">00000P2811000047666</td>
</tr>
<tr>
<td style=""text-align: left;"">300482</td>
<td style=""text-align: left;"">00000P2811000047666</td>
</tr>
<tr>
<td style=""text-align: left;"">325969</td>
<td style=""text-align: left;"">00000P2811000047670</td>
</tr>
<tr>
<td style=""text-align: left;"">351767</td>
<td style=""text-align: left;"">00000P2811000047670</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>101870 102595 102675 105139</li>
</ol>
<p>I want to have itemsets so that I can use it inside association rule model, I want to have a row like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">101870</th>
<th style=""text-align: left;"">102595</th>
<th style=""text-align: left;"">102675</th>
<th style=""text-align: left;"">105139</th>
<th style=""text-align: left;"">110736</th>
<th style=""text-align: left;"">111139</th>
<th style=""text-align: left;"">122787</th>
<th style=""text-align: left;"">122787</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">356897</td>
<td style=""text-align: left;"">356897</td>
<td style=""text-align: left;"">102675</td>
<td style=""text-align: left;"">105139</td>
<td style=""text-align: left;"">110736</td>
<td style=""text-align: left;"">111139</td>
<td style=""text-align: left;"">122787</td>
<td style=""text-align: left;"">122787</td>
</tr>
</tbody>
</table>
</div>",9376030.0,9376030.0,2022-03-10 00:11:04,2022-03-10 01:22:15,How to group items according to a specific column so that I can use it in association rule model?,<python><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
71353555,1,-1.0,2022-03-04 15:25:41,1,219,"<p>How do I keep a <code>character</code> ID variable <code>PERSON_ID</code> unchanged in a recipe? I tried <code>update_role(PERSON_ID , new_role = &quot;id variable&quot;)</code> and tried excluding it from step_dummy <code>step_dummy(all_nominal_predictors(), -all_numeric_predictors(), -all_outcomes(), -has_role(match = &quot;id variable&quot;)</code>. It does not work. It still converts PERSON_ID to factor. Any suggestion?</p>
",2182905.0,2182905.0,2022-03-04 16:27:37,2022-03-04 23:46:44,How to Exclude a Char Variable from recipes::step_dumm()?,<r><data-science><tidymodels><r-recipes>,1,3,N/A,CC BY-SA 4.0
71399924,1,71400015.0,2022-03-08 18:31:41,2,175,"<p>I have a dataset called <em>graphData</em> with three values, <code>wavelength</code>, <code>magnitude</code>, and <code>name</code>.</p>
<pre><code>let array = [
{name: &quot;dataset 1&quot;, wavelength: 2, magnitude: 20}
{name: &quot;dataset 1&quot;, wavelength: 3, magnitude: 22}
{name: &quot;dataset 1&quot;, wavelength: 4, magnitude: 19}
{name: &quot;dataset 2&quot;, wavelength: 2, magnitude: 14} //and so on...
]
</code></pre>
<p>I've grouped the dataset using <code>d3.group()</code> using the <code>name</code> value as a key</p>
<pre><code>let sortedGraphData = d3.group(graphData, d =&gt; d.fullId)
</code></pre>
<p>Which should return a map structure like this:</p>
<pre><code>{key: &quot;dataset 1&quot;
values: [
{wavelength: 2, magnitude: 20}
{wavelength: 3, magnitude: 22}
{wavelength: 4, magnitude: 19}
]}

{key: &quot;dataset 2&quot;
values: [
{wavelength: 2, magnitude: 14}
///and so on...
]}
</code></pre>
<p>But when I try and draw that data onto my graph with</p>
<pre><code> let valueline = d3.line()
.x(function(d){return x(+d.wavelength)})
.y(function(d){return y(+d.magnitude)});


graph.append(&quot;path&quot;)
.data(sortedSessionGraphData)
.attr(&quot;class&quot;, &quot;line&quot;)
.attr(&quot;d&quot;, valueline);
</code></pre>
<p>I get the following error: <code>Error: &lt;path&gt; attribute d: Expected number, &quot;MNaN,NaNLNaN,NaN&quot;.</code></p>
<p>I figure that <code>d3.line().x(function(d){return +d.wavelength})</code> isn't returning an array of wavelength and magnitude data pairs like I want it to, but I'm not sure how to expose the right data within the map.</p>
<p>How do you navigate through a map and graph new lines for each of the objects contained within a map?</p>
",15488999.0,-1.0,N/A,2022-03-08 19:48:24,How do I graph an individual line for each the groups within a Javascript map object using D3.js,<javascript><dictionary><d3.js><data-science><graph-data-science>,1,0,N/A,CC BY-SA 4.0
71404802,1,71404960.0,2022-03-09 05:41:22,1,77,"<p>I come from the C-Like language world and Python never ceases to amaze me. I realised I have no idea how += operator works in Python. In C-like if I do += on an integer variable, the result is integer:</p>
<pre class=""lang-java prettyprint-override""><code>int a = 1
a += 1;
print(a) // will be 2
</code></pre>
<p>But I have the following program in Python (it's not really important to understand what it is doing, just check out the += operators and the types it operates on):</p>
<pre class=""lang-python prettyprint-override""><code># cost of factories
from pulp import *
cf0 = 450
cf1 = 420
cf2 = 400

# factory throughput
f0 = 2000
f1 = 1500
f2 = 1000

# production goal
goal = 80000

# time limit
max_num_days = 30

problem = LpProblem(&quot;computerAssembly&quot;, LpMaximize)

f0days = LpVariable.dicts(&quot;f0eachDay&quot;, list(range(max_num_days)), cat=&quot;Binary&quot;)
f1days = LpVariable.dicts(&quot;f1eachDay&quot;, list(range(max_num_days)), cat=&quot;Binary&quot;)
f2days = LpVariable.dicts(&quot;f2eachDay&quot;, list(range(max_num_days)), cat=&quot;Binary&quot;)

produced = 0
cost = 0

for i in range(max_num_days):

    # only 2 active per day constraint
    problem += f0days[i] + f1days[i] + f2days[i] &lt;= 2

    # sum up what was produced
    produced += f0days[i]*f0 + f1days[i]*f1 + f2days[i]*f2

    # objective
    cost += f0days[i]*cf0 + f1days[i]*cf1 + f2days[i]*cf2

problem += produced &gt;= goal
problem += -cost

problem.solve()

print(problem)

# print(&quot;Problem solved: &quot;, problem.status())

print(&quot;Problem solved&quot;)
# print(str(problem.status()))

for day in range(max_num_days):
    print(&quot;Day configuration: {}-{}-{}&quot;.format(
        int(f0days[i].varValue), int(f1days[i].varValue), int(f2days[i].varValue)))
</code></pre>

<p>Now, when I print the problem (this is only an excerpt)</p>
<pre><code>MAXIMIZE
-450*f0eachDay_0 + -450*f0eachDay_1 + -450*f0eachDay_10 + -450*f0eachDay_11 +
 -450*f0eachDay_12 + -450*f0eachDay_13 + -450*f0eachDay_14 + -450*f0eachDay_15 +
 -450*f0eachDay_16 + -450*f0eachDay_17 + -450*f0eachDay_18 + -450*f0eachDay_19 + 
-450*f0eachDay_2 + -450*f0eachDay_20 + -450*f0eachDay_21 + -450*f0eachDay_22 + 
-450*f0eachDay_23 + -450*f0eachDay_24 + -450*f0eachDay_25 + -450*f0eachDay_26 + 
-450*f0eachDay_27 + -450*f0eachDay_28 + -450*f0eachDay_29 + -450*f0eachDay_3 + 
-450*f0eachDay_4 + -450*f0eachDay_5 + -450*f0eachDay_6 + -450*f0eachDay_7 + 
-450*f0eachDay_8 + -450*f0eachDay_9 + -420*f1eachDay_0 + -420*f1eachDay_1 + 
-420*f1eachDay_10 + -420*f1eachDay_11 + -420*f1eachDay_12 + -420*f1eachDay_13 + 
-420*f1eachDay_14 + -420*f1eachDay_15 + -420*f1eachDay_16 + -420*f1eachDay_17 + 
-420*f1eachDay_18 + -420*f1eachDay_19 + -420*f1eachDay_2 + -420*f1eachDay_20 + 
-420*f1eachDay_21 + -420*f1eachDay_22 + -420*f1eachDay_23 + -420*f1eachDay_24 + 
-420*f1eachDay_25 + -420*f1eachDay_26 + -420*f1eachDay_27 + -420*f1eachDay_28 + 
-420*f1eachDay_29 + -420*f1eachDay_3 + -420*f1eachDay_4 + -420*f1eachDay_5 + 
-420*f1eachDay_6 + -420*f1eachDay_7 + -420*f1eachDay_8 + -420*f1eachDay_9 + 
-400*f2eachDay_0 + -400*f2eachDay_1 + -400*f2eachDay_10 + -400*f2eachDay_11 + 
-400*f2eachDay_12 + -400*f2eachDay_13 + -400*f2eachDay_14 + -400*f2eachDay_15 + 
-400*f2eachDay_16 + -400*f2eachDay_17 + -400*f2eachDay_18 + -400*f2eachDay_19 + 
-400*f2eachDay_2 + -400*f2eachDay_20 + -400*f2eachDay_21 + -400*f2eachDay_22 + 
-400*f2eachDay_23 + -400*f2eachDay_24 + -400*f2eachDay_25 + -400*f2eachDay_26 + 
-400*f2eachDay_27 + -400*f2eachDay_28 + -400*f2eachDay_29 + -400*f2eachDay_3 + 
-400*f2eachDay_4 + -400*f2eachDay_5 + -400*f2eachDay_6 + -400*f2eachDay_7 + 
-400*f2eachDay_8 + -400*f2eachDay_9 + 0
</code></pre>
<p>It MAGICALLY Knows of every operation I was doing in the following line: <code>cost += f0days[i]*cf0 + f1days[i]*cf1 + f2days[i]*cf2</code></p>
<p>How is it that <code>cost += f0days[i]*cf0 + f1days[i]*cf1 + f2days[i]*cf2</code> does not produce an integer, but it actually remebers every single operation?</p>
<p>Thanks</p>
",1348303.0,-1.0,N/A,2022-03-09 06:03:29,+= Operator in Python,<python><data-science>,1,4,N/A,CC BY-SA 4.0
71409021,1,-1.0,2022-03-09 11:59:32,0,1212,"<p>I'm working on a dashboard in Google Data Studio and my manager would like to have the last edited time of the Data Studio report displayed on a page in the report. I'm using  Google Sheets as the data source for all the charts used. (I do not want to display the date when the Sheet was last edited, but the date when the Data Studio Report was last edited.)</p>
<p>I've tried adding a scorecard and messing around with the fields and formulas to display the date, but I'm only able to show the current date and not the date when the report was last edited.</p>
<p>I'm guessing maybe there's a way to connect to Google Analytics to make this possible, I'm not really sure how. Although, if there's a way without using Google Analytics, that would be wonderful.</p>
",18418434.0,5632629.0,2022-03-09 17:51:53,2022-03-10 14:30:08,Is there a way to add the last edited Date on a Google Data Studio report?,<google-analytics><data-science><data-analysis><looker-studio>,1,0,N/A,CC BY-SA 4.0
71423965,1,71424319.0,2022-03-10 12:12:00,0,692,"<p>I have a data set and there is a feature which containing numbers in string like</p>
<pre><code>&quot;153&quot;, &quot;45&quot;, &quot;13&quot;, &quot;345&quot;
</code></pre>
<p>I'd like to convert these values to integer with python and i wrote this line of code:</p>
<pre><code>df.column = df.column.astype(&quot;int&quot;)
</code></pre>
<p>But i'm getting this error:</p>
<pre><code>invalid literal for int() with base 10: '2,83E+05'
</code></pre>
<p>There is some value like:</p>
<pre><code>3.89E+05, 2.60E+05, 3,13E+05
</code></pre>
<p>How can i convert it to any numerical data type?
Thanks in advance</p>
<pre><code>enter code here
</code></pre>
",14837176.0,2613005.0,2022-03-10 13:03:45,2022-03-10 14:37:52,Invalid value error during converting string to int,<python><python-3.x><pandas><type-conversion><data-science>,1,0,N/A,CC BY-SA 4.0
71400211,1,-1.0,2022-03-08 19:02:11,-1,45,"<p>My csv file looks something like this</p>
<pre><code>Date     Value  otm  oty
Jan 2015  300    na   na
Feb 2015  302    2    na
Mar 2015  303    1    na
Apr 2015  305    2    na
May 2015  307    2    na
Jun 2015  307    0    na
Jul 2015  305    -2   na
Aug 2015  306    1    na
</code></pre>
<p>How can I change all the dates to mm/yyyy. eg Jan 2015 would be 01/2015? Thanks</p>
",18411646.0,18411646.0,2022-03-08 19:04:24,2022-03-08 19:22:01,How to formate dates in csv using python? eg Jan 2001 to 01/2001,<python><pandas><csv><datetime><data-science>,2,1,N/A,CC BY-SA 4.0
71413932,1,71524373.0,2022-03-09 17:56:54,0,526,"<p>I have a Pandas DataFrame with monthly events send by customers, like this:</p>
<pre><code>    df = pd.DataFrame(
[
    ('2017-01-01 12:00:00', 'SID1', 'Something', 'A. Inc'),
    ('2017-01-02 00:30:00', 'SID1', 'Something', 'A. Inc'),
    ('2017-01-02 12:00:00', 'SID2', 'Something', 'A. Inc'),
    ('2017-01-01 15:00:00', 'SID4', 'Something', 'B. GmbH')
],
    columns=['TimeStamp', 'Session ID', 'Event', 'Customer']
)
</code></pre>
<p>The Session IDs are unique, but could spann multiple days. In addition multiple sessions could occur on a given day.</p>
<p>I would like to calculate the minutes of usage for each day of the months per customer like this.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Customer</th>
<th>01.01</th>
<th>02.01</th>
<th>...</th>
<th>31.01</th>
</tr>
</thead>
<tbody>
<tr>
<td>A. Inc</td>
<td>720</td>
<td>30</td>
<td>...</td>
<td>50</td>
</tr>
<tr>
<td>B. GmbH</td>
<td>1</td>
<td>0</td>
<td>...</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>I suspect, that a split of Timestamp into Days and Time, followed by groupby('Customer', 'Day', 'Session ID') and then applying (via apply()) some maths is the way to go, but so far i could not produce any real progress.</p>
",16407479.0,-1.0,N/A,2022-03-18 08:43:06,Calculate Usage time per day and customer in Pandas,<python><pandas><data-science>,2,4,N/A,CC BY-SA 4.0
71423348,1,-1.0,2022-03-10 11:26:43,-2,38,"<p>I have to calculate &quot;totalAmountCollected + endingReceivables - totalSalesDone&quot;</p>
<p>All three columns from three dataframes. There is one common column in three dataframes is clientCode.</p>
<p>How to achieve that?</p>
",3655069.0,472495.0,2022-03-13 22:35:34,2022-03-13 22:35:34,How to calculate a formula contain different columns from different dataframes,<python><pandas><numpy><data-science>,1,1,N/A,CC BY-SA 4.0
71450106,1,-1.0,2022-03-12 13:28:32,1,190,"<p>I have been trying to get my generalized network flow problem in AMPL but I keep running into this error:</p>
<pre><code>presolve: constraint flow_balance['c5'] cannot hold:
    body &gt;= 0 cannot be &lt;= -2500; difference = 2500
presolve: constraint flow_balance['p1'] cannot hold:
    body &lt;= 0 cannot be &gt;= 4500; difference = -4500
presolve: constraint flow_balance['c5'] cannot hold:
    body &gt;= 0 cannot be &lt;= -5300; difference = 5300
presolve: constraint flow_balance['p1'] cannot hold:
    body &lt;= 0 cannot be &gt;= 4800; difference = -4800```


reset;

option solver cplex;

set NODES;                        # nodes in the network
set ARCS within {NODES, NODES};   # arcs in the network

param b {NODES} default 0;        # supply/demand for node i
param c {ARCS}  default 0;        # cost of one of flow on arc(i,j)
param l {ARCS}  default 0;        # lower bound on flow on arc(i,j)
param u {ARCS}  default Infinity; # upper bound on flow on arc(i,j)
param mu {ARCS} default 1;        # multiplier on arc(i,j) -- if one unit leaves i, mu[i,j] units arrive

var x {ARCS};                     # flow on arc (i,j)

data Prob3.dat

maximize profit: sum{(i,j) in ARCS} c[i,j] * x[i,j];  #objective: maximize arc flow profit

# Flow Out(i) - Flow In(i) = b(i)
subject to flow_balance {i in NODES}: sum{j in NODES: (i,j) in ARCS} x[i,j] - sum{j in NODES: (j,i) in ARCS} mu[j,i] * x[j,i] = b[i];
subject to capacity {(i,j) in ARCS}: l[i,j] &lt;= x[i,j] &lt;= u[i,j];
#subject to demand {i in NODES}: sum{j in NODES: (j,i) in ARCS} mu[j,i] * x[j,i] - sum{j in NODES: (i,j) in ARCS} x[i,j] = b[i];

solve;

display profit;
display NODES;
display ARCS;
display x;


#note: default arc costs and lower bounds are 0
#      default arc upper bounds are infinity
#      default node requirements are 0
#      default multiplier is 1
set NODES := p1 p2 p3 p4            #product time period nodes
             r1 r2 r3 r4            #raw material time period nodes
             c1 c2 c3 c4 c5 c5p;    #cash flow time period nodes
             
set ARCS :=  (p1,p2) (p2,p3) (p3,p4)           #inventory arcs
             (r1,r2) (r2,r3) (r3,r4)           #raw inventory arcs
             (c1,c2) (c2,c3) (c3,c4) (c4,c5)   #cash flow arcs  
             (c5,c5p)                          #virtual arc
             (p1,c2) (p2,c3) (p3,c4) (p4,c5)   #buy arcs final
             (r1,c2) (r2,c3) (r3,c4) (r4,c5)   #buy arcs raw
             (c1,p2) (c2,p3) (c3,p4)           #sell arcs final
             (c1,r2) (c2,r3) (c3,r4);          #sell arcs raw
             
param b:= p1  2000     #ending final product on-hand 
          p4  2000;    #initial final product on-hand   
          
#specify costs, upper bound, and multipliers for each arc
param:  c u mu l:=
        [p1, p2]  1.30  3000  0.94   .     #holding cost, capacity, 1-spoilage rate
        [p2, p3]  1.30  3000  0.94   .
        [p3, p4]  1.30  3000  0.94   .
        [c1, c2]  .     .     .      .     #final product period carry over cost
        [c2, c3]  .     .     .      .
        [c3, c4]  .     .     .      .    
        [c4, c5]  .     .     .      .
        [r1, r2]  11    7500  0.45   .     #raw material conversion
        [r2, r3]  11    9000  0.45   .
        [r3, r4]  11    8500  0.45   .
        [p1, c2]  .     3000  38     2000  #final price
        [p2, c3]  .     3000  40     2500
        [p3, c4]  .     5000  42     2800
        [p4, c5]  .     5000  42     2500
        [c1, p2]  .     .    -0.02631579  .     #1/final price
        [c2, p3]  .     .    -0.025       .
        [c3, p4]  .     .    -0.02380952  .
        [r1, c2]  .     7500 0.4    .     #raw price
        [r2, c3]  .     9000 0.4    .
        [r3, c4]  .     8500 0.333  .
        [r4, c5]  .     9200 0.286  .
        [c1, r2]  .     .    -2.5    .     #1/raw price
        [c2, r3]  .     .    -2.5    .
        [c3, r4]  .     .    -3.0    .
        [c5, c5p] -1    .     0      .;    #virtual arc has negative cost to          incentavize flow
</code></pre>
<p>I have posted my dat and mod files for reference. I know that the error is due somehow to the balance constraint but I am unsure why. I have tried to add min demand constraints but that seemed to make everything worse, I have tired redesigning the network flow chart multiple times and I've had no success. If anyone could provide some insight into why I can't figure this error out I would be extremely grateful.</p>
",18446673.0,18446673.0,2022-03-12 13:33:32,2022-03-13 14:05:18,Why does my AMPL code keep giving this error?,<optimization><data-science><ampl><network-flow>,1,0,N/A,CC BY-SA 4.0
71454802,1,-1.0,2022-03-13 07:47:18,0,475,"<p>I have a data frame where I want to replace the values <strong>'&lt;=50K'</strong> and <strong>'&gt;50K'</strong> in the 'Salary' column with <strong>'0'</strong> and <strong>'1'</strong> respectively. I have tried the replace function but it does not change anything. I have tried a lot of things but nothing seems to work. I am trying to do some logistic regression on the cells but the formulas do not work because of the datatype. The real data set has over 20,000 rows.</p>
<blockquote>
<pre><code>Age   Workclass   fnlwgt  education   education-num   Salary 
39  state-gov    455    Bachelors      13            &lt;=50K 
25   private     22      Masters       89             &gt;50K
</code></pre>
</blockquote>
<pre><code>df['Salary']= df['Salary'].replace(['&lt;=50K'],'0')
df['Salary']
</code></pre>
<p>This is the error i get when i try to do smf.logit(). See below code. I don't understand why i get an error because <strong>Age</strong> and <strong>education-num</strong> are both int64.</p>
<pre><code>mod = smf.logit(formula = 'education-num ~ Age', data= dftrn)

resmod = modelAdm.fit()
</code></pre>
<blockquote>
<p>ValueError: endog has evaluated to an array with multiple columns that
has shape (26049, 16). This occurs when the variable converted to
endog is non-numeric (e.g., bool or str).</p>
</blockquote>
",10830246.0,10830246.0,2022-03-13 10:48:10,2022-03-13 10:48:10,Values in a dataframe column will not change,<python><pandas><dataframe><data-science><logistic-regression>,2,0,N/A,CC BY-SA 4.0
71425402,1,71426383.0,2022-03-10 14:01:18,0,225,"<p>I have quite an advanced data wrangling issue in R, I hope you can help me with it. I have a data frame with a column called &quot;Markers&quot;, whereby I know three things:</p>
<ul>
<li>The amount of time between start_trial and tone_onset (variable amount of ms)</li>
<li>The amount of time between tone_onset and stimulus_onset (500ms)</li>
<li>The amount of time between stimulus_onset and end_trial (1000ms)</li>
</ul>
<p>I want to create a column that keeps track of the elapsed time per trial. Unfortunately, the amount of rows between the markers is not consistent with the elapsed time. Therefore, what I want to do is to evenly divide the rows in the amount of ms they should consist of. For example, one trial might have 50 rows between tone_onset and stimulus_onset, and therefore each row should progress 10ms in trial time. Another trial might have 100 rows in between, and then each row should progress 5ms. Furthermore, I want to continue counting the elapsed time until the start of the next trial (so the time between end_trial and start_trial). On top, I want the counting for each trial to centre around stimulus_onset (so everything before counts in negative, everything after in positive). Lastly, I want to label the trial according to their trial numbers. Dataframes speak better than words, so here is a very simplified example:</p>
<pre><code>df &lt;- data.frame(Marker = c(&quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;start_tone&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_stimulus&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;end_trial&quot;, &quot;&quot;, &quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_tone&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_stimulus&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;end_trial&quot;, &quot;&quot;, &quot;&quot;))
</code></pre>
<p>As said before, the time between tone_onset and stimulus_onset is always 500ms, and the time between stimulus_onset and end_trial is always 1000ms. The time between start_trial and tone_onset is variable however. I have a separate data frame with a list of the times between start_trial and tone_onset for each trial:</p>
<pre><code>trial_interval &lt;- (Trial_Interval = c(&quot;395&quot;, &quot;505&quot;))
</code></pre>
<p>What I want to end up with is the following:</p>
<pre><code>df2 &lt;- data.frame(Marker = c(&quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;start_tone&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_stimulus&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;end_trial&quot;, &quot;&quot;, &quot;start_trial&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_tone&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;start_stimulus&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;end_trial&quot;, &quot;&quot;, &quot;&quot;),
                    TrialTime = c(-895, -763.3, -631.7, -500, -400, -300, -200, -100, 0, 142.9, 285.7, 428.7, 571.4, 714.4, 857.3, 1000, 1142.9, -1005, -875.75, -752.5, -626.25, -500, -375, -250, -125, 0, 250, 500, 750, 1000, 1250, 1500),
                    Trial = c(&quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial1&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;, &quot;Trial2&quot;)
                    )
</code></pre>
<p>I tried to simplify this complex problem as best as I good. Let me know if I need to elaborate on something! Thanks a lot, I've been struggling with this for quite some time now.</p>
",15834079.0,15834079.0,2022-03-10 14:33:40,2022-03-10 15:07:48,Evenly divide certain values depending on the number of rows in a group (R),<r><dataframe><dplyr><data-science><data-wrangling>,1,0,N/A,CC BY-SA 4.0
71425483,1,71425563.0,2022-03-10 14:07:00,0,479,"<p>Suppose I am having dataframe like this:</p>
<pre><code>Length    Width    Height
 100       150      130
 120       140      150 
 200       170      120
 250       190      180
 270       200      195 
</code></pre>
<p>Now I want to filter the data from  three columns in a way like Length between (100-200) and width between (130-170) and Height between (120-150), if I extract the data like this, it will give me the data frame like</p>
<pre><code>Length    Width   Height
 100       150      130
 120       140      150 
 200       170      120
</code></pre>
<p>and the remaining data from the dataframe is like</p>
<pre><code>   Length    Width   Height
     250       190      180
     270       200      195 
</code></pre>
<p>Now, I want to see both filtered data and the remaining data from the data set in a separate variables. How to get that in python?</p>
",17976589.0,-1.0,N/A,2022-03-10 14:18:36,extract the remaining data from a dataset after filtering some data in Python,<python><pandas><dataframe><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
71425489,1,-1.0,2022-03-10 14:07:25,0,92,"<p>I have been learning LDA and QDA for predicting models in R, I picked up ISLR book and tried working on some problems, I used the Auto.csv dataset and wanted to run a model using LDA and QDA</p>
<p>here is my code, I get a NA value for mean and whenever I run the predict() function I get this warning message</p>
<pre><code>auto.qda.pred&lt;-predict(auto.qda,auto.test)
Warning message:
In FUN(newX[, i], ...) : no non-missing arguments to min; returning Inf
&gt; auto.qda.pred
  

#Here is my code for the whole process, sorry if its too long 
Auto&lt;-read.csv(file.choose(),header=TRUE)
auto&lt;-na.omit(Auto)
attach(auto)
colnames(auto)[1]&lt;-'Mpg'
library('GGally')
auto$mpg01&lt;-ifelse(auto$Mpg&gt;median(auto$Mpg),1,0)
auto$mpg01&lt;-as.factor(auto$mpg01)
auto$horsepower&lt;-as.numeric(auto$horsepower)
str(auto)
summary(auto)
#mpg highly correlated with mpg01, but not with other variables like horsepower, weight etc..
split_size=0.7
sample_size=floor(split_size*nrow(auto))
set.seed(123)
train_indices&lt;-sample(seq_len(nrow(auto)),size=sample_size)
train_indices
auto.train&lt;-auto[train_indices,]
auto.test&lt;-auto[-train_indices,]
nrow(auto.train)
nrow(auto.test)
library('MASS')
library('caret')
auto.lda&lt;-lda(mpg01~cylinders+displacement+horsepower+weight,data=auto.train)
auto.pred=predict(auto.lda,auto.test)
na.omit(auto.pred)
auto.test
auto.pred$class
mean(auto.pred$class!=auto.test$mpg01)

set.seed(123)
auto.qda&lt;-qda(mpg01~cylinders+displacement+horsepower+weight,data=auto.train)
auto.qda
auto.qda.pred&lt;-predict(auto.qda,auto.test)
na.omit(auto.qda.pred)
mean(auto.qda.pred$class!=auto.test$mpg01)
[1] NA



 
    
</code></pre>
",15077488.0,15077488.0,2022-03-10 14:24:34,2022-03-10 14:24:34,QDA and LDA returning NA mean in R,<r><regression><data-science><classification>,0,5,N/A,CC BY-SA 4.0
71435589,1,71435713.0,2022-03-11 08:25:40,2,171,"<p>I have a dataframe that looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>User</th>
<th>Product</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>a</td>
</tr>
<tr>
<td>1</td>
<td>b</td>
</tr>
<tr>
<td>2</td>
<td>a</td>
</tr>
<tr>
<td>2</td>
<td>c</td>
</tr>
<tr>
<td>3</td>
<td>b</td>
</tr>
</tbody>
</table>
</div>
<p>I want 1 row per user with the products as columns where it gives a 1 or 0 if the user purchased the product or not, how can I do this?</p>
",18409560.0,12416453.0,2022-03-11 08:42:09,2022-03-11 08:42:09,Python Dataframe Binary Encoding,<python><pandas><data-science><analytics><data-cleaning>,2,1,2022-03-11 08:43:35,CC BY-SA 4.0
71457034,1,-1.0,2022-03-13 13:06:44,0,180,"<p>i was wondering if its possible to change column values with another based on date and employee , <code>E/X=6</code> represent the time the employee has entered and <code>E/X=1</code> is the time he's out
each employee has <code>ID</code> and <code>Date</code> he can go out and enter multiple times a day</p>
<pre><code>import pandas as p
import numpy as n

df=p.read_excel(&quot;C:/Users/Hercules/Desktop/Employee.xls&quot;)
df

ID E/X            DateTime        Date     Time
107 6 2022-01-04  10:04:18 0 2022-01-04 10:04:18
107 6 2022-01-04  11:32:52 0 2022-01-04 11:32:52
107 6 2022-01-04  11:39:59 0 2022-01-04 11:39:59
107 1 2022-01-04  12:05:26 0 2022-01-04 12:05:26
107 6 2022-01-04  18:02:18 0 2022-01-04 18:02:18
107 6 2022-01-04  18:30:38 0 2022-01-04 18:30:38
107 1 2022-01-04  19:06:58 0 2022-01-04 19:06:58
107 1 2022-01-05  12:22:10 0 2022-01-05 12:22:10
107 6 2022-01-05  19:22:15 0 2022-01-05 19:22:15
122 1 2022-01-03  08:57:40 0 2022-01-03 08:57:40
122 6 2022-01-03  12:49:33 0 2022-01-03 12:49:33
122 1 2022-01-03  13:22:28 0 2022-01-03 13:22:28
122 6 2022-01-03  16:29:51 0 2022-01-03 16:29:51
122 1 2022-01-03  16:40:06 0 2022-01-03 16:40:06
</code></pre>
<p>I want to find a solution to modify some values in <code>E/X</code> in order to calculate the appropriate working time for each employee . every day for every employee have to start with <code>6</code> and ends with <code>1</code>, also every two consecutive rows should be like this and its possbile to drop rows because the count of rows for every id in a day must me pair (he entres and then he goes out,that's the algorithm <code>6</code> and then <code>1</code> , <code>6</code> and then <code>1</code> ):</p>
<pre><code>ID E/X             DateTime        Date     Time
107 6 2022-01-04  10:04:18 0 2022-01-04 10:04:18
107 1 2022-01-04  11:32:52 0 2022-01-04 11:32:52
</code></pre>
<p><strong>Desired Result</strong></p>
<pre><code>ID E/X            DateTime         Date     Time
107 6 2022-01-04  10:04:18 0 2022-01-04 10:04:18
107 1 2022-01-04  11:32:52 0 2022-01-04 11:32:52
107 6 2022-01-04  11:39:59 0 2022-01-04 11:39:59
107 1 2022-01-04  12:05:26 0 2022-01-04 12:05:26
107 6 2022-01-04  18:02:18 0 2022-01-04 18:02:18
107 1 2022-01-04  19:06:58 0 2022-01-04 19:06:58
107 6 2022-01-05  12:22:10 0 2022-01-05 12:22:10
107 1 2022-01-05  19:22:15 0 2022-01-05 19:22:15
122 6 2022-01-03  08:57:40 0 2022-01-03 08:57:40
122 1 2022-01-03  12:49:33 0 2022-01-03 12:49:33
122 6 2022-01-03  13:22:28 0 2022-01-03 13:22:28

122 1 2022-01-03  16:40:06 0 2022-01-03 16:40:06
</code></pre>
",18343968.0,18343968.0,2022-03-13 15:22:28,2022-03-13 15:22:28,Pandas: How to transform data and change column value?,<python><pandas><dataframe><pandas-groupby><data-science>,1,6,N/A,CC BY-SA 4.0
71409582,1,-1.0,2022-03-09 12:45:15,0,92,"<p>I have the following data:</p>
<pre><code>plate,part,posX,posY,rotation
1,FSHN01-R-58.stl,14.5,9.5,180
1,FSHN01-R-58.stl,14.5,9.5,180
1,FSHN01-E-2.stl,44.5,6.5,270
1,FSHN01-N-3.stl,88,7,0
2,FSHN01-N-7.stl,70.5,70.5,90
2,FSHN01-N-1.stl,128.5,64.5,180
2,FSHN01-N-1.stl,113.5,69.5,90
7,FSHN01-R-58.stl,14.5,9.5,180
7,FSHN01-R-58.stl,14.5,9.5,180
7,FSHN01-E-2.stl,44.5,6.5,270
7,FSHN01-N-3.stl,88,7,0
</code></pre>
<p>I want to group plates by part and find same plates. For example on this data 1,7 have same parts. posX, posY, rotation its not important for me and i need to find part counts of each plates.</p>
<p>For this, i wrote this code:</p>
<pre><code>import pandas as pd

df = pd.read_csv(&quot;public/33/stls/plates.csv&quot;)


result = df.groupby(['plate', 'part']).size()

print(result)
</code></pre>
<p>and i got.</p>
<pre><code>plate  part           
1      FSHN01-E-2.stl     1
       FSHN01-N-3.stl     1
       FSHN01-R-58.stl    2
2      FSHN01-N-1.stl     2
       FSHN01-N-7.stl     1
7      FSHN01-E-2.stl     1
       FSHN01-N-3.stl     1
       FSHN01-R-58.stl    2
</code></pre>
<p>so, 1. and 7. plates are same, how can i drop 7. plate from table and increase 1. plate plate count variable.</p>
<p>i need this result;</p>
<pre><code>plate  part           part count  plate count      
1      FSHN01-E-2.stl     1           2
       FSHN01-N-3.stl     1           2
       FSHN01-R-58.stl    2           2
2      FSHN01-N-1.stl     2           1
       FSHN01-N-7.stl     1           1
</code></pre>
",18418706.0,-1.0,N/A,2022-03-10 09:36:16,Pandas Group Data and Aggregate Variables and Find Duplicates,<python><pandas><csv><data-science>,2,1,N/A,CC BY-SA 4.0
71420482,1,71420603.0,2022-03-10 07:49:43,1,147,"<p>How to form a list of images of multiple formats in a Kaggle dataset where the image paths are like?
Working in Kaggle I wanted to convert the image paths into the list so that I can store and perform operations but couldn't find a proper image traversing Algo to give me the required list result.</p>
<p>Tree for the image is:</p>
<pre><code>|-data
   |-images
        |-ID0
          |--- img4tgh4r3.jpg
          |--- img324633.png
          |
          .
          .
        |-ID1
        .
        .
</code></pre>
<p>I tried using <code>ls -a</code> but how do you convert this structure and save it into a data type to reuse it.</p>
<pre><code>import os
  

path = &quot;/&quot;
dir_list = os.listdir(path)
  
print(&quot;Files and directories in '&quot;, path, &quot;' :&quot;) 
  
# print the list
print(dir_list)
</code></pre>
<p>This only lists the directories but not all the image types.</p>
",15142054.0,15142054.0,2022-04-06 17:58:59,2022-04-06 17:58:59,List image paths of multiple formats in a Kaggle Dataset,<python><data-science><kaggle>,1,1,2022-03-10 08:38:54,CC BY-SA 4.0
71430645,1,-1.0,2022-03-10 20:50:29,0,28,"<pre><code>len(df['education'].str.contains('Masters'))
45175

len(df['education'].str.contains('Bachelors'))
45175

df.shape
(45175, 12)
</code></pre>
<p>Someone please explain. Where's the mistake?</p>
",18210413.0,-1.0,N/A,2022-03-10 21:02:51,The column 'education' is a object type. Why is this code not working? Why show the same result in both cases?,<python><pandas><dataframe><data-science><data-analysis>,1,2,N/A,CC BY-SA 4.0
71466300,1,71469752.0,2022-03-14 10:32:20,0,57,"<p>i have extracted these values from a dictionary called 'mun_dict', now i want to take this extracted values and then put them in a new column of my dataframe, the column has 200 indexes but the values in the mun_dict are only 7.</p>
<p>I want to put the mun_dict values in the new column and then use 'np.nan' when a municipality is not found.</p>
<p>I tried this but it returned this error: 'Length of values (2) does not match length of index (200)'</p>
<p>This is the code i tried,</p>
<pre><code>mun_dict.values()

twitter_df['municipality'] =[mun_dict.values(), np.nan]

twitter_df[['municipality']]
</code></pre>
<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\common.py in require_length_match(data, index)
    529     &quot;&quot;&quot;
    530     if len(data) != len(index):
--&gt; 531         raise ValueError(
    532             &quot;Length of values &quot;
    533             f&quot;({len(data)}) &quot;

ValueError: Length of values (2) does not match length of index (200)
</code></pre>
",18461741.0,18461741.0,2022-03-14 10:52:08,2022-03-14 14:55:06,how do i put this dictionary values in a new column of a dataframe called 'municipality'?,<python><data-science>,1,2,N/A,CC BY-SA 4.0
71467702,1,-1.0,2022-03-14 12:20:44,1,37,"<p>Let's assume I have 5 features;</p>
<p><strong>feature 1, feature 2, feature 3, year, country and outcome</strong> that I trined a mode on them.
And now I want to predict the outcome in the new year (the new year does not include in the training dataset) and specific country but I do not have information for other features.</p>
<p>so how can I create assumptions for this missing data or there are any statistical techniques that can help to fill this missing whether numerical or category. ( I am not sure if I apply the mean or mode to fill these data it will help me to make a good prediction )</p>
",12454100.0,18454578.0,2022-03-17 18:02:03,2022-03-17 18:02:03,What is the best technique to deal with missing information in test data?,<time-series><data-science><forecasting>,0,0,N/A,CC BY-SA 4.0
74618290,1,-1.0,2022-11-29 17:54:31,1,554,"<p>So, I'm trying to develop a ml model for multiple linear regression that predicts the Y given n number of X variables.  So far, my model can read in a data set and give the predicted value with a coefficient of determination as well as the respective coefficients for a 1-unit increase in X.  The only issues are:</p>
<ol>
<li><p>I can't get the p-value for the life of me, it says most of the time the data isn't shaped right due to it being 5 columns and 1329 rows.  When I do get an output, they're just incorrect, I know because I did the regression in analysis toolpak in excel.</p>
</li>
<li><p>Is there a way to make the model recursive so that it recognizes the highest pvalue above .05 and calls itself again without said value until it hits the base case.  Which would be something like</p>
</li>
</ol>
<p>While dependent_v[pvalue] &gt; .05:</p>
<ol start=""3"">
<li>Also what would be the best visualization method to show my data?</li>
</ol>
<p>Thank you for any and all that help, I'm just starting to delve into machine learning on my own and want to hone my skills before an upcoming data science internship in the summer.</p>
<p>import matplotlib.pyplot as plt
import pandas as pd
from sklearn import linear_model</p>
<p>def multipleReg():</p>
<pre><code>dfreg = pd.read_csv(&quot;dfreg.csv&quot;)

#Setting dependent variables
dependent_v = ['Large_size', 'Mid_Level', 'Senior_Level', 'Exec_Level', 'Company_Location']
#Setting independent variable
independent_v  = 'Salary_In_USD'

X = dfreg[dependent_v] #Drawing dependent variables from dataframe
y = dfreg[independent_v] #Drawing independent variable from dataframe

reg = linear_model.LinearRegression() #Initializing regression model
reg.fit(X.values,y) #Fitting appropriate values 

predicted_sal = reg.predict([[1,0,1,0,0]]) #Prediction using 2 dimensions in array

percent_rscore = (reg.score(X.values,y)*100) #Model coefficient of determination 

print('\n')

print(&quot;The predicted salary is:&quot;, predicted_sal)

print(&quot;The Coefficient of deterimination is:&quot;, &quot;{:,.2f}%&quot;.format(percent_rscore))

#Printing coefficents of dependent variables(How much Y increases due to 1
#unit increase in X)
print(&quot;The corresponding coefficients for the dependent variables are:&quot;, reg.coef_)
</code></pre>
",20635426.0,-1.0,N/A,2022-11-29 18:07:33,How can I find the respective P-values for a multiple linear regression using the linear model from sklearn?,<python><machine-learning><statistics><regression><data-science>,1,0,N/A,CC BY-SA 4.0
74621189,1,-1.0,2022-11-29 23:02:48,0,61,"<p>It's about a data project. I have a problem with types of variables and I guess I am missing something that I can not see. I am beginner at this topic any help would be appreciated.</p>
<p>I have 8 normalised arrays and I want to put them into a dataframe so I can create a correlation matrix. But I have this error.</p>
<pre><code>

&gt; ValueError: Per-column arrays must each be 1-dimensional


</code></pre>
<p>I have tried to reshape my arrays but it did not work but I wanted to see that shape of arrays is equal or not so I wrote:</p>
<pre><code>print(date.shape,normalised_snp.shape,normalised_twybp.shape,normalised_USInflation.shape,normalised_USGDP.shape,normalised_USInterest.shape,normalised_GlobalInflation.shape,normalised_GlobalGDP.shape)

</code></pre>
<p>Then my output is</p>
<pre><code>
&gt; (4220, 1) (4220, 1) (4220, 1) (4220, 1) (4220, 1) (4220, 1) (4220, 1) (4220, 1)



</code></pre>
<p>After that I converted my arrays into a list and create a dataframe with those lists.</p>
<pre><code>normalised_snp = normalised_snp.tolist()
normalised_tybp = normalised_tybp.tolist()
normalised_twybp = normalised_twybp.tolist()
normalised_USInflation = normalised_USInflation.tolist()
normalised_USGDP = normalised_USGDP.tolist()
normalised_USInterest = normalised_USInterest.tolist()
normalised_GlobalInflation = normalised_GlobalInflation.tolist()
normalised_GlobalGDP = normalised_GlobalGDP.tolist()
</code></pre>
<p>I constructed the data frame:</p>
<pre><code>alldata = pd.DataFrame({'S&amp;P 500 Price':normalised_snp,
                        '10 Year Bond Price': normalised_tybp,
                        '2 Year Bond Price' : normalised_twybp,
                        'US Inflation' : normalised_USInflation,
                        'US GDP' : normalised_USGDP,
                        'US Insterest' : normalised_USInterest,
                        'Global Inflation Rate' : normalised_GlobalInflation,
                        'Global GDP' : normalised_GlobalGDP})
</code></pre>
<p>After that I have contstructed my correlation matrix</p>
<pre><code>correlation_matrix = alldata.corr()
print(correlation_matrix)
</code></pre>
<p>Since then I have no error but my correlation matrix looks empty</p>
<pre><code>
&gt; Empty DataFrame
Columns: []
Index: []


</code></pre>
<p>Is the problem caused by list type? If it is how can I solve the value error that occurs when I try to construct a data frame with matrices?</p>
",19017614.0,19017614.0,2022-11-29 23:06:10,2022-11-29 23:30:48,Correlation Matrix with Lists or Can not create DataFrame with Arrays,<python><arrays><dataframe><data-science><finance>,1,2,N/A,CC BY-SA 4.0
74617602,1,74617722.0,2022-11-29 17:00:18,1,41,"<p>I currently have a dataframe like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Person</th>
<th>Analysis</th>
<th>Dexterity</th>
<th>Skills</th>
</tr>
</thead>
<tbody>
<tr>
<td>174</td>
<td>3.76</td>
<td>4.12</td>
<td>1.20</td>
</tr>
<tr>
<td>239</td>
<td>4.10</td>
<td>3.78</td>
<td>3.77</td>
</tr>
<tr>
<td>557</td>
<td>5.00</td>
<td>2.00</td>
<td>4.40</td>
</tr>
<tr>
<td>674</td>
<td>2.23</td>
<td>2.40</td>
<td>2.80</td>
</tr>
<tr>
<td>122</td>
<td>3.33</td>
<td>4.80</td>
<td>4.10</td>
</tr>
</tbody>
</table>
</div>
<p>I want to add an column to compile all this information like below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Person</th>
<th>Analysis</th>
<th>Dexterity</th>
<th>Skills</th>
<th>new_column</th>
</tr>
</thead>
<tbody>
<tr>
<td>174</td>
<td>3.76</td>
<td>4.12</td>
<td>1.20</td>
<td>{&quot;Analysis&quot;:&quot;3.76&quot;,   &quot;Dexterity&quot;:&quot;4.12&quot;, &quot;Skills&quot;:&quot;1.20&quot;}</td>
</tr>
<tr>
<td>239</td>
<td>4.10</td>
<td>3.78</td>
<td>3.77</td>
<td>{&quot;Analysis&quot;:&quot;4.10&quot;,   &quot;Dexterity&quot;:&quot;3.78&quot;, &quot;Skills&quot;:&quot;3.77&quot;}</td>
</tr>
<tr>
<td>557</td>
<td>5.00</td>
<td>2.00</td>
<td>4.40</td>
<td>{&quot;Analysis&quot;:&quot;5.00&quot;,   &quot;Dexterity&quot;:&quot;2.00&quot;, &quot;Skills&quot;:&quot;4.40&quot;}</td>
</tr>
<tr>
<td>674</td>
<td>2.23</td>
<td>2.40</td>
<td>2.80</td>
<td>{&quot;Analysis&quot;:&quot;2.23&quot;,   &quot;Dexterity&quot;:&quot;2.40&quot;, &quot;Skills&quot;:&quot;2.80&quot;}</td>
</tr>
<tr>
<td>122</td>
<td>3.33</td>
<td>4.80</td>
<td>4.10</td>
<td>{&quot;Analysis&quot;:&quot;3.33&quot;,   &quot;Dexterity&quot;:&quot;4.80&quot;, &quot;Skills&quot;:&quot;4.10&quot;}</td>
</tr>
</tbody>
</table>
</div>",16769072.0,-1.0,N/A,2022-11-30 17:11:45,Create a custom column in a dict format,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
74617679,1,-1.0,2022-11-29 17:06:37,0,36,"<p>I've created a program that iterates 4 array values with its respective recalls,
also using shuffle function to dynamically shuffle them every time it execute.</p>
<pre><code>def predict_IT():
    new_Xdata_IT = X_IT.sample(5)
    new_Ydata_IT = Cat_Y[new_Xdata_IT.index.values]
    pred_IT = model_IT.predict(new_Xdata_IT)

    def recall(new_Ydata_IT, pred_IT, K):
            act_set = set(new_Ydata_IT)
            pred_set = set(pred_IT[:K])
            result = round(len(act_set &amp; pred_set) / float(len(act_set)), 2)
            return result

    actual = new_Ydata_IT
    prediction = np.array([&quot;Software Engineer / Programmer&quot;, &quot;Technical Support Specialist&quot;, &quot;Academician&quot;, &quot;Administrative Assistant&quot;])
    np.random.shuffle(prediction)
    
    def returnResult():
        output = []
        for K in range(0, 4):
            output.append(f&quot;{prediction[K]} = {recall(actual, prediction, K)}&quot;)
            return output
        
    return render_template(&quot;predictIT.html&quot;, prediction_text = returnResult())
</code></pre>
<p>It successfully return the first index but the remaining 3 values won't come after. I suspect the render_template function doesn't allow any iterations to be passed if so is there any other way for me to iterate the given array. any respond is highly appreciated.</p>
",14854165.0,14854165.0,2022-11-29 17:13:35,2022-11-29 17:13:35,Is there a way to return multiple values using for range in render_template flask,<python><flask><data-science>,0,2,N/A,CC BY-SA 4.0
74632631,1,-1.0,2022-11-30 18:34:29,0,97,"<p>I have a dataframe:</p>
<pre><code>df = C1  C2  E  
     1    2  3
     4    9  1
     3    1  1 
     8    2  8
     8    1  2
</code></pre>
<p>I want to add another columns that will have the count of the value that is in the columns 'E' in all the dataframe (in the column E)
So here the output will be:</p>
<pre><code>df = C1. C2. E. cou 
     1.   2. 3.  1 
     4.   9. 1.  2
     3.   1. 1   2
     8.   2. 8.  1
     8.   1. 2.  1 #2 appears only one it the column E
</code></pre>
<p>How can it be done efficiently ?</p>
",6057371.0,6057371.0,2022-11-30 18:52:08,2022-11-30 18:52:08,Pandas add column of count of another column across all the datafram,<python><pandas><dataframe><data-science><data-munging>,1,1,N/A,CC BY-SA 4.0
71455849,1,71456069.0,2022-03-13 10:32:37,0,60,"<p>was doing LDA analysis on some dataset in ISLR library,
I defined a another column as &quot;crime&quot;, I ran analysis through glm() and ran it again using lda, however it is not finding the crime variable column which is weird because its the same train dataset I used, here is my code</p>
<pre><code>Library(ISLR)
data(Boston)
Boston
nrow(Boston)
ncol(Boston)
colnames(Boston)
summary(Boston)
Boston$crime&lt;-ifelse(Boston$crim&gt;median(Boston$crim),1,0)
split_size=0.7
sample_size=floor(split_size*nrow(Boston))
set.seed(123)
train_indices&lt;-sample(seq_len(nrow(Boston)),size=sample_size)

boston.train&lt;-Boston[train_indices,]
boston.test&lt;-Boston[-train_indices,]
summary(boston.train)
summary(boston.test)

boston.lda&lt;-lda(crime~.,abs(-crime-crim),data=boston.train)
lda.pred&lt;-predict(boston.lda,boston.test)
boston.train
</code></pre>
",15077488.0,-1.0,N/A,2022-03-13 11:00:15,"LDA analysis, not finding object in data frame despite it being defined",<r><regression><data-science><classification>,1,0,N/A,CC BY-SA 4.0
74623282,1,-1.0,2022-11-30 05:34:49,0,263,"<p>I am working with m<em>ulticlass classifier</em>, so during calling <code>predicat_proba()</code> i am facing with such error:</p>
<pre><code>AttributeError: 'OutputCodeClassifier' object has no attribute 'predict_proba'
</code></pre>
<p>I checked documentation of <code>scikit-learn</code> (multioutput-classifier) that support <code>predict_proba()</code> but when I am calling this method I am facing with above error that I have shown.</p>
<p>I tried this code:</p>
<pre><code>X_train_logregr, X_test_logregr, y_train_logregr, y_test_logregr = train_test_split(X_train, y_train, test_size = 0.3, random_state=42)

scaler_logregr = StandardScaler() X_train_scaled_logregr = scaler_logregr.fit_transform(X_train_logregr)

X_test_scaled_logregr = scaler_logregr.transform(X_test_logregr)

logreg = LogisticRegression()
output = OutputCodeClassifier(logreg)
output.fit(X_train_scaled_logregr, y_train_logregr)
y_pred = output.predict(X_test_scaled_logregr)

y_pred_proba = output.predict_proba(X_test_scaled_logregr)
</code></pre>
<p>but when i am running it shows this error:</p>
<pre><code>AttributeError: 'OutputCodeClassifier' object has no attribute 'predict_proba'
</code></pre>
",9695766.0,16420204.0,2022-11-30 20:25:20,2022-11-30 20:25:20,OutputCodeClassifier object has no attribute 'predict_proba',<machine-learning><data-science><multiclass-classification>,1,2,N/A,CC BY-SA 4.0
74643176,1,-1.0,2022-12-01 14:08:02,0,57,"<p>As part of a machine learning architecture I'm building I need to parallelise a certain calculation in pytorch. For simplicity I'm going state a modified version of the problem and use numpy so it's easier to understand.</p>
<p>Suppose I have a collection of football teams (say 10) and they play a collection of matches (say 20). Each football team is represented by an ID (a number from 1-10). The match outcomes are saved as tuples <code>(t_1, t_2, win)</code> where <code>t_i</code> is the ID (int) for 'team i', and win=1 if team 1 wins (win=-1 if team 2 wins).</p>
<p>I want to calculate the total number of wins for every team. More specifically I want an numpy array <code>X</code> (of shape (10)) where <code>X[t_i]</code> := wins - losses (of 'team i' from the 20 matches). Assuming the match data is split into numpy arrays <code>match</code> (of shape (20, 2)), and <code>outcome</code> (of shape (20,1)), my current solution for solving this problem is as follows</p>
<pre><code>outcome = np.concatenate((outcome, -outcome), axis=1)
for i in range(20):
    X[match[i]] += outcome[i]
</code></pre>
<p>Now as you can guess, I want to get rid of the for loop. If I was to replace this code with</p>
<pre><code>X[match] += outcome
</code></pre>
<p>Then clearly the it will not work. Does anyone have any ideas how to solve this problem completely in parallel? Like I said, my problem is actually more complicated than what I've stated here. It's closer to wanting to calculate the win/loss total for each player on each team. If possible could someone provide a solution which is not dependant on there only being two teams in each match. Thanks in advance.</p>
",5578403.0,-1.0,N/A,2022-12-01 22:07:16,Calculating net wins for football teams,<python><numpy><machine-learning><pytorch><data-science>,1,2,N/A,CC BY-SA 4.0
71762251,1,-1.0,2022-04-06 06:51:59,0,232,"<blockquote>
<p>I am trying to build a word2vec using skip-gram and negative sampling
However, when I try to build the vocab dictionary, I have encountered an error as written on the title.
This is actually a sentiment analysis project, but I plan to use word2vec to carry out features extraction before the sentiment analysis.
Can anyone help me to solve this problem? thank you in advance</p>
</blockquote>
<pre><code>def vocab(self):
    # sentences: list of sentence token lists
    # [[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']], [[]], ...]
    sentences = self.sentences
    vocab = defaultdict(dict)
    vocab_words = ['int']
    vocab['int']['word_count'] = 0 
    vocab_size = 0
    for sent_tokens in sentences:
        for word1 in sent_tokens:
            vocab_size += len(word1)
            for word in word1:
                if not word.isdigit() and word not in vocab:
                    vocab[word]['word_count'] = 1
                    vocab_words.append(word)
                else:
                    if word.isdigit():
                        vocab['int']['word_count'] += 1 
                    else:
                        vocab[word]['word_count'] += 1
    low_freq_words = []
    for word in vocab:
        if vocab[word]['word_count'] &lt; self.min_count:
            low_freq_words.append(word)
    for word in low_freq_words:
        vocab_size -= vocab[word]['word_count']
        del vocab[word]
        vocab_words.remove(word)
    sorted_vocab = []
    for word in vocab:
        sorted_vocab.append((word, vocab[word]['word_count']))
    sorted_vocab.sort(key=lambda tup: tup[1], reverse=True)
    for idx, word in enumerate(sorted_vocab):
        vocab[word[0]]['word_freq'] = vocab[word[0]]['word_count'] / vocab_size
        vocab[word[0]]['word_index'] = idx
    return vocab
</code></pre>
",14811578.0,-1.0,N/A,2022-04-06 06:51:59,Word2vec TypeError: 'collections.defaultdict' object is not callable,<python><data-science><word2vec>,0,10,N/A,CC BY-SA 4.0
71764346,1,-1.0,2022-04-06 09:34:38,1,46,"<p>Here is the background information.
<a href=""https://stackoverflow.com/questions/71717528/how-to-identify-value-change-in-one-column-in-r/71717939?noredirect=1#comment126767162_71717939"">How to identify value change in one column in R?</a></p>
<p>Thanks to the help of two users, I gradually find the way to identify the drug changes in my data. However, now I need to check if there's a time gap before drug switch, if it is within 1 month, they will be kept.</p>
<p>So I used the following codes along with the codes used in the mentioned link.</p>
<pre class=""lang-r prettyprint-override""><code>df &lt;- data.frame(id = c(11,11,11,11,12,12,12,12,13,13,13,13),drug_type = c(&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;),drug_brand = c(1,1,2,2,2,3,3,3,1,1,2,3),
date = c(&quot;2020-01-01&quot;,&quot;2020-02-01&quot;,&quot;2020-03-01&quot;,&quot;2020-03-13&quot;,&quot;2019-04-05&quot;,&quot;2019-05-02&quot;,&quot;2019-06-03&quot;,&quot;2019-08-04&quot;,&quot;2021-02-02&quot;,&quot;2021-02-27&quot;,&quot;2021-03-22&quot;,&quot;2021-04-11&quot;))
</code></pre>
<p>Next, I used recommended codes to identify the subjects who switched their drugs.</p>
<pre class=""lang-r prettyprint-override""><code>switch &lt;- df %&gt;% group_by(id) %&gt;% filter(any(drug_type == &quot;B&quot; &amp; lag(drug_type) == &quot;A&quot;) |
           n_distinct(drug_brand) &gt; 1)
</code></pre>
<p>And then I tried only extracting the records on the switch day and the records 1 day before that, and I also created a column to show the calculated time gap between two dates for each patient.</p>
<pre class=""lang-r prettyprint-override""><code>switch %&gt;% 
  filter(drug_brand != lag(drug_brand)|drug_brand != lead(drug_brand)) %&gt;%
  mutate(time_gap = interval(lag(date),date)/dmonths(1)) %&gt;%
  filter(time_gap != NA) %&gt;% filter(time_gap &lt; 1)
</code></pre>
<p>It should have returned a result containing 4 records of 3 patients, but it returned nothing. I still haven't figured out why this happened because I checked my codes and they seem to be correct. Therefore, it would be great if you can share your advice to fix this.
Thank you in advance.</p>
",15610709.0,12505251.0,2022-04-06 10:20:26,2022-04-06 10:20:26,How can fix my codes to make my filter return correct results?,<r><database><data-science><medical>,0,5,N/A,CC BY-SA 4.0
71455436,1,-1.0,2022-03-13 09:30:22,-1,23,"<p>I was working on practicing EDA <a href=""https://miykael.github.io/blog/2022/advanced_eda/"" rel=""nofollow noreferrer"">FROM THIS LINK</a> and i saw a line in which there was some type error. i just want to understand how we a dataset have such type of error and how to avoid it ? <a href=""https://i.stack.imgur.com/LOykv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LOykv.png"" alt=""enter image description here"" /></a></p>
",13626547.0,7329832.0,2022-03-13 09:33:30,2022-03-13 17:30:25,How can a feature have numerical record and still be stored as non numeric?,<python><dataframe><data-science><data-analysis><exploratory-data-analysis>,1,0,N/A,CC BY-SA 4.0
71464193,1,71464921.0,2022-03-14 07:19:46,-1,235,"<p>i want to add a new row when the iteration reach the raw which has 'total charges'.
FYI: as shown in code, column number 1 is where it has to be performed.</p>
<pre><code>python
for row in df.itertuples():
    row[1] == 'Total Charges'
</code></pre>
<p><a href=""https://i.stack.imgur.com/Alw4r.png"" rel=""nofollow noreferrer"">this is how the data look like, i need to separate it with a row, right under total charges  </a></p>
",17040778.0,17040778.0,2022-03-14 07:58:58,2022-03-14 08:42:56,how to add new raw to the data frame during a loop and a certain condition is met?,<python><pandas><data-science><data-analysis>,2,2,N/A,CC BY-SA 4.0
71760048,1,71760226.0,2022-04-06 01:18:28,0,73,"<p>The data I'm working with (<a href=""https://mushroom.mathematik.uni-marburg.de/files/PrimaryData/primary_data_edited.csv"" rel=""nofollow noreferrer"">https://mushroom.mathematik.uni-marburg.de/files/PrimaryData/primary_data_edited.csv</a>) has lists in some entries and I'd like to expand these lists as a cartesian product of their elements. The image below is two columns in the dataframe's header.</p>
<p><a href=""https://i.stack.imgur.com/DYKiq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DYKiq.png"" alt=""data sample"" /></a></p>
<p>The output I'd like is something like:</p>
<pre><code>cap-diameter | cap-shape
10 | x
10 | f
20 | x
20 | f
5 | p
5 | x
10 | p
10 | x
...
</code></pre>
<p>So I don't want the cartesian product of all of the entries in each column, just that of the respective rows. I think pd.explode() might be a good place to start but I'm not sure how to accomplish this. Thanks in advance.</p>
",14845735.0,-1.0,N/A,2022-04-06 01:55:57,Cartesian product of rows in pandas?,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
74644592,1,74644655.0,2022-12-01 15:47:40,0,25,"<p>I am creating a tool to automate some tasks. These tasks generate two DataFrames, but when concatenating them the columns are messed up as follows:</p>
<pre><code>   col2  col4  col3 col1
0    A     2     0    a
1    A     1     1    B
2    B     9     9    c
3  NaN     8     4    D
4    D     7     2    e
5    C     4     3    F
</code></pre>
<p>But I need to rearrange them so that they look like this:</p>
<pre><code>   col1  col2  col3 col4
0    a     A     0    2
1    B     A     1    1
2    c     B     9    9
3    D   NaN     4    8
4    e     D     2    7
5    F     C     3    4
</code></pre>
<p>Can someone help me?</p>
<p>I tried with sort_values, but it didn't work, and I can't find anywhere another way to try to solve the problem.</p>
",17944590.0,-1.0,N/A,2022-12-01 15:52:45,How can I sort a column in python3?,<python><python-3.x><pandas><data-science>,3,0,N/A,CC BY-SA 4.0
74652276,1,-1.0,2022-12-02 07:37:19,0,26,"<p>Can't create X_train and X_test Dataframes ( from 2 differenet csv files) and also can't use them as integer</p>
<pre><code>data=pd.read_csv('action_train.csv', delimiter=';', header=0)
data=data.replace(to_replace ='[act1_]', value = '', regex = True).replace(to_replace ='[act2_]', value = '', regex = True).replace(to_replace ='[type ]', value = '', regex = True)
print(data.shape)
print(list(data.columns))
data1=pd.read_csv('action_test.csv', delimiter=';', header=0)
data1=data1.replace(to_replace ='[act1_]', value='', regex=True).replace(to_replace='[act2_]', value = '', regex = True).replace(to_replace ='[type ]', value = '', regex = True)
print(data1.shape)
print(list(data1.columns))
X_train=data['action_id', 'char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9', 'char_10']
print(X_train)
y_train=data['result']
X_test=data1['action_id', 'char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9', 'char_10']
print(X_test)
y_test=data1['result']
</code></pre>
<p>I tried to use them in different way but got tuple instead of array. Also can't convert object type in integer</p>
",18922465.0,-1.0,N/A,2022-12-02 07:37:19,I have several problems with Logistic Regression using pandas,<pandas><dataframe><data-science><logistic-regression><sklearn-pandas>,0,1,N/A,CC BY-SA 4.0
74645035,1,-1.0,2022-12-01 16:21:03,0,50,"<pre class=""lang-py prettyprint-override""><code>doc_1 = data.iloc[15]['Description']
doc_2 = &quot;Data is a new oil&quot;

data = [doc_1, doc_2]

count_vectorizer = CountVectorizer()
vector_matrix = count_vectorizer.fit_transform(data)

tokens = count_vectorizer.get_feature_names()
vector_matrix.toarray()

def create_dataframe(matrix, tokens):

    doc_names = [f'doc_{i+1}' for i, _ in enumerate(matrix)]
    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)
    return(df)

create_dataframe(vector_matrix.toarray(),tokens)

cosine_similarity_matrix = cosine_similarity(vector_matrix)
create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])
</code></pre>
<p>The code calculate the  Cosine Similarity between a cell and the string, but how can i improve my code so that i can calculate the Cosine Similarity between cells. So i want <code>doc1</code> compared with all the other cells in the column.</p>
<p>So i will get a table like this, where the dots is de cosine similarities:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">x</th>
<th style=""text-align: center;"">doc2</th>
<th style=""text-align: right;"">doc3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">doc1</td>
<td style=""text-align: center;"">....</td>
<td style=""text-align: right;"">....</td>
</tr>
</tbody>
</table>
</div>
<p>picture of the table how it should look like
<img src=""https://i.stack.imgur.com/jx8pZ.png"" alt=""enter image description here"" /></p>
",18455289.0,1409374.0,2022-12-01 19:34:04,2022-12-01 19:34:04,Cosine Similarity between 2 cells in a datafame,<pandas><dataframe><scikit-learn><data-science>,0,7,N/A,CC BY-SA 4.0
71767223,1,71767662.0,2022-04-06 12:59:06,1,701,"<p>I have a sample dataset. Here is:</p>
<pre><code>import pandas as pd
import numpy as np
df = {'Point1': [50,50,50,45,45,35,35], 'Point2': [48,44,30,35,33,34,32], 'Dist': [4,6,2,7,8,3,6]}
df = pd.DataFrame(df)
df
</code></pre>
<p>And its output is here:</p>
<p><a href=""https://i.stack.imgur.com/TFGXS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TFGXS.png"" alt=""enter image description here"" /></a></p>
<p>My goal is to find dist value with its condition and point2 value for each group of point1.
Here is my code. (It gives an error)</p>
<pre><code>if df['dist'] &lt; 5 :
    df1 = df[df['dist'].isin(df.groupby('Point1').max()['Dist'].values)]
else :
    df1 = df[df['dist'].isin(df.groupby('Point1').min()['Dist'].values)]

df1
</code></pre>
<p>And here is the expected output:</p>
<p><a href=""https://i.stack.imgur.com/9fJAm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9fJAm.png"" alt=""enter image description here"" /></a></p>
<p>So, if there is exist Dist value less than 5, I would like to take the max one of these groups. If no, I would like to take the min one. I hope it would be clear.</p>
",-1.0,-1.0,N/A,2022-04-06 13:29:13,"python, finding target value of the dataset",<python><pandas><numpy><data-science>,2,0,N/A,CC BY-SA 4.0
71767491,1,-1.0,2022-04-06 13:17:29,1,234,"<p>I'm trying to do some data analysis with python and pandas on a power consumption dataset.
However when I plot the data I get that stright line from <code>5-1-2007</code> to <code>13-1-2007</code><a href=""https://i.stack.imgur.com/484La.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/484La.png"" alt=""enter image description here"" /></a> but I have no missing values in my dataset which is a weird behavior as I made sure that my dataset in clean.
Anyone had similar issue? or can explain this behavior?
Thank you.</p>
<p>EDIT: Here is what the data looks like in that range
EDIT 2 : Here is the link to the original dataset (before cleaning) if that might help: <a href=""https://archive.ics.uci.edu/ml/machine-learning-databases/00235/"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/machine-learning-databases/00235/</a></p>
<p><a href=""https://i.stack.imgur.com/sWiG4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sWiG4.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/FRoBv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FRoBv.png"" alt=""enter image description here"" /></a></p>
",7561952.0,7561952.0,2022-04-06 13:52:57,2022-04-06 17:39:19,Visualizing Time series data,<python><pandas><data-science><data-analysis>,2,6,N/A,CC BY-SA 4.0
71755371,1,-1.0,2022-04-05 16:33:47,0,180,"<p>When I run the following chunk of code, I get a very small and unuseful correlation plot(see photo). I think it is because the column labels are too large. Is there any suggestion on how to fix this issue without modifying column names (maybe indicating only substring of names for example...)</p>
<pre><code>numeric.var &lt;- sapply(df_train, is.numeric)
corr.matrix &lt;- cor(df_train[,numeric.var])
corrplot(corr.matrix, main=&quot;\n\nCorrelation Plot for Numerical Variables&quot;, 
method=&quot;number&quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/t89Qf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t89Qf.png"" alt=""enter image description here"" /></a></p>
",18715406.0,8206434.0,2022-04-05 17:30:27,2022-04-05 17:30:27,How to fix corrplot formatting issue in R,<r><dataframe><data-science><data-analysis><r-corrplot>,0,3,N/A,CC BY-SA 4.0
71768892,1,-1.0,2022-04-06 14:46:19,0,36,"<p>I have a question... I am trying to implement clustering of words in the text using COM. But the examples I found do not allow processing a large amount of data. Could you please share links to projects that have implemented this task.</p>
",18726245.0,-1.0,N/A,2022-04-06 14:46:19,SOM for clustering words,<python><data-science><cluster-analysis><som>,0,2,N/A,CC BY-SA 4.0
74657539,1,-1.0,2022-12-02 14:46:04,0,39,"<p>The <strong>number of classes after test/train image generator applied</strong> is different from the <strong>original actual number of classes</strong>.</p>
<ul>
<li>Initially, I have 4 folders (4 classes)</li>
<li>After test/train generator I am getting 5 classes</li>
</ul>
<p>Code:</p>
<pre><code>input_shape = (128, 128, 3)
train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=(128, 128),
                                                    batch_size=32)
test_generator = test_datagen.flow_from_directory(test_dir,
                                                  shuffle=True,
                                                  target_size=(128, 128), 
                                                  batch_size=32)
</code></pre>
<pre><code>--------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-10-5ab82d751dd5&gt; in &lt;module&gt;
      1 model1.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])
----&gt; 2 history10 = model1.fit(
      3     train_generator,#egitim verileri
      4     steps_per_epoch=None,
      5     epochs=10,

1 frames
/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52   try:
     53     ctx.ensure_initialized()
---&gt; 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:

InvalidArgumentError: Graph execution error:

Detected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):
</code></pre>
",20667052.0,2347649.0,2022-12-09 20:13:39,2022-12-09 20:13:39,Mismatched in actual number of classes and classes after test/train generator,<tensorflow><machine-learning><keras><conv-neural-network><data-science>,0,3,N/A,CC BY-SA 4.0
74662308,1,-1.0,2022-12-02 22:47:09,1,82,"<p>I'm using RFECV for feature selection and I couldn't help but notice that I would get a different number of features on each run. I have 24 features sometimes the results would say that it only used 9 features sometimes it said 22 features.</p>
<p>I was wondering what's the reason for this and how do I interpret these different results and with which number of features should I go.</p>
<pre><code>from sklearn.feature_selection import RFECV

rfecv = RFECV(estimator=RandomForestClassifier())
model = RandomForestClassifier()
pipeline = Pipeline(steps=[('s',rfecv),('m',model)])
# evaluate model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))
# summarize all features
rfecv.fit(X, y)
#feat = []
for i in range(X.shape[1]):
    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfecv.support_[i], rfecv.ranking_[i]))
</code></pre>
",15019732.0,-1.0,N/A,2022-12-02 22:47:09,Why does RFECV results vary from iteration to iteration,<python><machine-learning><data-science><feature-selection><rfe>,0,3,N/A,CC BY-SA 4.0
74651022,1,-1.0,2022-12-02 04:48:26,0,107,"<p>I have a data set with the growth in student enrollments by college from one year to the next broken down by age bands (18-19, 20-24, etc.). I have another data set with the growth in student enrollments for the same colleges from one year to the next broken down by gender (M, F, O). Unfortunately, we don't have access to the raw data so I don't know the relationship between these (e.g. how many males 18-19, females 20-24, etc.).</p>
<p>Is there a way to do a correlation analysis on these separate datasets against each other to imply some relationships? E.g. I'm trying to see if I can reach any conclusions like &quot;the growth in the 20-24 age band was more strongly correlated to the growth in female vs. male students&quot;?</p>
<p>I have the two datasets loaded in dataframes and have already prepared some basis plots showing trend etc. <em>I did manage to brute-force an age by gender view in excel but wanted to hear others' ideas on the above before I attempt to replicate it in python...</em></p>
",4802868.0,-1.0,N/A,2022-12-02 12:12:42,Calculate implied correlation between two datasets,<python><excel><data-science><correlation>,1,1,N/A,CC BY-SA 4.0
74651077,1,-1.0,2022-12-02 04:59:50,0,16,"<p>I have value counts
'''<code>df[&quot;quality&quot;].value_counts</code>()'''
5    681</p>
<p>6    638</p>
<p>7    199</p>
<p>4     53</p>
<p>8     18</p>
<p>3     10</p>
<p>I want to add a new column with my data frame named as taste which is based on quality
less than 5 - bad</p>
<p>equal to or less than 7 - normal</p>
<p>greater than 7 - good</p>
<p>Please help me .
I'm trying this
'''<code>pd.cut(df['quality'], ['good', 'normal', 'bad'], include_lowest=True).value_counts()</code>'''</p>
<p>But it gives me error.</p>
<p>ValueError: could not convert string to float: 'good'</p>
",20212441.0,20212441.0,2022-12-02 05:29:24,2022-12-02 05:29:24,Ihave too many categories in target column .How to reduce them in just three categor,<python><pandas><dataframe><machine-learning><data-science>,0,0,2022-12-02 05:08:57,CC BY-SA 4.0
71767733,1,71767791.0,2022-04-06 13:33:07,0,66,"<p>I have two columns where I would like to artificially increase column A value to 1000 to see what happens to values in column B.</p>
<p><strong>Data</strong></p>
<pre><code>A           B
500         20
200         10
100         5
</code></pre>
<p><strong>Desired</strong></p>
<pre><code>A           B
500        20
200        10
100        5
1000       ?
</code></pre>
<p>I wish to artificially increase column A value to 1000 to see what happens to values in column B.</p>
<p><strong>Doing</strong></p>
<p>Using python I will test for correlation.
Treat this as linear regression problem.</p>
<pre><code>pyplot.scatter(x = ‘A’, y = ‘B’, s= 100)
pyplot.show()
</code></pre>
<p>Then I am thinking I can use linear regression to determine what the value of B will be if I increase the dependent value of A. Just not sure on how to input the what if A values.</p>
<pre><code>import numpy as np
from sklearn.linear_model import LinearRegression

x = np.array([500,200,100]).reshape((-1, 1))
y = np.array([20,10,5])
</code></pre>
<p>Any suggestion is appreciated</p>
",5942100.0,-1.0,N/A,2022-04-06 14:06:24,Artificially increase one value and observe affects on another- Python,<python><numpy><matplotlib><data-science><linear-regression>,1,0,N/A,CC BY-SA 4.0
74661896,1,-1.0,2022-12-02 21:57:10,0,86,"<p>Does anyone know whether the scaling functions mentioned here <a href=""https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/scale-properties/"" rel=""nofollow noreferrer"">https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/scale-properties/</a> exist within the python library, if so how can I call them?</p>
",20321674.0,-1.0,N/A,2022-12-03 09:01:05,"Neo4j, Graph Data Science Python Library Scaling Functions",<neo4j><graph-data-science><neo4j-python-driver>,1,0,N/A,CC BY-SA 4.0
74662109,1,-1.0,2022-12-02 22:20:10,0,72,"<p>I have a dataset having gdp of countries and their biofuel production named &quot;Merging2&quot;. I am trying to plot a bar chart of top 5 countries in gdp and in the same plot have the bar chart of their biofuel_production.</p>
<p>I plotted the top gdp's using :</p>
<pre><code>yr=Merging2.groupby(by='Years')
access1=yr.get_group(2019)
sorted=access1.sort_values([&quot;rgdpe&quot;], ascending=[False]) #sorting it
highest_gdp_2019=sorted.head(10) # taking the top 5 rgdpe
fig, ax=plt.subplots()
plt.bar(highest_gdp_2019.Countries,highest_gdp_2019.rgdpe, color ='black',width = 0.8,alpha=0.8)
ax.set_xlabel(&quot;Countries&quot;)
ax.set_ylabel(&quot;Real GDP&quot;)
plt.xticks(rotation = 90)

</code></pre>
<p>Is there a way to do that in Python ?</p>
",20521674.0,-1.0,N/A,2022-12-02 22:29:18,Plotting two variable in the same bar plot,<python><pandas><data-science><data-cleaning>,1,0,N/A,CC BY-SA 4.0
74652488,1,-1.0,2022-12-02 07:59:12,0,310,"<p><a href=""https://i.stack.imgur.com/JXXQQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JXXQQ.png"" alt=""img"" /></a>
In the picture, the example shows how to use Nested CV for hyperparameter tuning using a toy example. The outer CV is run for K=3 folds and 2 folds CV in the inner CV.</p>
<p>This is my understanding (please correct me if my understanding is wrong). In the first iteration (K = 1) using the outer fold training data we performed nested CV. Assume 'n_estimator = 2' gave an accuracy on the outer folds test set. Similarly for K = 2, we got accuracy = 0.92 for 'n_estimator = 5' and for K =3, accuracy = 0.96 using 'n_estimator = 5'.</p>
<p>My <strong>question</strong> is that since hyperparameter 'n_estimator = 2' gave the best accuracy = 0.98 on the first iteration so 'n_estimator = 2' hyperparameter should be selected OR 'n_estimator = 5' should be selected since this hyperparameter &quot;won&quot; the maximum time (based on frequency)</p>
<p>tack.imgur.com/K2uWT.png</p>
",389264.0,-1.0,N/A,2023-02-12 01:01:18,Hyperparameter Tuning and Training,<machine-learning><data-science><cross-validation><hyperparameters>,1,0,N/A,CC BY-SA 4.0
74666280,1,74666447.0,2022-12-03 11:27:01,0,44,"<p>I have two dataframes:</p>
<pre><code>df1 =  
    C0   C1. C2.  
4   AB. 1.  2
5   AC. 7   8
6   AD. 9.  9
7   AE. 2.  6
8   AG  8.  9

df2 = 
   C0    C1. C2
8  AB    0. 1
9  AE.   6. 3
10 AD.   1. 2
</code></pre>
<p>I want to apply a subtraction between these two dataframes, such that when the value of the columns C0 is the same - I will get the subsraction, and when is not - a bool column will have the value False. notice that current indeics are not aligned.
So new df1 should be:</p>
<pre><code>df1 =  
    C0   C1. C2. diff_C1 match  
4   AB.  1.  2.    1.    True
5   AC.  7   8.    0.    False
6   AD.  9.  9.    8.    True
7   AE.  2.  6.    -4.   True
8   AG   8.  9.    0    False
</code></pre>
<p>What is the best way to do it?</p>
",6057371.0,-1.0,N/A,2022-12-03 12:15:01,"pandas apply subtractions on columns function when indexes are not equal, based on alignment in another columns",<python><pandas><dataframe><data-science><data-munging>,2,0,N/A,CC BY-SA 4.0
74668460,1,74763863.0,2022-12-03 16:22:13,1,123,"<p>I have a database of news texts (100000 samples). Half of the dataset is tagged, and half is not, what methodology can I use to analyze the remaining news and fill them with tags?</p>
<p>Data example:</p>
<blockquote>
<p>Text = A cap on the price of Russian oil will restrict Russia's revenues for its &quot;illegal war Ukraine&quot;, the US says. The cap, approved by Western allies on Friday, is aimed at stopping countries paying more than $60 (£48) for a barrel of seaborne Russian crude oil. The measure - due to come into force on Monday - intensifies Western pressure on Russia over the invasion… [long test is cut]</p>
</blockquote>
<blockquote>
<p>Tags = ['russian', 'oil', 'war']</p>
</blockquote>
<p>I know how to use python, pandas. But I found only methods that predict whether the text is bad or good.</p>
",17647231.0,2347649.0,2022-12-11 19:10:44,2022-12-11 19:31:29,Which learning model should be chosen to predict text news tags?,<python><nlp><data-science><data-analysis><tagging>,1,0,N/A,CC BY-SA 4.0
74673328,1,-1.0,2022-12-04 06:26:16,0,269,"<p>For calculate accuracy, it's required that A prediction is accurate when the y_hat equals the test_y. Sum up all the instances when they are equal and divide by m.
convert both to one-dimensional arrays and compare them, then sum up and divide by len(test_x)
However, every time when I try to run it, I got TypeError expected string or bytes-like object</p>
<pre><code># Check performance using the test set
# UNQ_C5 GRADED FUNCTION: test_logistic_regression
def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):
    &quot;&quot;&quot;
    Input: 
        test_x: a list of tweets
        test_y: (m, 1) vector with the corresponding labels for the list of tweets
        freqs: a dictionary with the frequency of each pair (or tuple)
        theta: weight vector of dimension (3, 1)
    Output: 
        accuracy: (# of tweets classified correctly) / (total # of tweets)
    &quot;&quot;&quot;
    
    ### START CODE HERE ###

    
    # the list for storing predictions
    y_hat = []
    
    for tweet in test_x:
        # get the label prediction for the tweet
        y_pred = predict_tweet(test_x, freqs, theta)

        if y_pred &gt; 0.5:
            # append 1.0 to the list
            y_hat.append(1.0)
        else:
            # append 0 to the list
            y_hat.append(0.0)

    # With the above implementation, y_hat is a list, but test_y is (m,1) array
    # convert both to one-dimensional arrays in order to compare them using the '==' operator
    accuracy = (y_hat.flatten() == test_y.flatten()).sum() / len(test_x)


    ### END CODE HERE ###
    
    return accuracy

tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)
print(f&quot;Logistic regression model's accuracy = {tmp_accuracy:.4f}&quot;)


TypeError: expected string or bytes-like object
[Finished in 10.3s with exit code 1]
[cmd: ['python3', '-u', '/Users/mac/Desktop/python_work/NPL/C1_W1_Assignment.py']]
[dir: /Users/mac/Desktop/python_work/NPL]
[path: /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin]
</code></pre>
<p>I hope it could run successfully
I guess there's something wrong with</p>
<pre><code>    accuracy = (y_hat.flatten() == test_y.flatten()).sum() / len(test_x)

</code></pre>
",7002095.0,-1.0,N/A,2022-12-04 06:26:16,test_logistic_regression: TypeError: expected string or bytes-like object,<python-3.x><data-science><artificial-intelligence>,0,0,N/A,CC BY-SA 4.0
74663313,1,-1.0,2022-12-03 01:35:34,0,56,"<pre><code>ggplot(missense2)+
  geom_bar(aes(x=Variant.start.in.translation..aa.),stat=&quot;count&quot;)+
  theme_classic()+
  xlab(&quot;amino acid position&quot;)+
</code></pre>
<p>What do I add or use to allow the binwidth of the graph to be changed?</p>
<pre><code>ggplot(missense2)+
  geom_bar(aes(x=Variant.start.in.translation..aa.),stat=&quot;count&quot;)+
  theme_classic()+
  xlab(&quot;amino acid position&quot;)+
</code></pre>
<p>The command (<code>binwidth= &quot;2&quot;</code>) altered nothing no matter where placed in the code.</p>
",20243533.0,5792244.0,2022-12-03 03:16:44,2022-12-03 03:16:44,In R how do i change the binwidth on my graph,<r><ggplot2><data-science>,1,1,N/A,CC BY-SA 4.0
74677040,1,-1.0,2022-12-04 13:28:03,0,66,"<p>The excel file is in the link: <a href=""https://docs.google.com/spreadsheets/d/1k3PHDi_G7TDKS--Xv4YSxW74PHNkbVIq/edit?usp=sharing&amp;ouid=114185320765894103697&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/1k3PHDi_G7TDKS--Xv4YSxW74PHNkbVIq/edit?usp=sharing&amp;ouid=114185320765894103697&amp;rtpof=true&amp;sd=true</a></p>
<p>I tried to add a third column to this file, containing the number of children per parent, what I wrote :</p>
<pre><code>import pandas as pd
import networkx as nx

G = nx.from_pandas_edgelist(pd.read_excel(r'C:\Users\jalal.hasain\Desktop\CHILD--PARENT - Copy.xlsx'),
                            source='Parent', target='Child',
                            create_using=nx.DiGraph)
nx.descendants(G, '192IR')
</code></pre>
<p>Output :</p>
<pre><code>{'008ER', '578IR'}
</code></pre>
<p>len(nx.descendants(G, '192IR'))</p>
<pre><code>Output : 
</code></pre>
<p>2</p>
<pre><code>

the result is correct, but how can I can I pass the whole parent column into nx.descendants(G, Parent column), not only a specific cell from that column, so I can get in the output the whole third column that shows number of children per this parent.

I tried to do:
df1 = pd.read_excel(r'C:\Users\jalal.hasain\Desktop\CHILD--PARENT - Copy.xlsx')
df1
len(nx.descendants(G, df1['Parent']))

but the error is : 
</code></pre>
<p>TypeError: unhashable type: 'Series'</p>
<pre><code>how can I solve this error?
Plus, if you have an idea to create a fourth column containing a list of children for this parent, that will help a lot.
Appreciated and thanks for helping in advance.
</code></pre>
",13766202.0,-1.0,N/A,2022-12-04 13:28:03,get the number of children per parent from excel sheet using python,<python><excel><dataframe><data-structures><data-science>,0,0,N/A,CC BY-SA 4.0
74677451,1,-1.0,2022-12-04 14:17:43,0,132,"<p>In Python, when training some molecular data on the default parameters of XGB, I get very low Recall - precisely, 0.05. Optimizing the hyperparameters (tried most of them, really), the recall keeps staying low. Never went past 0.2.</p>
<p>On the other hand, ORANGE XGBoost, also with the same logic, should do the same, right? I don't make any sort of preprocessing in particular on Orange that I don't do in Python and vice-versa. That's not what happens.</p>
<p>In Orange, recall stays around 0.94, which is much closer to what I would expect. Precision is in all cases 0.95, and RF gives the same scores in Orange and Python. XGBoost only doesn't seem to be coherent across Python and Orange.</p>
<p>This is what my Orange model looks like:</p>
<p><a href=""https://i.stack.imgur.com/QmNpv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QmNpv.png"" alt=""enter image description here"" /></a></p>
<p>And, as said, this is the basic XGBoost part:</p>
<pre><code>    # Manual tryout

model = XGBClassifier()
model.fit(X_train_GM12878.loc[:, X_train_GM12878.columns != 'CLASS'], X_train_GM12878[&quot;CLASS&quot;])
</code></pre>
<p>The data is split between two datasets: K562 and GM12878. Training on K562 and testing on GM12878 gives more or less the same results, small difference - but the large difference comes from training GM12878 and testing on K562.</p>
<p>Does someone have any idea what could be happening?</p>
",2712153.0,-1.0,N/A,2022-12-04 14:17:43,Python XGBoost vs. Orange XGBoost - very different RECALLS,<python><machine-learning><data-science><xgboost><orange>,0,0,N/A,CC BY-SA 4.0
74662799,1,74663263.0,2022-12-03 00:02:44,0,1711,"<p>I have trained and saved an xgboost regressor model in Jupyter Notebook (Google Colab) and tried to load it in my local machine without success. I have tried to save and load the model in multiple formats: <code>.pkl</code> using <strong>pickle</strong> library, <code>.sav</code> using <strong>joblib</strong> library or <code>.json</code>.</p>
<p>When I load the model in VS Code, I get the following error:</p>
<blockquote>
<p>raise XGBoostError(py_str(_LIB.XGBGetLastError()))
xgboost.core.XGBoostError: [10:56:21] ../src/c_api/c_api.cc:846: Check
failed: str[0] == '{' (</p>
</blockquote>
<p>What is the problem here?</p>
",19352986.0,68587.0,2022-12-04 03:54:37,2022-12-04 03:54:37,"XGBoost Error when saving and loading xgboost model using Pickle, JSON and JobLib",<python><visual-studio-code><data-science><xgboost>,1,0,N/A,CC BY-SA 4.0
74664559,1,-1.0,2022-12-03 06:45:27,0,26,"<p>I was dealing with binary classification problem, where we need to predict which team wins {win:1,lost:0} , dataset have 8 teams is it good to do target encoding with the probability of wins of particular team or should go with OHE(one hot encoding)? if so why?</p>
<p>was trying to go with target encoding?</p>
",18176092.0,-1.0,N/A,2022-12-03 06:45:27,Target encoding of teams in binary classification problem,<machine-learning><data-science><feature-engineering><data-preprocessing>,0,0,N/A,CC BY-SA 4.0
74672338,1,74672432.0,2022-12-04 02:03:47,0,56,"<p>I have a data frame that looks like this.</p>
<pre><code>import pandas as pd
import numpy as np

data = [
  ['A',1,2,3,4],
  ['A',5,6,7,8],
  ['A',9,10,11,12],
  ['B',13,14,15,16],
  ['B',17,18,19,20],
  ['B',21,22,23,24],
  ['B',25,26,27,28],
  ['C',29,30,31,32],
  ['C',33,34,35,36],
  ['C',37,38,39,40],
  ['D',13,14,15,0],
  ['D',0,18,19,0],
  ['D',0,0,23,0],
  ['D',0,0,0,0],
  ['E',13,14,15,0],
  ['E',0,18,19,0],
  ['F',0,0,23,0],

]

df = pd.DataFrame(data, columns=['Name', 'num1', 'num2', 'num3', 'num4'])
df
</code></pre>
<p>Then I have the following code to calculate the group by weighted average.</p>
<pre><code>weights = [10,20,30,40] 

df=df.groupby('Name').agg(lambda g: sum(g*weights[:len(g)])/sum(weights[:len(g)]))
</code></pre>
<p>The problem lies in <code>sum(weights[:len(g)])</code> because all the groups do not have equal rows. As you can see above, <code>group A has 3 rows, B has 4 rows, C has 3 rows, D has 4 rows, E has 2 rows and F has 1 row</code>.
Depending upon the rows, it needs to calculate the sum.
Now, the above code returns me the weighted average by calculating</p>
<p><strong>For Group A, the first column calculates the weighted average as (1 X 10+5 X 20+9 X 30)/60 but it should calculate the weighted average as (1 X20+5 X 30+9 X 40)/90</strong></p>
<p><strong>For Group E, the first column calculates the weighted average as (13 X 10+0 X 20)/30 but it should calculate the weighted average as (13 X 30+0 X 40)/70</strong></p>
<p><strong>Current Result</strong></p>
<p><a href=""https://i.stack.imgur.com/IgEjl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IgEjl.png"" alt=""enter image description here"" /></a></p>
<p><strong>Expected result</strong></p>
<p><a href=""https://i.stack.imgur.com/kLwI5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kLwI5.png"" alt=""enter image description here"" /></a></p>
",16875907.0,4885169.0,2022-12-04 02:16:33,2022-12-04 03:29:12,How to take a sum (in denominator) for calculating group by weighted average in a dataframe?,<python><pandas><dataframe><data-science-experience>,2,0,N/A,CC BY-SA 4.0
71778635,1,71779394.0,2022-04-07 08:19:31,1,106,"<p>I have a <strong>subset</strong> (type &lt;class 'list'&gt;) which contain many subdataframe(subset[0],subset[1],subset[2]......so on).I want to do some filtering on each subset and after filtering i want to append it to a new dataframe.
My code:</p>
<pre><code>new_df = pd.DataFrame()
for i in range(20):
     a = subsets[i]
     a = a[((a[f'RUT1_Ang_meas_{i}'] &gt;= -1.047198) &amp; (a[f'RUT1_Ang_meas_{i}'] &lt;= 1.047198))]
     .
     .
     #some filtering
     .
     . 
     new_df= pd.concat([a],axis=1)

new_df.info() ```

i am getting an empty final dataframe.

How can i modify the code ?
</code></pre>
",18503721.0,18503721.0,2022-04-07 08:41:53,2022-04-07 09:42:38,How to append many range of subset to a new dataframe after some filtering?,<python><pandas><numpy><multiprocessing><data-science>,1,0,N/A,CC BY-SA 4.0
71769540,1,71781980.0,2022-04-06 15:26:50,0,380,"<p>I'm using MongoDB, in all my documents I have a field that is an array(all arrays same length), I want to retrieve all those arrays in one single 2d array, HOW?
my best effort was:
np.array(collection.find({},{&quot;array_field&quot;:1,&quot;_id&quot;:0})
but I'm getting pymongo.cursor.Cursor object</p>
",18726503.0,-1.0,N/A,2022-04-07 12:13:26,"how to convert Mongodb find({},{""array_field"":1,""_id"":0}) return into a 2d array in python",<python><arrays><mongodb><data-science><pymongo>,1,3,N/A,CC BY-SA 4.0
74667345,1,-1.0,2022-12-03 13:50:07,1,75,"<p>I am making PCA in python with this code:</p>
<pre><code>def OWN_PCA(X,num_components):

  #Step-1
  X_meaned = X - np.mean(X , axis = 0)
     
  #creating covariance matrix
  cov_mat = np.cov(X_meaned,rowvar=False)

  #calculating eigenvector and eigen value
  eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)

  #sorting the vectors based on eigen values
  sorted_index = np.argsort(eigen_values)[::-1]
  sorted_eigenvalue = eigen_values[sorted_index]
  sorted_eigenvectors = eigen_vectors[:,sorted_index]

  #choosing number of components
  eigenvector_subset = sorted_eigenvectors[:,0:num_components]

  X_reduced = np.dot( eigenvector_subset.transpose(), X.transpose() ).transpose()

  return X_reduced
</code></pre>
<p>The problem is when I apply it to the iris dataset and plot it, I will get this:
<a href=""https://i.stack.imgur.com/ic9sj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ic9sj.png"" alt=""enter image description here"" /></a></p>
<p>and when I use PCA in sklearn the image is reversed:
<a href=""https://i.stack.imgur.com/FCch2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FCch2.png"" alt=""enter image description here"" /></a></p>
<p>What is wrong with my code?</p>
",12044737.0,-1.0,N/A,2022-12-03 13:50:07,PCA algorithm makes iris dataset to be reversed on y axis,<python><data-science><pca>,0,1,N/A,CC BY-SA 4.0
74671443,1,-1.0,2022-12-03 22:54:46,-2,40,"<p>I have a dataframe column which contains lists of values.
I am interested in getting the count of each distinct value inside the list across the column using python.</p>
",20677779.0,20677779.0,2022-12-04 00:03:09,2022-12-04 00:03:09,Count of distinct values in pandas column which has list of values,<python><pandas><dataframe><numpy><data-science>,1,1,N/A,CC BY-SA 4.0
74674053,1,-1.0,2022-12-04 08:50:54,0,44,"<pre><code>df_movies = df[df['type'] == 'Movie']

# top 10 movie making country

df_movies['country'].value_counts().index[:10] 

# movie duration mean for each country

df_movies.groupby(['minute']).mean()
</code></pre>
<p>I'm learning python in Jupyter notebook. I can't sort</p>
<blockquote>
<p>&quot;Top 10 movie making country's video length mean&quot;</p>
</blockquote>
<p>Can anyone help me to do this?</p>
",20402917.0,20174226.0,2022-12-07 09:37:08,2022-12-07 09:37:08,Can you help me to find out this question (Top 10 movie making country's video length mean) from netfix dataset,<python-2.7><dataset><data-science><data-analysis><netflix>,1,2,N/A,CC BY-SA 4.0
74691393,1,-1.0,2022-12-05 16:30:07,1,27,"<p>I am trying to predict if a customer will convert to premium shoppers. The dataset looks as below:</p>
<pre><code>customer_id  category  sales   conversion
1234           1        18       converted
1234           2        24       converted
1568           2        49       not converted
1568           6        12       not converted
1568           1        17       not converted 
</code></pre>
<p>I am trying to predict if a customer will convert or not using 1)logistic regression 2) light GBM. However, usually the customer id is unique, corresponding with multiple spending information.</p>
<p>For my case, I cannot pivot category because there are 30 categories, but I am afraid duplicated customer_id will impact my final model prediction.</p>
<ol>
<li>How do I deal with the category code?</li>
<li>How do I deal with the duplicate of customer_id? Any good model that can handle this?</li>
</ol>
",19320246.0,-1.0,N/A,2022-12-06 05:03:57,Classification modeling with duplicated customer_id and multiple spending categories,<python><machine-learning><data-science><data-cleaning>,0,0,N/A,CC BY-SA 4.0
74683288,1,-1.0,2022-12-05 04:14:37,0,43,"<p>I pushed an app to heroku but kept getting applicaiton error when I visit the page, the heroku log looks like this:</p>
<pre><code>2022-12-05T04:02:19.743200+00:00 app[web.1]:   File &quot;/app/.heroku/python/lib/python3.10/site-packages/jinja2/tests.py&quot;, line 13, in &lt;module&gt;
2022-12-05T04:02:19.743201+00:00 app[web.1]:     from collections import Mapping
2022-12-05T04:02:19.743201+00:00 app[web.1]: ImportError: cannot import name 'Mapping' from 'collections' (/app/.heroku/python/lib/python3.10/collections/__init__.py)
2022-12-05T04:02:19.743308+00:00 app[web.1]: [2022-12-05 04:02:19 +0000] [10] [INFO] Worker exiting (pid: 10)
2022-12-05T04:02:19.892124+00:00 app[web.1]: [2022-12-05 04:02:19 +0000] [4] [INFO] Shutting down: Master
2022-12-05T04:02:19.892189+00:00 app[web.1]: [2022-12-05 04:02:19 +0000] [4] [INFO] Reason: Worker failed to boot.
2022-12-05T04:02:20.047524+00:00 heroku[web.1]: Process exited with status 3
2022-12-05T04:02:20.093161+00:00 heroku[web.1]: State changed from up to crashed
2022-12-05T04:02:58.986356+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;/&quot; host=week5-deliverable.herokuapp.com request_id=9ba670b2-e76d-4248-a592-208613f33dfe fwd=&quot;169.231.152.252&quot; dyno= connect= service= status=503 bytes= protocol=https
2022-12-05T04:02:59.182894+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;/favicon.ico&quot; host=week5-deliverable.herokuapp.com request_id=c6ce67d5-7c63-4f41-aa58-32edee7d3b4f fwd=&quot;169.231.152.252&quot; dyno= connect= service= status=503 bytes= protocol=https
</code></pre>
<p>Here is my github repository connected to this heroku app:
<a href=""https://i.stack.imgur.com/tgQni.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>My Procfile looks like this:</p>
<pre><code>web: gunicorn app:app
</code></pre>
<p>and my requirements looks like this:</p>
<pre><code>Flask==1.1.1
gunicorn==19.9.0
itsdangerous==1.1.0
Jinja2==2.10.1
MarkupSafe==1.1.1
Werkzeug==0.15.5
numpy&gt;=1.9.2
scipy&gt;=0.15.1
scikit-learn&gt;=0.18
matplotlib&gt;=1.4.3
pandas&gt;=0.19
</code></pre>
<p>Can someone help me to figure out how to deploy my model? Thanks a lot!</p>
<p>I already verified that all packages in the requirement has been installed locally, and those solutions relevant to changing Procfile or relevant packages did't work.</p>
",20687028.0,-1.0,N/A,2022-12-05 04:14:37,Heroku having application errors,<python><api><heroku><data-science><web-deployment>,0,1,N/A,CC BY-SA 4.0
74688095,1,74722382.0,2022-12-05 12:20:16,0,69,"<p>I have a 2 Dimensional set of time series data containing about 1000 samples. I.e I have a long list of 1000 elements each entry is a list of two numbers.</p>
<p>It can be thought of the position, x and y coordinates, of a car with a picture taken each second for 1000 seconds. When I plot this, as seen below, you get a decent idea of the trajectory but it's unclear where the car starts or finishes, i.e which direction it is traveling in. I was thinking about including arrows between each point but I think this would get quite clustered (maybe you know a way to overcome that issue?) Also, I thought of colouring each point with a spectrum that made it clear to see time increasing, i.e hotter points to colder points as time goes on. Any idea how to achieve this in matplotlib? <a href=""https://i.stack.imgur.com/cRAZV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cRAZV.png"" alt=""enter image description here"" /></a></p>
",14748984.0,2347649.0,2022-12-10 18:42:35,2022-12-10 18:42:35,How to helpfully plot time series data in python,<python><matplotlib><data-science><visualization>,1,1,N/A,CC BY-SA 4.0
71789322,1,-1.0,2022-04-07 21:50:35,1,200,"<p>I have big 3 CSV files and they are all 76 same columns.
The number of rows are different
17809 rows - 124262 rows -  108779 rows
I am trying to merge these 3 data frames but I am having a memory error. Can I solve this issue or is it impossible for my hardware?
16GB Ram, i5 11th.</p>
<p>I found this solution to merge them but there is an error. I want them to be in one dataframe.</p>
<pre><code>  from functools import reduce
    data_frames = [a, b, c]
    df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['Intrusion'], how='outer'), data_frames)
    df_merged
</code></pre>
<p>MemoryError: Unable to allocate 101. GiB for an array with shape (13517346950,) and data type int64</p>
",-1.0,-1.0,N/A,2022-05-12 15:54:37,Network Flow Dataframe - Merging Memory Error - Unable to allocate array with shape and data type,<pandas><data-science><network-flow>,1,0,N/A,CC BY-SA 4.0
74692140,1,-1.0,2022-12-05 17:30:59,1,294,"<p>Is there is a way to combine multiple ML models where each model use a dataset with different features but same outcome?</p>
<p>I find a way to combine models which is ensemble learning but its only applicable for datasets with the same features</p>
",20694082.0,-1.0,N/A,2022-12-10 18:52:23,combining Machine learning models,<machine-learning><deep-learning><dataset><data-science>,1,1,N/A,CC BY-SA 4.0
71781773,1,-1.0,2022-04-07 11:57:14,0,412,"<p>I getting &lt;urlopen error [errno 11001] getaddrinfo Failed&gt; error while trying to scrape data from a website. Can somebody help me with this error. I am new to Python coding.</p>
<p>here is the code :-</p>
<pre><code>from urllib.request import urlopen as uReq
my_url=  &quot;https://www.arket.com/en/women/coats-and-jackets.html&quot;
uClient= uReq(my_url)
page_html= uClient.read()
uClient.close()
</code></pre>
",18736185.0,-1.0,N/A,2022-04-07 11:57:14,scrapping data - urlopen error [errno 11001] getaddrinfo Failed,<python><web-scraping><data-science><urllib><urlopen>,0,2,N/A,CC BY-SA 4.0
71783689,1,71783741.0,2022-04-07 14:05:27,0,150,"<p>After a lot of research, I'm struggling to do what should be straightforward in R. I am trying to delete columns only (not rows) if a value from a <em>particular</em> row and <em>particular</em> column meets a condition. For example I have a dataframe:</p>
<pre><code>V1 &lt;- c(1:2)
V2 &lt;- c(2:3)
df1 &lt;- data.frame(V1, V2)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>V1</th>
<th>V2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>I want to delete the entire column V2 if Row2/V2 is less than 4. As you can see in this example, the value in question is 3.</p>
<p>In which case I would be left with a dataframe that is equivalent to:</p>
<pre><code>V1 &lt;- c(1:2)
df1 &lt;- data.frame(V1)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>V1</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
</tr>
<tr>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>Please help</p>
",18735916.0,-1.0,N/A,2022-04-08 10:03:43,Remove a column in dataframe if a particular value meets a condition in R,<r><data-science><data-wrangling>,2,0,N/A,CC BY-SA 4.0
74698666,1,-1.0,2022-12-06 07:19:44,0,14,"<p>I have a csv file with a column with non-English text and viewing it on excel leads to odd characters. I ended up saving the file. Now the saved file has the non english text replaced with excel encoded text. Is there a way to get the original text back?</p>
",5441912.0,-1.0,N/A,2022-12-06 07:19:44,How to get Excel encoded non english text back,<python><excel><csv><data-science>,0,0,N/A,CC BY-SA 4.0
74702124,1,-1.0,2022-12-06 11:57:53,0,99,"<p>I have to call Kedro nodes or pipelines in Django API, and need to use Kedro pipelines or nodes output as input in Django API. Not getting any solution. Please suggest some solution for this.
How to call Kedro pipeline or nodes in Django APIs?</p>
",9357106.0,-1.0,N/A,2022-12-06 12:19:30,how to call kedro pipline or nodes in Django framework,<python><django><django-rest-framework><data-science><kedro>,0,1,N/A,CC BY-SA 4.0
74707126,1,-1.0,2022-12-06 17:58:40,0,42,"<p>For my nlp problem I'm using a combination of TFIDF and KMeans from the sklearn package. The tfidf gets the vectors and then I use Kmeans to cluster the texts based on the vectors. I have a few parameters of the TFIDF that I play around with like the n_gram, the input features, and the stop_words. The problem is how do I evaluate this model? My guess is I don't have to evaluate the KMeans model since its role is just to calculate the distances between points and that I just have to focus on the TFIDF model and parameters I end up using. Is this correct? If it is, how do I evaluate this model apart from manually shifting through the clusters to see if items are grouped correctly?</p>
",11734835.0,-1.0,N/A,2022-12-06 17:58:40,how to evaluate the combination of tfidf and kmeans,<python><scikit-learn><data-science><k-means><tf-idf>,0,0,N/A,CC BY-SA 4.0
74685616,1,-1.0,2022-12-05 09:00:47,0,32,"<p>I recently updated my windows10 and after that my jupyter notebook is not functioning properly. I tried to import sklearn.datasets but it gave an error like</p>
<pre><code>HTTP Error 429: Too Many Requests
</code></pre>
<p>this one just after one time execution.
And also I am unable to get an option of run as administrator here. What should I do?</p>
<p>I tried updating anaconda navigator. Opening jupyter through command prompt. Launching through navigator power shell. But its not working.</p>
",20689307.0,-1.0,N/A,2022-12-05 09:00:47,What can I do so my jupyter notebook will work properly?,<python><jupyter-notebook><windows-10><data-science><data-analysis>,0,1,N/A,CC BY-SA 4.0
74689849,1,74689950.0,2022-12-05 14:36:07,-1,73,"<p><a href=""https://i.stack.imgur.com/prnua.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I want to make a new column from &quot;TotalPrice&quot; with qcut function but some values returns as NaN. I don't know why?</p>
<p>I tried to change the data type of the column. But nothing has changed.</p>
",11459099.0,-1.0,N/A,2022-12-05 15:10:42,pandas.qcut returning NaN values,<pandas><dataframe><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
74704230,1,-1.0,2022-12-06 14:30:52,0,107,"<p>I am using Prophet to predict a time serie. However, if I drop last month, <em>yhat</em> <strong>in the past</strong> changes. I think the expected result is to modify the future prediction, not the past. Is that behaviour correct?</p>
<p>How can I make a month yhat depends only past data?</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from prophet import Prophet
pd.set_option('display.float_format', lambda x: '%.2f' % x)

# Read the data
json_data = '{&quot;ds&quot;:{&quot;0&quot;:&quot;2017-10-01&quot;,&quot;1&quot;:&quot;2017-11-01&quot;,&quot;2&quot;:&quot;2017-12-01&quot;,&quot;3&quot;:&quot;2018-01-01&quot;,&quot;4&quot;:&quot;2018-02-01&quot;,&quot;5&quot;:&quot;2018-03-01&quot;,&quot;6&quot;:&quot;2018-04-01&quot;,&quot;7&quot;:&quot;2018-05-01&quot;,&quot;8&quot;:&quot;2018-06-01&quot;,&quot;9&quot;:&quot;2018-07-01&quot;,&quot;10&quot;:&quot;2018-08-01&quot;,&quot;11&quot;:&quot;2018-09-01&quot;,&quot;12&quot;:&quot;2018-10-01&quot;,&quot;13&quot;:&quot;2018-11-01&quot;,&quot;14&quot;:&quot;2018-12-01&quot;,&quot;15&quot;:&quot;2019-01-01&quot;,&quot;16&quot;:&quot;2019-02-01&quot;,&quot;17&quot;:&quot;2019-03-01&quot;,&quot;18&quot;:&quot;2019-04-01&quot;,&quot;19&quot;:&quot;2019-05-01&quot;,&quot;20&quot;:&quot;2019-06-01&quot;,&quot;21&quot;:&quot;2019-07-01&quot;,&quot;22&quot;:&quot;2019-08-01&quot;,&quot;23&quot;:&quot;2019-09-01&quot;,&quot;24&quot;:&quot;2019-10-01&quot;,&quot;25&quot;:&quot;2019-11-01&quot;,&quot;26&quot;:&quot;2019-12-01&quot;,&quot;27&quot;:&quot;2020-01-01&quot;,&quot;28&quot;:&quot;2020-02-01&quot;,&quot;29&quot;:&quot;2020-03-01&quot;,&quot;30&quot;:&quot;2020-04-01&quot;,&quot;31&quot;:&quot;2020-05-01&quot;,&quot;32&quot;:&quot;2020-06-01&quot;,&quot;33&quot;:&quot;2020-07-01&quot;,&quot;34&quot;:&quot;2020-08-01&quot;,&quot;35&quot;:&quot;2020-09-01&quot;,&quot;36&quot;:&quot;2020-10-01&quot;,&quot;37&quot;:&quot;2020-11-01&quot;,&quot;38&quot;:&quot;2020-12-01&quot;,&quot;39&quot;:&quot;2021-01-01&quot;,&quot;40&quot;:&quot;2021-02-01&quot;,&quot;41&quot;:&quot;2021-03-01&quot;,&quot;42&quot;:&quot;2021-04-01&quot;,&quot;43&quot;:&quot;2021-05-01&quot;,&quot;44&quot;:&quot;2021-06-01&quot;,&quot;45&quot;:&quot;2021-07-01&quot;,&quot;46&quot;:&quot;2021-08-01&quot;,&quot;47&quot;:&quot;2021-09-01&quot;,&quot;48&quot;:&quot;2021-10-01&quot;,&quot;49&quot;:&quot;2021-11-01&quot;,&quot;50&quot;:&quot;2021-12-01&quot;,&quot;51&quot;:&quot;2022-01-01&quot;,&quot;52&quot;:&quot;2022-02-01&quot;,&quot;53&quot;:&quot;2022-03-01&quot;,&quot;54&quot;:&quot;2022-04-01&quot;,&quot;55&quot;:&quot;2022-05-01&quot;,&quot;56&quot;:&quot;2022-06-01&quot;,&quot;57&quot;:&quot;2022-07-01&quot;,&quot;58&quot;:&quot;2022-08-01&quot;,&quot;59&quot;:&quot;2022-09-01&quot;,&quot;60&quot;:&quot;2022-10-01&quot;,&quot;61&quot;:&quot;2022-11-01&quot;},&quot;y&quot;:{&quot;0&quot;:3065,&quot;1&quot;:3127,&quot;2&quot;:8506,&quot;3&quot;:2527,&quot;4&quot;:2376,&quot;5&quot;:2753,&quot;6&quot;:2964,&quot;7&quot;:3750,&quot;8&quot;:4445,&quot;9&quot;:3502,&quot;10&quot;:3968,&quot;11&quot;:3195,&quot;12&quot;:3232,&quot;13&quot;:3377,&quot;14&quot;:7823,&quot;15&quot;:2452,&quot;16&quot;:2563,&quot;17&quot;:2747,&quot;18&quot;:2877,&quot;19&quot;:3617,&quot;20&quot;:3620,&quot;21&quot;:4044,&quot;22&quot;:3491,&quot;23&quot;:2853,&quot;24&quot;:3447,&quot;25&quot;:3346,&quot;26&quot;:7835,&quot;27&quot;:2543,&quot;28&quot;:2412,&quot;29&quot;:1860,&quot;30&quot;:759,&quot;31&quot;:3630,&quot;32&quot;:2216,&quot;33&quot;:1247,&quot;34&quot;:4455,&quot;35&quot;:3178,&quot;36&quot;:3502,&quot;37&quot;:3475,&quot;38&quot;:7311,&quot;39&quot;:2296,&quot;40&quot;:2136,&quot;41&quot;:1717,&quot;42&quot;:2200,&quot;43&quot;:3764,&quot;44&quot;:3697,&quot;45&quot;:4007,&quot;46&quot;:3566,&quot;47&quot;:3043,&quot;48&quot;:3457,&quot;49&quot;:3256,&quot;50&quot;:8564,&quot;51&quot;:2218,&quot;52&quot;:2815,&quot;53&quot;:3389,&quot;54&quot;:3816,&quot;55&quot;:4853,&quot;56&quot;:4406,&quot;57&quot;:3859,&quot;58&quot;:4152,&quot;59&quot;:3421,&quot;60&quot;:3965,&quot;61&quot;:3590}}'
data = pd.read_json(json_data)
data['ds'] = pd.to_datetime(data['ds'])

# Predict helper function
def predict(df, periods=12):
    m = Prophet().fit(df)
    future = m.make_future_dataframe(periods=periods, freq='MS')
    forecast = m.predict(future)
    return forecast[['ds', 'yhat']].merge(df, on='ds', how='left')

# Predict the next 12 months
prediction = predict(data, 12)

# Drop last month from data and predict the next 12 months
prediction_without_last_month = predict(data[:-1], 12)

# Compare the predictions
comparison = prediction.merge(prediction_without_last_month, on='ds', suffixes=['', '_without_last_month'])

# Output the comparison
comparison[comparison['ds'].dt.month == 10]
</code></pre>
<p><a href=""https://i.stack.imgur.com/EyevM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EyevM.png"" alt=""Predictions comparison"" /></a></p>
<p>PS: Note that all october month in past year have different yhat only by excluding november 2022, that is in the future comparing to these months.</p>
",1647466.0,6361531.0,2022-12-09 17:52:47,2022-12-09 17:52:47,Why adding new data in time series affects yhat (predictions) in the past in Prophet?,<python><data-science><arima><prophet>,2,0,N/A,CC BY-SA 4.0
74704664,1,-1.0,2022-12-06 15:00:59,-1,50,"<p>I'm working on a dataset that has both numerical and categorical columns. One of the numerical columns is <code>fare rates ($)</code> which has just 4 distinct values (<code>200</code>, <code>400</code>, <code>600</code> and <code>800</code>). I have done feature scaling on other numerical features but I'm stuck here to decide whether should I need to apply <code>normalization</code> here or make it categorical to <code>encode</code> this feature. I want to use Neural Networks, if I treat it as a numerical feature, the weights for this feature will affect the output. If anyone has any leads then please help me in this regard.
Thanks</p>
<p>I'm trying to find a perfect solution.</p>
",14577396.0,14577396.0,2022-12-06 15:10:33,2022-12-06 18:19:53,Should I need to convert ordered numerical data into categorical for encoding or normalize it?,<machine-learning><deep-learning><data-science><feature-selection><feature-engineering>,1,0,N/A,CC BY-SA 4.0
74707842,1,-1.0,2022-12-06 19:00:58,0,28,"<p>I want only displot / countplot to return based on the choice, I am not sure where to place the return function</p>
<pre><code>def viz(plottype, col):
    
    if plottype == &quot;count&quot;:
        plt.figure(figsize=(15,15))
        sns.countplot(x=df[col],palette=&quot;mako&quot;, hue=df['Credit_Score'])
        plt.xticks(rotation=45)
        plt.show()
        
    elif plottype == 'dist':
        
        plt.figure(figsize=(12,12))
        sns.displot(df[col], color='crimson', kde=True, bins=50)
        plt.xticks(rotation=45)
        plt.show()
       
        
occ =viz(&quot;count&quot;, &quot;Occupation&quot;)
cm = viz(&quot;count&quot;, &quot;Credit_Mix&quot;)
mSal = viz('dist', 'Monthly_Inhand_Salary')
cm
</code></pre>
<p>I want cm to return only a countplot and mSal to return only the displot. I am not sure what to return and where to place the return. Or is there another way to carry this out?</p>
",19657272.0,-1.0,N/A,2022-12-06 19:00:58,Where should I place the return to only get the plots I want?,<python><matplotlib><data-science><visualization><data-analysis>,0,1,N/A,CC BY-SA 4.0
74708259,1,74708354.0,2022-12-06 19:47:57,2,86,"<p>I have a large DataFrame (named: complete) of data (only two columns). I want to filter it using complete words only, instead of substrings. Ex:</p>
<p><strong>complete dataframe:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">comment</th>
<th style=""text-align: center;"">sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">fast running</td>
<td style=""text-align: center;"">0.9</td>
</tr>
<tr>
<td style=""text-align: center;"">heavily raining</td>
<td style=""text-align: center;"">0.5</td>
</tr>
<tr>
<td style=""text-align: center;"">in the house</td>
<td style=""text-align: center;"">0.1</td>
</tr>
<tr>
<td style=""text-align: center;"">coming in</td>
<td style=""text-align: center;"">0.0</td>
</tr>
<tr>
<td style=""text-align: center;"">rubbing it</td>
<td style=""text-align: center;"">-0.5</td>
</tr>
</tbody>
</table>
</div>
<p>if I set a substring to filter my table:</p>
<pre><code>substring = 'in'
comp = complete[complete.apply(lambda row: row.astype(str).str.contains(substring, case=False).any(), axis=1)]
</code></pre>
<p><strong>output comp:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">comment</th>
<th style=""text-align: center;"">sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">fast running</td>
<td style=""text-align: center;"">0.9</td>
</tr>
<tr>
<td style=""text-align: center;"">heavily raining</td>
<td style=""text-align: center;"">0.5</td>
</tr>
<tr>
<td style=""text-align: center;"">in the house</td>
<td style=""text-align: center;"">0.1</td>
</tr>
<tr>
<td style=""text-align: center;"">coming in</td>
<td style=""text-align: center;"">0.0</td>
</tr>
<tr>
<td style=""text-align: center;"">rubbing it</td>
<td style=""text-align: center;"">-0.5</td>
</tr>
</tbody>
</table>
</div>
<p>It returns the same DF since all words do have &quot;in&quot; as a substring.</p>
<p><strong>My desired output:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">comment</th>
<th style=""text-align: center;"">sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">in the house</td>
<td style=""text-align: center;"">0.1</td>
</tr>
<tr>
<td style=""text-align: center;"">coming in</td>
<td style=""text-align: center;"">0.0</td>
</tr>
</tbody>
</table>
</div>
<p>Filter it only if the substring is found as a word, not as a substring.</p>
<p>¿How can this be done?</p>
",2857611.0,2857611.0,2022-12-06 19:57:00,2022-12-06 20:28:26,filter a DataFrame using complete word only,<python><pandas><dataframe><data-science>,2,2,N/A,CC BY-SA 4.0
74697092,1,-1.0,2022-12-06 03:53:16,0,40,"<p>I'm new to data science and machine learning, I came accros a task where i was to do a binary split on a categorical variable that has 5 levels, I'm supposed to only slit into two, please how do i go about this?</p>
<p>Thank you</p>
<p>I tried thinkink of it as getting every possible two-way split combination of 5 levels i.e</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column A</th>
<th>Column B</th>
</tr>
</thead>
<tbody>
<tr>
<td>LEVEL 1,</td>
<td>LEVEL 2,LEVEL 3,LEVEL 4,LEVEL 5,</td>
</tr>
<tr>
<td>LEVEL 1, LEVEL 2,</td>
<td>LEVEL 3,LEVEL 4,LEVEL 5,</td>
</tr>
</tbody>
</table>
</div>
<p>etc... but this might grow up to 16 combinations.
Then i find the eentropy of the 16 columns + the target variable but i'm not sure if that's the way to do it. It looks long and will take time.</p>
",6686671.0,-1.0,N/A,2022-12-06 03:53:16,How to do a binary split on a categorical variable with 5 levels,<machine-learning><data-science>,0,0,N/A,CC BY-SA 4.0
71788199,1,-1.0,2022-04-07 19:55:07,0,47,"<p><a href=""https://i.stack.imgur.com/SUOpw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SUOpw.png"" alt=""enter image description here"" /></a></p>
<p>Hi, I have .txt file for public data in this format with no delimiters. All I have is the headers of the fields based on their positions. Example:</p>
<pre><code>          1- 2       FIPS State code (00 for US record)
          4- 8       District ID
         10-81       District Name
</code></pre>
<p>I am trying to load data to Tableau but the column values get messed up. Especially, the Name values get split which is okay but they shift right into the 4th column.</p>
<p>Can someone please help me what is the best way to extract these fields in python when all we have is their positions?</p>
",5174650.0,5174650.0,2022-04-07 20:02:45,2022-04-07 20:02:45,Extract fields in python when all we have is their positions and no delimiter,<python><python-3.x><data-science><data-extraction>,0,6,N/A,CC BY-SA 4.0
74709037,1,74709068.0,2022-12-06 21:06:26,0,394,"<p>I work on a finance forecasting project I did my preprocessing part but when I try to construct train and test set I got an error</p>
<p>I have constructed my DataFrame as</p>
<pre><code>alldata = pd.DataFrame({'Date':date,
                        'S&amp;P 500 Price':normalised_snp,
                        'S&amp;P 500 Open': normalised_snpopen,
                        '10 Year Bond Price': normalised_tybp,
                        '10 Year Bond Open': normalised_tybpopen,
                        '2 Year Bond Price': normalised_twybp,
                        '2 Year Bond Open': normalised_twybpopen,
                        'US Inflation' : normalised_USInflation,
                        'US GDP' : normalised_USGDP,
                        'US Insterest' : normalised_USInterest,
                        'Global Inflation Rate' : normalised_GlobalInflation,
                        'Global GDP' : normalised_GlobalGDP})
</code></pre>
<p>It looks like this</p>
<pre><code>        Date  S&amp;P 500 Price  ...  Global Inflation Rate  Global GDP
0 2006-01-03       0.143754  ...               0.588237         0.0
1 2006-01-04       0.144885  ...               0.588237         0.0
2 2006-01-05       0.144890  ...               0.588237         0.0
3 2006-01-06       0.147795  ...               0.588237         0.0
4 2006-01-09       0.148936  ...               0.588237         0.0
5 2006-01-10       0.148824  ...               0.588237         0.0
6 2006-01-11       0.149914  ...               0.588237         0.0
7 2006-01-12       0.147943  ...               0.588237         0.0
8 2006-01-13       0.148319  ...               0.588237         0.0
9 2006-01-17       0.147208  ...               0.588237         0.0
</code></pre>
<p>and then I have tried to construct test and train for this set as</p>
<pre><code>X= alldata['S&amp;P 500 Price'].to_numpy()
y= alldata.drop(columns=['Date','S&amp;P 500 Price','10 Year Bond Open','2 Year Bond Open']).to_numpy()
print(y)
X_train,X_test,y_train,y_test = train_test_split(test_size=0.25,random_state=0)
print(X_test.shape)
print(X_train.shape)
</code></pre>
<p>But I got an error as</p>
<blockquote>
<p>ValueError: At least one array required as input</p>
</blockquote>
<p>I couldn't find my mistake is ther any solution for this?</p>
",19017614.0,19017614.0,2022-12-06 21:11:17,2022-12-06 21:11:17,ValueError: At least one array required as input error when dividing the set,<python><pandas><scikit-learn><data-science><finance>,1,0,N/A,CC BY-SA 4.0
74718879,1,-1.0,2022-12-07 15:12:59,0,133,"<p>I'm trying to create a confusion_matrix but I get the following error:</p>
<pre><code>TypeError: Labels in y_true and y_pred should be of the same type.
Got y_true=[False  True] and y_pred=['False' 'True'].
Make sure that the predictions provided by the classifier coincide with the true labels.
</code></pre>
<p>This is my code.</p>
<pre class=""lang-py prettyprint-override""><code>predict_test = best_cat.predict(features_test)
sns.heatmap(confusion_matrix(target_test, predict_test),annot=True,fmt='3.0f')
plt.title('Confusion Matrix', y=1.05, size=15)
</code></pre>
<p>How can the error be solved?</p>
",19983751.0,2347649.0,2022-12-10 18:28:50,2022-12-10 18:33:48,type error while creating confusion matrix,<python><machine-learning><scikit-learn><data-science><confusion-matrix>,1,8,N/A,CC BY-SA 4.0
74723291,1,-1.0,2022-12-07 22:02:28,0,89,"<p>I wish to extract tables to excel from a scanned booklet. <a href=""https://drive.google.com/file/d/1KJkejeHY_bw0NW-JKAEQKa6x0qn3Il5D/view?usp=share_link"" rel=""nofollow noreferrer"">sample pdf can be found here</a>
It is in .pdf format though the tables appear as an image as it is scanned.
Tried Camelot and PyMuPDF but seem to get it wrong somewhere.</p>
<p>Here is the code I used :</p>
<pre><code>import camelot
import PyPDF2
import pandas as pd

file = r&quot;C:/Users/Vibes/Desktop/Projects/.pdf/Camelot.pdf&quot;

tables = camelot.read_pdf(file, pages='all', flavor=&quot;stream&quot;, encoding=&quot;utf-8&quot;)

master_DF = pd.DataFrame()

for i in range(tables.n):
    if i == 0:
        new_header = tables[i].df.iloc[4] 
        tables[i].df = tables[i].df[5:]
        tables[i].df.columns = new_header
        master_DF = pd.concat([master_DF, tables[i].df], axis=0, ignore_index=True)
    else:
         tables[i].df = tables[i].df[1:]
         tables[i].df.columns = new_header
         master_DF = pd.concat([master_DF, tables[i].df], axis=0, ignore_index=True)
         print(tables[i].df)
         print(&quot;&quot;)

</code></pre>
<p>Tried camelot but got this error:</p>
<p><code>page-1 is image-based, camelot only works on text-based pages.</code></p>
",13384545.0,-1.0,N/A,2022-12-07 22:07:54,Extracting Unstructured data with python - Extracting tables from a scanned book,<python><python-3.x><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
71796390,1,-1.0,2022-04-08 11:40:53,-1,2125,"<p>I am writing Python code in Kaggle. The csv file has 8 columns in which column 0 is 'date' and column1 is 'stringency_index'</p>
<pre><code>    series = pd.read_csv(&quot;&lt;csv path&gt;&quot;, 
                         na_values=['nan','?'], parse_dates = ['date'])
    series1 = series.iloc[:, [0,1]]
    series1
</code></pre>
<p>Some of the rows of series1 are:</p>
<pre><code>    date        stringency_index

0   2020-01-29      2.78

1   2020-01-30      2.78

2   2020-01-31      2.78

3   2020-01-02      2.78

4   2020-02-02      2.78

... ... ...

716 2022-01-14      58.33

717 2022-01-15      58.33

718 2022-01-16      58.33

719 2022-01-17      NaN

720     2022-01-18      NaN

</code></pre>
<p>Then, I convert <code>series1</code> to <code>TimeSeries</code> as follows</p>
<pre><code>series1 = TimeSeries.from_dataframe(series1, 'stringency_index')
</code></pre>
<p>I have already imported <code>TimeSeries</code> from darts. I am getting the following error</p>
<pre><code>AttributeError: Invalid type of `time_col`: it needs to be of either 'str', 'datetime' or 'int' dtype.
</code></pre>
<p>How can I fix this error?</p>
",2830154.0,1832058.0,2022-04-08 12:41:22,2022-11-02 23:47:57,Converting dataframe to TimeSeries in Python is giving Invalid type of `time_col` error,<python><time-series><data-science>,2,2,N/A,CC BY-SA 4.0
74723908,1,-1.0,2022-12-07 23:23:26,0,39,"<p>Notice that ProviderID, Vender and PCP in df_claims are float strings (Ex. 12231.0), which isn't accurate representation. These columns are ID's for respective entities, and ideally should be int (Ex. 12231). Convert these columns to int, handling the missing values separately.</p>
<p>This is my question in the assignment.</p>
<p><a href=""https://i.stack.imgur.com/tO0QD.png"" rel=""nofollow noreferrer"">display(df_claim)</a></p>
<p>So, I use astype to convert datatype.</p>
<pre><code>df_claim['ProviderID'] = df_claim['ProviderID'].fillna(0)

df_claim['ProviderID'] = df_claim['ProviderID'].astype('int64')

df_claim['ProviderID'] = df_claim['ProviderID'].replace(0, np.nan)
</code></pre>
<p><a href=""https://i.stack.imgur.com/DeZXU.png"" rel=""nofollow noreferrer"">after add the codes</a></p>
<p>Nothing is changed.</p>
<p>I tried astype(int), astype('int), astype(np.int), astype('int64').
But noting of those work.</p>
<p>I want convert whole values in column 'ProviderID' to integer without any decimal.</p>
",20718328.0,4583620.0,2022-12-07 23:27:15,2022-12-07 23:27:15,Converting float64 column to int column,<python><pandas><database><dataframe><data-science>,0,2,N/A,CC BY-SA 4.0
74722346,1,-1.0,2022-12-07 20:06:21,0,57,"<p>I have a csv file called purchases.csv and I am trying to find the least and most shopped months. I added the csv file to the end of the question. Here what i got so far;</p>
<pre><code>import pandas as pd
from collections import Counter

df = pd.read_csv('purchases.csv')

df['date']=pd.to_datetime(df['date']) #Filtering the date to 2020 only
dataset=df[df['date'].dt.year == 2020]

#Exploratory Data Analysis

totalShop = len(dataset) # Total shopping
print(totalShop)

product = len(dataset['item_id'].value_counts()) #How many products there are
print(product) # Ürün Sayısı

count = Counter(dataset['item_id']) #most selled 10 products
maxSold = count.most_common(10)
print(maxSold)
</code></pre>
<p>I believe I can find it with counter and most_common() but it does not work well. This is what I tried;</p>
<pre><code>count = Counter(dataset['date'])
maxShopping = count.most_common()
print(maxShopping)
</code></pre>
<p>I would appriciate if anyone can help me. Thank you so much!</p>
<p><a href=""https://i.stack.imgur.com/ckcmJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ckcmJ.png"" alt=""This is what the csv file looks like"" /></a></p>
",10964038.0,10964038.0,2022-12-07 20:18:11,2022-12-07 21:38:43,How to find the least shopped month in a csv file with Python?,<python-3.x><pandas><numpy><csv><data-science>,0,3,N/A,CC BY-SA 4.0
74724904,1,74725683.0,2022-12-08 02:38:17,0,44,"<p>This might seem very basic but I am new to Pandas, the title might not be accurate but I have good screenshots below
This is sort of a voting app</p>
<p>I have the vote_results dataframe:
<a href=""https://i.stack.imgur.com/Nk80W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nk80W.png"" alt=""enter image description here"" /></a></p>
<p>And the legislators dataframe:
<a href=""https://i.stack.imgur.com/FhnfE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FhnfE.png"" alt=""enter image description here"" /></a></p>
<p>This is the dataframe that I want to accomplish:
<a href=""https://i.stack.imgur.com/30lIA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/30lIA.png"" alt=""enter image description here"" /></a></p>
<p>supported bills means a vote for yes
opposed bills means a vote for no
<strong>So I want to know how many times each legislator voted for yes (1) and no(2)
I also need a join on the legislator's dataframe in order to include the legislator name</strong></p>
<p>Really need some insight on this to understand better what can I do with dataframes in terms of manipulation.</p>
<p>I tried making some joins and group_by's but honestly I am not handling it very well, I can't print my group_by's and also don't know how to create an alias for count positive and negative votes grouped by legislator_id</p>
",10402252.0,-1.0,N/A,2022-12-08 04:56:11,Is there a way to count how many entries exists with a certain filter for python pandas?,<python><pandas><dataframe><pyspark><data-science>,1,1,N/A,CC BY-SA 4.0
74737053,1,-1.0,2022-12-08 22:33:29,0,40,"<p>I have a dataframe called &quot;df&quot; that I would like to make compatible with a time series.</p>
<pre><code>

    GeoName     TimePeriod     Population

0   Alamance, NC    2015        157,367
1   Alamance, NC    2016        160,663
2   Alamance, NC    2017        163,671
3   Alamance, NC    2018        166,901
4   Alamance, NC    2019        169,590
... ... ... ... ... ... ...
422 York, SC        2017        262,998
423 York, SC        2018        270,090

</code></pre>
<p>I created another dataframe 'df2'</p>
<pre><code>    Datetime
0   2015-07-01 00:00:00
1   2015-07-01 01:00:00
2   2015-07-01 02:00:00
3   2015-07-01 03:00:00
4   2015-07-01 04
</code></pre>
<p>My goal is 1) have df2 have column names of the distinct geonames/counties_population (there are 61 distinct geonames but they appear more than once for distinct years)</p>
<p>I would like for ex. to have every hour of df2 for 2015 and Alamance, NC to show the population of 157,367 --&gt; repeat for all 61 counties/every year</p>
<pre><code>    Datetime               Alamance_NC_pop
0   2015-07-01 00:00:00    157,367
1   2015-07-01 01:00:00    157,367
2   2015-07-01 02:00:00    157,367
3   2015-07-01 03:00:00    157,367
4   2015-07-01 04:00:00    157,367
</code></pre>
",20728175.0,20728175.0,2022-12-08 22:39:37,2022-12-08 22:39:37,Converting Dataframe to Be Compatible With Time Series,<python><pandas><dataframe><data-science>,0,1,N/A,CC BY-SA 4.0
74725071,1,74725385.0,2022-12-08 03:05:16,1,78,"<p>I have a dataset of over 1000 txt files which contains information of books</p>
<pre><code>The Project Gutenberg EBook of Apocolocyntosis, by Lucius Seneca

This eBook is for the use of anyone anywhere at no cost and with
almost no restrictions whatsoever.  You may copy it, give it away or
re-use it under the terms of the Project Gutenberg License included
with this eBook or online at www.gutenberg.org


Title: Apocolocyntosis

Author: Lucius Seneca

Release Date: November 10, 2003 [EBook #10001]
[Date last updated: April 9, 2005]

Language: English

Character set encoding: ASCII

*** START OF THIS PROJECT GUTENBERG EBOOK APOCOLOCYNTOSIS ***

</code></pre>
<p>I'm trying to use pandas to read these files and create a data frame from it getting Title, Author, Release Date, and Language as columns and its values but so far I have been having errors</p>
<p>Reading from a single file</p>
<pre><code>df = pd.read_csv('dataset/10001.txt')
</code></pre>
<p>Error</p>
<pre><code>ParserError                               Traceback (most recent call last)
Input In [30], in &lt;cell line: 1&gt;()
----&gt; 1 df = pd.read_csv('dataset/10001.txt')

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\util\_decorators.py:311, in deprecate_nonkeyword_arguments.&lt;locals&gt;.decorate.&lt;locals&gt;.wrapper(*args, **kwargs)
    305 if len(args) &gt; num_allow_args:
    306     warnings.warn(
    307         msg.format(arguments=arguments),
    308         FutureWarning,
    309         stacklevel=stacklevel,
    310     )
--&gt; 311 return func(*args, **kwargs)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\io\parsers\readers.py:680, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)
    665 kwds_defaults = _refine_defaults_read(
    666     dialect,
    667     delimiter,
   (...)
    676     defaults={&quot;delimiter&quot;: &quot;,&quot;},
    677 )
    678 kwds.update(kwds_defaults)
--&gt; 680 return _read(filepath_or_buffer, kwds)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\io\parsers\readers.py:581, in _read(filepath_or_buffer, kwds)
    578     return parser
    580 with parser:
--&gt; 581     return parser.read(nrows)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\io\parsers\readers.py:1254, in TextFileReader.read(self, nrows)
   1252 nrows = validate_integer(&quot;nrows&quot;, nrows)
   1253 try:
-&gt; 1254     index, columns, col_dict = self._engine.read(nrows)
   1255 except Exception:
   1256     self.close()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\io\parsers\c_parser_wrapper.py:225, in CParserWrapper.read(self, nrows)
    223 try:
    224     if self.low_memory:
--&gt; 225         chunks = self._reader.read_low_memory(nrows)
    226         # destructive to chunks
    227         data = _concatenate_chunks(chunks)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\_libs\parsers.pyx:805, in pandas._libs.parsers.TextReader.read_low_memory()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\_libs\parsers.pyx:861, in pandas._libs.parsers.TextReader._read_rows()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\_libs\parsers.pyx:847, in pandas._libs.parsers.TextReader._tokenize_rows()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\_libs\parsers.pyx:1960, in pandas._libs.parsers.raise_parser_error()

ParserError: Error tokenizing data. C error: Expected 2 fields in line 60, saw 3
</code></pre>
",13595250.0,-1.0,N/A,2022-12-08 04:18:09,creating data frame from text file,<python><pandas><dataframe><data-science><read.csv>,1,3,N/A,CC BY-SA 4.0
71797812,1,72119311.0,2022-04-08 13:22:31,-2,29,"<p>One of the biggest struggle with ML research is the creation of objective functions which capture the researcher's goals. Especially when talking about generalizable AI, the definition of the objective function is very tricky.</p>
<p><a href=""https://arxiv.org/abs/1705.05363"" rel=""nofollow noreferrer"">This excellent paper</a> for instance attempts to define an objective function to reward an agent's curiosity.</p>
<p>If we could measure intelligent behavior well, it would perhaps be possible to perform an optimization in which the parameters of a simulation such as a cellular automaton are optimized to maximize the emergence of increasingly intelligent behavior.</p>
<p>I vaguely remember having come across a group of cross-discipline researchers who were attempting to use the information theory concept of entropy to measure intelligent behavior but cannot find any resources about it now. So is there a scientific field dedicated to the quantification of intelligent behavior?</p>
",8853612.0,8853612.0,2022-04-08 13:31:09,2022-05-06 07:29:18,Is there a scientific field dedicated to the quantification of intelligent behavior?,<data-science><artificial-intelligence><computer-science><information-theory>,1,1,2022-05-06 07:39:20,CC BY-SA 4.0
74725653,1,-1.0,2022-12-08 04:50:24,0,62,"<p>I want to split my .txt file by starting 4 digits and create column in vaex.</p>
<p><strong>I do this by pandas and want to do the same as below by vaex.</strong></p>
<pre><code>
df= pd.read_csv(r&quot;D:\Personal Projects\Testing_file.txt&quot;)

df['First'] = df['address'].str[:4]
df['Second'] = df['address'].str[4:]

new = df[['First', 'Second']]
aaa = new.groupby('First')['Second'].apply(list)
dff = aaa.to_frame()
newDf = dff.transpose()

</code></pre>
",20719878.0,-1.0,N/A,2022-12-08 04:54:10,How to split a file after read by VAEX in python,<python><pandas><dataframe><data-science><vaex>,0,0,N/A,CC BY-SA 4.0
74729470,1,-1.0,2022-12-08 11:17:59,0,58,"<p>I'm new to pandas library and I need to find the summation value for some parameters in 30 sheets of an excel file. the parameters  are in the same column but their rows differ from each other in every sheet. I import the sheets into jupyter notebook with pandas as a series of dataframes. now I don't know how to find different parameters with their corresponding value( in another column) in these dataframes so that I would be able to calculate their sumation and create a new dataframe with these parameters and their added values.</p>
<pre><code>our data is like this:
1       a      56     unemployed
2       b      76     employer
3       d      85     employee
4       c      92     self-employed
</code></pre>
<p>the a, b, c and d parameters rows differ in every sheets. but their column is the same
I want to write a code to iterate over every dataframe and find the parameter. get its value for instane in here for a, is 56, add it to its value in every dataframe and do this for other parameters and give the results as a dataframe with this parameter and their overall values.
I really need this. thanks</p>
<p>I already import the data as follow:</p>
<pre><code>import numpy as np
import pandas as pd

dict_df = pd.read_excel(&quot;location of file&quot;, sheet_name=[i for i in range(0, 30)])

## I made a list of sheets as dataframes dfList=[df1, df2, ..., df30]

dfList = list(dict_df.values())
</code></pre>
",20720865.0,20720865.0,2022-12-08 20:26:41,2022-12-08 20:26:41,adding corresponding values of parameters in each row for 30 sheets of excel files using pandas,<python><pandas><jupyter-notebook><data-science>,0,0,N/A,CC BY-SA 4.0
74738205,1,-1.0,2022-12-09 02:02:08,1,41,"<p>I am learning pandas and doing some exercies but without much source</p>
<p>So basically I have these 4 dataframes below:
<a href=""https://i.stack.imgur.com/6YtUo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6YtUo.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/NFNKH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NFNKH.png"" alt=""enter image description here"" /></a></p>
<p>So for every bill in the dataset, i want to know how many legislators supported the bill and how many legislators opposed the bill, also who was the primary sponsor of the bill?</p>
<p>This is what I am trying to achieve:
<a href=""https://i.stack.imgur.com/l5rIF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l5rIF.png"" alt=""enter image description here"" /></a></p>
<p>I was able to solve this one:
<a href=""https://stackoverflow.com/questions/74724904/is-there-a-way-to-count-how-many-entries-exists-with-a-certain-filter-for-python"">Is there a way to count how many entries exists with a certain filter for python pandas?</a></p>
<p>But what I'm asking now involves 3 tables I guess(?)</p>
",10402252.0,-1.0,N/A,2022-12-14 00:57:21,How do I gather information from 3 dataframes in pandas with custom headers?,<python><pandas><dataframe><pyspark><data-science>,1,0,N/A,CC BY-SA 4.0
74742670,1,74744602.0,2022-12-09 11:44:57,0,57,"<p>I am attempting to make a Bayesian classification algorithm using multivariate Gaussian distribution, but I can't figure out how to extract the probabilities from the resultant matrix from this equation:</p>
<p><a href=""https://i.stack.imgur.com/qWVOJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qWVOJ.png"" alt=""enter image description here"" /></a></p>
<p>So far I have managed to classify a set of given two features into either of two classes using univariate Gaussian distribution, however I would like to have six features, and so using univariate would become quite a lot of code. To achieve this I used the following equations:</p>
<p><a href=""https://i.stack.imgur.com/uus6b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uus6b.png"" alt=""enter image description here"" /></a></p>
<p>And my MATLAB code:</p>
<pre><code>grass_trained_data = [300, 5; 278, 6; 320, 4];
water_trained_data = [320, 12; 329, 8; 305, 9];

test_data = [300, 5]; 
td_x1 = test_data(1);
td_x2 = test_data(2);

% P(grass)
p_grass = 3/6;

% P(water)
p_water = 3/6;

% Calculate P(x | grass)
grass_td_mean_vector = mean(grass_trained_data);
grass_td_mean_x1 = grass_td_mean_vector(1);  % the mean of x1 in the trained data
grass_td_mean_x2 = grass_td_mean_vector(2);  % the mean of x2 in the trained data

grass_td_variance = var(grass_trained_data);  % the variance of x1 and x2
grass_td_variance_x1 = grass_td_variance(1);
grass_td_variance_x2 = grass_td_variance(2);

p_x1_given_grass = (2 * pi * grass_td_variance_x1).^(-1/2) * exp(-1/2 * ((td_x1 - grass_td_mean_x1).^2) / grass_td_variance_x1);
p_x2_given_grass = (2 * pi * grass_td_variance_x2).^(-1/2) * exp(-1/2 * ((td_x2 - grass_td_mean_x2).^2) / grass_td_variance_x2);

p_x_given_grass = p_x1_given_grass * p_x2_given_grass;

% Calculate P(x | water)
water_td_mean_vector = mean(water_trained_data);
water_td_mean_x1 = water_td_mean_vector(1);  % the mean of x1 in the trained data
water_td_mean_x2 = water_td_mean_vector(2);  % the mean of x2 in the trained data

water_td_variance = var(water_trained_data);  % the variance of x1 and x2
water_td_variance_x1 = water_td_variance(1);
water_td_variance_x2 = water_td_variance(2);

p_x1_given_water = (2 * pi * water_td_variance_x1).^(-1/2) * exp(-1/2 * ((td_x1 - water_td_mean_x1).^2) / water_td_variance_x1);
p_x2_given_water = (2 * pi * water_td_variance_x2).^(-1/2) * exp(-1/2 * ((td_x2 - water_td_mean_x2).^2) / water_td_variance_x2);

p_x_given_water = p_x1_given_water * p_x2_given_water;

% Now calculate the probability of the test data belonging to each class
p_grass_given_x = (p_x_given_grass * p_grass) / (p_x_given_grass * p_grass + p_x_given_water * p_water);
p_water_given_x = (p_x_given_water * p_water) / (p_x_given_grass * p_grass + p_x_given_water * p_water);
</code></pre>
<p>This gives <code>P(X | grass)</code> as a probability from 0 to 1, however, in my code for my multivariate attempt:</p>
<pre><code>grass_trained_data = [300, 5; 278, 6; 320, 4];
water_trained_data = [320, 12; 329, 8; 305, 9];

test_data = [300, 5];

% P(grass)
p_grass = 3/6;

% P(water)
p_water = 3/6;

% Calculate P(x | grass)
grass_td_mean_vector = mean(grass_trained_data);
grass_td_cov_matrix = cov(grass_trained_data);

p_x_given_grass = (2 .* pi).^(-1/2) .* det(grass_td_cov_matrix).^(-1/2) .* exp((-1/2) .* transpose(test_data - grass_td_mean_vector) .* inv(grass_td_cov_matrix) .* (test_data - grass_td_mean_vector))
</code></pre>
<p>This gives <code>P(X | grass)</code> as a 2 x 2 matrix. What is the meaning of each of these values, and how can I use them?</p>
",12307227.0,12307227.0,2022-12-09 13:28:32,2022-12-10 11:08:51,How is the resultant likelihood matrix from a multivariate Gaussian distribution used in Bayesian classification?,<matlab><machine-learning><statistics><data-science><bayesian>,1,0,N/A,CC BY-SA 4.0
74745340,1,-1.0,2022-12-09 15:48:24,-1,543,"<p>I've been provided with a PoweBI license and I have a set of x and y coordinates. I would like to plot each coordinate over a rectangle which represents a football pitch. At a later stage overlay an actual football pitch (see images below).</p>
<p>I would just like to get information/guidance on what I need to achieve it. I would then search for information on how to get to it. I need a starting point.</p>
<p>Do I need python or R skills to achieve it?</p>
<p><a href=""https://i.stack.imgur.com/Xnp49.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xnp49.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/gI2yn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gI2yn.png"" alt=""field view"" /></a></p>
<p><a href=""https://i.stack.imgur.com/TYw65.png"" rel=""nofollow noreferrer"">Heat Map</a></p>
<p>Thanks</p>
",13606690.0,-1.0,N/A,2022-12-10 07:01:46,Plot X Y coordinates in PowerBI,<r><python-3.x><powerbi><data-science>,1,2,N/A,CC BY-SA 4.0
74721480,1,-1.0,2022-12-07 18:38:42,0,22,"<p>I have accuracy scores from two models on different datasets and languages, for example, I have this table for two models.
<a href=""https://i.stack.imgur.com/JMibM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JMibM.png"" alt=""enter image description here"" /></a></p>
<p>is it correct if I take avg accuracy from model1 and model2 and do a significance t.test on this to see what model does best on these datasets?</p>
<p>Kind regards</p>
",11719536.0,-1.0,N/A,2022-12-07 18:43:05,Is it correct to do t.test on accuracy scores,<data-science><t-test>,1,0,N/A,CC BY-SA 4.0
74732457,1,74734612.0,2022-12-08 15:12:59,0,114,"<p>I need to solve the following system of non-linear inequalities:</p>
<pre><code>x*y &gt;= 420
x*z &gt;= 14
x*y*z &lt; 5000
</code></pre>
<p>I have tried to find a similar question/solution or package that helps, but I struggle to apply it to that specific case.</p>
<p>The expected outcome should be a list of tuples (x,y,z). In the end, a 3-dimensional plot would be awesome but not really necessary (though should be easy to do it as soon as the solution list exists).</p>
<p>Edit: x, y and z are positive integers.</p>
",20724942.0,20724942.0,2022-12-09 11:47:14,2022-12-09 13:30:07,How to solve the following 3-dimensional system of non-linear inequalities in R?,<r><data-science><nonlinear-optimization><inequalities>,2,4,N/A,CC BY-SA 4.0
71804910,1,-1.0,2022-04-09 02:50:58,1,53,"<p>So, I have this column
<a href=""https://i.stack.imgur.com/wQh9V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wQh9V.png"" alt=""enter image description here"" /></a></p>
<p>And, I wanted to convert into this column
<a href=""https://i.stack.imgur.com/yWqbt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yWqbt.png"" alt=""enter image description here"" /></a></p>
<p>I want to replace zero with number. So, how can I do that using pandas python?</p>
",16875907.0,-1.0,N/A,2022-04-09 02:52:37,How to remove duplicate rows by substituting the zero value with another number in pandas python?,<python><pandas><dataframe><pandas-groupby><data-science>,1,0,N/A,CC BY-SA 4.0
71817700,1,71818337.0,2022-04-10 14:49:09,0,72,"<p>i've already posted here my problem and since then i've been trying to find solution to help me optimise my results , in the previous post ,Yaloa understood what i wanted to do but sadly i always end up in a dead end
<a href=""https://stackoverflow.com/questions/71577895/pandas-how-to-create-an-algorithm-that-helps-me-improve-results-and-creating-n"">My previous Post</a></p>
<p>the fact is that I want to improve my results in order to visualize them
This is my dataframe:</p>
<pre><code>ID           TimeandDate        Date       Time
10   2020-08-07 07:40:09  2022-08-07   07:40:09
10   2020-08-07 08:50:00  2022-08-07   08:50:00
10   2020-08-07 12:40:09  2022-08-07   12:40:09
10   2020-08-08 07:40:09  2022-08-08   07:40:09
10   2020-08-08 17:40:09  2022-08-08   17:40:09
12   2020-08-07 08:03:09  2022-08-07   08:03:09
12   2020-08-07 10:40:09  2022-08-07   10:40:09
12   2020-08-07 14:40:09  2022-08-07   14:40:09
12   2020-08-07 16:40:09  2022-08-07   16:40:09
13   2020-08-07 09:22:45  2022-08-07   09:22:45
13   2020-08-07 17:57:06  2022-08-07   17:57:06
</code></pre>
<p>first of all the data is collected from time clock , i want to create new dataframe with 2 new columns the first one is <code>df[&quot;Check-in&quot;]</code> , as you can see my data doesnt have any indicator to show what time the <code>id</code> has checked in , so i will suppose that the first time for every <code>id</code> is a <code>check-in</code> , and the next row is a check-out and it will be inserted in <code>df[&quot;Check-out&quot;]</code> , also if a <code>check-in</code> doesnt have a <code>check-out</code> time it has to be registred as the <code>check-out</code> for the previous <code>check-out</code> of the same day(sometimes <code>id</code> forgot to <code>check-out</code>) because it has to be the same number of rows for <code>check-in</code> and <code>check-out</code> cant have 2 <code>check-ins</code> and 3 <code>check-outs</code></p>
<p><strong>What i've tried ?</strong>
what i meant with i need better results is because what i've tried is not the best solution , i took the <code>min</code> as a <code>check-in</code> and the <code>max</code> is a <code>check-out</code> of <code>time</code> for every <code>id</code> without adding the two columns , and after that i started calculating the time difference , now imagine <code>ID=13</code> has entered at <code>07:40:09</code> and the he check out at <code>08:40:09</code> , later that day he returns at <code>19:20:00</code> and leave in the next 10 minutes <code>19:30:00</code> if i do that fonction it will show that he worked for 12 hours while his real working time is 1 hour</p>
<p><strong>Result Desired</strong></p>
<pre><code>ID         Date   Check-in    Check-out
10   2020-08-07   07:40:09     12:40:09
10   2020-08-08   07:40:09     17:40:09
12   2020-08-07   08:03:09     10:40:09
12   2020-08-07   14:40:09     16:40:09 
13   2020-08-07   09:22:45     17:57:06
</code></pre>
",18392496.0,17562044.0,2022-06-11 05:50:23,2022-06-11 05:50:23,How to create an algorithm that helps me improve results and automate process?,<python><pandas><dataframe><algorithm><data-science>,3,0,N/A,CC BY-SA 4.0
71811153,1,71813420.0,2022-04-09 19:15:57,-1,28,"<p>So for example say there's a website that shows all the events available around my area in the next 2 weeks. But there's also another website that provides the same data labeled a little differently. Say the data from both websites was provided in json format and it looked like this:</p>
<pre><code>&quot;Events&quot;:{
  &quot;id&quot;:1,
  &quot;Name&quot;: &quot;Rally&quot;,
  &quot;Start time&quot;: &quot;5pm&quot;
}
</code></pre>
<p>and the second website also gives the data in json but instead of rally, the event is called rallies. Here's the json:</p>
<pre><code>&quot;Events&quot;:{
  &quot;id&quot;:1,
  &quot;Name&quot;: &quot;Rallies&quot;,
  &quot;Start time&quot;: &quot;5pm&quot;
}
</code></pre>
<p>It's obvious that these 2 events are the same thing but how do I map them together? What methods can I use to recognize them as the same thing? Imagine there were 1000 of these events. How would that affect the speed of the program?</p>
",18301773.0,-1.0,N/A,2022-04-11 01:37:38,How do I analyze data from two different sources with a little different structers?,<python><string><web-scraping><data-science><string-comparison>,1,0,N/A,CC BY-SA 4.0
71811374,1,-1.0,2022-04-09 19:45:08,0,169,"<p>i am trying to create an ARIMA model. This has been working for me without issue up to today. However, I now get a TypeError with respect to the order argument in the model.</p>
<p>I have upgraded my version of StatsModels also but still no change. Would anyone have any suggestions?</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller
from pmdarima import auto_arima
from statsmodels.tsa.arima.model import ARIMA


df = pd.read_csv('Aryzta AG - Historic Closing Price.csv', index_col='Date', parse_dates=True)
df = df.dropna()
print('Shape of data', df.shape)
df.head()
print(df.head())

df['Close'].plot(figsize=(12, 5))
plt.title('Aryzta AG Closing Share Price 2012 - 2022')
plt.ylabel('Close Price')
plt.show()


def adf_test(dataset):
    dftest = adfuller(dataset, autolag='AIC')
    print(&quot;1. ADF : &quot;, dftest[0])
    print(&quot;2. P-Value : &quot;, dftest[1])
    print(&quot;3. Num Of Lags : &quot;, dftest[2])
    print(&quot;4. Num Of Observations Used For ADF Regression:&quot;, dftest[3])
    print(&quot;5. Critical Values :&quot;)
    for key, val in dftest[4].items():
        print(&quot;\t&quot;, key, &quot;: &quot;, val)
    if dftest[0] &lt; dftest[4][&quot;5%&quot;]:
        print(&quot;Reject Ho - Time Series is Stationary&quot;)
    else:
        print(&quot;Failed to Reject Ho - Time Series is Non-Stationary&quot;)


adf_test(df['Close'])

stepwise_fit = auto_arima(df['Close'], trace=True, suppress_warnings=True)

print(df.shape)
train = df.iloc[:-500]
test = df.iloc[-500:]
print(train.shape, test.shape)

model = ARIMA(train, order=(1, 1, 1))
model_fit = model.fit()
model_fit.summary()


</code></pre>
",18723865.0,-1.0,N/A,2022-04-09 19:45:08,Creating ARIMA Model - Getting Error on Order argument,<python><time-series><data-science><data-modeling><arima>,0,3,N/A,CC BY-SA 4.0
71814174,1,-1.0,2022-04-10 06:06:32,1,496,"<p>I am working on a dataset which consists of average age of marriage. On this dataset I am doing data cleaning job. While performing this process, I came across a feature where I had to fill the 'NaN' values in the location column. But in location column there are multiple unique values and I want to fill the nan values in location. I need some suggestion on how to fill these Nan values in column which had many unique values.</p>
<p><a href=""https://i.stack.imgur.com/Bddg8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bddg8.png"" alt=""enter image description here"" /></a></p>
<p>I have attached the dataset for reference, <a href=""https://github.com/atharva07/Age-of-marriage"" rel=""nofollow noreferrer"">DataSet</a></p>
",9149925.0,4685471.0,2022-04-10 14:34:20,2022-04-10 14:34:20,Fill nan values in one column based on other columns,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
71818555,1,-1.0,2022-04-10 16:34:15,0,118,"<p>Hi i have two vectors: predicts and targets and i have 3 classes &quot;A&quot;, &quot;B&quot; and &quot;C&quot;.</p>
<pre><code>  targets = c(&quot;B&quot;, &quot;A&quot;, &quot;C&quot;, &quot;C&quot;, &quot;B&quot;)
  predicts = c(&quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;B&quot;)
</code></pre>
<p>and i want to create confusion matrix using this data. I want my output to look like this:</p>
<pre><code>    A B C
  A 0 1 0
  B 0 2 0
  C 0 0 2
</code></pre>
<p>but for <code>table(targets, predicts)</code> i get this:</p>
<pre><code>    B C
  A 1 0
  B 2 0
  C 0 2
</code></pre>
<p>Is it possible to fill the table with default values when missing class, like 0 for example?</p>
",11395159.0,-1.0,N/A,2022-04-10 16:35:35,"Create confusion matrix in R, using table() problem with missing values",<r><machine-learning><data-science><confusion-matrix>,1,0,2022-04-10 19:19:37,CC BY-SA 4.0
71818341,1,-1.0,2022-04-10 16:07:14,0,50,"<p>I would like to use a repeatability test for normalized data (Rtpr for Gaussian data). However, I cannot figure out how to normally distribute my data set, as it is strongly skewed, and most values converge into one large column when looking into the histogram (very similar value numbers for most individuals, as seen in the picture below).</p>
<p><a href=""https://i.stack.imgur.com/QBw7k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QBw7k.png"" alt=""enter image description here"" /></a></p>
<p>I used the most known data transformation techniques, but none seems to work (square root, cube, log, log10, reciprocal and Box-Cot). I know I am missing something, but I am quite new to statistics and transformation/ normalization, so I am unsure what I am doing wrong.</p>
<p>Here is the code I used with the data sets:
<a href=""https://github.com/SoundsF1shy/Normalise-data-set"" rel=""nofollow noreferrer"">https://github.com/SoundsF1shy/Normalise-data-set</a></p>
<p>Thank you, and have a great day.</p>
",16434569.0,16434569.0,2022-04-10 16:16:05,2022-04-10 16:16:05,Fix up and normalize data set,<r><statistics><data-science>,0,2,N/A,CC BY-SA 4.0
71826115,1,-1.0,2022-04-11 10:17:57,2,429,"<p>While working on a project I have come across a weird error, where fitting my model works perfectly but when I apply <code>gridsearch</code> it gives me an error.</p>
<p>The code puts all the necessary objects created and uses them in the pipeline.</p>
<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import pandas as pd
import numpy as np
import time
from numpy.fft import fft

class DataPreprocess(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass
    def fit(self, X, y=None):
        self.X_m = X.merge(y, on= ['year', 'weekofyear'])
        
        return self
    
    def transform(self, X):
        dt = pd.to_datetime(self.X_m[&quot;week_start_date&quot;], format=&quot;%Y-%m-%d&quot;)
        unix = []
        for i in dt:
            unix.append(time.mktime(i.timetuple()))
        X_t = (self.X_m).reset_index().assign(date = unix).set_index(['date'])
        
        return X_t 



class FourierComponents(BaseEstimator, TransformerMixin):
    &quot;&quot;&quot;creat features based on sin(2*pi*f*t) and cos(2*pi*f*t)&quot;&quot;&quot;
    def __init__(self, n=10):
        self.n = n
        
    def fit(self, X, y= None):
        self.labels = X['total_cases']
        
        self.Y_t = fft(self.labels - (self.labels).mean())
        self.Y_t = self.Y_t[:len(self.labels)//2]

        
        self.ind_max = np.abs(self.Y_t).argsort()
        
        self.t_span = len(self.labels)

        self.f = np.linspace(0, len(self.Y_t), len(self.Y_t)) / self.t_span
        
        self.f_ind = self.f[self.ind_max]
        
        self.ind = pd.RangeIndex(start = 1, stop=(len(X.index.get_level_values('date')) +1)).values.reshape(-1, 1)
    
        return self
    
    def transform(self, X):
        
        Xt = np.zeros((X.shape[0], 2*len(self.f_ind[-self.n:])))
        
        for i, f in enumerate(self.f_ind[-self.n:]):
            Xt[:, 2*i]     = np.cos(2*np.pi*f*self.ind).reshape(-1)
            Xt[:, 2*i + 1] = np.sin(2*np.pi*f*self.ind).reshape(-1)
        
        return Xt

Unixdata = DataPreprocess()
fourier = FourierComponents()

model = Pipeline([
    ('indices', Unixdata),
    ('fourier', fourier),
    ('scalar', StandardScaler()),
    ('regressor', Ridge())
])

param_grid = {'fourier__n' : list(range(3,5)),
              'regressor__alpha' : np.logspace(1, 4, 20)}

grid_search = GridSearchCV(model, param_grid, cv = 5, verbose = 1, scoring='neg_mean_absolute_error')

grid_search.fit(sj_train_features, sj_train_labels)
</code></pre>
<p>fitting the <code>grid_search</code> here gives me this error:</p>
<pre><code>    Fitting 5 folds for each of 40 candidates, totalling 200 fits
[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-167-cfce20172a59&gt; in &lt;module&gt;
----&gt; 1 grid_search.fit(sj_train_features, sj_train_labels)

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     71                           FutureWarning)
     72         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---&gt; 73         return f(**kwargs)
     74     return inner_f
     75 

~\anaconda3\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
    734                 return results
    735 
--&gt; 736             self._run_search(evaluate_candidates)
    737 
    738         # For multi-metric evaluation, store the best_index_, best_params_ and

~\anaconda3\lib\site-packages\sklearn\model_selection\_search.py in _run_search(self, evaluate_candidates)
   1186     def _run_search(self, evaluate_candidates):
   1187         &quot;&quot;&quot;Search all candidates in param_grid&quot;&quot;&quot;
-&gt; 1188         evaluate_candidates(ParameterGrid(self.param_grid))
   1189 
   1190 

~\anaconda3\lib\site-packages\sklearn\model_selection\_search.py in evaluate_candidates(candidate_params)
    706                               n_splits, n_candidates, n_candidates * n_splits))
    707 
--&gt; 708                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),
    709                                                        X, y,
    710                                                        train=train, test=test,

~\anaconda3\lib\site-packages\joblib\parallel.py in __call__(self, iterable)
   1027             # remaining jobs.
   1028             self._iterating = False
-&gt; 1029             if self.dispatch_one_batch(iterator):
   1030                 self._iterating = self._original_iterator is not None
   1031 

~\anaconda3\lib\site-packages\joblib\parallel.py in dispatch_one_batch(self, iterator)
    845                 return False
    846             else:
--&gt; 847                 self._dispatch(tasks)
    848                 return True
    849 

~\anaconda3\lib\site-packages\joblib\parallel.py in _dispatch(self, batch)
    763         with self._lock:
    764             job_idx = len(self._jobs)
--&gt; 765             job = self._backend.apply_async(batch, callback=cb)
    766             # A job can complete so quickly than its callback is
    767             # called before we get here, causing self._jobs to

~\anaconda3\lib\site-packages\joblib\_parallel_backends.py in apply_async(self, func, callback)
    206     def apply_async(self, func, callback=None):
    207         &quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;
--&gt; 208         result = ImmediateResult(func)
    209         if callback:
    210             callback(result)

~\anaconda3\lib\site-packages\joblib\_parallel_backends.py in __init__(self, batch)
    570         # Don't delay the application, to avoid keeping the input
    571         # arguments in memory
--&gt; 572         self.results = batch()
    573 
    574     def get(self):

~\anaconda3\lib\site-packages\joblib\parallel.py in __call__(self)
    250         # change the default number of processes to -1
    251         with parallel_backend(self._backend, n_jobs=self._n_jobs):
--&gt; 252             return [func(*args, **kwargs)
    253                     for func, args, kwargs in self.items]
    254 

~\anaconda3\lib\site-packages\joblib\parallel.py in &lt;listcomp&gt;(.0)
    250         # change the default number of processes to -1
    251         with parallel_backend(self._backend, n_jobs=self._n_jobs):
--&gt; 252             return [func(*args, **kwargs)
    253                     for func, args, kwargs in self.items]
    254 

~\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)
    558     else:
    559         fit_time = time.time() - start_time
--&gt; 560         test_scores = _score(estimator, X_test, y_test, scorer)
    561         score_time = time.time() - start_time - fit_time
    562         if return_train_score:

~\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py in _score(estimator, X_test, y_test, scorer)
    605         scores = scorer(estimator, X_test)
    606     else:
--&gt; 607         scores = scorer(estimator, X_test, y_test)
    608 
    609     error_msg = (&quot;scoring must return a number, got %s (%s) &quot;

~\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py in __call__(self, estimator, *args, **kwargs)
     85         for name, scorer in self._scorers.items():
     86             if isinstance(scorer, _BaseScorer):
---&gt; 87                 score = scorer._score(cached_call, estimator,
     88                                       *args, **kwargs)
     89             else:

~\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py in _score(self, method_caller, estimator, X, y_true, sample_weight)
    210                                                  **self._kwargs)
    211         else:
--&gt; 212             return self._sign * self._score_func(y_true, y_pred,
    213                                                  **self._kwargs)
    214 

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     71                           FutureWarning)
     72         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---&gt; 73         return f(**kwargs)
     74     return inner_f
     75 

~\anaconda3\lib\site-packages\sklearn\metrics\_regression.py in mean_absolute_error(y_true, y_pred, sample_weight, multioutput)
    176     0.85...
    177     &quot;&quot;&quot;
--&gt; 178     y_type, y_true, y_pred, multioutput = _check_reg_targets(
    179         y_true, y_pred, multioutput)
    180     check_consistent_length(y_true, y_pred, sample_weight)

~\anaconda3\lib\site-packages\sklearn\metrics\_regression.py in _check_reg_targets(y_true, y_pred, multioutput, dtype)
     82 
     83     &quot;&quot;&quot;
---&gt; 84     check_consistent_length(y_true, y_pred)
     85     y_true = check_array(y_true, ensure_2d=False, dtype=dtype)
     86     y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_consistent_length(*arrays)
    254     uniques = np.unique(lengths)
    255     if len(uniques) &gt; 1:
--&gt; 256         raise ValueError(&quot;Found input variables with inconsistent numbers of&quot;
    257                          &quot; samples: %r&quot; % [int(l) for l in lengths])
    258 

ValueError: Found input variables with inconsistent numbers of samples: [188, 748]
</code></pre>
<p>but</p>
<pre><code>model.fit(sj_train_features, sj_train_labels)
</code></pre>
<p>fits perfectly.</p>
<p>Now I am wondering why and where is the mistake in the code?
Can anyone point me in the right direction?</p>
<p>A small example (hopefully representative):</p>
<pre class=""lang-py prettyprint-override""><code>sj_train_features = pd.DataFrame({
    'year': [1990] * 10,
    'weekofyear': np.arange(18, 28),
    'week_start_date': pd.date_range('1990-04-30', periods=10, freq='w'),
    'ndvi_ne': np.random.random(10),
    'station_precip': np.random.random(10)*10,
}).set_index(['year', 'weekofyear'])

sj_train_labels = pd.Series(np.random.random(10)*20, index=sj_train_features.index, name='total_cases')
</code></pre>
",18587285.0,10495893.0,2022-04-22 14:41:50,2022-04-22 14:41:50,Model works perfectly but GridSearch causes error,<python><scikit-learn><data-science><pipeline><grid-search>,1,4,N/A,CC BY-SA 4.0
71835889,1,-1.0,2022-04-12 01:08:05,-1,196,"<p>I have a dataframe with an age column. It looks like this: <a href=""https://i.stack.imgur.com/ItFhB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ItFhB.png"" alt=""enter image description here"" /></a></p>
<p>However, some values are missing. For now, I replaced them with the most occurring values like this:</p>
<pre><code>df_processed = df_processed.apply(lambda x: x.fillna(x.value_counts().index[0]))
</code></pre>
<p>but I want to replace them with an unknown category. However, it is weird to me to put a text 'unknown' in a numeric categorical column. What should I replace the missing values to? I want  NUMERIC 'unknown' category. I heard thAT 0 is a bad idea for age.</p>
",17220769.0,1454176.0,2022-04-12 01:19:04,2022-04-12 03:25:09,Missing Values in Numeric Columns,<python><pandas><data-science>,2,0,N/A,CC BY-SA 4.0
71850187,1,-1.0,2022-04-12 23:30:23,-2,40,"<p>In the housing file, there is the median income column with values ranging from <code>0.499</code> to <code>15.001</code>. I need to a code to get the number of houses that have a median income value between 2 and 5.</p>
<p>Anyways, I just tried this. It works, but I think there are cleaner ways</p>
<pre><code>def mid_class(i):
    if 2&lt;i&lt;5:
        return True
    else:
        return False
sum(housing['median_income'].apply(lambda y: mid_class(y)))
</code></pre>
",17889362.0,2681662.0,2022-04-13 01:14:25,2022-04-13 01:32:42,getting the frequecy/count of a range of numbers in a python dataframe,<python><pandas><dataframe><group-by><data-science>,1,2,N/A,CC BY-SA 4.0
71855308,1,-1.0,2022-04-13 09:44:29,0,55,"<p>I had created a model for predicting type of website by looking at text.</p>
<p>But it is seems to be not working. I had stored the model, vectorizer, label encoder in the pickle file and loading here</p>
<p>code :</p>
<pre><code>import pandas as pd
import sklearn.metrics as sm
import nltk
import string
from nltk.tokenize import word_tokenize
import re
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
import pickle
import os

def clean_text(text):
    #### cleaning the text 
    ###1. Convert the text to lower case
    text= text.lower()

    ###2. tokenize the sentences to words
    text_list= word_tokenize(text)

    ###3. Removes the special charcters
    special_char_non_text= [re.sub(f'[{string.punctuation}]+','',i) for i in text_list]

    ###4.  remove stopwords
    non_stopwords_text= [i for i in special_char_non_text if i not in stopwords.words('english')]

    ###5. lemmatize the words
    lemmatizer= WordNetLemmatizer()
    lemmatized_words= [lemmatizer.lemmatize(i) for i in non_stopwords_text]

    cleaned_text= ' '.join(lemmatized_words)

    return cleaned_text

text_input= input('Please enter the text: ')
cleaned_text= clean_text(text_input)

temp_df= pd.DataFrame({'input_text':[cleaned_text.strip()]})
vectorizer_filepath= 'tf_idf_vectorizer.pkl'
tf_idf_vectorizer= pickle.load(open(vectorizer_filepath,'rb'))
temp_df_1= tf_idf_vectorizer.transform(temp_df)
input_df= pd.DataFrame(temp_df_1.toarray(),columns=tf_idf_vectorizer.get_feature_names())

### load the model

model_path='multinomial_clf.pkl'
model_clf= pickle.load(open(model_path,'rb'))

y_pred= model_clf.predict(input_df)

#print(y_pred)
### load the label encoder
label_encoder_file= 'label_encoder.pkl'
label_encoder= pickle.load(open(label_encoder_file,'rb'))

label_class= label_encoder.inverse_transform(y_pred.ravel())
print(f'{label_class} is the predicted class')

</code></pre>
<p>I am getting an error:</p>
<pre><code>KeyError                                  Traceback (most recent call last)
~\anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode_python(values, uniques, encode)
     65         try:
---&gt; 66             encoded = np.array([table[v] for v in values])
     67         except KeyError as e:

~\anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in &lt;listcomp&gt;(.0)
     65         try:
---&gt; 66             encoded = np.array([table[v] for v in values])
     67         except KeyError as e:

KeyError: 'website booking flight  bus ticket'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-21-b92cbf8dfe74&gt; in &lt;module&gt;
      5 vectorizer_filepath= 'tf_idf_vectorizer.pkl'
      6 tf_idf_vectorizer= pickle.load(open(vectorizer_filepath,'rb'))
----&gt; 7 temp_df_1= tf_idf_vectorizer.transform(temp_df)
      8 input_df= pd.DataFrame(temp_df_1.toarray(),columns=tf_idf_vectorizer.get_feature_names())
      9 

~\anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in transform(self, y)
    275             return np.array([])
    276 
--&gt; 277         _, y = _encode(y, uniques=self.classes_, encode=True)
    278         return y
    279 

~\anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode(values, uniques, encode, check_unknown)
    111     if values.dtype == object:
    112         try:
--&gt; 113             res = _encode_python(values, uniques, encode)
    114         except TypeError:
    115             types = sorted(t.__qualname__

~\anaconda3\lib\site-packages\sklearn\preprocessing\_label.py in _encode_python(values, uniques, encode)
     66             encoded = np.array([table[v] for v in values])
     67         except KeyError as e:
---&gt; 68             raise ValueError(&quot;y contains previously unseen labels: %s&quot;
     69                              % str(e))
     70         return uniques, encoded

ValueError: y contains previously unseen labels: 'website booking flight  bus ticket'

</code></pre>
<p>I had used the input text value as <strong>This is the website for booking flight,bus tickets</strong></p>
<p>I am not sure why it is happening like this</p>
<p>Could anyone help me to solve the issue?</p>
",13615535.0,4685471.0,2022-04-13 13:20:22,2022-04-13 13:20:22,Issue while deploying an model locally,<python><machine-learning><nlp><data-science><tfidfvectorizer>,1,0,N/A,CC BY-SA 4.0
71856782,1,-1.0,2022-04-13 11:32:14,1,1784,"<p>I have a dataset that I am analysing to find the optimal number of clusters using k-means.</p>
<p>I am testing the number of clusters from [1..11] - which produces the following plot:</p>
<p><a href=""https://i.stack.imgur.com/MqHvx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MqHvx.png"" alt=""enter image description here"" /></a></p>
<p>The original dataset has six classes but the elbow plot shows the bend really occurring at 3 clusters. For curiosity I overlaid a line on the plot from 11 clusters and back and it is almost a straight line to 6 clusters - which indicates to me that the real elbow is at 6, but it is subtle to see.</p>
<p>So, visually 3 looks to be the right answer, but given the known number of classes (6) the straight line I drew indicates 6...</p>
<p>Question:</p>
<ul>
<li>How should you correctly interpret an elbow plot like this
(especially when you are not given the classes)?</li>
<li>Would you say the
elbow is at 3 or 6?</li>
</ul>
",5485232.0,5485232.0,2022-04-13 11:39:54,2022-04-17 13:06:24,Elbow Method for optimal no. of clusters,<data-science><k-means>,1,0,N/A,CC BY-SA 4.0
74745386,1,74745494.0,2022-12-09 15:52:32,1,42,"<p>Hi everyone? i white a function to replace NaN values in DataFrame (1750 000 lines):</p>
<pre><code>def Android_iOs_device_os_cange(df):
    def find_Android_brand(df):
        list_for_android = list(df[df['device_os'] == 'Android'].device_brand.unique())
        list_for_android.remove('(not set)')
        return list_for_android
    def find_iOS_brand(df):
        list_for_iOS = list(df[df['device_os'] == 'iOS'].device_brand.unique())
        list_for_iOS.remove('(not set)')
        return list_for_iOS
    for i in list(df[df.device_os.isnull() &amp; df.device_brand.notnull()].index):
        if df.device_brand[i] in find_Android_brand(df) and pd.isnull(df.loc[i, 'device_os']) == True:
            df['device_os'][i] = df.loc[i, 'device_os'] = 'Android'
        elif df.device_brand[i] in find_iOS_brand(df) and pd.isnull(df.loc[i, 'device_os']) == True:
            df['device_os'][i] = df.loc[i, 'device_os'] = 'iOS'
        else:
            df['device_os'][i] = df.loc[i, 'device_os'] = '(not set)'
    return df
</code></pre>
<p>It fulfills its purpose, but but he replaced only 20,000 lines in 3.5 hours. I understand that the catch here is the for loop, but I don't understand how to make the function better. Who can advise anything?</p>
<p>I try to make it with function loc, but for my it always ended with</p>
<pre><code>'Series' object has no attribute 'device_os'
</code></pre>
",19632927.0,-1.0,N/A,2022-12-09 16:39:23,pd.fillna or pd.DataFrame.loc for NaN values by function with condition,<python><pandas><function><loops><data-science>,1,2,N/A,CC BY-SA 4.0
74747082,1,74747212.0,2022-12-09 18:37:14,1,68,"<p>I read data from a csv file and then calculate the amount of bins according to sturges rule. Then I make a histogram using matplotlib, but I don't get what I expect.</p>
<pre><code>import matplotlib.pyplot as plot

height = [167, 170, 173, 173, 173, 174, 175, 178, 180, 180, 182, 182, 184, 185, 187, 188, 189, 190, 192, 193, 195, 197, 199, 202]
plot.hist(height, bins=5)
plot.xlabel(&quot;Sizes&quot;)
plot.ylabel(&quot;Count&quot;)
plot.show()

</code></pre>
<p>Which gets me the following output:</p>
<p><a href=""https://i.stack.imgur.com/gRzJN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gRzJN.png"" alt=""Resulting hist plot"" /></a></p>
<p>But I would expect the counts in the different ranges to be:</p>
<pre><code>167 - 174.0: 6
174.0 - 181.0: 4
181.0 - 188.0: 6
188.0 - 195.0: 5
195.0 - 202.0: 3
</code></pre>
<p>What can I do to fix the plot or am I thinking about this wrong?</p>
",20102671.0,-1.0,N/A,2022-12-09 19:23:43,Matplotlib hist doesn't calculate the correct heights of bars,<python><matplotlib><data-science>,1,3,N/A,CC BY-SA 4.0
71837774,1,-1.0,2022-04-12 06:01:14,0,393,"<p>I am currently working with a data set of minecraft usernames:</p>
<pre><code>0            ivixxw_18         4228398
1              slayne_         4228398
2            _megasik_         4228398
3               player         4228398
4            koteevski         4228398
...                ...             ...
4228393     _______ban         4228398
4228394  ________1_2_3         4228398
4228395  ________i___i         4228398
4228396      ________s         4228398
4228397     _________m         4228398
</code></pre>
<p>I want to create a column with the length of each user name without using a for loop. Is this possible? Right now I have</p>
<pre><code>un['Length of Name'] = len(un)
</code></pre>
<p>This returns the length of the table (which makes sense to me, I just don't know how to fix it), not a specific row. How can I grab just one row?</p>
",16450485.0,18329678.0,2022-04-12 13:46:51,2022-04-12 13:46:51,How to find the length of a certain object in a data frame?,<python><pandas><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
71854467,1,71854935.0,2022-04-13 08:42:04,0,96,"<p>I am filtering in ElasticSearch. I want doc_count to return 0 on non-data dates, but it doesn't print those dates at all, only dates with data are returned to me. do you know how i can do it? Here is the Python output:</p>
<pre><code>0                                                     NaN
1                                                     NaN
2                                                     NaN
3                                                     NaN
4                                                     NaN
                               ...                       
33479    {'date': '2022-04-13T08:08:00.000Z', 'value': 7}
33480    {'date': '2022-04-13T08:08:00.000Z', 'value': 7}
33481    {'date': '2022-04-13T08:08:00.000Z', 'value': 7}
33482    {'date': '2022-04-13T08:08:00.000Z', 'value': 7}
33483    {'date': '2022-04-13T08:08:00.000Z', 'value': 7}
</code></pre>
<p>And here is my ElasticSearch filter:</p>
<pre><code>&quot;from&quot;: 0,
    &quot;size&quot;: 0,
    &quot;query&quot;: {
        &quot;bool&quot;: {
            &quot;must&quot;:
                [
                    {
                        &quot;range&quot;: {
                            &quot;@timestamp&quot;: {
                                &quot;gte&quot;: &quot;now-1M&quot;,
                                &quot;lt&quot;: &quot;now&quot;
                            }
                        }
                    }
                ]
        }
    },
    &quot;aggs&quot;: {
        &quot;continent&quot;: {
            &quot;terms&quot;: {
                &quot;field&quot;: &quot;source.geo.continent_name.keyword&quot;
            },
            &quot;aggs&quot;: {
                &quot;_source&quot;: {
                    &quot;date_histogram&quot;: {
                        &quot;field&quot;: &quot;@timestamp&quot;, &quot;interval&quot;: &quot;8m&quot;
                    }}}}}}
</code></pre>
",18353852.0,-1.0,N/A,2022-04-13 09:17:06,Returning data that is not in ElasticSearch as 0 in doc_count,<python><elasticsearch><data-science><bigdata>,1,0,N/A,CC BY-SA 4.0
71799347,1,71803578.0,2022-04-08 15:09:58,0,260,"<p>i would like to create a correlation function between a column and the others, passing the dataframe with all columns, corelating wiht a specif colum and returning a list of metrics and correlation i`am doing this like this.</p>
<pre><code>correlations = df.corr().unstack().sort_values(ascending=True) 
correlations = pd.DataFrame(correlations).reset_index() 
correlations.columns = ['corr_matrix', 'dfbase', 'correlation'] 
correlations.query(&quot;corr_matrix == 'venda por m2' &amp; dfbase != 'venda por m2'&quot;) 
</code></pre>
<p><a href=""https://i.stack.imgur.com/skWHv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/skWHv.png"" alt=""enter image description here"" /></a></p>
<p>but i would like to know a way to make this with a function.</p>
",17338791.0,17338791.0,2022-04-08 16:28:38,2022-04-08 22:04:54,how to make a Correlation One Column to Many Columns and return a list?,<python><pandas><dataframe><data-science><pearson-correlation>,1,0,N/A,CC BY-SA 4.0
71828461,1,-1.0,2022-04-11 13:10:08,0,489,"<p>I am trying to add a vertical line to a Pandas.dataframe.plot(subplot), but am struggling big time. Current Code:</p>
<pre><code>df_final = df_final / df_final.max(axis=0)
df_final.loc[30102040].plot(subplots=True, layout=(17, 1), figsize=(25, 25), sharex=True, sharey=True)
plt.legend(loc='center left')
plt.axvline(sepsis_onset, color='red', linestyle='dashed')

plt.show()
</code></pre>
<p>what it currently looks like:
<img src=""https://i.stack.imgur.com/3j2Iy.png"" alt=""Current Output"" /></p>
<p>Neither the legend, nor the axvline is currently displayed correctly. What am I missing?</p>
",18772178.0,5012099.0,2022-04-11 13:10:41,2022-04-11 13:30:46,Adding a vertical line to Pandas subplot,<python><pandas><matplotlib><plot><data-science>,1,1,N/A,CC BY-SA 4.0
71843174,1,-1.0,2022-04-12 12:57:28,0,628,"<p>I am working on a project which uses Selenium however, Spyder simply refuses to import or even install it. I have read a few answers to similar problems and have tried:</p>
<pre><code>pip install selenium

pip3 install selenium

sudo pip install selenium

sudo pip3 install selenium
</code></pre>
<p>as is usually suggested.</p>
<p>Each time the console says the syntax is invalid. Whenever I try to import selenium it says Module not found. Why?</p>
",18111004.0,7429447.0,2022-04-12 22:00:06,2022-04-12 22:00:43,How to install and import selenium into spyder,<python><selenium><data-science><spyder>,1,1,N/A,CC BY-SA 4.0
71851592,1,-1.0,2022-04-13 03:41:17,1,68,"<p>I am trying the following. I am applying a detect and translate function to a column containing free text variable which are the professional role of some customers:</p>
<pre><code>from langdetect import detect
from google_trans_new import google_translator  

#simple function to detect and translate text 
def detect_and_translate(text,target_lang='en'):

    result_lang = detect(text)

    if result_lang == target_lang:
       return text 

    else:
       translator = google_translator()
       translate_text = translator.translate(text,lang_src=result_lang,lang_tgt=target_lang)
       return translate_text 
df_processed['Position_Employed'] = df_processed['Position_Employed'].replace({'0':'unknown', 0:'unknown'})
df_processed['Position_Employed'] = df_processed['Position_Employed'].apply(detect_and_translate)
</code></pre>
<p>But I am getting the following error;</p>
<p>JSONDecodeError: Extra data: line 1 column 433 (char 432)</p>
<p>I have tried to update the solution from this link but it did not work to editing line 151 in google_trans_new/google_trans_new.py which is: response = (decoded_line + ']') to response = decoded_line</p>
<p><a href=""https://stackoverflow.com/questions/68214591/python-google-trans-new-translate-raises-error-jsondecodeerror-extra-data"">Python google-trans-new translate raises error: JSONDecodeError: Extra data:</a></p>
<p>What can I do?</p>
",18789196.0,-1.0,N/A,2022-04-13 03:41:17,Applying google_translator to a column of a pandas dataframe,<python><data-science><google-translate>,0,0,N/A,CC BY-SA 4.0
71864019,1,-1.0,2022-04-13 21:29:20,4,638,"<p>I am using a dataset of company names with that may contains not identical duplicates.</p>
<p>The list may contains : company A but also c.o.m.p.a.n.y A or comp A</p>
<p>Is there any python script using NLP for example that can find similar names from a dataset.</p>
<p>Thanks in advance</p>
",18797635.0,-1.0,N/A,2022-04-13 22:59:27,Python programming finding similar names from a list of names,<python><database><nlp><data-science>,1,4,N/A,CC BY-SA 4.0
71862078,1,-1.0,2022-04-13 18:09:04,0,170,"<p>I have a scenario where I am provided a list of clusters and pairwise distance only between items in same cluster. I need to rank these clusters based on some kind of relative score from this info.</p>
<p>e.g. a if [A,B,C] is a given cluster, then I have d(A,B), d(B,C) and d(C,A), where d is their distances. However, I do not have distance of A or B or C with points belonging to other clusters.</p>
<p>I was thinking of ranking the clusters based on ascending order of median value of inside-cluster pairwise-distances after computing this median value for all clusters. Please let me know if there is any other ideas or major drawback in the current approach. In my scenario it is fine if the inter clusters distances are not optimized because the goal is to rank given clusters with some kind of relative confidence score</p>
<p>NOTE: I also have an information that if d(X,Y) for any point X and Y is &lt; 5, then they are meant to be in the same cluster, else it is an incorrect assignment. It is also a possibility that X,Y,Z share the same cluster because d(X,Y) &lt;5 and d(Y,Z) &lt; 5 but d(Z,X) may NOT be &lt;5, they share the same cluster because of linkage between (X,Y) and (Y,Z).</p>
",11463249.0,-1.0,N/A,2022-04-13 18:09:04,Cluster confidence scoring,<python><data-science><cluster-analysis><ranking><scoring>,0,2,N/A,CC BY-SA 4.0
71873793,1,71873978.0,2022-04-14 15:08:00,0,389,"<p>I have a pandas dataframe as shown below:</p>
<p><a href=""https://i.stack.imgur.com/0MCT3.png"" rel=""nofollow noreferrer"">Pandas Dataframe</a></p>
<p>I want to drop the rows that has only one non zero value. What's the most efficient way to do this?</p>
",17927850.0,-1.0,N/A,2022-04-14 15:31:50,dropping rows that has only one non zero value from a pandas dataframe in python,<python><pandas><dataframe><data-science>,3,0,N/A,CC BY-SA 4.0
71873810,1,-1.0,2022-04-14 15:08:59,0,40,"<p>I want to use the column 'Qual' in Table 1 and categorise them according to the 'Level' in Table 2 in a separate worksheet, further displaying a separate column 'Qual level' of those levels in table 1</p>
<p>Table1:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Qual</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>a</td>
</tr>
<tr>
<td>3454</td>
<td>b</td>
</tr>
<tr>
<td>2767</td>
<td>c</td>
</tr>
<tr>
<td>2098</td>
<td>a</td>
</tr>
</tbody>
</table>
</div>
<p>Table2:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Qual</th>
<th>level</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>AA</td>
</tr>
<tr>
<td>b</td>
<td>BB</td>
</tr>
<tr>
<td>c</td>
<td>CC</td>
</tr>
</tbody>
</table>
</div>
<p>Final output required:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Qual</th>
<th>Qual Level</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>a</td>
<td>AA</td>
</tr>
<tr>
<td>3454</td>
<td>b</td>
<td>BB</td>
</tr>
<tr>
<td>2767</td>
<td>c</td>
<td>CC</td>
</tr>
<tr>
<td>2098</td>
<td>a</td>
<td>AA</td>
</tr>
</tbody>
</table>
</div>
<p>this is what I have tried so far:</p>
<pre><code>df.groupby(['Qual','Level'])['Qual Level']
</code></pre>
<p>its been giving me an error and im not sure if this is the right method.</p>
",5084594.0,5084594.0,2022-04-14 15:22:08,2022-04-14 15:34:23,how to categorise data depending upon a category column in another worksheet and displaying them in a new column,<python><pandas><dataframe><jupyter-notebook><data-science>,2,0,2022-04-14 19:00:23,CC BY-SA 4.0
71887519,1,-1.0,2022-04-15 18:17:20,1,260,"<p>If I have 2 lists of numbers (shown below), how could I find the KL Divergence? Do I first have to find the probability distribution in them (if so, how could one do that)?</p>
<p>I've tried putting the data through a kernel density function but it has not worked</p>
<pre class=""lang-py prettyprint-override""><code>data = [18, 16, 46, 4, 10, 7, 14, 51, 7, 4, 49, 9, 7, 7]
data = np.reshape(data, (-1, 1)) # Reshape data for KernelDensity() function
data2 = [0, 17, 0, 20, 77, 23, 7, 8, 8, 19, 0, 48, 19, 7, 4, 7, 16]
data2 = np.reshape(data2, (-1, 1)) # Reshape data for KernelDensity() function

from sklearn.neighbors import KernelDensity
kd = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(data)
kd2 = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(data2)

from scipy.special import kl_div
kl_div(kd, kd2)
</code></pre>
<p>When I run the code, I receive the following error</p>
<blockquote>
<p>TypeError: ufunc 'kl_div' not supported for the input types, and the
inputs could not be safely coerced to any supported types according to
the casting rule ''safe''</p>
</blockquote>
<p>I've been trying to figure this out for a few hours. Thanks in advance.</p>
",16940961.0,16940961.0,2022-04-15 18:35:47,2022-04-15 18:35:47,Finding KL Divergence from set of numbers,<python><statistics><data-science><probability-distribution>,0,4,N/A,CC BY-SA 4.0
71872238,1,71872256.0,2022-04-14 13:11:59,1,27,"<p>I want to select the year '2019/0'(string) from the column 'year of entry' and only multiply their 'grades' times 2 which is in another column</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>year of entry</th>
<th>Grades</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019/0</td>
<td>14</td>
</tr>
<tr>
<td>2010/0</td>
<td>21</td>
</tr>
<tr>
<td>2019/0</td>
<td>15</td>
</tr>
</tbody>
</table>
</div>
<p>this is what I have tried so far:</p>
<pre><code>df.loc[df(&quot;Year of Entry&quot;),'2018/9'] = df(&quot;Grades&quot;)*2
</code></pre>
<p>its been giving me an error and im not sure if this is the right method.</p>
",5084594.0,-1.0,N/A,2022-04-14 13:14:08,How to change the data of a particular column and multiply them depending upon the specific values in another column using pandas?,<python><pandas><jupyter-notebook><data-science>,1,0,N/A,CC BY-SA 4.0
71885212,1,-1.0,2022-04-15 14:21:23,1,282,"<h3>Introduction and background</h3>
<p>Hi! I hope you are all fine... My name is Bryan, I recentely graduated in Biochemical Engineering and I'm very interested in Data Science to apply it to my field. I had a course in undergrad on algorithms, and it made me passionate about programming. However, I didn't have time to try to deepen it. When the pandemic started, I decided to learn Python and took a Stanford course that they were offering for free. I learned a lot about the language, but when college classes started again, I was forced to put programming aside. About 15 days ago I decided to learn R. I thought there wouldn't be so many differences to Python, but I was surprised at how different they are...
To learn a little more about R for Data Science, I created a little project involving a subject I am passionate about (soccer). I know I skipped some steps in learning the language and, for this reason, I am already thinking about taking some classes specifically for R, about the syntax of the language, how it works, etc. (suggestions for courses and materials will be welcome too).</p>
<h3>Idea</h3>
<p>My idea is to extract some data from the Brazilian Championship (Serie A) in the era of points scored (2003-2021) on the <a href=""https://besoccer.com"" rel=""nofollow noreferrer"">besoccer</a> site.</p>
<h3>Tools used</h3>
<ul>
<li>R language</li>
<li>Rstudio</li>
<li>robotstxt&quot;, &quot;rvest&quot;, &quot;dplyr&quot;, &quot;writexl&quot; libraries</li>
<li>Extension &quot;SelectorGadget&quot; for Google Chrome</li>
</ul>
<h3>Code</h3>
<pre class=""lang-r prettyprint-override""><code># Importing libraries
library(&quot;robotstxt&quot;)
library(&quot;rvest&quot;)
library(&quot;dplyr&quot;)
library(&quot;writexl&quot;)

# Verifying if besoccer accepts automated extraction
links &lt;- c(&quot;https://www.besoccer.com/&quot;, &quot;https://www.besoccer.com/competition/scores/serie_a_brazil/NNN&quot;)

paths_allowed(links)

# Pages and HTML extraction
years &lt;- 2003:2021
br_links &lt;- paste(&quot;https://www.besoccer.com/competition/scores/serie_a_brazil/&quot;,
                  years, sep = &quot;&quot;)

htmls &lt;- br_links %&gt;%
  lapply(read_html)

# Getting informations (sample)  
for (html in htmls) {
  matches &lt;- htmls %&gt;%
    html_nodes(&quot;#mod_competition_season .item-col:nth-child(1) .main-line&quot;) %&gt;%
    html_text()
  total_matches &lt;- as.numeric(matches)
}
</code></pre>
<h3>Explaining the code</h3>
<ol>
<li>I used the &quot;robotstxt&quot; library to check if the site accepts data extraction. I took a look at the HTML of the page and verified that the &quot;NNN&quot; is replaced by the year of the competition. So, I concluded that, if it passed the test, there would be no problem extracting data from the pages of the championships from 2003 to 2021.</li>
<li>As I said before, I noticed that the championship url was always the same, only changing the year at the end. To facilitate the access to the pages, I created a vector with the years (2003 through 2021) and created an object to store the links of the 19 competitions that I obtained using the &quot;paste&quot; function, in which I used the &quot;prefix&quot; of the page and the vector with the years. The result is a character object with 19 entries (one for each championship year).</li>
<li>I used the function &quot;read_html&quot; from the package &quot;rvest&quot; to get the HTML of the pages. Since I had an array of character type, I chose to use the function &quot;lapply&quot; to iterate over the array and extract the HTML. The result is a list with the HTML of the 19 pages of the competition.</li>
<li>Finally, I bring an example of information that I want to extract (number of championship matches). For this, I used the &quot;html_nodes&quot; function from the &quot;rvest&quot; package to point to which CSS selector I want. I used the Chrome extension to get the exact selector. Then, I used the &quot;html_text&quot; function from the &quot;rvest&quot; package to transform the information into text, and finally convert it to numeric information for later compute (since you don't do calculation with strings/texts). I used a repetition loop to iterate through all the pages.</li>
</ol>
<h3>The Problem</h3>
<p>After performing what I explained in step 4 above, I got the following error:</p>
<blockquote>
<p><strong>Error in UseMethod(&quot;xml_find_all&quot;) : no applicable method for 'xml_find_all' applied to an object of class &quot;list&quot;</strong>.</p>
</blockquote>
<p>My interpretation of the error is that the method of the function is not applicable for a list, and then my head bugged, because I tried to undo the list by applying the indexes in the loop and was not successful. I believe there is some problem with my logic in the problem, but unfortunately I am not able to find the error myself.<br />
<strong>Side note:</strong> I tested what was inside the for loop on just one page (2003 Championship) to see if what I wrote in the &quot;matches&quot; object would run if it was on just one HTML (without for loop), and not on a list of HTML's... and the answer is that it did!</p>
<h3>Questions</h3>
<p>My questions are: how can I extract the same information from all 19 pages, since the selector is the same on all pages? What is wrong with my loop? If someone can point me to the error, the solution and explain it to me, I'd really appreciate it! See you later! o/</p>
",-1.0,-1.0,N/A,2022-04-16 03:43:53,Web-scraping with R: how to perform the extraction of the same information from a list of links,<r><list><web-scraping><data-science><rvest>,1,2,N/A,CC BY-SA 4.0
71885888,1,71887330.0,2022-04-15 15:25:37,0,143,"<p>There is a table of the following type:</p>
<p>id      |   title.  |   parent_id</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">id</th>
<th style=""text-align: center;"">title.</th>
<th style=""text-align: right;"">parent_id</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">245</td>
<td style=""text-align: center;"">Fruits</td>
<td style=""text-align: right;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: center;"">All</td>
<td style=""text-align: right;"">0</td>
</tr>
</tbody>
</table>
</div>
<p>and there are several thousand lines in it, according to a hierarchical scheme, i.e. all by type of tree and these are categories in the store from the most general to the narrowest.</p>
<p>I need a few more columns for each id, with the name of the parent category up to the most general category. That is, so that there are more columns in the row: Depth 6: id_title ; Depth 5 parent_id title; Depth 4 grandparent_id title and so on the most general.</p>
<p>Or, instead of such a number of columns, I need a code that will make a path to each id</p>
<p>Let's say the category <code>2642</code> has the heading &quot;Small tools&quot;, and the path in the category tree - <code>10016-&gt;10072-&gt;10690-&gt;2642</code>. If we replace the category IDs in this path with headers, we get the following tree:</p>
<ul>
<li>Construction and repair</li>
<li>Hand tools and accessories</li>
<li>Carpentry and locksmith tools</li>
<li>Small tools</li>
</ul>
<p>I don't know how to do it...</p>
",18811535.0,-1.0,N/A,2022-04-15 17:55:50,Converting flat data into a tree,<python><machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
72827563,1,-1.0,2022-07-01 09:59:24,0,60,"<p>i have a csv table that looks like this:</p>
<p><a href=""https://i.stack.imgur.com/e0hJ0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e0hJ0.png"" alt=""enter image description here"" /></a></p>
<p>Table data:</p>
<pre><code>rows = [['0', 'False', 'False', '0'], 
['1', 'False', 'False', '0'], 
['2', 'True', 'False', '80'], 
['3', 'False', 'False', '80'], 
['4', 'False', 'False', '80'], 
['5', 'False', 'False', '80']]

fields= ['id', 'i1', 'i2', 'm1']

</code></pre>
<p>The table has over 1000 rows. There are obviously dependencies in the table. For example, m1 has the value 80 as soon as i1 becomes true.</p>
<p>What are the possibilities in Python to identify these dependencies? I have already tried the corr function - unfortunately I get a correlation of 0.0025 for m1 and i1</p>
<pre><code>dat = pd.read_csv(data.csv)
dat.head(5)
corr = dat.corr()
plt.figure(figsize=(12,8))
sns.heatmpa(corr, cmap=&quot;Greens&quot;, annot=True)
</code></pre>
<p>The correlation between i1 and m1 0.0025.</p>
",10110081.0,10110081.0,2022-07-01 10:12:26,2022-07-01 10:12:26,Python - Relation between variables,<python><dataframe><csv><data-science><correlation>,0,3,N/A,CC BY-SA 4.0
71819435,1,-1.0,2022-04-10 18:27:33,0,285,"<p>I'm trying to detect outliers using a no code platform called Weka. I've installed the packages maintained by Weka within it's package manager. However, I've imported the KD-CUP 1999 dataset into Weka and I'm trying to use the classifer option (The Classify tab is for training and evaluating the performance of different machine learning algorithms) The issue is that the Local Outlier Factor algorithm is not activated  or simply isn't allowing me to use it depsite me installing via Weka package manager, an image is provided for better insight: <a href=""https://i.stack.imgur.com/9FLTr.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/9FLTr.png</a></p>
",14580096.0,14580096.0,2022-04-11 15:33:47,2022-04-11 15:33:47,Weka - Local Outlier Factor not activated in the classifer option,<algorithm><machine-learning><data-science><weka><unsupervised-learning>,1,0,N/A,CC BY-SA 4.0
71834253,1,71834384.0,2022-04-11 20:59:26,0,80,"<p>I did go through multiple StackOverflow posts to get an idea of how to solve this but couldn't come up with anything.</p>
<p>So, I have a dataframe with three attributes: id, X1, Y1.
I need to pass each instance/entry of the dataframe to a function(e.g., func) which returns two values: X2, Y2. The operation basically looks like this:</p>
<pre><code>X2, Y2 = func(X1, Y1)
</code></pre>
<p>I need to save the X2, Y2 for each entry as a new column so that the new dataframe looks like: id, X1, Y1, X2, Y2</p>
<p>I am not sure how to perform this with pandas. Could you please give me some pointers?</p>
<p>Thanks a lot for your effort and time!</p>
",15374305.0,-1.0,N/A,2022-04-11 21:15:02,Use multiple columns of a dataframe for an operation and save the result in multiple columns,<python><pandas><dataframe><data-science>,2,2,N/A,CC BY-SA 4.0
71847351,1,71847544.0,2022-04-12 18:11:00,1,931,"<p>I am trying to figure out how to or if there is a way to use <code>dagit</code> command on multiple python job files.</p>
<p>Example: <code>dagit -f hello_cereal_job.py</code></p>
<p>With in the directory I have multiple <code>*_job.py</code> files and when I launch the dagster ui locally with <code>dagit</code> I'd like to have all my jobs visible in the UI.</p>
<p>With this command it only shows me the one job that I have in the file, but I'd like to have all jobs from all files in my directory show up.</p>
",3738936.0,-1.0,N/A,2022-04-12 18:33:19,Load multiple python files in to dagster via dagit locally,<python><data-science><etl><dagster>,1,0,N/A,CC BY-SA 4.0
71847542,1,-1.0,2022-04-12 18:33:06,-1,141,"<p>I'm trying to code a spider that takes name, gdp and countries' birth and death values but I can't find where is the error or a solution for an error while processing the starting url, don't know if it's a problem with xpath or other thing, most of the threads that I found about talks about wrong indentation, but this is not my case.</p>
<p>Code:</p>
<pre><code>import scrapy
import logging
</code></pre>
<pre><code>class GdpsSpider(scrapy.Spider):
</code></pre>
<pre><code>name = 'gdps'
allowed_domains = ['www.worldpopulationreview.com']
start_urls = ['https://worldpopulationreview.com/countries/countries-by-national-debt/']

def parse(self, response):
    countries = response.xpath('//tr')
    for country in countries:
        name = country.xpath(&quot;.//td[1]/a/text()&quot;).get()
        link = country.xpath(&quot;.//td[1]/a/@href&quot;).get()
        gdp = country.xpath(&quot;.//td[2]/text()&quot;).get()

        # yield{
        #     'name': name,
        #     'gdp': gdp,
        #     'link': link
        # }

        yield response.follow(url=link, callback=self.parse_bd, meta={'name':name, 'gdp':gdp})

def parse_bd(self, response):
    name = response.request.meta['name']
    gdp = response.request.meta['gdp']

    # rows = response.xpath('//*[@id=&quot;popClock&quot;]/div/div[1]/div/div/table/tbody')

    rows = response.xpath(&quot;(//table[@class='table table-striped'])[1]/tbody&quot;)
    for row in rows:
        births = row.xpath(&quot;.//tr[3]/td[@class='number']/text()&quot;).get()
        deaths = row.xpath(&quot;.//tr[4]/td[@class='number']/text()&quot;).get()

        yield{
            'name': name,
            'gdp': gdp,
            'births': births,
            'deaths': deaths
        }
</code></pre>
<p>Error:</p>
<pre><code>(virtual_workspace) C:\Users\Bugra\projects\gdp&gt;scrapy crawl gdps
2022-04-12 21:43:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: gdp)
2022-04-12 21:43:57 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 36.0.2, Platform Windows-10-10.0.22000-SP0
2022-04-12 21:43:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'gdp',
 'NEWSPIDER_MODULE': 'gdp.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['gdp.spiders']}
2022-04-12 21:43:57 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2022-04-12 21:43:57 [scrapy.extensions.telnet] INFO: Telnet Password: 10f8998db04ef71d
2022-04-12 21:43:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2022-04-12 21:43:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2022-04-12 21:43:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2022-04-12 21:43:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2022-04-12 21:43:57 [scrapy.core.engine] INFO: Spider opened
2022-04-12 21:43:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2022-04-12 21:43:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2022-04-12 21:43:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (307) to &lt;GET https://worldpopulationreview.com/robots.txt&gt; from &lt;GET https://www.worldpopulationreview.com/robots.txt&gt;
2022-04-12 21:43:58 [py.warnings] WARNING: C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\core\engine.py:276: ScrapyDeprecationWarning: Passing a 'spider' argument to ExecutionEngine.download is deprecated
  return self.download(result, spider) if isinstance(result, Request) else result

2022-04-12 21:43:58 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET https://worldpopulationreview.com/robots.txt&gt; (referer: None)
2022-04-12 21:43:58 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2022-04-12 21:43:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (307) to &lt;GET https://worldpopulationreview.com/countries/countries-by-national-debt/&gt; from &lt;GET https://www.worldpopulationreview.com/countries/countries-by-national-debt/&gt;
2022-04-12 21:43:58 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET https://worldpopulationreview.com/robots.txt&gt; (referer: None)
2022-04-12 21:43:58 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2022-04-12 21:43:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (308) to &lt;GET https://worldpopulationreview.com/countries/countries-by-national-debt&gt; from &lt;GET https://worldpopulationreview.com/countries/countries-by-national-debt/&gt;
2022-04-12 21:43:58 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://worldpopulationreview.com/countries/countries-by-national-debt&gt; (referer: None)
2022-04-12 21:43:58 [scrapy.core.scraper] ERROR: Spider error processing &lt;GET https://worldpopulationreview.com/countries/countries-by-national-debt&gt; (referer: None)
Traceback (most recent call last):
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\utils\defer.py&quot;, line 132, in iter_errback
    yield next(it)
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\utils\python.py&quot;, line 354, in __next__
    return next(self.data)
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\utils\python.py&quot;, line 354, in __next__
    return next(self.data)
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\core\spidermw.py&quot;, line 66, in _evaluate_iterable
    for r in iterable:
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\spidermiddlewares\offsite.py&quot;, line 29, in process_spider_output
    for x in result:
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\core\spidermw.py&quot;, line 66, in _evaluate_iterable
    for r in iterable:
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\spidermiddlewares\referer.py&quot;, line 342, in &lt;genexpr&gt;
    return (_set_referer(r) for r in result or ())
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\core\spidermw.py&quot;, line 66, in _evaluate_iterable
    for r in iterable:
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\spidermiddlewares\urllength.py&quot;, line 40, in &lt;genexpr&gt;
    return (r for r in result or () if _filter(r))
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\core\spidermw.py&quot;, line 66, in _evaluate_iterable
    for r in iterable:
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\spidermiddlewares\depth.py&quot;, line 58, in &lt;genexpr&gt;
    return (r for r in result or () if _filter(r))
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\core\spidermw.py&quot;, line 66, in _evaluate_iterable
    for r in iterable:
  File &quot;C:\Users\Bugra\projects\gdp\gdp\spiders\gdps.py&quot;, line 22, in parse
    yield response.follow(url=link, callback=self.parse_bd, meta={'name':name, 'gdp':gdp})
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\http\response\text.py&quot;, line 158, in follow
    return super().follow(
  File &quot;C:\Users\Bugra\anaconda3\envs\virtual_workspace\lib\site-packages\scrapy\http\response\__init__.py&quot;, line 163, in follow
    raise ValueError(&quot;url can't be None&quot;)
ValueError: url can't be None
2022-04-12 21:43:58 [scrapy.core.engine] INFO: Closing spider (finished)
2022-04-12 21:43:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1522,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 17956,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/307': 2,
 'downloader/response_status_count/308': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 1.021947,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 4, 12, 18, 43, 58, 619959),
 'httpcompression/response_bytes': 56201,
 'httpcompression/response_count': 3,
 'log_count/DEBUG': 9,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2022, 4, 12, 18, 43, 57, 598012)}
2022-04-12 21:43:58 [scrapy.core.engine] INFO: Spider closed (finished)
</code></pre>
",16909672.0,16909672.0,2022-04-12 18:44:27,2022-04-13 09:29:53,"scrapy ValueError(""url can't be None"")",<python><web-scraping><scrapy><web-crawler><data-science>,1,0,N/A,CC BY-SA 4.0
72827876,1,-1.0,2022-07-01 10:26:35,1,360,"<pre><code>from flair.models import TextClassifier
from flair.data import Sentence
</code></pre>
<p>I am receiving error:</p>
<blockquote>
<p>cannot import name '_BaseLazyModule' from 'transformers.file_utils'</p>
</blockquote>
<p>Can anyone help me?</p>
",18115988.0,2760692.0,2022-07-01 16:34:37,2022-07-01 16:34:37,python flair module is giving error cannot import name '_BaseLazyModule' from 'transformers.file_utils',<python><data-science><flair>,0,0,N/A,CC BY-SA 4.0
72831277,1,72831722.0,2022-07-01 15:09:22,0,1245,"<p>I am currently learning segmentation and was using this project <a href=""https://github.com/jalajthanaki/Customer_segmentation"" rel=""nofollow noreferrer"">https://github.com/jalajthanaki/Customer_segmentation</a> I found to learn but I reached the 99th cell and got a:
<a href=""https://i.stack.imgur.com/DM3uV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DM3uV.png"" alt=""enter image description here"" /></a></p>
<p>Here is the cell that is causing the problem:</p>
<pre><code># sum of purchases / user &amp; order
temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False) 
['TotalPrice'].sum()
basket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})

# percentage of the price of the order / product category
for i in range(5):
col = 'categ_{}'.format(i) 
temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)[col].sum()
basket_price.loc[:, col] = temp 

# date of the order

df_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')
temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False) 
['InvoiceDate_int'].mean()
df_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)
basket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])

# selection of significant entries:
basket_price = basket_price[basket_price['Basket Price'] &gt; 0]
basket_price.sort_values('CustomerID', ascending = True)[:5]
</code></pre>
<p>The reason this confuses me is I am not sure what array is causing this problem and google has not helped me. I am absolutely new to this so any and all help is appreciated.</p>
",14366304.0,-1.0,N/A,2022-07-01 15:54:45,"Value error where a 1D array was expected but got an array with shape (18632, 3)",<python><pandas><data-science>,1,1,N/A,CC BY-SA 4.0
71869607,1,71873054.0,2022-04-14 09:46:25,1,1189,"<p>all code:</p>
<pre><code>def rgb2hex(r,g,b):
    return '#{:02x}{:02x}{:02x}'.format(r,g,b)

def rg(num):
    num = int(np.round((num / 100) * 124))
    r = (124 - num)
    g = (124 + num)
    b = (0)
    x = rgb2hex(r,g,b)
    return x

def colourmap(value):
    if math.isnan(value) == False:
        y = rg(value)
    else:
        y = '#808080'
    return y

m = homes.explore(
     column=&quot;Percentage&quot;,
     cmap=lambda value: colourmap(value),#map to custom colour scheme
     marker_kwds=dict(radius=15, fill=True), # make marker radius 15px with fill
     tooltip=labels, # show labels in the tooltip
     legend=False, 
     name=&quot;Homes&quot; # name of the layer in the map
)
m #plot
</code></pre>
<p>Hi, I'm relatively new to stackoverflow and using geopandas so any comments are appreciated.</p>
<p>I'm trying to create a custom colour scheme that handles NaN values.</p>
<p>Green being high percentages, yellow being low but gray meaning NaN.</p>
<p>However I get this error:</p>
<pre><code>
ValueError                                Traceback (most recent call last)
_______________________________________.ipynb Cell 33' in &lt;module&gt;
      3 labels = [&quot;Worker Type&quot;,&quot;Postcode&quot;,&quot;Name&quot;,&quot;Percentage&quot;]
---&gt; 16 m = homes.explore(
     17      column=&quot;Percentage&quot;,
     18      cmap=lambda value: colourmap(value)
     19      marker_kwds=dict(radius=15, fill=True)
     20      tooltip=labels
     22      legend=False, # do not show column label in the tooltip
     23      name=&quot;Homes&quot; # name of the layer in the map

File ____________________\ref_env\lib\site-packages\geopandas\geodataframe.py:1858, in GeoDataFrame.explore(self, *args, **kwargs)
   1855 @doc(_explore)
   1856 def explore(self, *args, **kwargs):
   1857     &quot;&quot;&quot;Interactive map based on folium/leaflet.js&quot;&quot;&quot;
-&gt; 1858     return _explore(self, *args, **kwargs)

File ________________\ref_env\lib\site-packages\geopandas\explore.py:457, in _explore(df, column, cmap, color, m, tiles, attr, tooltip, popup, highlight, categorical, legend, scheme, k, vmin, vmax, width, height, categories, classification_kwds, control_scale, marker_type, marker_kwds, style_kwds, highlight_kwds, missing_kwds, tooltip_kwds, popup_kwds, legend_kwds, **kwargs)
    454     nan_color = missing_kwds.pop(&quot;color&quot;, None)
    456     gdf[&quot;__folium_color&quot;] = nan_color
--&gt; 457     gdf.loc[~nan_idx, &quot;__folium_color&quot;] = color
    458 else:
    459     gdf[&quot;__folium_color&quot;] = color

File _______________\ref_env\lib\site-packages\pandas\core\indexing.py:723, in _LocationIndexer.__setitem__(self, key, value)
    720 self._has_valid_setitem_indexer(key)
    722 iloc = self if self.name == &quot;iloc&quot; else self.obj.iloc
--&gt; 723 iloc._setitem_with_indexer(indexer, value, self.name)

File _________________\ref_env\lib\site-packages\pandas\core\indexing.py:1730, in _iLocIndexer._setitem_with_indexer(self, indexer, value, name)
   1727 # align and set the values
   1728 if take_split_path:
   1729     # We have to operate column-wise
-&gt; 1730     self._setitem_with_indexer_split_path(indexer, value, name)
   1731 else:
   1732     self._setitem_single_block(indexer, value, name)

File ______________\ref_env\lib\site-packages\pandas\core\indexing.py:1785, in _iLocIndexer._setitem_with_indexer_split_path(self, indexer, value, name)
   1780     if len(value) == 1 and not is_integer(info_axis):
   1781         # This is a case like df.iloc[:3, [1]] = [0]
   1782         #  where we treat as df.iloc[:3, 1] = 0
   1783         return self._setitem_with_indexer((pi, info_axis[0]), value[0])
-&gt; 1785     raise ValueError(
   1786         &quot;Must have equal len keys and value &quot;
   1787         &quot;when setting with an iterable&quot;
   1788     )
   1790 elif lplane_indexer == 0 and len(value) == len(self.obj.index):
   1791     # We get here in one case via .loc with a all-False mask
   1792     pass

ValueError: Must have equal len keys and value when setting with an iterable
</code></pre>
<p>Can someone describe to me this error and direct me on where to look at next?</p>
<p><strong>EDIT:</strong> I should have included that I am using Point Geometry</p>
",18287111.0,18287111.0,2022-04-19 12:35:43,2022-05-09 20:57:43,Creating custom colourmap for geopandas.explore plot,<python><pandas><data-science><geospatial><geopandas>,2,0,N/A,CC BY-SA 4.0
71875064,1,71875586.0,2022-04-14 16:46:27,0,1725,"<p>My dataset looks like this :</p>
<p><a href=""https://i.stack.imgur.com/uMoBG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uMoBG.png"" alt=""Here's a sample of the dataset. Just want to plot the columns marked in yellow"" /></a></p>
<p>Here's a table with a sample of the data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>City</th>
<th>AverageClientUsage</th>
<th>AverageClientEst</th>
<th>EstimatedTraffic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Atlanta</td>
<td>2695.68</td>
<td>3555.62</td>
<td>2812.89</td>
</tr>
<tr>
<td>Boston</td>
<td>559.48</td>
<td>1080.49</td>
<td>583.81</td>
</tr>
<tr>
<td>Chicago</td>
<td>3314.44</td>
<td>5728</td>
<td>3458.56</td>
</tr>
</tbody>
</table>
</div>
<p>I'd like ggplot to use City as the X axis and have three bars for each point on the x axis, one for AverageClientUsage, one for AverageClientEst, one for EstimatedTraffic. How do I go about doing this? At the end I'd like the ggplot to look like this:</p>
<p><a href=""https://i.stack.imgur.com/XirTX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XirTX.png"" alt=""Plot created using Microsoft Excel"" /></a></p>
",7059675.0,1855677.0,2023-03-03 03:00:30,2023-03-03 03:00:30,How to create a ggplot bar chart with multiple columns of data for y?,<r><ggplot2><data-science>,1,1,2023-03-06 11:34:24,CC BY-SA 4.0
72852755,1,-1.0,2022-07-04 06:51:16,0,164,"<p>Is it possible to calculate the Confidence score while predicting the class labels from the NVIDIA NeMo Text Classification model?</p>
<p>I have attached the script for predicting the accuracy of the Nemo Text Classification model for reference.</p>
<pre><code># Test the model
with torch.no_grad():
    correct = 0
    total = 0
    for i, batch in enumerate(validation_dataloader):
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch
        # Forward pass
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
        # print (outputs)
        prediction = torch.argmax(outputs[0],dim=1)
        total += b_labels.size(0)
        correct+=(prediction==b_labels).sum().item()

print('Test Accuracy of the model on the data is: {} %'.format(100 * correct / total))
</code></pre>
",19339191.0,19339191.0,2022-07-19 19:16:37,2022-07-19 19:16:37,Is it possible to evaluate confidence score from accuracy of an NVIDIA NeMo Text Classification model?,<machine-learning><nlp><data-science><artificial-intelligence><text-classification>,0,4,N/A,CC BY-SA 4.0
72852858,1,72854734.0,2022-07-04 07:01:51,0,64,"<p>This is the code that I have been writing, but unable to add labels to the data points. Have tried multiple ways but getting error one after the other!!
The data set in 9th line: 'country' is to be used as labelling. I want to label the 1st and last data point.
Please Help!</p>
<pre><code>```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

data = pd.read_csv('happy_income1.csv')
happy = data['happyScore']
satis = data['avg_satisfaction']
country = data['country']

# Zapping 2 arrays together
satis_happy = np.column_stack((satis,happy))

# Sorting
data.sort_values('avg_satisfaction', inplace=True) #Sorting Data Column

# Filtering
satisfied = data[data['avg_satisfaction']&gt;4] #Making Section as per requirement
print(satisfied)

# Making clusters as required
k_res = KMeans(n_clusters=3).fit(satis_happy)
cluster = k_res.cluster_centers_
print(cluster)

# Plotting
fig, week4 = plt.subplots()
week4.scatter(x=happy, y=satis)
week4.scatter(x=cluster[:,0], y=cluster[:,1], s=9999, alpha=0.25)
week4.set_xlabel('Happiness')
week4.set_ylabel('Satisfaction')
week4.set_title('Happiness versus Satisfaction')

# Labelling
# ----------------------------------------------

plt.show()
```
</code></pre>
<p>CSV File Link: <a href=""https://drive.google.com/file/d/15NjdiMR5D6nJD4jgVbSh7V5GP3AV2o9d/view?usp=sharing"" rel=""nofollow noreferrer"">Click Here</a></p>
",19434068.0,19434068.0,2022-07-04 08:37:13,2022-07-04 09:45:10,I am unable to label the data points on the graph using matplotlib,<python><matplotlib><data-science>,1,2,N/A,CC BY-SA 4.0
71889338,1,-1.0,2022-04-15 22:02:30,0,490,"<p>trying to create a new column on a cudf dataframe based on VWMA from ta_py :</p>
<pre><code>#creating df
CJ_m30 = cudf.read_csv(&quot;/media/f333a/Data/CJ_m30.csv&quot;, 
                         names = [&quot;DateTime&quot;,&quot;Bid&quot;,&quot;Ask&quot;,&quot;Open&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Close&quot;])

#trying to create new column based on func

import ta_py as ta

length = 40

def process_vwma(data):
    VWMA = ta.vwma(data,length)
    return  VWMA
    

CJ_m30['VWMA'] = CJ_m30['Close'].apply(process_vwma, axis = 0)
</code></pre>
<p>returns error :</p>
<blockquote>
<p>ValueError: UDFs using *args or **kwargs are not yet supported.</p>
</blockquote>
<p>updated:
now error is :</p>
<blockquote>
<p>TypingError: Failed in cuda mode pipeline (step: nopython frontend)
Failed in cuda mode pipeline (step: nopython frontend) Unknown
attribute 'vwma' of type Module(&lt;module 'ta_py' from
'/home/f320x/anaconda3/envs/rapids-22.02/lib/python3.9/site-packages/ta_py/<strong>init</strong>.py'&gt;)</p>
<p>File &quot;../../../../../tmp/ipykernel_3478/1395824149.py&quot;, line 6:</p>

<p>During: typing of get attribute at /tmp/ipykernel_3478/1395824149.py
(6)</p>
<p>File &quot;../../../../../tmp/ipykernel_3478/1395824149.py&quot;, line 6:</p>

<p>During: resolving callee type: type(&lt;numba.cuda.compiler.Dispatcher
object at 0x7f05b67a47c0&gt;) During: typing of call at
/home/f320x/anaconda3/envs/rapids-22.02/lib/python3.9/site-packages/cudf/core/series.py
(2495)</p>
<p>File
&quot;../../../anaconda3/envs/rapids-22.02/lib/python3.9/site-packages/cudf/core/series.py&quot;,
line 2495:
&gt;&gt;&gt; def f(x):
</p>
<pre><code>    return df.apply(lambda row: f_(row[name]))
    ^
</code></pre>
</blockquote>
<p>Can someone give an explanation ?
Thank you</p>
",18817999.0,18817999.0,2022-04-16 15:56:45,2022-05-02 19:32:58,Apply ta_py function to Cudf dataframe - RAPIDS,<python><data-science><rapids><cudf>,1,2,N/A,CC BY-SA 4.0
72829882,1,-1.0,2022-07-01 13:16:37,0,76,"<p>Good Afternoon,</p>
<p>I'm a a little stumped as to how to implement this formula into Excel, I already have the data and understand what the formula is supposed to do, but I cannot fathom for the life of me how to write the formula</p>
<p>KANDS = {[KSCOEFS]t[KSCOEFS]}(to power-1)[KSCOEFS]t[OBS]</p>
<p>I understand the first part of the rhs of the equation is an array calculation and usues MMULT with a transpose for the second array which makes sense as the array size would be the same and returns a value when used alone, but the next part, the OBS is only one column wide but the same number of rows as the KSCOEFS, that just returns a #VALUE.</p>
<p>But how do I get it to calculate the KANDS section which is one column wide and 8 rows long? this would need to be calculated 31 times</p>
<p>In my mind, the RHS is ={POWER(MMULT(KSCOEFS),TRANSPOSE(KSCOEFS)),-1)}*MMULT(KSCOEFS,TRANSPOSE(OBS)), but this doesn't help answer how I get a solution vector in KANDS.</p>
<p>Link to example sheet - This only needs to work for one wavelength as it should be repeatable all the way along the wavelength as KANDS needs to be solved for each wavelength</p>
<p><a href=""https://1drv.ms/x/s!AljqtqHbhTTigcsSgL69h1YkeE9_MA?e=xBGDbJ"" rel=""nofollow noreferrer"">https://1drv.ms/x/s!AljqtqHbhTTigcsSgL69h1YkeE9_MA?e=xBGDbJ</a></p>
",19460685.0,19460685.0,2022-07-04 16:22:52,2022-07-04 16:22:52,Calculating Vectors,<excel><excel-formula><data-science>,0,9,N/A,CC BY-SA 4.0
72830100,1,-1.0,2022-07-01 13:34:08,0,60,"<p>I used a dataframe on which I'm applying a Decision Tree Classifier, but it is showing me following error :</p>
<p>could not convert string to float: ' mood_swings, weight_loss, weight_loss'</p>
<p>My code is :</p>
<p>clf.fit(X_train, y_train)</p>
",19437300.0,-1.0,N/A,2022-07-01 13:34:08,Getting an error while dealing with Decision Tree fitting,<python><pandas><dataframe><data-science><data-analysis>,0,3,N/A,CC BY-SA 4.0
72853572,1,72856470.0,2022-07-04 08:11:07,0,111,"<p>I have a large dataframe like <a href=""https://www.kaggle.com/datasets/nelakurthisudheer/dataset-for-predicting-watering-the-plants"" rel=""nofollow noreferrer"">this</a>, not used for time time but for a binary classification task. It contains two important feature columns which have more than 60% NaN values. Instead of removing those columns or shrinking the dataframe are there other ways to resample the data and removing those NaNs or substituting them with synthetic values? I was thinking about the SMOTE package but I know it's used for unbalanced dataframes, not for NaNs. Could I use interpolation through NN or I'll risk to generate misleading data?</p>
",-1.0,-1.0,N/A,2022-07-04 12:06:27,How to resample a dataframe removing nan values?,<python><statistics><data-science><nan><resampling>,1,1,N/A,CC BY-SA 4.0
72855092,1,-1.0,2022-07-04 10:13:10,0,214,"<p>I want to freeze the layers in pytorch efficentnet model. My usual way of dooing this doesn't work.</p>
<pre><code>from torchvision.models import efficientnet_b0
from torch import nn
from torch import optim

efficientnet_b0_fine = efficientnet_b0(pretrained=True)

for param in efficientnet_b0_fine.parameters():
  param.requires_grad = False

efficientnet_b0_fine.fc = nn.Linear(512, 10)

optimizer = optim.Adam(efficientnet_b0_fine.parameters(), lr=0.0001)

loss_function = nn.CrossEntropyLoss()

training(net=efficientnet_b0_fine, n_epochs=epochs, optimizer=optimizer, loss_function=loss_function, train_dl = train_dl)
</code></pre>
<p>The Error I get says:</p>
<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>The Training function looks like this:</p>
<pre><code>for xb, yb in train_dl:
  optimizer.zero_grad()  
  xb = xb.to(device)
  yb = yb.to(device)

  y_hat = net(xb)  
  loss = loss_function(y_hat, yb) 
  
  loss.backward()  
  optimizer.step()  
</code></pre>
<p>Would be great if one of you has a solution!</p>
",13497495.0,-1.0,N/A,2022-07-04 10:13:10,Pytorch Runtime Error: efficientnet freeze layers by setting requires grad = false,<pytorch><data-science><torch><torchvision>,0,4,N/A,CC BY-SA 4.0
72866451,1,-1.0,2022-07-05 08:42:39,0,424,"<p>I have a dataset with about 200 columns/features all with numerical values and taking its <code>corr()</code>  gives me values very close to 0 (like -0.0003 to +0.0003), and so by plotting its heatmap also gives me a big black box with white diagonal - I hope you get the picture. Anyway, here it is:</p>
<p><a href=""https://i.stack.imgur.com/WUmWx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WUmWx.png"" alt=""enter image description here"" /></a></p>
<p>After this, when I try to perform PCA on the dataset, it doesn't really help, as there's barely any correlation between any two features. Did I assume right?</p>
<p>Here's the PCA code:</p>
<pre><code>from sklearn.decomposition import PCA

pca = PCA(n_components = .99) # 99% of variance (selecting components while retaining 99% of the variability in data)
pca.fit(X_scaled)
X_PCA = pca.transform(X_scaled)
</code></pre>
<p>And here's the plot to determine the principle components (Elbow method):</p>
<p><a href=""https://i.stack.imgur.com/rlegT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rlegT.png"" alt=""enter image description here"" /></a></p>
<p>Code for the above:</p>
<pre><code>sns.set(rc = {'figure.figsize': (20, 10)})

plt.ylabel('Eigenvalues')
plt.xlabel('Number of features')
plt.title('Elbow method to determine the principle components')
plt.ylim(0, max(pca.explained_variance_))
plt.axhline(y = (max(pca.explained_variance_) + min(pca.explained_variance_))/2, color = 'r', linestyle = '--')
plt.plot(pca.explained_variance_)
plt.show()
</code></pre>
<p>What I was able to determine from the plot is that there's not really a way to get the principal components, except maybe at PC1, but that'd mean there's only one PC and that would be like discarding 99.5% of data, so I am assuming all the 200 features are necessary.</p>
<p>So my question boils down to this:</p>
<ol>
<li>Is that the right assumption?</li>
<li>If not, what is an ideal way to deal with scenarios like this (where there are a lot of features and no correlations between most (or all) of them)?</li>
<li>Is the correlation between variables a deciding factor for PCA? I read somewhere it is.</li>
</ol>
",1725974.0,-1.0,N/A,2022-07-20 16:51:36,PCAs and Feature correlation,<python><machine-learning><data-science><pca>,1,0,N/A,CC BY-SA 4.0
72880499,1,-1.0,2022-07-06 08:51:13,-1,109,"<p>I have a dataset with several indicators related to some geographical entities ,I want to study factors that influence an indicator A (among the other indicator) .I need to determine which indicators affect it the most (correlation)
which ML algo should I use
I want to have a kind of scoring function for my indicator A to allow its prediction</p>
<p><a href=""https://i.stack.imgur.com/xO9zv.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",19489199.0,404970.0,2022-07-06 10:40:14,2022-07-06 10:40:14,Machine learning algorithm for correlation between indicators,<algorithm><machine-learning><dataset><data-science><correlation>,2,0,N/A,CC BY-SA 4.0
72861930,1,72862122.0,2022-07-04 20:52:31,-1,55,"<p>This likely isn't an easy question.  I am looking at public trade data, and trying to group 'related' items.  But the data itself isn't perfect (the human element) or is misleading.  The good news, is that related items generally follow similar patterns.</p>
<p>Here's an example</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">Notional</th>
<th style=""text-align: left;"">Premium</th>
<th style=""text-align: right;"">Strike</th>
<th style=""text-align: left;"">Structure</th>
<th style=""text-align: left;"">StartDate</th>
<th style=""text-align: left;"">EndDate</th>
<th style=""text-align: left;"">Index</th>
<th style=""text-align: left;"">ExecutionTimestamp</th>
<th style=""text-align: right;"">ResetFreq</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">10</td>
<td style=""text-align: left;"">500,000,000</td>
<td style=""text-align: left;"">9,125,000</td>
<td style=""text-align: right;"">0.02925</td>
<td style=""text-align: left;"">Call</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">2025-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T13:41:01</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">500,000,000</td>
<td style=""text-align: left;"">9,125,000</td>
<td style=""text-align: right;"">0.02925</td>
<td style=""text-align: left;"">Call</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">2025-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T13:41:01</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">12</td>
<td style=""text-align: left;"">250,000,000</td>
<td style=""text-align: left;"">3,837,500</td>
<td style=""text-align: right;"">0.03255</td>
<td style=""text-align: left;"">Call</td>
<td style=""text-align: left;"">2023-06-30</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T14:36:15</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">13</td>
<td style=""text-align: left;"">250,000,000</td>
<td style=""text-align: left;"">3,837,500</td>
<td style=""text-align: right;"">0.03255</td>
<td style=""text-align: left;"">Call</td>
<td style=""text-align: left;"">2023-06-30</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T14:37:11</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">14</td>
<td style=""text-align: left;"">380,000,000</td>
<td style=""text-align: left;"">1,633,999.99462</td>
<td style=""text-align: right;"">0.02473</td>
<td style=""text-align: left;"">Put</td>
<td style=""text-align: left;"">2023-06-30</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T16:40:37</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">15</td>
<td style=""text-align: left;"">380,000,000</td>
<td style=""text-align: left;"">1,633,999.99462</td>
<td style=""text-align: right;"">0.02473</td>
<td style=""text-align: left;"">Put</td>
<td style=""text-align: left;"">2023-06-30</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T16:40:37</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">16</td>
<td style=""text-align: left;"">130,000,000</td>
<td style=""text-align: left;"">987,999.99952</td>
<td style=""text-align: right;"">0.03223</td>
<td style=""text-align: left;"">Call</td>
<td style=""text-align: left;"">2023-06-30</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T16:41:00</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">17</td>
<td style=""text-align: left;"">130,000,000</td>
<td style=""text-align: left;"">987,999.99952</td>
<td style=""text-align: right;"">0.03223</td>
<td style=""text-align: left;"">Call</td>
<td style=""text-align: left;"">2023-06-30</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T16:41:00</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">18</td>
<td style=""text-align: left;"">130,000,000</td>
<td style=""text-align: left;"">987,999.99952</td>
<td style=""text-align: right;"">0.03223</td>
<td style=""text-align: left;"">Put</td>
<td style=""text-align: left;"">2023-06-30</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T16:41:16</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">19</td>
<td style=""text-align: left;"">130,000,000</td>
<td style=""text-align: left;"">987,999.99952</td>
<td style=""text-align: right;"">0.03223</td>
<td style=""text-align: left;"">Put</td>
<td style=""text-align: left;"">2023-06-30</td>
<td style=""text-align: left;"">2024-06-30</td>
<td style=""text-align: left;"">SOFR-OIS</td>
<td style=""text-align: left;"">2022-06-28T16:41:16</td>
<td style=""text-align: right;"">3</td>
</tr>
</tbody>
</table>
</div>
<p>A lot of real similar looking things here! One specific note on the columns:</p>
<p><strong>Structure</strong> is misleading.  In reality, there are 3 possible values [Call, Put, Straddle].  However, it seems like Put = Put, but Call = Call OR Straddle in the data.  A straddle = 1 Put + 1 Call with the same notional, dates and strike.</p>
<p>So in the example I can see that index 16, 17, 18, 19 are <em>very</em> likely to be 2 straddle trades [and I know this to be true]. However, index 14 &amp; 15 are also related to these two straddle trades!  The way I would know this is:</p>
<ol>
<li>Timestamp is very close in time (&lt;5min-ish)</li>
<li>The difference in (strike*10000) between 16/17/18/19 and 14/15 is divisible by a
factor of 25 [people like round numbers].</li>
<li>StartDate, EndDate,
Index, Reset Freq match as well.</li>
</ol>
<p>In this case (14,15,16,17,18,19) I would want to return 2x “1 straddle vs 1 put trade with details xyz”.  In pandas terms, I would convert the put+call into a straddle where applicable, and be left with two “Group” IDs to identify related trade.</p>
<p>So in general, I guess I'm asking if there is a way to create a list of rules that determine the probability of it being a related trade?  I'm open to any suggestions though!</p>
<p>I can edit the question for more specific rules etc if needed.</p>
<p>Tx!</p>
",558619.0,558619.0,2022-07-04 21:06:21,2022-07-04 21:18:52,"Pandas - Grouping related data entries, with imperfect data",<python><pandas><machine-learning><data-science><data-analysis>,1,1,N/A,CC BY-SA 4.0
72862539,1,72862611.0,2022-07-04 22:34:34,0,1178,"<p>I have these three lists:</p>
<pre><code>amountOfYes = [38, 33, 30, 29, 29, 29, 28, 28, 27, 27, 27, 26, 26, 26, 26]
amountOfNo = [39, 35, 33, 32, 30, 28, 24, 22, 21, 21, 21, 20, 20, 20, 19]
index = [0, 118, 393, 317, 1, 8, 9, 29, 226, 297, 331, 52, 105, 251, 316]
</code></pre>
<p>amountOfYes &amp; amountOfNo are lists filled with the amount of Yes and No that the top 15 sessions with most attendance had and Index is a list filled with the correspondent position that voting.</p>
<p>the first voting had 38 yes and 39 no, the 118th had 33 yes and 35 no.</p>
<p>How do I plot the amount of yes and no with the corresponding index?</p>
<p><strong>EDIT:</strong></p>
<p>The idea is to create a line graph similar to this one:
<a href=""https://i.stack.imgur.com/6RMEn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6RMEn.png"" alt=""Graph with the first 20 votings"" /></a></p>
<p>This chart has the first 20 votings, I need the votings specific votings of Index</p>
<p>I was going with</p>
<pre><code>plt.plot(amountOfYes[index], label = 'Yes', color = 'red')
plt.plot( amountOfNo[index], label = 'No', color = 'black')
</code></pre>
<p>The X-axis should be the index and the Y the number of votes each option got</p>
",9784600.0,9784600.0,2022-07-04 22:47:59,2022-07-04 23:29:59,How to plot specific index from array?,<python><matplotlib><data-science>,1,5,N/A,CC BY-SA 4.0
72871896,1,72871945.0,2022-07-05 15:22:34,2,81,"<p>I would like to get the corresponding values of a vector in a table from a column in another column. (just look below)</p>
<p>example:</p>
<p>Vector:</p>
<pre><code>v = c('A', 'B', 'C')
</code></pre>
<p>Table :</p>
<pre class=""lang-py prettyprint-override""><code># key     Value
 'C'        3
 'A'        1
 'B'        2
</code></pre>
<p>When I give the vector <code>v</code> (<code>A, B, C</code>) I want to get back the corresponding values in the good order <code>1, 2, 3</code>.</p>
<p>In reality, the vector is the <code>rownames</code> of a dataset, and I need to replace it with the corresponding values.</p>
<p>I was thinking about using the <code>left_join</code> function from <code>Dplyr</code> but I would need 2 tables for this.</p>
<p>Thanks for your help</p>
",17965313.0,-1.0,N/A,2022-07-05 17:49:33,R join a table column from a vector,<r><dplyr><data-science>,2,0,N/A,CC BY-SA 4.0
72884634,1,-1.0,2022-07-06 13:42:31,0,288,"<p>I am working with data frame while after running the specific code and check for head() function I got the error &quot;AttributeError: 'NoneType' object has no attribute 'head'
&quot;</p>
<p>The relevant piece of code is below:</p>
<pre><code>import numpy as np
import pandas as pd
rfilepath=&quot;Advertising.csv&quot;
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.model_selection import train_test_split

def loaddata(rfilepath):
    data=pd.read_csv(rfilepath)
    return(data)

try:
    data_df=loaddata(rfilepath)
    print(data_df)
except:
    print(&quot;error&quot;)


data_df.head() #Here no error is showing



def processdata(data_df):
    for (columnName, columnData) in data_df.iteritems():


        print(columnName)
        sns.boxplot(data_df[columnName])
        plt.show()
        q1=stats.scoreatpercentile(data_df[columnName],25)
        print(&quot;Q1&quot;,q1)

        q3=stats.scoreatpercentile(data_df[columnName],75)
        print(&quot;Q3&quot;,q3)

        iqr=stats.iqr(data_df[columnName])
        print(&quot;iqr&quot;,iqr)

        lower_bound= q1- 1.5*(iqr)
        print(&quot;Lowebound&quot;,lower_bound)

        upper_bound= q3+ 1.5*(iqr)
        print(&quot;upperbound&quot;,upper_bound)
        print(&quot;\n&quot;)
        outliers= data_df[columnName][((data_df[columnName]&lt;lower_bound) | (data_df[columnName]&gt;upper_bound))]
        outliers
        median=stats.scoreatpercentile(data_df[columnName],99)
        median
        for i in outliers:
            data_df[columnName]=np.where(data_df[columnName]==i,median,data_df[columnName])
        sns.boxplot(data_df[columnName])
        plt.show()


try:
    data_df=processdata(data_df)
except:
    print(&quot;error&quot;)

data_df.head()#after calling the function processdata(data_df) here shows the &quot;AttributeError: 'NoneType' object has no attribute 'head'&quot;
</code></pre>
<p>I think the issue is with the function processdata(data_df).If anyone know what exactly the issue?</p>
",19495076.0,-1.0,N/A,2022-07-06 14:56:34,Why is my pandas dataframe data type turning into 'None' type?,<python><python-3.x><data-science>,1,1,N/A,CC BY-SA 4.0
72880688,1,-1.0,2022-07-06 09:05:40,0,1942,"<p>I have a table in data.table format in R:</p>
<pre><code>dt &lt;- data.table(currency = c(&quot;EUR&quot;,&quot;EUR&quot;,&quot;EUR&quot;,&quot;USD&quot;,&quot;USD&quot;,&quot;USD&quot;, &quot;RON&quot;,&quot;RON&quot;,&quot;RON&quot;,&quot;RON&quot;,&quot;RON&quot;,&quot;EUR&quot;,&quot;EUR&quot;,&quot;USD&quot;,&quot;USD&quot;,&quot;USD&quot;, &quot;RON&quot;,&quot;RON&quot;,&quot;RON&quot;,&quot;RON&quot;,&quot;RON&quot;), date = c(&quot;2019-02-25&quot;,&quot;2019-02-25&quot;,&quot;2019-02-25&quot;,&quot;2019-02-25&quot;,&quot;2019-02-25&quot;,&quot;2019-02-25&quot;, &quot;2019-02-25&quot;,&quot;2019-02-25&quot;,&quot;2019-02-25&quot;,&quot;2019-02-25&quot;,&quot;2019-02-25&quot;,&quot;2019-03-01&quot;,&quot;2019-03-01&quot;,&quot;2019-03-01&quot;,&quot;2019-03-01&quot;,&quot;2019-03-01&quot;, &quot;2019-03-01&quot;,&quot;2019-03-01&quot;,&quot;2019-03-01&quot;,&quot;2019-03-01&quot;,&quot;2019-03-01&quot;), y = c(&quot;0,1&quot;,&quot;0,2&quot;,&quot;0,2&quot;,&quot;0,1&quot;,&quot;0,1&quot;,&quot;0,15&quot;,&quot;0,1&quot;,&quot;0,2&quot;,&quot;0,3&quot;,&quot;0,1&quot;,&quot;0,1&quot;,&quot;0,15&quot;,&quot;0,1&quot;,&quot;0,1&quot;,&quot;0,25&quot;,&quot;0,3&quot;,&quot;0,1&quot;,&quot;0,1&quot;,&quot;0,15&quot;,&quot;0,1&quot;,&quot;0,2&quot;)
dt
</code></pre>
<p>I needed to use the following code:</p>
<pre><code>dt &lt;- dt|&gt;
  group_by(currency, date)  |&gt;
  mutate(N = row_number())  |&gt;
  ungroup()  |&gt;
  complete(currency, date, N) |&gt;
  arrange(date, currency, N)
dt
</code></pre>
<p>Then I needed to do approximation: to create a new column &quot;x&quot; in the table which will include approximation of column &quot;y&quot; and fulfill missing values. For doing this I used code:</p>
<pre><code>#a table with selected columns
m &lt;- dt[,c(&quot;currency&quot;,&quot;date&quot;,&quot;N&quot;,&quot;y&quot;)]

#add column x to the table
m$x &lt;- na.approx(m$y)

</code></pre>
<p>And then I get an error:</p>
<pre><code>Error: Assigned data `na.approx(zero_rates_hist$zero_rate)` must be compatible with existing data.
x Existing data has 1654800 rows.
x Assigned data has 1654653 rows.
ℹ Only vectors of size 1 are recycled.
</code></pre>
<p>Help me, please! Thank you.</p>
",17508349.0,17508349.0,2022-07-06 09:56:07,2022-07-06 09:56:07,ERROR in R: Assigned data must be compatible with existing data,<r><error-handling><datatable><data-science>,1,3,N/A,CC BY-SA 4.0
72887820,1,-1.0,2022-07-06 17:40:36,0,49,"<p>In our Data Science project we are playing around pandas dataframe, numpy and scipy libraries and we want to change the code into Pyspark, We are facing issues like:</p>
<p><strong>wst = cur_buck[:, [0]]</strong></p>
<p><strong>cur_buck[:, :-1] = cur_buck[:, 1:] - wst</strong></p>
<p><strong>cur_buck[:, -1] = cur_buck[:, -2]</strong></p>
<p>number_of_particles=100</p>
<p><strong>particles_matrix[10] = cur_buck[:, -1].reshape(number_of_particles).copy()</strong></p>
<p><strong>Same thing how can we achieve in Pyspark?</strong></p>
<p>Any leads would be highly appreciable like these 4 lines I want to convert in pyspark.</p>
<p>Thanks in advance</p>
",19496895.0,-1.0,N/A,2023-11-11 10:48:35,Pandas index operations in Pyspark,<python><pandas><pyspark><data-science><pyspark-pandas>,1,3,N/A,CC BY-SA 4.0
72887848,1,-1.0,2022-07-06 17:43:06,1,22,"<p>I have a order table where I have a timestamp field for each order as well as the user_id:</p>
<pre><code>CREATE TABLE order(
id BIGINT, 
created TIMESTAMP,
user_id INT
PRIMARY KEY(id)
)
</code></pre>
<p>I'm trying to get something like this where I get the count of new and recurrent users for each month. A user is new for the whole month they made their first purchase and recurrent every purchase after that.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Customer   Type</th>
<th>month 1</th>
<th>month 2</th>
<th>month 3</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>new</td>
<td>count</td>
<td>count</td>
<td>count</td>
<td></td>
</tr>
<tr>
<td>recurrent</td>
<td>count</td>
<td>count</td>
<td>count</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>So far I have this:</p>
<pre><code>SELECT 'New' AS 'Customer Type',
SUM(CASE WHEN MONTH(created) = 1 THEN 1 ELSE 0 END) month_1,
SUM(CASE WHEN MONTH(created) = 2 THEN 1 ELSE 0 END) month_2,
SUM(CASE WHEN MONTH(created) = 3 THEN 1 ELSE 0 END) month_3
FROM order
HAVING MONTH(MIN(created))
</code></pre>
<p>I was planning on using a UNION for the recurrent with a NOT IN (MIN(MONTH(created)) statement but i'm sure there's a better approach to this.</p>
<p>Any help will be greatly appreciated!</p>
",19496781.0,-1.0,N/A,2022-07-06 17:43:06,New vs recurring users with months as columns mysql,<mysql><database><data-science><mysql-workbench>,0,2,N/A,CC BY-SA 4.0
72835752,1,72835806.0,2022-07-02 00:56:30,-1,36,"<p>Why is it ok to write this:</p>
<pre><code>example1 = df_name[df_name[['column1','column2']].isin(['val1', 'val2'])]
print(example1)
</code></pre>
<p>but not this:</p>
<pre><code>example1 = df_name[df_name[['column1','column2']]].isin(['val1', 'val2'])
print(example1)
</code></pre>
<p>I was hoping to better understand filtering syntax, insofar as why you don't append something like .isin() to an entire (subsetted) df, instead of including it within the bracketed df subsection itself.</p>
",19176762.0,19176762.0,2022-07-04 22:03:55,2022-07-04 22:03:55,Why do you bracket the entire line after a df subset that uses .isin() instead of just bracketing the df subset half?,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
72835883,1,-1.0,2022-07-02 01:38:54,-2,65,"<p>I am stuck in this list problem, I am unable to solve it.</p>
<p>list1= [&quot;aaditya-2&quot;, &quot;rahul-9&quot;, &quot;shubhangi-4&quot;]</p>
<p>I need to sort this list without using sort/sorted function.... and also it should sort on the basis of numbers at the last..</p>
<p>Output:
[&quot;aaditya-2&quot;, &quot;shubhangi-4&quot;, &quot;rahul-9&quot;]</p>
<p>OR</p>
<p>[&quot;rahul-9&quot;, &quot;shubhangi-4&quot;, &quot;aaditya-2&quot;  ]</p>
",19464354.0,-1.0,N/A,2022-07-02 02:21:56,List Problem in python to sort a list of string without using sort or sorted function,<python><string><list><sorting><data-science>,2,3,N/A,CC BY-SA 4.0
72863835,1,72863851.0,2022-07-05 03:55:39,1,6526,"<p>I have exported the gold price data from the brokers. the file has no column names like this</p>
<p>2014.02.13,00:00,1291.00,1302.90,1286.20,1302.30,41906
2014.02.14,00:00,1301.80,1321.20,1299.80,1318.70,46244
2014.02.17,00:00,1318.20,1329.80,1318.10,1328.60,26811
2014.02.18,00:00,1328.60,1332.10,1312.60,1321.40,46226</p>
<p>I read csv to pandas dataframe and it take the first row to be the column names. I am curious how can I set the column names and still have all the data</p>
<p>Thank you</p>
",19483761.0,495455.0,2022-07-05 04:00:00,2022-07-05 04:05:37,How to create column names in pandas dataframe?,<python-3.x><pandas><data-science>,1,1,N/A,CC BY-SA 4.0
72870821,1,72870860.0,2022-07-05 14:05:51,0,35,"<p><strong>Context</strong></p>
<p>I am currently processing some data and encountered a problem.
I would like to filter a Pandas DataFrame using Values from a Series.
However, this always throws the following Error:</p>
<blockquote>
<p>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</p>
</blockquote>
<p><strong>Code</strong></p>
<pre class=""lang-py prettyprint-override""><code># DataFrame
userID (Int) | startTS (Int) | endTS (Int) | placeID (String)

# Group Data into Subgroups, one for each User.
stayGroup = stayData.groupby('userID')

for userID, data in stayGroup:

    for index, row in data.iterrows():

        # Stays starting during this Stay.
        staysA = data[row['startTS'] &lt; data['startTS'] &lt; row['endTS']]

        # Stays ending during this Stay.
        staysB = data[row['startTS'] &lt; data['endTS'] &lt; row['endTS']]

        # Stays starting before and ending after this Stay.
        staysC = data[(row['startTS'] &gt;= data['startTS']) &amp; (row['endTS'] &lt;= data['endTS'])]
</code></pre>
<p><strong>Question</strong></p>
<p>Does anyone have an idea what's this error means and how I can solve it?
Thanks a lot for your assistance in advance!</p>
",13345744.0,13345744.0,2022-07-05 14:10:24,2022-07-05 14:10:24,Python: How to filter a Pandas DataFrame using Values from a Series?,<python><pandas><dataframe><data-science><series>,1,2,N/A,CC BY-SA 4.0
72893719,1,72894622.0,2022-07-07 07:18:19,1,98,"<p>I combined two Csv files i now have an Data-Frame age column that contains data in the form:
51-55<br />
41-45<br />
41  45<br />
46-50<br />
36-40<br />
46  50<br />
26-30<br />
21  25<br />
36  40<br />
31  35<br />
26  30<br />
21-25<br />
56 or older<br />
31-35<br />
56-60<br />
61 or older<br />
20 or younger</p>
<p>i will like to make them be in the this range 15-25,26-35,36-45........</p>
",19500489.0,19500489.0,2022-07-07 07:39:09,2022-07-07 08:30:50,My pandas dataframe age column contains whitespace and string,<python><pandas><dataframe><data-science><data-cleaning>,1,0,N/A,CC BY-SA 4.0
72889811,1,72889880.0,2022-07-06 21:01:34,1,173,"<p>I have a DF where some values have a prefix and I want to make a loop to remove it.
The DF looks like this:
<a href=""https://i.stack.imgur.com/kKylC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kKylC.png"" alt=""enter image description here"" /></a></p>
<p>I want to remove the &quot;null-&quot; and just leave the numbers.
I tried that, but I got and error message:
<a href=""https://i.stack.imgur.com/ruzUM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ruzUM.png"" alt=""enter image description here"" /></a></p>
",16769072.0,16769072.0,2022-07-06 21:04:38,2022-07-07 00:42:00,Loop to remove prefix in dataframe,<python><pandas><dataframe><data-science>,3,2,N/A,CC BY-SA 4.0
72894922,1,72895209.0,2022-07-07 08:53:29,0,167,"<p>I used RandomizedSearchCV (RSCV) with the default 5-fold CV for LGBMClassifier with an evaluation set.</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
model_LGBM=LGBMClassifier(objective='binary',metric='auc',random_state=0,early_stopping_round=100)

distributions = dict(max_depth=range(1,10),
                     num_leaves=[50,100,150],
                     learning_rate=[0.1,0.2,0.3],
                     )

clf = RandomizedSearchCV(model_LGBM, distributions, random_state=0,n_iter=100,verbose=10)
clf.fit(X_train,y_train,eval_set=(X_test,y_test))
</code></pre>
<p>So the output of the RSCV looks like:</p>
<pre><code>First iter: CV 1/5, &quot;valid0's&quot; CV 2/5 &quot;valid0's&quot;, ..., CV 5/5 &quot;valid0's&quot;;
Second iter: CV 1/5 &quot;valid0's&quot;, CV 2/5 &quot;valid0's&quot;, ..., CV 5/5 &quot;valid0's&quot;;
...
Last iter: CV 1/5 &quot;valid0's&quot;, CV 2/5 &quot;valid0's&quot;, ..., CV 5/5 &quot;valid0's&quot;;
+1 fit with &quot;valid0's&quot;
</code></pre>
<p>I suppose the last fit is the refitted best estimator. Does it use the whole training set? Where does it use the evaluation set?</p>
",2614233.0,-1.0,N/A,2022-07-07 09:14:50,How does best estimator fitting work in RandomizedSearchCV?,<python><optimization><data-science><lightgbm>,1,0,N/A,CC BY-SA 4.0
72895577,1,-1.0,2022-07-07 09:39:43,0,27,"<p>I have a simple LogisticRegression model that I am using to classify some training data as below:</p>
<pre><code>clf = LogisticRegression()
training_output = clf.fit(x_train, y_train)
</code></pre>
<p>Once the model is trained I am using some test data to evaluate its performance:</p>
<pre><code>score = clf.score(x_test, y_test)
</code></pre>
<p>So far so good. I am able to get the score and create confusion matrix.</p>
<p>But, is there a way for me to get how the model classified the x_test , so I can compare if against y_test row by row?</p>
",2686775.0,-1.0,N/A,2022-07-07 09:52:53,How do I find out how LogisticRegression in sklearn classified my test data?,<python-3.x><machine-learning><scikit-learn><data-science>,2,0,2022-07-07 12:00:01,CC BY-SA 4.0
72899066,1,-1.0,2022-07-07 13:52:17,1,231,"<p>I have used pandas function to create dates with this period of time, but it returned empty list.</p>
<pre class=""lang-py prettyprint-override""><code>date_df = pd.date_range(start='1/1/1', end='31/12/1999')
</code></pre>
",8753176.0,-1.0,N/A,2022-07-07 14:10:32,is there a way to generate dates for 10000 years ? starting from 1-1-1 to 9999-12-31,<python><pandas><datetime><numbers><data-science>,1,3,N/A,CC BY-SA 4.0
72902448,1,72902484.0,2022-07-07 18:10:06,-1,930,"<p>My data-frame age column looks like this</p>
<p>20 or younger =14</p>
<p>61 or older   =45</p>
<p>56-60         = 34</p>
<p>31-35        =30</p>
<p>56 or older  =31</p>
<p>21-25     =23</p>
<p>26  30    =56</p>
<p>31  35  =44</p>
<p>36  40  =32</p>
<p>21  25  =26</p>
<p>26-30 =14</p>
<p>46  50  =14</p>
<p>36-40 =15</p>
<p>46-50  =33</p>
<p>41  45  =24</p>
<p>41-45  =29</p>
<p>51-55 =35</p>
<p>so i wrote this function to categorize it better but i got this typeerror message that says '&lt;' not supported between instance of str and int</p>
<p>def age_buckets(x):</p>
<pre><code>if x &lt; 30: 
    return '18-29' 
elif x &lt; 40: 
    return '30-39' 
elif x &lt; 50: 
    return '40-49' 
elif x &lt; 60: 
    return '50-59' 
elif x &lt; 70: 
    return '60-69' 
elif x &gt;=70: 
    return '70+' 
else: return 'other'
</code></pre>
<p>Here is a link to what i am doing
<a href=""https://deepnote.com/workspace/eddie-abfa350f-f15e-43fe-8960-fab53a2def2e/project/Welcome-e6ac66b9-19f2-4973-bbc2-7adfda9366f3/%2FReasons%20for%20resignation%20analysis.ipynb"" rel=""nofollow noreferrer"">https://deepnote.com/workspace/eddie-abfa350f-f15e-43fe-8960-fab53a2def2e/project/Welcome-e6ac66b9-19f2-4973-bbc2-7adfda9366f3/%2FReasons%20for%20resignation%20analysis.ipynb</a></p>
",19500489.0,19500489.0,2022-07-08 06:55:57,2022-07-08 06:55:57,pandas dataframe column contains string and int,<python><pandas><dataframe><data-science><typeerror>,1,3,N/A,CC BY-SA 4.0
72909936,1,-1.0,2022-07-08 10:17:19,1,78,"<p>I'm working on a project that began on an older version of Neo4j (3.5) and has slightly different syntax, particularly regarding algorithms. I'm trying to 'update' the following query to work with GDS:</p>
<pre><code>CALL algo.labelPropagation.stream(
'MATCH (p:Publication) RETURN id(p) as id',
'MATCH (p1:Publication)-[r1:HAS_WORD]-&gt;(w)&lt;-[r2:HAS_WORD]-(p2:Publication) 
WHERE r1.occurrence &gt; 5 AND r2.occurrence &gt; 5
RETURN id(p1) as source, id(p2) as target, count(w) as weight',
{graph:'cypher',write:false, weightProperty : &quot;weight&quot;}) yield nodeId, label
with label, collect(algo.asNode(nodeId)) as nodes where size(nodes) &gt; 2
MERGE (c:PublicationLPACommunity {id : label})
FOREACH (n in nodes |
   MERGE (n)-[:IN_LPA_COMMUNITY]-&gt;(c)
)
return label, nodes
</code></pre>
<p>The main issues are likely the first part (algo.labelPropagation) and (algo.asNode) since these have changed in GDS. Here is the error that is returned:</p>
<pre><code>Procedure call provides too many arguments: got 3 expected no more than 2.

Procedure gds.labelPropagation.stream has signature: gds.labelPropagation.stream(graphName :: STRING?, configuration  =  Map{} :: MAP?) :: nodeId :: INTEGER?, communityId :: INTEGER?
meaning that it expects at least 1 argument of type STRING?
Description: The Label Propagation algorithm is a fast algorithm for finding communities in a graph. (line 1, column 1 (offset: 0))
&quot;CALL gds.labelPropagation.stream(&quot;
 ^
</code></pre>
<p>Any help much appreciated!</p>
",19480934.0,19480934.0,2022-07-08 10:22:05,2022-07-08 13:07:46,Algorithm queries in latest version of Neo4j - GDS syntax updates,<algorithm><neo4j><cypher><graph-theory><graph-data-science>,1,0,N/A,CC BY-SA 4.0
72914889,1,72958456.0,2022-07-08 17:21:37,0,138,"<p>I've used gensim Word2Vec to learn the embedding of monetary amounts and other numeric data in bank transaction memos. The goal is to use this to be able to extract these amounts and currencies from future input strings.</p>
<p><strong>Design</strong>
Our input strings are something like</p>
<pre><code>&quot;AMAZON.COM TXNw98e7r3347 USD 49.00 @ 1.283&quot;
</code></pre>
<p>During preprocessing, I tokenize and also replace all tokens that have the possibility of being a monetary amount (string consisting only of digits, commas, and &lt;= 1 decimal point/period) with a special VALUE_TOKEN. And I also manually replace exchange rates with RATE_TOKEN. The result would be</p>
<pre><code>[&quot;AMAZON&quot;, &quot;.COM&quot;, &quot;TXNw&quot;, &quot;98&quot;, &quot;e&quot;, &quot;7&quot;, &quot;r&quot;, &quot;3347&quot;, &quot;USD&quot;, &quot;VALUE_TOKEN&quot;, &quot;@&quot;, &quot;RATE_TOKEN&quot;]
</code></pre>
<p>With all my preprocessed lists of strings in list <code>data</code>, I generate model</p>
<pre><code>model = Word2Vec(data, window=3, min_count=3)
</code></pre>
<p>The embeddings of model that I'm most interested in are that of VALUE_TOKEN, RATE_TOKEN, as well as any currencies (USD, EUR, CAD, etc.). Now that I generated the model, I'm not sure what to do with it.</p>
<p><strong>Problem</strong>
Say I have a new string that the model has never seen before,</p>
<pre><code>new_string = &quot;EUR 299.99 RATE 1.3289 WITH FEE 5.00&quot;
</code></pre>
<p>I would like to use <code>model</code> to identify which tokens of <code>new_string</code> is most contextually similar to VALUE_TOKEN (which should return [&quot;299.99&quot;, &quot;5.00&quot;]), which is closest to RATE_TOKEN (&quot;1.3289&quot;). It should be able to classify these based on the learned embedding. I can preprocess <code>new_string</code> the way I do with the training data, but because I don't know the exchange rate before hand, all three tokens of [&quot;299.99&quot;, &quot;5.00&quot;, &quot;1.3289&quot;] will be tagged the same (either with VALUE_TOKEN or a new UNIDENTIFIED_TOKEN).</p>
<p>I've looked into methods like <code>most_similar</code> and <code>similarity</code> but don't think they work for tokens that are not necessarily in the vocabulary. What methods should I use to do this? Is this the right approach?</p>
",14048440.0,-1.0,N/A,2022-07-12 21:14:11,Using a Word2Vec Model to Extract Data,<machine-learning><nlp><data-science><word2vec><word-embedding>,1,2,N/A,CC BY-SA 4.0
72893026,1,-1.0,2022-07-07 06:13:00,0,269,"<p>I have a simulation pipeline where I run experiments on patients. Everything's in Python (SciPy, Numpy, Pandas, etc.) in a local Docker container running on 2017 MacBook Pro (3.5 GHz Dual-Core Intel Core i7 and 16 GB 2133 MHz LPDDR3).</p>
<p>The <strong>experiments</strong> DataFrame looks like this:</p>
<pre><code>       trial  experiment  observation  compound     value
0          1          10            1         4  7.578612
1          7           8            1         4  1.751288
2         11           7            1         4  0.754702
3         30           6            1         4  7.762336
4         35           4            1         3  3.613458
</code></pre>
<p>and here's the <strong>patients</strong> one:</p>
<pre><code>       patient  compound    lab    unit  metric_1  metric_2  metric_3   metric_4
0        72070         7  lab_a  unit_c   1292774     44351      3454  17.219036
1        43025         3  lab_a  unit_b    661842     30200      6147  11.882615
2        45878         8  lab_b  unit_b    292885     30928      7864  28.959206
3          697         7  lab_a  unit_a   1352669     81372      3769   3.728837
4        51402         8  lab_a  unit_c    517981     48154       381  45.606934
</code></pre>
<p>At the end of this simulation, I generate statistics on the individual observation. To get there, I merge both DataFrames to get the individual results (inner merge on <strong>compound</strong>). Both DataFrames are currently relatively small (50,000 rows for <strong>patients</strong> and 15,000 rows for <strong>experiments</strong>) but I'm trying to increase the size of the simulation (10x the number of experiments), which gets the simulation to crash.</p>
<h2>1 - Initial Approach</h2>
<p>I started by doing a simple Pandas merge (&quot;inner&quot; is the default)</p>
<pre><code>import pandas as pd
results = pd.merge(experiments, patients, on=&quot;compound&quot;)

results

          trial experiment  observation compound     value  patient    lab    unit  metric_1  metric_2  metric_3   metric_4
0             1         10            1        4  7.578612    11437  lab_a  unit_b   1022481     24955      7312  43.395134
1             1         10            1        4  7.578612    60952  lab_a  unit_c    873872     98759      1348   5.580664
2             1         10            1        4  7.578612    41207  lab_a  unit_b    421455     88188      9705  27.077997
3             1         10            1        4  7.578612    62537  lab_a  unit_a    645139     24159      6014   3.610864
4             1         10            1        4  7.578612    59984  lab_a  unit_c   1176892     96816      6099  45.588840
...         ...        ...          ...      ...       ...      ...    ...     ...       ...       ...       ...        ...
83320766  99898          5            1        5  6.272213    33903  lab_a  unit_b    894670     98514      6620   9.320307
...
</code></pre>
<p>I get <strong>83.34M</strong> rows as a results (which is what I want). The simulation took 01:04 to run and 7.45 GB of RAM (using <code>results.info(memory_usage=&quot;deep&quot;</code>).</p>
<h2>2 - Reduce DataFrame memory usage</h2>
<p>I then changed downcasted the default data types to reduce the memory usage of both DataFrames.</p>
<pre><code>experiments_schema = {
    &quot;trial&quot;: &quot;uint32&quot;,
    &quot;experiment&quot;: &quot;category&quot;,
    &quot;observation&quot;: &quot;uint8&quot;,
    &quot;compound&quot;: &quot;category&quot;,
    &quot;value&quot;: &quot;float32&quot;
}

patients_schema = {
    &quot;patient&quot;: &quot;uint32&quot;,
    &quot;compound&quot;: &quot;category&quot;,
    &quot;lab&quot;: &quot;category&quot;,
    &quot;unit&quot;: &quot;category&quot;,
    &quot;metric_1&quot;: &quot;uint64&quot;,
    &quot;metric_2&quot;: &quot;uint32&quot;,
    &quot;metric_3&quot;: &quot;uint32&quot;,
    &quot;metric_4&quot;: &quot;float32&quot;,
}

experiments = experiments.astype(experiments_schema)
patients = patients.astype(patients_schema)
</code></pre>
<p>As expected, I get the same number of rows. The simulation took a bit less time to run (00:58) but used less than half RAM (3.49 GB).</p>
<h2>3 - Reduce runtime with Dask</h2>
<p>I then replaced my Pandas merge with a Dask one.</p>
<pre><code>import dask.dataframe as dd
results = dd.merge(experiments, patients, on=&quot;compound&quot;, how=&quot;inner&quot;)
</code></pre>
<p>Again, same number of rows. The simulation only took 00:26 to run and used 3.49 GB of memory.</p>
<hr />
<h2>What to do next to improve the simulation performance / scale?</h2>
<p>I need to run 150,000 experiments (10x more) on the same 50,000 patients. If I run this on my machine it'll just run out of memory with an error 137 (and I've maxed the Mac docker desktop settings to use as much RAM as possible). What can I do to run the simulation with 10x experiments?</p>
<ul>
<li>Should I expand Dask usage to capture the next steps of my pipeline and only do a <code>results.compute()</code> at the very end?</li>
<li>Should I look into Cython or Numba implementations of the merge operation?</li>
<li>Should I store to disk?</li>
<li>Any other idea (except Spark / distributed workflow as everything's expected to be local)?</li>
</ul>
<p>Thanks</p>
<hr />
<p><strong>Edit</strong>: here's a short script to regenerate sample data (if needed)</p>
<pre><code>import numpy as np
import pandas as pd

rng = np.random.default_rng()

experiment_count = 15000
experiments = pd.DataFrame(
    {
        &quot;trial&quot;: np.sort(rng.choice(a=100000, size=experiment_count, replace=False)),
        &quot;experiment&quot;: rng.choice(a=[3, 4, 5, 6, 7, 8, 9, 10], size=experiment_count, replace=True),
        &quot;observation&quot;: rng.choice(a=[1, 2, 3], size=experiment_count, p=[0.9, 0.09, 0.01], replace=True),
        &quot;compound&quot;: rng.choice(a=[1, 2, 3, 4, 5, 6, 7, 8, 9], size=experiment_count, replace=True),
        &quot;value&quot;: 10*rng.random(size=experiment_count, dtype='float32')
    }
)

patient_pool = 50000
patients = pd.DataFrame(
    {
        &quot;patient&quot;: rng.choice(a=75000, size=patient_pool, replace=False),
        &quot;compound&quot;: rng.choice(a=[1, 2, 3, 4, 5, 6, 7, 8, 9], size=patient_pool, replace=True),
        &quot;lab&quot;: rng.choice(a=[&quot;lab_a&quot;, &quot;lab_b&quot;, &quot;lab_c&quot;], size=patient_pool, p=[0.6, 0.3, 0.1], replace=True),
        &quot;unit&quot;: rng.choice(a=[&quot;unit_a&quot;, &quot;unit_b&quot;, &quot;unit_c&quot;], size=patient_pool, p=[0.3, 0.4, 0.3], replace=True),
        &quot;metric_1&quot;: rng.choice(a=1500000, size=patient_pool, replace=True),
        &quot;metric_2&quot;: rng.choice(a=100000, size=patient_pool, replace=True),
        &quot;metric_3&quot;: rng.choice(a=10000, size=patient_pool, replace=True),
        &quot;metric_4&quot;: 50*rng.random(size=patient_pool, dtype='float32'),
    }
)
</code></pre>
",2097233.0,-1.0,N/A,2022-07-08 06:35:43,Reduce Pandas / Dask DataFrame memory usage to scale up simulation,<python><pandas><data-science><simulation><dask>,1,9,N/A,CC BY-SA 4.0
72903381,1,-1.0,2022-07-07 19:39:31,0,38,"<p>I'm trying to find the average of each row without taking into account the &quot;unnamed column&quot; which is the year.<br />
Currently I have:</p>
<pre><code>print(df.mean(axis=0))
</code></pre>
<p>But this just finds the average WITH the year which obviously is a huge outlier and skews the data.</p>
<p>Below is the dataset I am using:</p>
<p><img src=""https://i.stack.imgur.com/6vfnL.png"" alt=""enter image description here"" /></p>
",19505619.0,7117003.0,2022-07-07 19:46:10,2022-07-07 19:56:16,How to find the average of a row in pandas barring one column?,<python><pandas><data-science><data-cleaning>,2,3,N/A,CC BY-SA 4.0
72928972,1,72929139.0,2022-07-10 13:48:00,0,577,"<p>I wonder how to change the names of columns which name has begin on &quot;Unnamed:&quot;.
Want to replace those columns with years from 1960 to 2019.
Have you guys any idea how?</p>
<p><img src=""https://i.stack.imgur.com/7wIez.png"" alt=""enter image description here"" /></p>
",19520743.0,1019850.0,2022-07-10 14:32:37,2022-07-11 07:08:28,Rename of huge number of columns - python / pandas,<python><dataframe><data-science><rename>,1,5,N/A,CC BY-SA 4.0
72932768,1,72934834.0,2022-07-11 00:35:26,0,79,"<p>I have Datframe with Multiindex and I want to transfer it to Json</p>
<p>this is Datframe</p>
<p><a href=""https://i.stack.imgur.com/Zfe5C.jpg"" rel=""nofollow noreferrer"">This is the dataframe</a></p>
<p>When I try to convert it to JSON, and I put to_json() on the orient = index, it converts all the keys to tuple</p>
<p>That's why I tried to put data in this format, but it didn't know how to do it correctly</p>
<pre><code>    data = {
        '2022-01-21': {
            'SF1_8-1':
                {&quot;Lock_Auto&quot;:0,
                 &quot;Lock_Man&quot;:0,
                 &quot;lability_Auto&quot;:0,
                 &quot;lability_Man&quot;:67499,
                 &quot;ANd&quot;:40,
                 &quot;ANR_Remove&quot;:0,
                 &quot;AveI&quot;:11.01470647,
                 &quot;AveLoss&quot;:113.8496936
                },
            'S01_8-2':
                {&quot;Lock_Auto&quot;:0,
                 &quot;Lock_Man&quot;:0,
                 &quot;Unavailability_Auto&quot;:0,
                 &quot;Unavailability_Man&quot;:8475,
                 &quot;ANd&quot;:40,
                 &quot;ANR_Remove&quot;:0,
                 &quot;AveI&quot;:11.01470647,
                 &quot;AveLoss&quot;:113.8496936
                },

            'S01_8-3':
                {&quot;Lock_Auto&quot;:0,
                 &quot;Lock_Man&quot;:0,
                 &quot;Unavailability_Auto&quot;:0,
                 &quot;Unavailability_Man&quot;:8475,
                 &quot;ANd&quot;:40,
                 &quot;ANR_Remove&quot;:0,
                 &quot;AveI&quot;:11.01470647,
                 &quot;AveLoss&quot;:113.8496936
                }
        },
        '2022-01-22': {
            '001_P87-1':
                {&quot;Lock_Auto&quot;:0,
                 &quot;Lock_Man&quot;:0,
                 &quot;Unavailability_Auto&quot;:0,
                 &quot;Unavailability_Man&quot;:8475,
                 &quot;ANd&quot;:40,
                 &quot;ANR_Remove&quot;:0,
                 &quot;AveI&quot;:11.01470647,
                 &quot;AveLoss&quot;:113.8496936
                },

            },
            '001_P-2':
                {&quot;Lock_Auto&quot;:0,
                 &quot;Lock_Man&quot;:0,
                 &quot;Unavailability_Auto&quot;:0,
                 &quot;Unavailability_Man&quot;:8475,
                 &quot;ANd&quot;:40,
                 &quot;ANR_Remove&quot;:0,
                 &quot;AveI&quot;:11.01470647,
                 &quot;AveLoss&quot;:113.8496936
                },

            '001_P-3':
                {&quot;Lock_Auto&quot;:0,
                 &quot;Lock_Man&quot;:0,
                 &quot;Unavailability_Auto&quot;:0,
                 &quot;Unavailability_Man&quot;:8475,
                 &quot;ANd&quot;:40,
                 &quot;ANR_Remove&quot;:0,
                 &quot;AveI&quot;:11.01470647,
                 &quot;AveLoss&quot;:113.8496936
                },
        }
</code></pre>
<ul>
<li>Question 1: Is there a way to transfer Multi_index Dataframe to Json</li>
<li>Question 2: How can I put data like this?</li>
</ul>
",15695523.0,15695523.0,2022-07-11 00:44:54,2022-07-11 07:17:54,MultiIndex dataframe into Json,<javascript><python><json><pandas><data-science>,1,0,N/A,CC BY-SA 4.0
72921024,1,-1.0,2022-07-09 11:35:16,0,323,"<p>I'm writing a small programm for data exploration: I have data of about 3-4 different groups (with subgroups, so probably about 10-20 different pairings) with about 50+ tests (float point number values) each. My goal is to graph and statistically analyze this data. The idea is not to have publication-ready statistics/graphs but to get an initial overview over the data.</p>
<p>I've successfully implemented the normality testing via scipy.shapiro and depending on that either a RM-anova via statsmodels or a Friedman-test via scipy, both followed by tukeyhsd post-hoc testing via statsmodels.
Then a combined boxplot/swarmplot and friedmann/RManova + post-hoc tables are saved as a figure to a folder.
This works fine for most of the data/tests, but I get a few errors like this:</p>
<pre><code>site-packages\scipy\stats\_stats_py.py:7933: RuntimeWarning: divide by zero encountered in
double_scalars chisq = (12.0 / (k*n*(k+1)) * ssbn - 3*n*(k+1)) / c
</code></pre>
<p>or this</p>
<pre><code>statsmodels\sandbox\stats\multicomp.py:1300: RuntimeWarning: invalid value encountered in divide
st_range = np.abs(meandiffs) / std_pairs #studentized range statistic
</code></pre>
<p>I am also missing a few graphs, the number of which corresponds directly to the number of these error messages. Therefore, I assume that the program does not export a graph if this error occurs but skips to the next iteration.</p>
<p>As such I would like to implement some sort of try/except rule where if this error occurs, the data is still graphed (the iteration not ended prematurely).
Thank you in advance for any help!</p>
<p>Sidenote: I also get the warning</p>
<pre><code>scipy\stats\_morestats.py:1758: UserWarning: Input data for shapiro has range zero. The results may not be accurate.
  warnings.warn(&quot;Input data for shapiro has range zero. The results &quot;
</code></pre>
<p>for some Data, but this does not stop the current iteration (and I would like to not go to &quot;except&quot; because of this)</p>
",19496871.0,-1.0,N/A,2022-07-09 12:04:13,Python: How configure 'Try' and 'Except' in for statsmodels RuntimeWarning when 'divide by zero' is encountered,<python><statistics><try-catch><data-science><warnings>,1,0,N/A,CC BY-SA 4.0
72923865,1,72925608.0,2022-07-09 18:48:19,0,150,"<p>I'm writing a program which will show the candlestick chart of Gold and detect patterns. I'm getting the data from yfinance and trying to draw the chart with plotly, but I see that some parts of the data are missing. I checked the data with mplfinance and everything worked successfully, but I need it in plotly.</p>
<pre><code>import plotly.graph_objects as go
import pandas as pd
import yfinance as yf
import talib

import mplfinance as mpf



data = yf.download(tickers=&quot;GC=F&quot;, period=&quot;5d&quot;, interval=&quot;5m&quot;)
fig = go.Figure(data=[go.Candlestick(x=data.index,
            open=data['Open'], high=data['High'],
            low=data['Low'], close=data['Close'])
                 ])

fig.update_layout(xaxis_rangeslider_visible=False)
fig.write_html('first_figure.html', auto_open=True)
</code></pre>
",19517679.0,-1.0,N/A,2022-07-10 01:25:01,Missing part of the data in graph,<python><pandas><plotly><data-science>,2,0,N/A,CC BY-SA 4.0
72935959,1,-1.0,2022-07-11 08:56:26,0,134,"<p>I have created ridge regression model to predict sales of an item say X. My final model contains around 180 features. I have pickled this model and now I want to see how model I created is performing on new data set containing same features as present in different model but on different timeframe.
I have to pass entire new dataset(dataframe) into existing model and check relevant score say r-square or any other score relevant to regression model.</p>
<p>Below is the code I'm using:</p>
<pre><code># loading library
import pickle

    with open('ridge_model', 'wb') as p:
        pickle.dump(ridge, p)
    
    y_test = df.pop('Target')
    x_test = df
    
    # load saved model
    with open('ridge_model' , 'rb') as p:
        new_reg = pickle.load(p)
    
    r_squared = new_reg.score(x_test,y_test)
    print(r-squared)
</code></pre>
<p>Here ridge is the regression model I created</p>
<p>df is the  new data on which prediction needs to be done</p>
<p>y_test contains target variable
x_test contains features in dataset except target variable</p>
<p>I want to understand:</p>
<ol>
<li><p>Can we pass entire new data set onto existing model for prediction and
see how model is performing or not?</p>
</li>
<li><p><code>r_squared = new_reg.score(x_test,y_test)</code> in this line does .score calculates r-square as
calculated on existing model or any other score it calculates?</p>
</li>
</ol>
",18951121.0,4420967.0,2022-07-11 17:48:39,2022-07-11 17:48:39,Prediction on new data after pickling on regression model,<python><machine-learning><data-science><linear-regression>,0,8,N/A,CC BY-SA 4.0
72906923,1,-1.0,2022-07-08 05:13:30,0,9,"<p><a href=""https://i.stack.imgur.com/l9kfc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l9kfc.png"" alt=""enter image description here"" /></a></p>
<p>I have trouble sending my data frame to panelOLS model.
the dataframe didn't using any column as index</p>
<p>then it returns an error massage:
ValueError: Series can only be used with a 2-level MultiIndex</p>
<p>Doeas that mean I need to use double index and use datetime type as my indices?</p>
",10330922.0,10330922.0,2022-07-08 05:15:59,2022-07-08 05:15:59,How to solve this Pandas and PanelOLS indexing problem,<python><pandas><time-series><data-science>,0,3,2022-07-08 05:15:59,CC BY-SA 4.0
72917674,1,72917724.0,2022-07-08 22:45:24,1,81,"<p>I'm trying to loop through columns 10 - 12 of the following data set df2, replacing 1 with a.2. with b. 3 with c and 4 with d</p>
<p>I've figured out how to perform the substitution on a single column using the code below.</p>
<pre><code>df2[df2$DX == 1,]$DX &lt;-'a'
df2[df2$DX == 2,]$DX &lt;-'b'
df2[df2$DX == 3,]$DX &lt;- 'c'
df2[df2$DX == 4,]$DX &lt;- 'd'
df2$DX &lt;- as.factor(df2$DX)
</code></pre>
<p>I'm not sure how to set up the for loop to loop over the other two columns. Thank you for your help.</p>
",11938167.0,-1.0,N/A,2022-07-13 19:00:41,Replacing values in a R dataframe given certain conditions,<r><data-science>,3,0,N/A,CC BY-SA 4.0
72943778,1,72944248.0,2022-07-11 19:36:53,0,35,"<p>I am still learning Data Engeineering and Data Science, new and appreciate any advice.</p>
<p>Long story short, I have managed to create Pandas script which updates already existing csv file into another one I need for further actions. Data sorted out, I lave learned so many things there. Good job. Script:</p>
<pre><code>import pandas as pd
dataset = &quot;csv_file.csv&quot;
df = pd.read_csv(dataset, usecols=[0,1], header=0, names=['title', 'product_id'])
df['title'] = df['title'].str.replace('&quot;', &quot;''&quot;)
df['title'] = '&quot;' + df['title'] + '&quot;'
print(df.head())
df.to_csv(f'edited_csv.csv')
</code></pre>
<p>Now, I have been thinking about automation for running some bulk tasks I have. So, I am working on a small script so I can make numerous API calls where I am creating the product, using the title and product_id from the edited csv file.</p>
<p>I was googling for a few days and found some sources. Until I get approval for testing, I would like to check this one out, to see if I am on the right path, or I should change the way I am thinking and learn some new ways.</p>
<p>The edited csv file looks something like this:</p>
<pre><code>title, product_id
&quot;Name 1&quot;, 12345
&quot;Name 2&quot;, 98765
&quot;Name 3&quot;, 34566
&quot;Nime 4&quot;, 78930
&quot;Name 5&quot;, 99850
</code></pre>
<p>So, I have been implementing another small script for making those API calls. I hope I somehow got the point, but may miss a few things:</p>
<pre><code>api_token = &quot;my_token&quot;
df = pd.read_csv('edited_csv.csv')

    for i in range(len(df)):
        
        endpoint = f&quot;/api/mystore/products&quot;
        request_body = {
            &quot;product_id&quot;: df['product_id'],
            &quot;title&quot;: df['title'],
            &quot;my_discount_type&quot;: &quot;percentage&quot;,
            &quot;my_discount_amount&quot;: 15,
            &quot;store&quot;: &quot;Store3&quot;,
            &quot;versions&quot;: &quot;Black/White/Red&quot;
        }
        }
</code></pre>
<p>So, the goal would be to make products one by one, line by line, until I reach the end.
I was a bit in dilemma if I should use while loop.</p>
<p>Thank you in advance, good people.</p>
",17326438.0,17326438.0,2022-07-11 19:50:53,2022-07-11 20:23:38,How would I generate API call via Python while reading data from csv flle? I am using Pandas library and would need some guidance,<python><pandas><dataframe><rest><data-science>,1,0,N/A,CC BY-SA 4.0
72943817,1,72982355.0,2022-07-11 19:40:01,0,100,"<p><a href=""https://www.glassdoor.com/Job/us-data-scientist-jobs-SRCH_IL.0,2_IN1_KO3,17.htm?srs=JOBS_HOME_RECENT_SEARCHES"" rel=""nofollow noreferrer"">Glassdoor Job Search Link</a></p>
<p>Currently attempting to select the &quot;sector&quot; for a job posting. Not having an luck with xpath or css selector thus far...any assistance would be appreciated! My code is already iterating through each job posting successfully and pulling company name, location, job description etc. My vs code is below.</p>
<p>Original code credit: Omer Sakarya and Ken Jee.</p>
<p>Here are some of my attempts:</p>
<pre><code>'.//span[@class=&quot;css-1ff36h2 e1pvx6aw0&quot;]'

 './/div[@id=&quot;EmpBasicInfo&quot;]//div[@class=&quot;d-flex flex-wrap&quot;]/div[5]/span[@class=&quot;css-1ff36h2 e1pvx6aw0&quot;]'

'.//div[@class=&quot;EmpBasicInfo&quot;]//span[text()=&quot;Sector&quot;]//following-sibling::*'
</code></pre>
<p><a href=""https://i.stack.imgur.com/0Je9b.png"" rel=""nofollow noreferrer"">Glassdoor Job Posting/Sector Element</a></p>
<pre><code>from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException
from selenium import webdriver
import time
import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


def get_jobs(keyword, num_jobs, verbose, path, slp_time):

'''Gathers jobs as a dataframe, scraped from Glassdoor'''

#Initializing the webdriver
options = webdriver.ChromeOptions()

#Uncomment the line below if you'd like to scrape without a new Chrome window every time.
#options.add_argument('headless')

#Change the path to where chromedriver is in your home folder.
driver = webdriver.Chrome(executable_path=path, options=options)
driver.set_window_size(1120, 1000)

url='https://www.glassdoor.com/Job/' + keyword + '-jobs-SRCH_KO0,14.htm'
driver.get(url)
jobs = []

while len(jobs) &lt; num_jobs:  #If true, should be still looking for new jobs.

    #Let the page load. Change this number based on your internet speed.
    #Or, wait until the webpage is loaded, instead of hardcoding it.
    time.sleep(slp_time)

    #Test for the &quot;Sign Up&quot; prompt and get rid of it.
    try:
        driver.find_element(By.CSS_SELECTOR,  '[data-selected=&quot;true&quot;]').click()
    except ElementClickInterceptedException:
        pass

    time.sleep(.1)

    try:
        driver.find_element(By.XPATH,('.//div[@id=&quot;JAModal&quot;]//span[@alt=&quot;Close&quot;]')).click()
    except NoSuchElementException:
        pass

    #Going through each job in this page
    job_buttons = driver.find_elements(By.CSS_SELECTOR,'[data-test=&quot;job-link&quot;]') #jl for Job Listing. These are the buttons were going to click.
    for job_button in job_buttons:  

        print(&quot;Progress: {}&quot;.format(&quot;&quot; + str(len(jobs)) + &quot;/&quot; + str(num_jobs)))
        if len(jobs) &gt;= num_jobs:
            break

        job_button.click()  #You might 
        time.sleep(1)
        collected_successfully = False
        
        while not collected_successfully:
            try:
                company_name = driver.find_element(By.XPATH,'.//div[@class=&quot;css-xuk5ye e1tk4kwz5&quot;]').text
                location = driver.find_element(By.XPATH,'.//div[@class=&quot;css-56kyx5 e1tk4kwz1&quot;]').text
                job_title = driver.find_element(By.XPATH,'.//div[contains(@class, &quot;css-1j389vi e1tk4kwz2&quot;)]').text
                job_description = driver.find_element(By.XPATH,'.//div[@class=&quot;jobDescriptionContent desc&quot;]').text
                collected_successfully = True
            except:
                time.sleep(5)

        try:
            salary_estimate = driver.find_element(By.XPATH,'.//span[@class=&quot;css-1hbqxax e1wijj240&quot;]').text
        except NoSuchElementException:
            salary_estimate = -1 #You need to set a &quot;not found value. It's important.&quot;
        
        try:
            rating = driver.find_element(By.CSS_SELECTOR,'[data-test=&quot;detailRating&quot;]').text
        except NoSuchElementException:
            rating = -1 #You need to set a &quot;not found value. It's important.&quot;

        #Printing for debugging
        if verbose:
            print(&quot;Job Title: {}&quot;.format(job_title))
            print(&quot;Salary Estimate: {}&quot;.format(salary_estimate))
            print(&quot;Job Description: {}&quot;.format(job_description[:500]))
            print(&quot;Rating: {}&quot;.format(rating))
            print(&quot;Company Name: {}&quot;.format(company_name))
            print(&quot;Location: {}&quot;.format(location))

        #Going to the Company tab...
        #clicking on this:
        #&lt;div class=&quot;tab&quot; data-tab-type=&quot;overview&quot;&gt;&lt;span&gt;Company&lt;/span&gt;&lt;/div&gt;
        try:
            driver.find_element(By.XPATH,'.//div[@class=&quot;tab&quot; and @data-tab-type=&quot;overview&quot;]').click()

            try:
                #&lt;div class=&quot;infoEntity&quot;&gt;
                #    &lt;label&gt;Headquarters&lt;/label&gt;
                #    &lt;span class=&quot;value&quot;&gt;San Francisco, CA&lt;/span&gt;
                #&lt;/div&gt;
                headquarters = driver.find_element(By.XPATH,'.//div[@class=&quot;infoEntity&quot;]//label[text()=&quot;Headquarters&quot;]//following-sibling::*').text
            except NoSuchElementException:
                headquarters = -1

            try:
                size = driver.find_element(By.XPATH,'.//div[@id=&quot;EmpBasicInfo&quot;]//div[@class=&quot;d-flex flex-wrap&quot;]/div[1]/span[@class=&quot;css-1ff36h2 e1pvx6aw0&quot;]').text
            except NoSuchElementException:
                size = -1

            try:
                founded = driver.find_element(By.XPATH,'.//div[@class=&quot;css-1pldt9b e1pvx6aw1&quot;]//span[text()=&quot;Founded&quot;]//following-sibling::*').text
            except NoSuchElementException:
                founded = -1

            try:
                type_of_ownership = driver.find_element(By.XPATH,'.//div[@class=&quot;infoEntity&quot;]//label[text()=&quot;Type&quot;]//following-sibling::*').text
            except NoSuchElementException:
                type_of_ownership = -1

            try:
                industry = driver.find_element(By.XPATH,'.//div[@id=&quot;EmpBasicInfo&quot;]//div[@class=&quot;d-flex flex-wrap&quot;]/div[4]/span[@class=&quot;css-1ff36h2 e1pvx6aw0&quot;]').text
            except NoSuchElementException:
                industry = -1

            try:
                sector = driver.find_element(By.XPATH,&quot;.//div[@id='EmpBasicInfo']//div[@class='d-flex flex-wrap']/div[5]/span[@class='css-1pldt9b e1pvx6aw1']//following-sibling::*&quot;).text
            except NoSuchElementException:
                sector = -1

            try:
                revenue = driver.find_element(By.XPATH,'.//span[@class=&quot;css-1ff36h2 e1pvx6aw0&quot;]').text
            except NoSuchElementException:
                revenue = -1

            try:
                competitors = driver.find_element(By.XPATH,'.//div[@class=&quot;infoEntity&quot;]//label[text()=&quot;Competitors&quot;]//following-sibling::*').text
            except NoSuchElementException:
                competitors = -1

        except NoSuchElementException:  #Rarely, some job postings do not have the &quot;Company&quot; tab.
            headquarters = -1
            size = -1
            founded = -1
            type_of_ownership = -1
            industry = -1
            sector = -1
            revenue = -1
            competitors = -1

            
        if verbose:
            print(&quot;Headquarters: {}&quot;.format(headquarters))
            print(&quot;Size: {}&quot;.format(size))
            print(&quot;Founded: {}&quot;.format(founded))
            print(&quot;Type of Ownership: {}&quot;.format(type_of_ownership))
            print(&quot;Industry: {}&quot;.format(industry))
            print(&quot;Sector: {}&quot;.format(sector))
            print(&quot;Revenue: {}&quot;.format(revenue))
            print(&quot;Competitors: {}&quot;.format(competitors))
            print(&quot;@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@&quot;)

        jobs.append({&quot;Job Title&quot; : job_title,
        &quot;Salary Estimate&quot; : salary_estimate,
        &quot;Job Description&quot; : job_description,
        &quot;Rating&quot; : rating,
        &quot;Company Name&quot; : company_name,
        &quot;Location&quot; : location,
        &quot;Headquarters&quot; : headquarters,
        &quot;Size&quot; : size,
        &quot;Founded&quot; : founded,
        &quot;Type of ownership&quot; : type_of_ownership,
        &quot;Industry&quot; : industry,
        &quot;Sector&quot; : sector,
        &quot;Revenue&quot; : revenue,
        &quot;Competitors&quot; : competitors})
        #add job to jobs

    #Clicking on the &quot;next page&quot; button
    try:
        driver.find_element(By.CSS_SELECTOR, &quot;[alt='next-icon']&quot;).click
    except NoSuchElementException:
        print(&quot;Scraping terminated before reaching target number of jobs. Needed {}, got {}.&quot;.format(num_jobs, len(jobs)))
        break

return pd.DataFrame(jobs)  #This line converts the dictionary object into a pandas DataFrame.
</code></pre>
",19518784.0,19518784.0,2022-07-12 22:50:41,2022-07-14 14:40:29,Difficulty Selecting Element using Selenium,<python><selenium><web-scraping><data-science>,2,5,N/A,CC BY-SA 4.0
72943838,1,-1.0,2022-07-11 19:42:06,0,20,"<p>I know that the title could be a Little confussing, but basically this is what I want.</p>
<p>Lets supposse I have a a dataframe of this form</p>
<pre><code>P_SEXO P_EDADR COD_ENCUESTAS
   &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;
1      2      16            74
2      2       9            75
3      2      18            75
4      2      11            75
5      2      11            76
6      2       1            76
</code></pre>
<p>As you can see, theres some values that repeats on <code>COD_ENCUESTAS</code>, this is because its like an índex in another dataframe (df2).</p>
<p>Df2 have a form like this:</p>
<pre><code>COD_ENCUESTAS VA1_ESTRATO
          &lt;dbl&gt;       &lt;dbl&gt;
1           74          24
2           75          23
3           76          12
4           77           23
5           78           14
6           79          11
</code></pre>
<p>I want output:</p>
<pre><code>P_SEXO P_EDADR COD_ENCUESTAS    VA1_ESTRTO 
   &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;
1      2      16            74    24
2      2       9            75    23
3      2      18            75    23
4      2      11            75    23
5      2      11            76    12
6      2       1            76    12
</code></pre>
<p>Thnaks beforehand
-lasagna</p>
",18473320.0,-1.0,N/A,2022-07-11 20:17:30,Match data between dataframes columns and then paste the data avaliable on one of them,<r><data-science>,1,2,N/A,CC BY-SA 4.0
72943939,1,72944164.0,2022-07-11 19:51:17,0,249,"<p>I was trying to use ggplot2 to plot multiple lines in a window. this is my code here:</p>
<pre><code>ggplot(type1_age, aes(x = Year, y = Value, group = Age, color = Age)) + geom_line()+ggtitle('The percentage distribution of type1 diabetes patients in different age groups')+ylab(&quot;percentage (%)&quot;)
</code></pre>
<p>type1_age file looks like this:
<a href=""https://i.stack.imgur.com/6seGU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6seGU.png"" alt=""enter image description here"" /></a>
the result figure is this:
<a href=""https://i.stack.imgur.com/IgzDm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IgzDm.png"" alt=""enter image description here"" /></a></p>
<p>the problem is that y-axis in the result figure is not in order. can you please help me to figure out? Thanks!</p>
",15525911.0,13513328.0,2022-07-11 19:56:19,2022-07-11 20:13:05,how to make the y-axis value in order when using ggplot2 in R language,<r><ggplot2><data-science>,1,4,N/A,CC BY-SA 4.0
72948209,1,-1.0,2022-07-12 06:58:03,2,218,"<p>I am trying to make a cluster to analyze DNA sequences and find the less matched patterns among them (for say &lt;25% match).
Is it possible to perform cluster analysis (k-means or any other approach) for the DNA in a fasta file.</p>
<p>for example I want to generate a cluster graph (image) for the given data (below is the fasta sequences) and find the less similar (&lt;25%) sequence (encircled dots)</p>
<pre><code>&gt;1
 GATGTACTTCGTTCAGTTACATTGCTGCTCAAGGAGATTTTCAACGTGAAAAAAATTATTATTCGCAATTCCTTTAGTTGTTCCTT
 &gt;2
 CAGTGTACTTCGTTCAGTTACGTATTGCTGCTCAAGGAGATTTTCAACGTGAAAAAATTATTATTCGCAATTCCTTTAGTTTGTTC
 &gt;3
 CGGTATTACTTCGTTCAGTTACGTATTATGCTCGAAAGGAATTTCTATTGAAAGGTATTGCAATTCCTTTAGTTGTTCCTTTCTAT
 &gt;4
 CGGTGTACTTCGTTCAGTTACGTATTGCTGCTCAAGGAGATTTTCAACGTGAAAAAAATTATTATTCGCAATTCCTTTAGTTGTTC
 &gt;5
 GATGTACTTCGTTCCAGTTGTGTGTGCTGCTCAAGGAGATTTTTCAACGTGAAAAAATTATTATTCGCAATTCAACTTTGAATTTG
 &gt;6
 CAATGTACTTCGTTCGGTTACGTATTGCTGCTCAAGAGATTTTCAACGTGAAAAAAATTATTATTCGCAATTCCTTTAGTTGTTCC
 &gt;7
 CAAACACTTCGTTCAGTTACGTATTGCTGCTCAAGGAGATTTTCAACGTGAAAAAAATTATTATTCGCAATTCCTTTAGTTGTTCC 
</code></pre>
<p><a href=""https://i.stack.imgur.com/NMwEN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NMwEN.png"" alt=""enter image description here"" /></a></p>
",16933406.0,-1.0,N/A,2022-07-12 06:58:03,clustering of a fasta file having DNA sequences to find the most unmatched clone,<python><data-science><cluster-analysis><biopython>,0,1,N/A,CC BY-SA 4.0
72949405,1,72949672.0,2022-07-12 08:44:20,-1,66,"<p>I want to convert the following DataFrame</p>
<p><a href=""https://i.stack.imgur.com/CNttz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CNttz.png"" alt=""Data Frame 1"" /></a></p>
<p>into a new DataFrame</p>
<p><a href=""https://i.stack.imgur.com/iwwHR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iwwHR.png"" alt=""enter image description here"" /></a></p>
<p>using pandas unstack function. Please help me out?</p>
",8221992.0,-1.0,N/A,2022-07-12 09:18:04,How to convert pandas dataframe using Unstack?,<python><dataframe><data-science>,1,2,N/A,CC BY-SA 4.0
72936261,1,-1.0,2022-07-11 09:21:46,-1,379,"<p>I have a data frame that looks like this -</p>
<pre><code>License_type | User
A1           |  U1
A2           |  U1
A2           |  U1
A2           |  U1
A3           |  U1
A4           |  U1
A1           |  U2
A2           |  U2
A2           |  U2
A2           |  U2
A2           |  U3
A4           |  U3
</code></pre>
<p>I want to create a stacked histogram where for each User, the licenses types are stacked...</p>
<p><img src=""https://i.stack.imgur.com/2JJmB.png"" alt=""example"" /></p>
",12041890.0,4238408.0,2022-07-11 09:27:05,2022-07-11 09:47:18,How to created a stacked histogram from a dataframe for two related columns?,<python><pandas><matplotlib><data-science>,3,0,N/A,CC BY-SA 4.0
72936682,1,-1.0,2022-07-11 09:54:18,1,100,"<p>From <code>sklearn.model_selection</code> family I have imported <code>train_test_split</code> and I want to train my model and test the model in order to predict variable <code>y</code>.</p>
<p>I assigned string data type as my <code>X</code> (features/variable of my dataset) and my <code>y</code> is an integer dataset (response).</p>
<p>After doing that I have imported <code>LinearRegression</code> function/method from <code>sklearn.linear_model</code> family, now when I try to fit the model it displays an error</p>
<pre><code>can’t convert strings(X) to variable y
</code></pre>
<p>Why?</p>
<pre><code>X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area 
         Number of Rooms',
        'Avg. Area Number of Bedrooms', 'Area Population', 'Price', 
        'Address']]   
                                
y = df['Price']                                                                                                                    
</code></pre>
<pre><code>from sklearn.model_selection import train_test_split 
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.40 , random_state=101)         
</code></pre>
<pre><code>from sklearn.linear_model import LinearRegression

lm = LinearRegression()                                                                                                        
lm.fit(X_train,y_train)
</code></pre>
",19279743.0,11989081.0,2022-07-11 11:09:55,2022-07-11 11:09:55,I can’t fit my model using linear_model.fit,<python><scikit-learn><data-science>,1,3,N/A,CC BY-SA 4.0
72952529,1,-1.0,2022-07-12 12:42:39,-1,134,"<p>I am writing a Google Bigquery for doing RFM analysis by using following sql queries but getting errors. Unable to calculate the Recency and Bigquery is not recognizing Day in DAY_DFF Function.
Your help will be appreciated!</p>
<pre class=""lang-sql prettyprint-override""><code>    -- Customer_ID = vstr_id, Order_ID = vst_id, Order_Date = evnt_dt, Sales = tot_sale_amt
    --STEP 1: FILTER THE DATASET
    WITH dataset AS (
        SELECT vstr_id, 
               vst_id, 
               evnt_dt, 
               tot_sale_amt
        FROM item_purchase
        WHERE evnt_dt &lt; CURRENT_DATE - 17
    ),

    -- STEP 3: SUMMARIZE THE DATASET

    Order_Summary as (
        SELECT vstr_id, vst_id, evnt_dt,
            SUM(tot_sale_amt) as tot_sale_amt
        FROM dataset
        GROUP BY vstr_id, vst_id, evnt_dt
    )

    -- Step 4: Put together the RFM Report

    SELECT 
    t1.vstr_id,
    --(SELECT MAX(evnt_dt) FROM Order_Summary) as max_order_date,
    --(SELECT MAX(evnt_dt) FROM Order_Summary) WHERE vstr_id = t1.vstr_id) as max_customer_order_date
    -- Now we want to substract max_customer_order_date from max_order_date.
    DATE_DIFF(day, (SELECT MAX(evnt_dt) FROM Order_Summary WHERE vstr_id = t1.vstr_id), (SELECT 
    MAX(evnt_dt) FROM Order_Summary)) as Recency,

    COUNT(t1.vst_id) as Frequency,
    SUM(t1.tot_sale_amt) as Monetary
    FROM Order_Summary t1
    GROUP BY t1.vstr_id
    ORDER BY 1,3 DESC
</code></pre>
",15010665.0,15010665.0,2022-07-12 13:04:01,2022-07-12 13:14:41,Getting error in finding out Recency in Bigquery(SQL)? (RFM Analysis),<sql><database><google-bigquery><data-science><data-analysis>,1,3,N/A,CC BY-SA 4.0
72955632,1,-1.0,2022-07-12 16:35:55,1,68,"<p>I'm currently helping a client identify fraud activity using login activity. I've boiled the data down to a specific week and aggregated over day.
for example:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>IP Address</th>
<th>Total Login Attempts</th>
<th>Login Failure</th>
</tr>
</thead>
<tbody>
<tr>
<td>7/11/2022</td>
<td>1.256.xxx.xxx</td>
<td>1000</td>
<td>.5</td>
</tr>
<tr>
<td>7/12/2022</td>
<td>1.256.xxx.xxx</td>
<td>50000</td>
<td>.2</td>
</tr>
<tr>
<td>7/11/2022</td>
<td>1.126.xxx.xxx</td>
<td>1000</td>
<td>.5</td>
</tr>
</tbody>
</table>
</div>
<p>IP address and Date are the primary keys in this table.
Would it be appropriate to randomly sample the Total Login Attempts or Login Failure per day to create a normal distribution to determine outliers in the dataset?
Is there a rule in statistics that would stop this form being effective? i.e. the timeseries components not being independent</p>
",19535992.0,14909621.0,2022-07-18 18:30:00,2022-07-18 18:30:00,Statistics and random sampling of daily data over a week,<python><statistics><data-science>,0,2,N/A,CC BY-SA 4.0
72868341,1,-1.0,2022-07-05 11:03:09,0,311,"<p>I am trying to offset a date by 1 month from the following code</p>
<pre><code>import pandas as pd
import datetime
from pandas.tseries.offsets import DateOffset
ts = pd.to_datetime('2017-03-30')
ts=ts + DateOffset(months=1)
ts.date()
</code></pre>
<p>the answer comes out to be <code>datetime.date(2017, 4, 30)</code></p>
<p>and for the</p>
<pre><code>ts = pd.to_datetime('2017-03-31')
ts=ts + DateOffset(months=1)
ts.date()
</code></pre>
<p>the answer is same <code>datetime.date(2017, 4, 30)</code>
why is this happening ? I am trying to offset one month date for <code>pd.date_range('2022-03-27',end='2022-06-04',freq='D')</code></p>
",18168227.0,-1.0,N/A,2022-07-05 11:03:09,Pandas dateoffset giving wrong offset month,<python><pandas><dataframe><datetime><data-science>,0,3,N/A,CC BY-SA 4.0
72898611,1,-1.0,2022-07-07 13:21:05,-1,63,"<p>I want to add coordinates to this cities, but the dataframe does not have one..</p>
<p>group_metro_manila = metro_manila.groupby(['City'])[['Price (PHP)', 'Floor_area (sqm)', 'Price Per SQM']].mean().sort_values('Price (PHP)')</p>
<p>So I manually created them...</p>
<p>city_location = pd.DataFrame({
'City': ['Dasmarinas', 'General Trias', 'Imus', 'Las Pinas', 'Makati', 'Mandaluyong','Manila', 'Muntinlupa','Paranaque','Pasay', 'Pasig','Quezon City', 'Silang', 'Taguig'],
'Latitude': [14.2990, 14.3214, 14.4064, 14.4445, 14.5547, 14.5794, 14.5995, 14.4081, 14.4793, 14.5378, 14.5764, 14.6760, 14.2142, 14.5176],
'Longitude': [120.9590, 120.9073, 120.9405, 120.9939, 121.0244, 121.0359, 120.9842, 121.0415, 121.0198, 121.0014, 121.0851, 121.0437, 120.9687, 121.0509]
})</p>
<p><a href=""https://i.stack.imgur.com/rMDD8.png"" rel=""nofollow noreferrer"">group_metro_manila</a></p>
<p>I want to add the city_location latitude and longitude to their designated city
<a href=""https://i.stack.imgur.com/R3vpc.png"" rel=""nofollow noreferrer"">city_location</a></p>
<p>So what I do is I join city_location to group_metro_manila</p>
<p>merge_df = group_metro_manila.join(city_location[['City','Latitude','Longitude']].set_index('City'), on='City')</p>
<p>but the values becomes NaN</p>
<p><a href=""https://i.stack.imgur.com/aHVxr.png"" rel=""nofollow noreferrer"">Join but values becomes NaN</a></p>
<p>can someone enlighten me on how to add columns without making the values NaN?
I am new to pandas and want to become data analyst, please can someone help me correct my code? Thank you.</p>
",19166909.0,19166909.0,2022-07-07 15:57:43,2022-07-08 20:57:31,Pandas join values become NaN,<python><pandas><data-science>,3,0,N/A,CC BY-SA 4.0
72929694,1,-1.0,2022-07-10 15:32:05,0,94,"<p>Java has the <code>@override</code> annotation which only applies for overriden methods. and it gives a compiler error if you apply it for something else.</p>
<p>In my project I am trying to use some thing similar. Since the project uses recursion too much. I created an annotation called <code>@recurisve</code> and I add it before recursive functions</p>
<p>It won't be a big deal but I'd love if there's a way to make it actually detect recursive functions</p>
",17945917.0,1019850.0,2022-07-10 16:53:04,2022-07-10 17:08:37,Adding an annotation that can only apply for recursive methods,<java><recursion><annotations><data-science>,1,3,N/A,CC BY-SA 4.0
72957829,1,-1.0,2022-07-12 20:06:00,0,32,"<p>I've faced a challange to join two dataframes in one.</p>
<p>For example, I've 2 dataframes:</p>
<ol>
<li>The first df (df1) is the base with more than 300K lines, with multiple dates for every ID.</li>
</ol>
<p>e.g:</p>
<p>ID |   date    |<br/>
001| 01-01-2021|<br/>
001| 02-01-2021|<br/>
001| 03-01-2021|<br/>
001| 04-01-2021|<br/>
001| 05-01-2021|<br/></p>
<p>...<br/></p>
<p>002| 01-01-2021|<br/>
002| 02-01-2021|<br/>
002| 03-01-2021|<br/></p>
<ol start=""2"">
<li>The second df (df2) is where I have some extra info that I should join on the first data frame (not so big as the first one, 40k lines)</li>
</ol>
<p>e.g:</p>
<p>ID | start_date | end_date  | status     |<br/>
001| 01-01-2021 | 02-01-2021| working    |<br/>
001| 02-02-2021 | 01-03-2021| not working|<br/></p>
<p>The challenge is to identify all the status on df1 based on the id and the start and end date of df2..</p>
<p>Basically, if ID_df1 == ID_df2 and date_df1 &gt;= start_date &amp; date_df1 &lt;= end_date, so I've to capture the status of df2, which one in this case is &quot;Working&quot;</p>
<p>To solve this issue, I've created a function called status:</p>
<pre><code>def status (df2, id, date):
    pos = -1
    for i in range(len(df2)):
        if ((df2.iat[i,0] == id) &amp; (pd.to_datetime(df2.iat[i,1] &lt;= date) &amp; (pd.to_datetime(df2.iat[i,2] &gt;= date):
           pos = i
        break
    if pos &gt; -1:
       return (df2.iat[pos,3])
    else:
       return &quot;Not Found&quot;
</code></pre>
<p>And my issue is to apply for the function &quot;status&quot; on every df1 data.</p>
<p>I've tried to create a column in blank and apply the function on a  loop:</p>
<pre><code>df1['Status'] = &quot;&quot;

for i in range(len(df1)):
    df1.[i,2] = status(df2, df1[i,0], df1[i,1])
</code></pre>
<p>It worked for me, but it took a loooot of time.. for the 300K, I've covered the first 4k in one hour.</p>
<p>Is there an easier way to do that?</p>
<p>Thank you very much!</p>
",14507861.0,-1.0,N/A,2022-07-12 20:42:59,How to apply a multiple argument function on a large dataframe with pandas?,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
72942037,1,-1.0,2022-07-11 16:53:46,1,48,"<p>I have two large databases, but I want to know how much of the data its already in the other, those values are random in the rows, so I need a filter that indícates what of the values in a df1$column are in the df2$column.</p>
<p>For example, lets Think this like vectors.
I have two vectors</p>
<pre><code>a=c(&quot;q&quot;,&quot;w&quot;,&quot;e&quot;,&quot;r&quot;,&quot;t&quot;,&quot;y&quot;,&quot;u&quot;,&quot;i&quot;,&quot;o&quot;)
b=c(&quot;o&quot;,&quot;u&quot;,&quot;y&quot;,&quot;t&quot;,&quot;r&quot;,&quot;e&quot;,&quot;w&quot;,&quot;q&quot;,&quot;a&quot;)
</code></pre>
<p>I want an output thats says the ones from b that are NOT in a, for example</p>
<pre><code>&gt;&quot;i&quot;
</code></pre>
<p>Hope this is understandable</p>
",18473320.0,18473320.0,2022-07-11 17:11:09,2022-07-11 17:20:17,How to know if its findable a value thats in a column in another column?,<r><data-science>,3,2,N/A,CC BY-SA 4.0
72969835,1,-1.0,2022-07-13 16:45:54,0,33,"<p>I have a dataframe example shown below. I need to add a new category 3 in Facility and a new category 4 in product. For the added categories i would like to either add NA to the mean or zero.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">name</th>
<th style=""text-align: center;"">product</th>
<th style=""text-align: right;"">Facility</th>
<th style=""text-align: right;"">mean</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0.5</td>
</tr>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">0.3</td>
</tr>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0.2</td>
</tr>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0.1</td>
</tr>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">2.5</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2.3</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">5.2</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1.2</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">8.0</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2.3</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">0.9</td>
</tr>
</tbody>
</table>
</div>
<p>Can you please assist me on how i can add the new categories? I cannot seem to figure it out.</p>
",16941310.0,-1.0,N/A,2022-07-13 17:24:10,I have a data frame that I need to add 2 new categories in existing columns using python,<python><pandas><dataframe><data-science>,1,1,N/A,CC BY-SA 4.0
72978027,1,-1.0,2022-07-14 09:11:30,-1,72,"<p>I have been training a temperature dataset with TCN model. I Have tested it on small data available. I am training a bivariate TCN model with the data which I am giving input is Maximum temperature and minimum temperature to predict maximum temperature.  I want to know if this is overfitting or if the graph is right <a href=""https://i.stack.imgur.com/Amv50.png"" rel=""nofollow noreferrer"">Graph here</a></p>
<p>Below is my model</p>
<pre><code>i = Input(shape=(lookback_window, 2))
m = TCN(nb_filters = 64)(i)
m = Dense(3, activation='linear')(m)

model = Model(inputs=[i], outputs=[m])
model.summary()
</code></pre>
<p>The summary of the model is <a href=""https://i.stack.imgur.com/Hxq0D.png"" rel=""nofollow noreferrer"">given here</a></p>
",18356738.0,18356738.0,2022-07-14 09:52:48,2022-07-14 10:37:17,TCN model result seems like offset by one time step,<machine-learning><keras><deep-learning><data-science><overfitting-underfitting>,1,0,N/A,CC BY-SA 4.0
72984094,1,72985165.0,2022-07-14 16:52:33,2,2593,"<p>I am using a Q-Q Plot to test if the residuals of my linear regression follow a normal distribution but the result is a vertical line.</p>
<p>It looks like linear regression is a pretty good model for this dataset, so <strong>shouldn't the residuals be normally distributed</strong>?</p>
<p><a href=""https://i.stack.imgur.com/wkX9Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wkX9Z.png"" alt=""The regression line and the non-normalized points used to predict."" /></a></p>
<p><a href=""https://i.stack.imgur.com/kQwC8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kQwC8.png"" alt=""Residual Plot"" /></a></p>
<p><a href=""https://i.stack.imgur.com/IoDct.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IoDct.png"" alt=""Q-Q Plot"" /></a></p>
<p>The points were created randomly:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

x_values = np.linspace(0, 5, 100)[:, np.newaxis]
y_values = 29 * x_values + 30 * np.random.rand(100, 1)
</code></pre>
<p>Then, I fitted a Linear Regression model:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.linear_model import LinearRegression

reg = LinearRegression()
reg.fit(x_values, y_values)
predictions = reg.predict(x_values)
residuals = y_values - predictions
</code></pre>
<p>Finally, I used the statsmodel module to plot the Q-Q Plot of the residuals:</p>
<pre class=""lang-py prettyprint-override""><code>fig = sm.qqplot(residuals, line='45')
</code></pre>
",19550950.0,19550950.0,2022-07-14 22:21:37,2022-07-14 22:21:37,Why is my Normal Q-Q Plot of residuals a vertical line?,<python><data-science><linear-regression>,2,0,N/A,CC BY-SA 4.0
72984111,1,-1.0,2022-07-14 16:53:30,0,189,"<p>I'm a novice at DAX language, and I'm attempting to create a measure which displays the accumulative sum of records by day; problem is, instead of showing the sum of each record, I see the same results of the column ID, which is the one i'm using ot calculate it (meaning: nothing changes :)</p>
<pre><code>ID_Cum_IF = 
var vMaxDate = CALCULATE(Max([DATA_Subscription]), ALL(Calendar[Date]))
//This Calendar date is an aux table with the dates same as from my original table

var vDateToday = MAX(Calendar[Date]

var RESULT = 
IF(
        vDateToday &lt;= vDataMaxDate,

    CALCULATE(
        [Value_Acum_sum], 
        FILTER(ALL(Calendar), Calendar[Date] &lt;= vDateToday)
    ),[Value_Acum_sum]
)

RETURN RESULT
</code></pre>
<p>So the result I'm expecting is, in the last column, instead of seeing the same numbers as of the other measure I created (Total Count/Day) or the ID column, I need to see
1 in the first line
3 in the second line
8 in the third.....</p>
<p><a href=""https://i.stack.imgur.com/1t8PI.png"" rel=""nofollow noreferrer"">Table I'm using to see if what I'm doing is working x aux table</a></p>
<p>Any ideas of what's wrong with my code?
I made it using some online tutorials, tweaked it a little bit, but I think something is missing here.</p>
<p>Thanks for the experts for the help with this! :D</p>
",19550980.0,-1.0,N/A,2022-07-14 16:53:30,Cummulative SUM Measure in DAX is not working,<powerbi><data-science><dax><data-analysis><data-manipulation>,0,6,N/A,CC BY-SA 4.0
72987843,1,-1.0,2022-07-15 00:13:00,0,135,"<p>I want to create a new column called ('Zoning Status) from the selection of 3 columns, namely the column (RT, RW, Kelurahan). For example, you want to search for Kelurahan 'Batu Ampar', RT 12 RW 2, RT 15 RW 5, RT 7 RW 4. Then the value will be zonasi_1 in the column ('zoning status'). Then Kelurahan 'Batu Ampar', RT 1 RW 4, RT 9 RW 3, RT 6 RW 5 then the value becomes zonation_2. Apart from the 2 conditions above, the value becomes zonation_3. I've tried using for and if else but it's not what I expected. sorry if my english is very bad. thank you before</p>
<pre><code>status_zonasi = []
for x,y,z in zip(df_51['RT'], df_51['RW'], df_51['Kelurahan']):
    if x in [12, 15, 7, 11, 11, 10, 9 ,14] and y == [2, 5, 4, 4, 2, 2, 2, 2] and z == ['Batu Ampar']:
        c = 'Zonasi 1'
    elif x in [1, 9 , 6, 5, 8, 15, 17, 1, 10, 13, 4, 6, 12] and y == [4, 3, 5, 5, 2, 2, 2, 6, 4, 4, 4, 4, 4] and z == ['Batu Ampar']:
        c = 'Zonasi 2'
    else:
        c = 'Zonasi 3'
    status_zonasi.append(c)
</code></pre>
<p><a href=""https://i.stack.imgur.com/sMtrT.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",19477773.0,19477773.0,2022-07-15 02:26:27,2022-07-15 11:52:52,create a new column from 3 existing columns (python pandas),<python><pandas><data-science><data-analysis><feature-engineering>,1,4,N/A,CC BY-SA 4.0
72953840,1,-1.0,2022-07-12 14:18:10,0,669,"<p>I need to get the salary range from span tag. I tried using find().text but doesn't work as the span tag has other tags in it.</p>
<pre><code>job_list = soup.find_all(&quot;div&quot;, class_=&quot;d-flex flex-column pl-sm css-1buaf54 job-search-key- 1mn3dn8 e1rrn5ka0&quot;)

for job in job_list:
   salary = job.find(&quot;span&quot;, class_=&quot;job-search-key-1hbqxax e1wijj240&quot;)
   print(salary)
</code></pre>
<p>The output I have:</p>
<p><a href=""https://i.stack.imgur.com/1RQ9D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1RQ9D.png"" alt=""enter image description here"" /></a></p>
<p>₹362,870 - ₹955,252 is my desired output</p>
",13389656.0,10216112.0,2022-07-12 14:34:44,2022-07-12 14:34:44,Get first text inside a tag using beautiful soup,<python><web-scraping><beautifulsoup><data-science>,1,2,N/A,CC BY-SA 4.0
72958368,1,-1.0,2022-07-12 21:04:17,0,114,"<p>Hello Stack overflow community;
I am working in a scholar project using Neo4j database and i need help from members which are worked before with neo4j gds in order to finding a solution for my problem;
i want to apply a community detection algorithm called &quot;Newman-Girvan&quot; but it doesn't exist any algorithm with this name in neo4j gds library; i found an algorithm called &quot;Modularity Optimization&quot;, is it the Newman-Girvan algorithm and just the name is changed or it is a different algorithm?
Thanks in advance.</p>
",18640989.0,18640989.0,2022-07-14 18:06:54,2022-07-14 18:06:54,Neo4j Community detection,<neo4j><graph-data-science>,1,0,N/A,CC BY-SA 4.0
72976488,1,-1.0,2022-07-14 07:03:36,1,27,"<p>I want to group consecutive growth and falls in pandas series. I have tried this, but it seems not working:</p>
<pre><code>consec_rises = self.df_dataset.diff().cumsum()
group_consec = consec_rises.groupby(consec_rises)
</code></pre>
<p>My dataset:</p>
<pre><code>date
2022-01-07     25.847718
2022-01-08     29.310294
2022-01-09     31.791339
2022-01-10     33.382136
2022-01-11     31.791339
2022-01-12     29.310294
2022-01-13     25.847718
2022-01-14     21.523483
2022-01-15     16.691068
2022-01-16     11.858653
2022-01-17      7.534418
</code></pre>
<p>I want to get result as following:</p>
<pre><code>Group #1 (consecutive growth)
2022-01-07     25.847718
2022-01-08     29.310294
2022-01-09     31.791339
2022-01-10     33.382136
    
Group #2 (consecutive fall)
2022-01-12     29.310294
2022-01-13     25.847718
2022-01-14     21.523483
2022-01-15     16.691068
2022-01-16     11.858653
2022-01-17      7.534418
</code></pre>
",18134326.0,-1.0,N/A,2022-07-14 09:09:00,Group consecutive rises and falls using Pandas Series,<python><pandas><pandas-groupby><data-science><series>,1,0,N/A,CC BY-SA 4.0
72996952,1,-1.0,2022-07-15 16:19:58,0,43,"<p>I have deployed my Flask App in Heroku and the HTML,CSS pages were loaded from the domain created.</p>
<p>In my local machine, I have tried the functionalities with the URL <pre>
<a href=""http://127.0.0.1:5000"" rel=""nofollow noreferrer"">http://127.0.0.1:5000</a> </pre></p>
<p>But In order to make it work after deployment in Heroku, I tried using use_for, which I can use it for rendering css,js files in html.</p>
<p>I couldn't find any possibilities to use that in .js file for calling the routing function defined in the python file.</p>
<p>Hereby Attached my .js code,</p>
<pre><code>    function sendResponse(rawText) {
        var url = &quot;http://127.0.0.1:5000/get_response&quot;
        $.post(url,{
            user_text:rawText
        },function(data, status){
        if(data !== &quot;undefined&quot;){
          console.log(data['bot_response'`enter code here`])
          var botHtml = '&lt;p class=&quot;botText&quot;&gt;&lt;span&gt;' + data['bot_response'] + '&lt;/span&gt; &lt;/p&gt;';
            $(&quot;#chatbox&quot;).append(botHtml);
            document.getElementById('userInput').scrollIntoView({block: 'start', behavior: 
            'smooth'});
            return data['bot_response']
            
        }
    })
  }


 
</code></pre>
",14497974.0,-1.0,N/A,2022-07-15 16:19:58,How to use url_for in .js function with FLASK App?,<python><flask><heroku><data-science>,0,5,N/A,CC BY-SA 4.0
72988186,1,-1.0,2022-07-15 01:28:42,0,85,"<p>I'm trying to segment a csv file of names and emails. The file has first_name, last_name, email, and email domain. I need to classify them by 'business' or 'individual' email.</p>
<p>the data looks like this:
<a href=""https://i.stack.imgur.com/er4yD.png"" rel=""nofollow noreferrer"">CSV File</a></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = pd.read_csv(&quot;C:\\Users\\Path\\to\\file\\5.csv&quot;)  

domains =['gmail.com',
'att.com',
'netzero.net',
'msn.com',
'yahoo.com',
'aol.com',
'bellsouth.net',
'hotmail.com',
'comcast.net',
'mail.com',
'verizon.net',
'ymail.com',
'live.com',
'netscape.net',
'icloud.com',
'cableone.net',
'alltel.net',
'worldnet.att.com',
'centurytel.net',
'earthlink.net',
'adelphia.com',
'alltell.net',]

individual = data.loc[data['Email-Domain'] == str(domains)]
business = data.loc[data['Email-Domain'] !== str(domains)]

print(individual[['First_Name_01','Last_Name_01','Email']])
print(business[['First_Name_01','Last_Name_01','Email']])
</code></pre>
<p>However when I run this script it just prints an empty list for the first output and then everything in the csv file one the next output.</p>
",19552134.0,19552134.0,2022-12-15 16:34:57,2022-12-15 16:34:57,Saving segmented csv files after separating them by a column value,<python><pandas><csv><data-science><business-intelligence>,1,1,N/A,CC BY-SA 4.0
72996318,1,-1.0,2022-07-15 15:28:26,0,20,"<p>I was curious of how can I do a kind of function that remplace the name of columna based on matches.</p>
<p>For example, lets say that I have a <code>df1</code> that contain the followings colnames</p>
<pre><code>colnames(df1)
 [1] &quot;IES PADRE&quot;                               &quot;Institución de Educación Superior (IES)&quot;
 [3] &quot;Sector IES&quot;                              &quot;Carácter IES&quot;                           
 [5] &quot;Departamento de \ndomicilio de la IES&quot;   &quot;Municipio de\ndomicilio de la IES&quot;      
 [7] &quot;Programa Académico&quot;                      &quot;Nivel de Formación&quot;                     
 [9] &quot;Nivel de Formación&quot;                      &quot;Metodología&quot;                            
[11] &quot;Área de Conocimiento&quot;                    &quot;Núcleo Básico del Conocimiento (NBC)&quot;   
[13] &quot;Departamento de oferta del programa&quot;     &quot;Municipio de oferta del programa&quot;       
[15] &quot;Género&quot;                                  &quot;Año&quot;                                    
[17] &quot;Semestre&quot;                                &quot;Matriculados&quot;       
</code></pre>
<p>And a <code>df2</code> that contains these:</p>
<pre><code> colnames(a21)
 [1] &quot;CÓDIGO DE LA INSTITUCIÓN&quot;                &quot;IES_PADRE&quot;                              
 [3] &quot;INSTITUCIÓN DE EDUCACIÓN SUPERIOR (IES)&quot; &quot;PRINCIPAL O SECCIONAL&quot;                  
 [5] &quot;SECTOR IES&quot;                              &quot;IES ACREDITADA&quot;                         
 [7] &quot;CARACTER IES&quot;                            &quot;CÓDIGO DEL DEPARTAMENTO (IES)&quot;          
 [9] &quot;DEPARTAMENTO DE DOMICILIO DE LA IES&quot;     &quot;CÓDIGO DEL MUNICIPIO (IES)&quot;             
[11] &quot;MUNICIPIO DE DOMICILIO DE LA IES&quot;        &quot;CÓDIGO SNIES DEL PROGRAMA&quot;              
[13] &quot;PROGRAMA ACADÉMICO&quot;                      &quot;PROGRAMA ACREDITADO&quot;                    
[15] &quot;NIVEL ACADÉMICO&quot;                         &quot;NIVEL DE FORMACIÓN&quot;                     
[17] &quot;METODOLOGÍA&quot;                             &quot;ÁREA DE CONOCIMIENTO&quot;                   
[19] &quot;NÚCLEO BÁSICO DEL CONOCIMIENTO (NBC)&quot;    &quot;DESC CINE CAMPO AMPLIO&quot;                 
[21] &quot;DESC CINE CAMPO ESPECIFICO&quot;              &quot;DESC CINE CODIGO DETALLADO&quot;             
[23] &quot;CÓDIGO DEL DEPARTAMENTO (PROGRAMA)&quot;      &quot;DEPARTAMENTO DE OFERTA DEL PROGRAMA&quot;    
[25] &quot;CÓDIGO DEL MUNICIPIO (PROGRAMA)&quot;         &quot;MUNICIPIO DE OFERTA DEL PROGRAMA&quot;       
[27] &quot;SEXO&quot;                                    &quot;AÑO&quot;                                    
[29] &quot;SEMESTRE&quot;                                &quot;MATRICULADOS&quot;          
</code></pre>
<p>As you can see, theres some titles that have the same name but in uppercase, others thant change in typing a Little and others that simply not match.</p>
<p>I want to keep those that are similar and change it to coincide with the typing on df1, its possible to deal with that?</p>
",18473320.0,-1.0,N/A,2022-07-15 17:22:41,"Theres and way to compare column names and if they match, change the name to the one thats in the first Database",<r><data-science>,1,1,N/A,CC BY-SA 4.0
72968922,1,-1.0,2022-07-13 15:33:25,0,51,"<p>At work, we have a tool written in R, as well as a legacy version of the same thing written as an Excel Spreadsheet (we're working on getting rid of that, but for now it's still here).</p>
<p>I've written some new function in R, let's say</p>
<pre><code>predictor &lt;- function(value) return(predict(model, x=value))
</code></pre>
<p>(in reality, it's a bit more complex, but close enough for the sake of this question)</p>
<p>We'd like to back-port said function into the old Excel version of the tool, so I am looking for a way bring that R function (the function itself, not the output) into an Excel spreadsheet.</p>
<p>I don't really care if it's transpiled into an Excel-Formula, a VBA snippet, called directly, or whatever.
All I need is the ability to write a value into a cell in Excel, throw that function at it, and get the result back in another cell.
Is there a way to achieve this?</p>
",3582019.0,-1.0,N/A,2022-07-13 15:33:25,Export R function to excel,<r><excel><data-science>,0,2,N/A,CC BY-SA 4.0
72986245,1,72988192.0,2022-07-14 20:19:59,1,74,"<p>Original code credit-Ken Jee</p>
<pre><code>Salary = df['Salary Estimate'].apply(lambda x:x.split('(')[0])
minus_Kd = Salary.apply(lambda x:x.replace('K','').replace('$',''))

min_hr=minus_Kd.apply(lambda x:x.lower().replace('per hour;','').replace('employer 
provided salary:',''))

df['min_salary'] = min_hr.apply(lambda x: x.split('-')[0])
df['max_salary'] = min_hr.apply(lambda x: x.split('-')[1])
df['avg_salary']=(df.min_salary+df.max_salary)/2
</code></pre>
<p>Having the following error when running this portion of my code:</p>
<pre><code>  df['max_salary'] = min_hr.apply(lambda x: x.split('-')[1])
  IndexError: list index out of range
</code></pre>
<p>Here is some sample data that I am attempting to parse through:</p>
<p>Employer Provided Salary:$78K - $191K</p>
<p>$77K - $107K (Glassdoor est.)</p>
<p>Being a zero index, shouldn`t 0 and 1 be left/right of the &quot;-&quot; respectively?</p>
",19518784.0,19518784.0,2022-07-14 20:23:14,2022-07-15 01:35:09,Index issues while data cleaning,<python><pandas><visual-studio-code><data-science><data-cleaning>,1,7,N/A,CC BY-SA 4.0
72997787,1,-1.0,2022-07-15 17:34:02,1,20,"<p>Context: We run a collection of book clubs where members meet each week. Members are assigned to a specific club, but are free to join any club on any week. E.g., I may be assigned to the Tuesday club, but I can join the Monday or Thursday club if it better fits my schedule for a given week</p>
<p>Additionally, members are constantly leaving and joining the collective. E.g., member A might join for weeks 4-8, and member B might join for weeks 6-10. Thus, the specific composition of a club on any given week can vary significantly</p>
<p>We have an attendance table at the member-club-week level (e.g., on the week of 7/12, the Tuesday club had members A, C, D, F, and G in attendance)</p>
<p>I am trying to create a single quantitative measure of the consistency of co-members at the member level. If a member has the same co-members every single week that they attend, they should have a very high score. If a member has different co-members every time they attend, they should have a very low score.</p>
<p>My first thought was to use a market basket analysis to measure the co-occurence and confidence between each member. This gives us a quantitative score of how likely one member is to occur with any other member</p>
<p>However, where I am stuck is how to translate those two-member confidences into a single number at the member level which describes how often one members was in a club with consistent co-members</p>
",19558012.0,-1.0,N/A,2022-07-15 17:34:02,How to measure consistency of members within a group?,<data-science><predictive><graph-data-science>,0,0,N/A,CC BY-SA 4.0
73002872,1,73003284.0,2022-07-16 09:01:03,1,947,"<p>We have this homework assignment.</p>
<p>Problem 1: Suppose we are interested in the buying habits of shoppers at a particular grocery store with regards to whether they purchase apples, milk, and/or bread. Now suppose 30% of all shoppers at this particular grocery store buy apples, 45% buy milk, and 40% buy a loaf of bread. Let 𝐴 be the event that a randomly selected shopper buys apples, 𝐵 be the event that the same randomly selected shopper buys milk, and 𝐶 the event that they buy bread. Suppose we also know (from data collected) the following information:</p>
<p>The probability that the shopper buys apples and milk is 0.20. The probability that the shopper buys milk and bread is 0.25. The probability that the shopper buys apples and bread is 0.12. The probability that the shopper buys all three items is 0.07. Use this information to answer the following questions.</p>
<p>a) For our purposes, we will use a numeric representation for each event. For example, (010) would be an event in the sample space where a zero in the first place represents no apples were bought while a 1 means they were. Similarly, the second place is the presence of milk and the third place of bread. The example given (010) represents the purchase of milk but not apples or bread.</p>
<p>Insert into vector S the events that belong to the sample space. Then insert the events from the sample space that would correspond to 𝐴 occuring into vector A. Repeat this with vector B for 𝐵 and vector C for 𝐶.</p>
<p>According to set notation, the sample space should look like this: 𝑆={…}. However, due to data storage syntax in R, we will be storing these events in vectors. For example, for some arbitrary event 𝑊 would be stored as follows 𝑊=𝑐(010). c() is a command we can use to construct a vector where commas separate each entry. Complete the code below. Do not worry about the order in which events are placed in vector.</p>
<p>My answer: (Obviously wrong)</p>
<p>S = c(111) A = c(100) B = c(010) C = c(001)</p>
<p>To clarify my answer (As specified above):</p>
<p>S (Sample space)
A (Apples)
B (Milk)
C (Bread)</p>
<p>Is anyone able to assist me with how we are meant to declare these variables in R?</p>
<p>I am new to this language and cannot seem to figure out the correct notation for this question.</p>
<p>Thanks.</p>
",18296355.0,-1.0,N/A,2022-07-16 10:04:40,R notation for declaring vectors - Axioms of Probability,<r><statistics><data-science><probability>,1,0,N/A,CC BY-SA 4.0
72982696,1,-1.0,2022-07-14 15:03:22,0,179,"<p>I am analyzing the text of some literary works and I want to look at the distance between certain words in the text. Specifically, I am looking for parallelism.</p>
<p>Since I can’t know the specific number of tokens in a text I can’t simply put all words in the text in the training data because it would not be uniform across all training data.</p>
<p>For example, the text:</p>
<p>“I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character. I have a dream today.&quot;</p>
<p>Is not the same text length as</p>
<p>&quot;My fellow Americans, ask not what your country can do for you, ask what you can do for your country.&quot;</p>
<p>So therefore I could not columns out of each word and then assign the distance in a row because the lengths would be different.</p>
<p>How could I go about representing this in training data?  I was under the assumption that training data had to be the same type and length.</p>
",3098629.0,3098629.0,2022-07-14 20:30:21,2022-07-15 00:35:54,How to train data of different lengths in machine learning?,<data-science><training-data>,1,1,N/A,CC BY-SA 4.0
72999151,1,73000538.0,2022-07-15 20:11:41,0,119,"<p>I am having an issue with MySQL Community Server Workbench. I input this command on MySQL File Queries and get this error</p>
<p>Command:</p>
<pre><code>use CIA_DATA;

SELECT * FROM new_table;

SELECT Country, GDP, Electricity;
FROM new_table;
WHERE GDP &lt; 4000;
ORDER BY Electricity;

select now(); database();
</code></pre>
<p>Error:
22:03:14 SELECT Country, GDP, Electricity LIMIT 0, 50000 Error Code: 1054. Unknown column 'Country' in 'field list' 0.000 sec</p>
<p>Please let me know what mistakes I have made. Keep in mind that my Result Grid has these field names/columns:
Water, Sanitation, GDP, Life, Underweight, Literacy, Electricity, Country</p>
<p>Thanks</p>
",18849788.0,-1.0,N/A,2022-07-15 23:46:39,MySQL Workbench Error Code 1054 for Invalid Column Even When I Have the Corresponding Column Name,<mysql><sql><data-science>,1,3,N/A,CC BY-SA 4.0
73004959,1,-1.0,2022-07-16 14:22:41,0,45,"<pre><code>from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Convolution2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten

model=Sequential()`model=Sequential()
model.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation=&quot;relu&quot;))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten(input_shape=(64,64,3)))
model.add(Dense(units=128,activation=&quot;relu&quot;))
model.add(Dense(units=64,activation=&quot;relu&quot;))
model.add(Dense(units=6,activation=&quot;softmax&quot;)) 
model.compile(loss=&quot;categorical_crossentropy&quot;,optimizer=&quot;adam&quot;,metrics=[&quot;accuracy&quot;])


def main():
file_uploaded = st.file_uploader(&quot;Choose File&quot;, type=[&quot;png&quot;,&quot;jpg&quot;,&quot;jpeg&quot;])
class_btn = st.button(&quot;Classify&quot;)
if file_uploaded is not None:    
    image = Image.open(file_uploaded)
    st.image(image, caption='Uploaded Image', use_column_width=True)    
if class_btn:
    if file_uploaded is None:
        st.write(&quot;Invalid command, please upload an image&quot;)
    else:
        with st.spinner('Model working....'):
            plt.imshow(image)
            plt.axis(&quot;off&quot;)
            predictions = predict(image)
            time.sleep(1)
            st.success('Classified')
            st.write(predictions)
            st.pyplot(fig)

def predict(image):
classifier_model = &quot;C:/Users/houcem/Desktop/Doctor Sina/code/ECG1.h5&quot;
IMAGE_SHAPE = (64,64,3)
model = load_model(classifier_model, compile=False)
test_image = image.resize((64,64))
test_image = preprocessing.image.img_to_array(test_image)

test_image = np.expand_dims(test_image, axis=0)
preds = np.argmax(model.predict(test_image),axis=-1)
index = ['Fusion Beat','Normal Beat','Unknown Beat','Supraventricular ectopic Beat','Ventricular ectopic beat']
print(&quot;------------------------------------\n&quot;)
print(&quot;\nprediction -&gt; &quot;,index[preds[0]])
print(&quot;\n-------------------------------------&quot;)
                   

text = &quot;The predicted type is &quot; + str(index[preds[0]]) 
return text
</code></pre>
<p>i got this error any help please</p>
<p>ValueError( ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape [None, 64, 64, 4]</p>
",14595641.0,14595641.0,2022-07-16 23:07:58,2022-07-16 23:07:58,ValueError: Input 0 of layer sequential is incompatible with the laye,<python><deep-learning><data-science>,0,2,N/A,CC BY-SA 4.0
72996550,1,-1.0,2022-07-15 15:44:54,1,62,"<p>the goal is to convert the given JSON into the CSV data Frame according to the below sample output. this is a part of the whole project. anyone would like to exercise.</p>
<p><strong>response.json</strong></p>
<pre><code>[
    {
      
      &quot;fiscalPeriodYearMonth&quot;: &quot;2012-09&quot;,
      
      &quot;revenuePer&quot;: {
        &quot;yearOverYear&quot;: 19.57,
        &quot;threeYearAvg&quot;: 28.24,
        &quot;fiveYearAvg&quot;: 21.240000000000002,
        &quot;tenYearAvg&quot;: 28.96
      },
      &quot;operatingIncome&quot;: {
        &quot;yearOverYear&quot;: 21.57,
        &quot;threeYearAvg&quot;: 50.019999999999996,
        &quot;fiveYearAvg&quot;: 30.3,
        &quot;tenYearAvg&quot;: null
      },
      &quot;netIncomePer&quot;: {
        &quot;yearOverYear&quot;: 14.000000000000002,
        &quot;threeYearAvg&quot;: 44.330000000000005,
        &quot;fiveYearAvg&quot;: 29.01,
        &quot;tenYearAvg&quot;: null
      },
      &quot;epsPer&quot;: {
        &quot;yearOverYear&quot;: 16.55,
        &quot;threeYearAvg&quot;: 44.65,
        &quot;fiveYearAvg&quot;: 30.830000000000002,
        &quot;tenYearAvg&quot;: null
        }
    },

    {
     
      &quot;fiscalPeriodYearMonth&quot;: &quot;2013-09&quot;,
      &quot;revenuePer&quot;: {
        &quot;yearOverYear&quot;: 7.5600000000000005,
        &quot;threeYearAvg&quot;: 18.87,
        &quot;fiveYearAvg&quot;: 17.9,
        &quot;tenYearAvg&quot;: 29.020000000000003
      },
      &quot;operatingIncome&quot;: {
        &quot;yearOverYear&quot;: 1.06,
        &quot;threeYearAvg&quot;: 23.27,
        &quot;fiveYearAvg&quot;: 34.11,
        &quot;tenYearAvg&quot;: 58.93000000000001
      },
      &quot;netIncomePer&quot;: {
        &quot;yearOverYear&quot;: 0.77,
        &quot;threeYearAvg&quot;: 22.42,
        &quot;fiveYearAvg&quot;: 30.12,
        &quot;tenYearAvg&quot;: 52.459999999999994
      },
      &quot;epsPer&quot;: {
        &quot;yearOverYear&quot;: 1.4500000000000002,
        &quot;threeYearAvg&quot;: 23.46,
        &quot;fiveYearAvg&quot;: 31.5,
        &quot;tenYearAvg&quot;: 47.88
      }
    }
  
  ] 

</code></pre>
<p><strong>My Code</strong>:</p>
<pre><code>import pandas as pd
df = pd.read_json(r'PATH TO JSON FILE', orient ='values')

print(df.T.to_csv(&quot;final_output.csv&quot;))

</code></pre>
<p>But it does not giving me the desired following format.</p>
<p><strong>Output I need</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>2012-09</th>
<th>2013-09</th>
</tr>
</thead>
<tbody>
<tr>
<td>Revenue</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Year Over Year</td>
<td>19.57</td>
<td>7.56</td>
</tr>
<tr>
<td>3-Year Average</td>
<td>28.24</td>
<td>18.87</td>
</tr>
<tr>
<td>5-Year Average</td>
<td>21.24</td>
<td>17.90</td>
</tr>
<tr>
<td>10-Year Average</td>
<td>28.96</td>
<td>29.02</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>operatingIncome</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Year Over Year</td>
<td>21.57</td>
<td>1.06</td>
</tr>
<tr>
<td>3-Year Average</td>
<td>50.02</td>
<td>23.27</td>
</tr>
<tr>
<td>5-Year Average</td>
<td>30.30</td>
<td>34.11</td>
</tr>
<tr>
<td>10-Year Average</td>
<td>-</td>
<td>58.93</td>
</tr>
</tbody>
</table>
</div>
<p>and so on.</p>
<p>I know its a bit logical but very interesting, and will be so easy for someone good in python pandas.</p>
",12560892.0,12560892.0,2022-07-15 18:16:31,2022-07-17 16:43:51,JSON data into csv by using python maybe pandas specific format,<python><json><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
73011220,1,-1.0,2022-07-17 11:03:49,0,135,"<p>I have a column called &quot;feedback&quot;, and have 1 field called &quot;emotions&quot;. In those emotions field, we can see the random values and random length like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">emotions</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">sad, happy</td>
</tr>
<tr>
<td style=""text-align: left;"">happy, angry, boring</td>
</tr>
<tr>
<td style=""text-align: left;"">boring</td>
</tr>
<tr>
<td style=""text-align: left;"">sad, happy, boring, laugh</td>
</tr>
</tbody>
</table>
</div>
<p>etc with different values and different length.
so, the question is, what's query to serve the mysql or postgre data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">emotion</th>
<th style=""text-align: center;"">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">happy</td>
<td style=""text-align: center;"">3</td>
</tr>
<tr>
<td style=""text-align: left;"">angry</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">sad</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">boring</td>
<td style=""text-align: center;"">3</td>
</tr>
<tr>
<td style=""text-align: left;"">laugh</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table>
</div>
<p>based on <a href=""https://stackoverflow.com/questions/59243804/sql-count-of-items-in-comma-separated-column-in-a-table"">SQL: Count of items in comma-separated column in a table</a> we could try using</p>
<pre><code>    SELECT value as [Holiday], COUNT(*) AS [Count]
FROM OhLog
CROSS APPLY STRING_SPLIT([Holidays], ',')
GROUP BY value
</code></pre>
<p>but it wont help because that is for sql server, not mysql or postgre. or anyone have idea to translation those sqlserver query to mysql?
thank you so much.. I really appreciate it</p>
",13706390.0,13706390.0,2022-07-17 11:22:46,2022-07-22 10:08:41,counting comma separated values mysql-postgre,<mysql><sql><database><postgresql><data-science>,3,4,N/A,CC BY-SA 4.0
73004755,1,-1.0,2022-07-16 13:52:08,1,53,"<p>I am try to fit a liner regression model. I am geeting this error: <code>TypeError: float() argument must be a string or a number, not 'method'</code>.
The data type of all the colums is float64.</p>
<p>Data ussed for the model:</p>
<pre><code>    ASSETS  REVENUES    AUDIT_FEES
0   78485.38    55098.67    391.9
1   259054.6    324518.85   599.17
2   545000.86   661234.06   1201.83
3   290087.75   19177.09    413.37
4   327593.87   406270.92   691.48

X = Audit_Data[['ASSETS','REVENUES']]
Y = Audit_Data['AUDIT_FEES']
</code></pre>
<p>And the code is:</p>
<pre><code>Model_reg = linear_model.LinearRegression()
Model_reg.fit(X,Y)
</code></pre>
<p>Can you please let me know what is the issue?</p>
",17790711.0,-1.0,2022-07-19 07:30:56,2022-07-19 07:30:56,"Linear Regression model error: TypeError: float() argument must be a string or a number, not 'method'",<python><machine-learning><data-science><linear-regression>,1,0,N/A,CC BY-SA 4.0
73014037,1,-1.0,2022-07-17 17:44:39,0,42,"<p>I'm trying to train a CNN model to give a prediction of format</p>
<p><code>array([ 0.,  0.,  0.,  0., -1.], dtype=float32)</code>.</p>
<p>My Training data looks like this :</p>
<pre><code>0        [[-1.0, -1.0, -1.0, -1.0, -1.0], [0.0, 0.0, 0....
1        [[0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 1.0, 0....
2        [[0.0, 0.0, 0.0, -1.0, -1.0], [0.0, 0.0, 0.0, ...
3        [[-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, ...
4        [[-1.0, -1.0, -1.0, -1.0, -1.0], [0.0, 0.0, 0....
                               ...                        
15484    [[-1.0, -1.0, -1.0, -1.0, -1.0], [0.0, 2.0, 1....
15485    [[-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, ...
15486    [[-1.0, -1.0, -1.0, -1.0, -1.0], [0.0, 2.0, 0....
15487    [[1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0....
15488    [[-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, ...
</code></pre>
<p>With each row of shape (24,5) looking like this :</p>
<pre><code>array([[-1., -1., -1., -1., -1.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  3.,  0.,  0.,  1.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0., -1.],
       [ 0.,  1.,  2.,  0.,  0.],
       [ 0.,  3.,  0.,  0., -1.],
       [ 0.,  1.,  0.,  0., -1.],
       [ 0.,  1.,  0.,  0., -1.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0., -1.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0., -1.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0., -1.],
       [ 0.,  0.,  0.,  0., -1.],
       [ 0.,  1.,  0.,  0., -1.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0., -1.],
       [ 0.,  0.,  0.,  0., -1.],
       [ 0.,  1.,  0.,  0., -1.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0., -1.],
       [ 1.,  2.,  0.,  0., -1.]], dtype=float32)
</code></pre>
<p>The model I'm using looks like this :</p>
<pre><code>model = tf.keras.Sequential(layers=[
                tf.keras.layers.Conv2D(32,kernel_size = 3, activation=tf.nn.relu, input_shape=(28,28,1)),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(32,kernel_size = 3, activation=tf.nn.relu),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(32,kernel_size = 5,strides=2,padding='same', activation=tf.nn.relu),
                tf.keras.layers.BatchNormalization(),

                tf.keras.layers.Conv2D(64,kernel_size = 3, activation=tf.nn.relu),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(64,kernel_size = 3, activation=tf.nn.relu),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(64,kernel_size = 3, strides=2, padding='same', activation=tf.nn.relu),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(128,activation=tf.nn.relu),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Dense(1, activation=tf.nn.softmax)])

        model.compile(optimizer=tf.keras.optimizers.Adam(0.1), 
                      loss=tf.keras.losses.CategoricalCrossentropy(), 
                      metrics=tf.keras.metrics.Accuracy())

</code></pre>
<p>I'm new to the field and is currently getting the following error :</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).
</code></pre>
<p>Can someone tell me what I'm doing wrong here? I added a <code>tf.convert_to_tensor()</code> function on the training data but is still getting the same error.</p>
<p>Adding <code>.astype(&quot;float32&quot;)</code> seems to not be working either.</p>
",10036976.0,-1.0,N/A,2022-07-17 18:38:27,CNN Model unable to train,<python><machine-learning><conv-neural-network><data-science>,1,0,N/A,CC BY-SA 4.0
73023075,1,73023602.0,2022-07-18 13:26:22,0,60,"<p>I have some data that looks like this:</p>
<pre><code>this_data = [{
    &quot;Name&quot;: &quot;Bluefox&quot;,
    &quot;Sub Name&quot;: &quot;Moonglow&quot;,
    &quot;Time Series&quot;: &quot;{'2022-07-06': 9.5, '2022-07-07': 7.2, '2022-07-08': 10.3}&quot;,
    &quot;Probability&quot;: &quot;{'2022-07-06': 0.2, '2022-07-07': 0.3, '2022-07-08': 0.5}&quot;,
    &quot;Max Value&quot;: 466888785.24275005,
},{
    &quot;Name&quot;: &quot;Blackbird&quot;,
    &quot;Sub Name&quot;: &quot;Skylight&quot;,
    &quot;Time Series&quot;: &quot;{'2022-07-06': -16240599.020647092, '2022-07-07': -17984033.390385196}&quot;,
    &quot;Probability&quot;: &quot;{'2022-07-06': 0.6, '2022-07-07': 0.7}&quot;,
    &quot;Max Value&quot;: 81509865.34667145,
},{
    &quot;Name&quot;: &quot;Bluefox&quot;,
    &quot;Sub Name&quot;: &quot;Skylight&quot;,
    &quot;Time Series&quot;: &quot;{'2022-07-06': -123000, '2022-07-07': -245100}&quot;,
    &quot;Probability&quot;: &quot;{'2022-07-06': 0.0, '2022-07-07': 0.0}&quot;,
    &quot;Max Value&quot;: 90409417.34667145,
}]
</code></pre>
<p>And I want to transform this into:</p>
<pre><code>{'Bluefox': {
    'Moonglow': {
        'date': {
            '2022-07-06': {
                'Time Series' : 9.5,
                'Probability' : 0.2,
                
            },
            '2022-07-07' : {
                'Time Series' : 7.2,
                'Probability' : 0.3,

            },
            '2022-07-08' : {
                'Time Series' : 10.3,
                'Probability' : 0.5,

            }
        },
        'Max Value' : 466888785.24275005
    },
    'Skylight':{
        'date': {}
        }
    }
},
{'Blackbird': {
    'Moonglow': {
        'date': {
            '2022-07-06': {
                'Time Series' : 9.5,
                'Probability' : 0.2,
                }
            },
        'Max Value' : 466888785.24275005
        }
    }
}
</code></pre>
<p>I am trying something like this:</p>
<pre><code>import json
import ast
from collections import defaultdict

entity_to_cp = {
    'Bluefox' : ['Moonglow', 'Skylight'],
    'Blackbird' : ['Skylight']
}


inner = defaultdict(list)
between = defaultdict(dict)
between2 = defaultdict(dict)
outer = defaultdict(dict)
for each_dict in this_data:
    for label, all_values in each_dict.items():
        if label == &quot;Name&quot;:
            outer[all_values] = between
            cur_e = all_values
        if label == &quot;Sub Name&quot;:
            between[all_values] = between2
            cur_cp = all_values
        
        try:
            if cur_cp in entity_to_cp[cur_e]:
                try:
                    all_values = ast.literal_eval(all_values)
                    for k,v in all_values.items():
                        print(k)
                        between2[k] = inner
                        inner[label].append(v)
                    # inner[label].append(all_values)
                except AttributeError as e:
                    print(e)
                except SyntaxError as e:
                    print(e)
                except ValueError as e:
                    print(e)
        except NameError as e:
            print(e)

</code></pre>
<p>But this doesn't work and I get duplicated values all over the place. Help!</p>
",11317776.0,11317776.0,2022-07-18 13:40:02,2022-07-18 14:39:11,Data manipulation to nested dictionaries?,<python><json><python-3.x><dictionary><data-science>,1,5,N/A,CC BY-SA 4.0
73033757,1,-1.0,2022-07-19 08:38:52,1,147,"<p>I am working with Pyspark to build regression models for each group in the data. To do so, I am using Pandas_UDF and am able to build the models successfully and generate the outputs and return them as a DataFrame.</p>
<p>At the same time, I wanted to save the model for future use but somehow I could not save it to the GCS Bucket.</p>
<p>If I simply save the model to the CWD, the models are saved but don't have access to the models saved locally in the worker nodes</p>
<p>Code used to save the model locally as follows:</p>
<pre><code>model.save(l2_category+'.pickle')
</code></pre>
<p>The code used to save the model to the GCS bucket is as follows:</p>
<pre><code>with open(os.path.join(gcs_path,l2_category+'.pkl'),'w') as f:
    pickle.dump(model,f)
</code></pre>
<p>But saving to GCS throws an error</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'gs://darkstores-data-eng_stg/multi_sku_test/models/610a967aac6e434026bb7fa9.pkl'
</code></pre>
<p>Can someone help me with the best way to tackle this?</p>
",2771169.0,-1.0,N/A,2022-07-19 08:38:52,Pyspark - Save statsmodel trained models in GCS Buckets (or Driver),<apache-spark><machine-learning><pyspark><data-science><statsmodels>,0,0,N/A,CC BY-SA 4.0
73011522,1,-1.0,2022-07-17 11:52:26,0,65,"<p>I have a column called &quot;feedback&quot;, and have 1 field called &quot;emotions&quot;. In those emotions field, we can see the random values and random length like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">emotions</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">sad, happy</td>
</tr>
<tr>
<td style=""text-align: left;"">happy, angry, boring</td>
</tr>
<tr>
<td style=""text-align: left;"">boring</td>
</tr>
<tr>
<td style=""text-align: left;"">sad, happy, boring, laugh</td>
</tr>
</tbody>
</table>
</div>
<p>etc with different values and different length.
so, the question is, what's query to serve the mysql or postgre data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">emotion</th>
<th style=""text-align: center;"">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">happy</td>
<td style=""text-align: center;"">3</td>
</tr>
<tr>
<td style=""text-align: left;"">angry</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">sad</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">boring</td>
<td style=""text-align: center;"">3</td>
</tr>
<tr>
<td style=""text-align: left;"">laugh</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table>
</div>
<p>based on <a href=""https://stackoverflow.com/questions/59243804/sql-count-of-items-in-comma-separated-column-in-a-table"">SQL: Count of items in comma-separated column in a table</a> we could try using</p>
<pre><code>    SELECT value as [Holiday], COUNT(*) AS [Count]
FROM OhLog
CROSS APPLY STRING_SPLIT([Holidays], ',')
GROUP BY value
</code></pre>
<p>but it wont help because that is for sql server, not mysql or postgre. or anyone have idea to translation those sqlserver query to mysql?
thank you so much.. I really appreciate it</p>
",19566016.0,5193536.0,2022-07-17 12:57:28,2022-07-17 12:57:28,mysql counting comma separated values at field,<mysql><sql><database><data-science>,2,1,2022-07-17 12:57:39,CC BY-SA 4.0
73036098,1,73036867.0,2022-07-19 11:31:46,0,237,"<p>I have a dataframe with following structure:</p>
<pre><code>df &lt;- 
  data.frame(
    Replicate = c(rep(&quot;N1&quot;, 50), rep(&quot;N2&quot;, 50)),
    feature1 = rnorm(100, 0, 1),
    feature2 = rnorm(100, 0, 3),
    feature3 = rnorm(100, 0.1, 1)  
)
</code></pre>
<p>I am calculating the correlation between my (biological) replicates for each of my data columns (here &quot;feature 1-3&quot;) with following code:</p>
<pre><code>results_table &lt;- data.frame(feature = NA,
                            correlation = NA)

for(i in colnames(df)[2:4]){
  cor_i &lt;- cor(df %&gt;% filter(Replicate == &quot;N1&quot;) %&gt;% pull(i), 
               df %&gt;% filter(Replicate == &quot;N2&quot;) %&gt;% pull(i), 
               use = &quot;pairwise.complete&quot;)
  
  results_table_temp &lt;- data.frame(feature = i,
                                   correlation = cor_i)
  
  results_table &lt;- rbind(results_table, results_table_temp)
}

results_table &lt;- results_table[2:nrow(results_table),]
results_table
</code></pre>
<p>I basically filter my initial dataframe for the respective replicate and calculate correlation between these replicate for each column (using a a for loop with <code>cor()</code> and store the output in dataframe).</p>
<p>For my dataset (240 rows with &gt;7000 colums), the computing time is quite long! Is there a more efficient way to calculate this? Maybe a specific function or preprocessing of data to make the computation more efficient?</p>
",14259242.0,-1.0,N/A,2022-07-19 12:24:40,Optimizing correlation calculation between (biological) replicates in R,<r><data-science><correlation>,1,0,N/A,CC BY-SA 4.0
73036117,1,-1.0,2022-07-19 11:33:05,0,35,"<p>just need advice, correction or even direction where should I go further with the project.</p>
<p>I am building a script which will make for loop API calls for creating products. I am pulling data for API call from edited csv which looks something like this:</p>
<pre><code>title, product_id
&quot;Name 1&quot;, 12345
&quot;Name 2&quot;, 98765
&quot;Name 3&quot;, 34566
&quot;Nime 4&quot;, 78930
&quot;Name 5&quot;, 99850
</code></pre>
<p>So, what I have created so far is for loop which makes API calls and at the end it should save responses and create new csv file with product_ids and responses for every one of them. It is very important for me to save it so i can investigate what happened afterwards.</p>
<p>The script I have now is this one:</p>
<pre><code>from urllib import response
import requests
import json 
import pandas as pd
import csv
from edit_csv_script import edited_csv

api_token = api_token
api_base_url = &quot;https://api.mystore.net&quot;

headers = {
        &quot;Accept&quot;: &quot;application/json&quot;,
        &quot;Content-Type&quot;: &quot;application/json&quot;, 
        &quot;X-Recharge-Access-Token&quot;: api_token
}

df = pd.read_csv('test.csv')
for i in range(len(df)):
    endpoint = f&quot;{api_base_url}/products&quot;
    data = {
            &quot;product_id&quot;: df['product_id'][i],
            &quot;title&quot;: df['title'][i],
            &quot;discount_type&quot;: &quot;percentage&quot;,
            &quot;discount_amount&quot;: 5,
            &quot;interval&quot;: &quot;week&quot;,
        }
    response = requests.post(endpoint, json.dumps(data), headers=headers)
        
    for i in range(len(df)):
        response_api = response.json()
        bad_responses = response.status_code !=200 or response.status_code !=201
        if bad_responses:
            df_bad_responses = pd.DataFrame({'product_id': df[&quot;product_id&quot;][i],'response': response_api, 'status_code': response.status_code, 'message': response.text})
            df_bad_responses.to_csv('bulk_process_bad_responses.csv')
                
            df_ok_responses = pd.DataFrame({'product_id': df[&quot;product_id&quot;][i], 'response': response_api, 'status_code': response.status_code})
            df_ok_responses.to_csv('bulk_process_ok_responses.csv')
</code></pre>
<p>So the error now I am getting is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;Desktop/testing_scripts/bulk_products_create.py&quot;, line 36, in &lt;module&gt;
    response = requests.post(endpoint, json.dumps(data), headers=headers)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/json/__init__.py&quot;, line 231, in dumps
    return _default_encoder.encode(obj)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/json/encoder.py&quot;, line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/json/encoder.py&quot;, line 257, in iterencode
    return _iterencode(o, 0)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/json/encoder.py&quot;, line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
</code></pre>
<p>Maybe I should not use Pandas library for this one, not sure what do you think. Also, would like to underline that I have checked this discussion:<a href=""https://stackoverflow.com/questions/50916422/python-typeerror-object-of-type-int64-is-not-json-serializable"">Python - TypeError: Object of type &#39;int64&#39; is not JSON serializable</a></p>
<p>but not one of the answers helped me.</p>
<p>Thank in advance!</p>
",17326438.0,17326438.0,2022-07-26 16:12:21,2022-07-26 16:12:21,Review for the script which which makes API calls in for loop and saves the ids and responses in the csv file which can (so far),<python><pandas><dataframe><api><data-science>,0,4,N/A,CC BY-SA 4.0
73022027,1,73022054.0,2022-07-18 12:03:37,1,73,"<p>I have a vector of dates</p>
<pre><code>L = [Timestamp('2018-07-15 00:00:00'),
  Timestamp('2019-07-15 00:00:00')]
</code></pre>
<p>and a dataframe with a date column:</p>
<pre><code>df = c1 c2 Date
     1. 2. 2018-07-13 16:00:00
     1. 7. 2018-07-15 16:00:00
     3. 7. 2018-07-15 16:50:00
     4. 7. 2018-07-15 19:50:00
     2. 2. 2018-07-16 16:00:00
     5. 1. 2020-10-10 16:00:00
     8. 4. 2018-06-13 16:00:00   
     5. 4. 2021-12-13 16:00:00
     2. 9. 2019-01-01 16:00:00
     2. 6. 2019-01-01 17:00:00
</code></pre>
<p>I want to add a column that is the rank in the dates vector, and delete rows from the same date (regardless of hour/minutes).
So I will have:</p>
<pre><code>df = c1 c2 d.                  new
     1. 2. 2018-07-13 16:00:00  0
     1. 4. 2018-06-13 16:00:00  0
     2. 2. 2018-07-16 16:00:00  1
     5. 1. 2020-10-10 16:00:00  2
     8. 4. 2018-06-13 16:00:00  0
     5. 4. 2021-12-13 16:00:00  2
     2. 9. 2019-01-01 16:00:00  1
     2. 6. 2019-01-01 17:00:00. 1
</code></pre>
<p>What is the best way to do it?</p>
",6057371.0,6057371.0,2022-07-18 12:40:50,2022-07-18 12:47:58,Pandas dataframe how to add a columns based on rank in a dates vector,<python><pandas><dataframe><data-science><data-munging>,1,0,N/A,CC BY-SA 4.0
73025693,1,-1.0,2022-07-18 16:33:35,0,101,"<p>I have a data frame with over 2000 rows, but in the first column, there are a number of duplicates. I want to add up the data in each duplicate together. An example of the data is seen below
<a href=""https://i.stack.imgur.com/HJENT.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",13017203.0,-1.0,N/A,2022-07-18 17:41:41,How add or merge duplicate rows and columns,<python><pandas><dataframe><data-science><exploratory-data-analysis>,1,0,N/A,CC BY-SA 4.0
73038957,1,73040234.0,2022-07-19 14:40:34,0,45,"<p>I am trying to get a certain graph from this url: <br />
<a href=""https://sasri.sasa.org.za/agronomy/mycanesimlite/mcl_single_run_get_input.php?start_date=2021-06-15&amp;harvest_date=2022-06-15&amp;ratoon=R&amp;residue=0&amp;tam=150&amp;irrigation=Rainfed&amp;weather_station=14&amp;forecast=Normal"" rel=""nofollow noreferrer"">https://sasri.sasa.org.za/agronomy/mycanesimlite/mcl_single_run_get_input.php?start_date=2021-06-15&amp;harvest_date=2022-06-15&amp;ratoon=R&amp;residue=0&amp;tam=150&amp;irrigation=Rainfed&amp;weather_station=14&amp;forecast=Normal</a> <br />
When you inspect the website, you can find the following link to a php file that, supposedly, generates a graph. That's the graph Im trying to get. <br />
<a href=""https://sasri.sasa.org.za/agronomy/mycanesimlite/mcl_crop_status_data.php"" rel=""nofollow noreferrer"">https://sasri.sasa.org.za/agronomy/mycanesimlite/mcl_crop_status_data.php</a></p>
<p>This is the code I am currently using. By the way, It's obviously Python</p>
<pre><code>import requests

actionURL = &quot;https://sasri.sasa.org.za/agronomy/mycanesimlite/mcl_crop_status_data.php&quot;

res = requests.get(actionURL)

cookies = res.cookies

post_data = {&quot;method&quot;:&quot;POST&quot;}

headers = {
                &quot;Host&quot;: &quot;sasri.sasa.org.za&quot;,
                &quot;Connection&quot;: &quot;keep-alive&quot;,
                &quot;Accept&quot;: &quot;*/*&quot;,
                &quot;Origin&quot;: &quot;https://sarsi.sasa.org.za&quot;,
                &quot;Sec-Fetch-Dest&quot;:&quot;empty&quot;,
                &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded; charset=UTF-8&quot;,
                &quot;X-Requested-With&quot;: &quot;XMLHttpRequest&quot;,
                &quot;User-Agent&quot;: &quot;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0&quot;,
                &quot;sec-ch-ua-platform&quot;: &quot;Linux&quot;,
                &quot;Content-Length&quot;: &quot;38&quot;,
                &quot;Origin&quot;: &quot;https://sasri.sasa.org.za&quot;,
                &quot;Referer&quot;: &quot;https://sasri.sasa.org.za/agronomy/mycanesimlite/mcl_single_run_get_input.php?start_date=2021-06-15&amp;harvest_date=2022-06-15&amp;ratoon=R&amp;residue=0&amp;tam=150&amp;irrigation=Rainfed&amp;weather_station=14&amp;forecast=Normal&quot;,
                &quot;Accept-Encoding&quot;: &quot;gzip, deflate, br&quot;,
                &quot;Accept-Language&quot;: &quot;es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3&quot;,
                &quot;Cookie&quot;: &quot;ga=GA1.3.669250053.1658240582; _gid=GA1.3.65875213.1658240582; _gat=1; PHPSESSID=62d6be5ae92de&quot;,
                &quot;Sec-Fetch-Site&quot;: &quot;same-origin&quot;,
                &quot;Sec-Fetch-Mode&quot;: &quot;cors&quot;
}

res_post = requests.post(actionURL, data=post_data, cookies=cookies, headers=headers)

values = res_post.json()

print(values)
</code></pre>
<p>However, when I execute it, I get the following error</p>
<pre><code>'\nNotice: Undefined index: field in D:\\inetpub\\wwwroot\\Client\\sasa\\Sasex\\Apps\\MyCanesim_Lite\\mcl_crop_status_data.php on line 18\n[] \r\n'
</code></pre>
<p>By the way, this happens to me when I'm using chrome. When I use Firefox, this whole issue seems to be different as I still cannot retrieve the data, but I can click on the previous link and that php errors disappears.</p>
<pre><code></code></pre>
",-1.0,-1.0,N/A,2022-07-20 13:17:04,How can I get a dataset from a graph in a website using dev tools and Python?,<python><php><web-scraping><graph><data-science>,1,3,N/A,CC BY-SA 4.0
73040621,1,73058918.0,2022-07-19 16:37:55,-1,692,"<p>I have created a binary classification model. During training the model I am getting both loss and accuracy as constant. I tried the same code for other datasets and it is giving the same thing in every case. How to fix this?
Here is the code</p>
<pre><code>model=Model()
lossfn=nn.BCEWithLogitsLoss()
optimizer=torch.optim.Adam(model.parameters(),lr=0.01)
#training model
epochs=10
train_acc=[]
test_acc=[]
losses=torch.zeros(epochs)

#training over loops
for i in range(epochs):
    model.train()
    batchAcc=[]
    batchLoss=[]

    for X,y in train_loader:

        #forward pass
        yHat=model(X)

        #lossfunction
        loss=lossfn(y,yHat)

        #backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        batchLoss.append(loss.item())
        # predictions=(torch.sigmoid(yHat)&gt;0.5).float()
        predictions=yHat&gt;0
        batchLoss.append(loss.item())
        batchAcc.append(100*torch.mean(((yHat&gt;0) == y).float()))

    losses[i]=np.mean(batchLoss)
    train_acc.append(np.mean(batchAcc))

    #Evaluation mode
    model.eval()
    X,y=next(iter(test_loader))
    with torch.no_grad():
        yHat=model(X)
    test_acc.append( 100*torch.mean(((yHat&gt;0) == y).float()))

    print(f'Epoch:{i+1} loss:{loss.item()} train accuracy:{train_acc[-1]} test accuracy:{test_acc[-1]}')
</code></pre>
<p>Output of the code</p>
<pre><code>Epoch:1 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:2 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:3 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:4 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:5 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:6 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:7 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:8 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:9 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
Epoch:10 loss:0.6931471824645996 train accuracy:33.33646011352539 test accuracy:0.0
</code></pre>
",18168227.0,-1.0,N/A,2022-07-20 22:51:03,constant loss and accuracy in Pytorch model,<python><deep-learning><pytorch><data-science>,2,2,N/A,CC BY-SA 4.0
70786248,1,-1.0,2022-01-20 12:28:33,0,69,"<p>This work for the string &quot;AAA&quot; :</p>
<pre><code>df['B'] = df.A.str.replace(r'(^.*AAA.*$)', 'ZZZ',  regex=True)
</code></pre>
<p>How can I do if I want to replace multiple string by 'ZZZ'?</p>
<pre><code>df['B'] = df.A.str.replace(r'(^.*AAA.*$)' or r'(^.*BBB.*$)', 'ZZZ',  regex=True)
</code></pre>
<p>I tried this but it does not work</p>
",10987256.0,-1.0,N/A,2022-01-20 12:28:33,Replace multiple string containing a string with Pandas,<python><pandas><data-science>,0,2,N/A,CC BY-SA 4.0
71891029,1,-1.0,2022-04-16 04:11:06,2,659,"<p>I am new to data science and trying to get a grip on exploratory data analysis. My goal is to get a correlation matrix between all the variables. For numerical variables I use Pearson's R, for categorical variables I use the corrected Cramer's V. The issue now is to get a meaningful correlation between categorical and numerical variables. For that I use the correlation ratio, as outlined <a href=""https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9"" rel=""nofollow noreferrer"">here</a>. The issue with that is that categorical variables with high cardinality show a high correlation no matter what:</p>
<p><a href=""https://i.stack.imgur.com/0SRjN.png"" rel=""nofollow noreferrer"">correlation matrix cat vs. num</a></p>
<p>This seems nonsensical, since this would practically show the cardinality of the the categorical variable instead of the correlation to the numerical variable. The question is: how to deal with the issue in order to get a meaningful correlation.</p>
<p>The Python code below shows how I implemented the correlation ratio:</p>
<pre><code>import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

train = pd.DataFrame({
    'id': [0,1,2,3,4,5,6,7,8,9,10,11], 'num3': [6,3,3,9,6,9,9,3,6,3,6,9],
    'cat2': [0,1,0,1,0,1,0,1,0,1,0,1], 'cat3': [0,1,2,0,1,2,0,1,2,0,1,2],
    'cat6': [0,4,8,2,6,10,0,4,8,2,6,10], 'cat12': [0,7,2,9,4,11,6,1,8,3,10,5],
})
cat_cols, num_cols = ['cat2','cat3','cat6','cat12'], ['id','num3']

def corr_ratio(cats, nums):
    avgtotal = nums.mean()
    elements_avg, elements_count = np.zeros(len(cats.index)), np.zeros(len(cats.index))
    cu = cats.unique()
    for i in range(cu.size):
        cn = cu[i]
        filt = cats == cn
        elements_count[i] = filt.sum()
        elements_avg[i] = nums[filt].mean(axis=0)
    numerator = np.sum(np.multiply(elements_count, np.power(np.subtract(elements_avg, avgtotal), 2)))
    denominator = np.sum(np.power(np.subtract(nums, avgtotal), 2))  # total variance
    return 0.0 if numerator == 0 else np.sqrt(numerator / denominator)

rows = []
for cat in cat_cols:
    col = []
    for num in num_cols:
        col.append(round(corr_ratio(train[cat], train[num]), 2))
    rows.append(col)

df = pd.DataFrame(np.array(rows), columns=num_cols, index=cat_cols)
sns.heatmap(df)
plt.tight_layout()
plt.show()
</code></pre>
",18360039.0,-1.0,N/A,2022-09-28 10:07:37,Numerical vs. categorical vars: Why 100% correlation for categorical variable with high cardinality?,<python><statistics><data-science><correlation><exploratory-data-analysis>,2,0,N/A,CC BY-SA 4.0
73041999,1,-1.0,2022-07-19 18:37:14,0,28,"<p>I have the following code in R</p>
<pre><code>v &lt;- c(&quot;featureA&quot;, &quot;featureB&quot;)
newdata &lt;- unique(data[v])
print(unique(data[v])
print(predict(model, newdata, type='response', allow.new.level = TRUE)
</code></pre>
<p>And I got the following result</p>
<pre><code>  featureA       featureB
1    bucket_in_10_to_30 bucket_in_90_to_100
2    bucket_in_10_to_30  bucket_in_50_to_90
3     bucket_in_0_to_10  bucket_in_50_to_90
4     bucket_in_0_to_10 bucket_in_90_to_100
7    bucket_in_10_to_30  bucket_in_10_to_50
10  bucket_in_30_to_100 bucket_in_90_to_100
19    bucket_in_0_to_10   bucket_in_0_to_10
33    bucket_in_0_to_10  bucket_in_10_to_50
36  bucket_in_30_to_100  bucket_in_10_to_50
38   bucket_in_10_to_30   bucket_in_0_to_10
52  bucket_in_30_to_100   bucket_in_0_to_10
150 bucket_in_30_to_100  bucket_in_50_to_90

   1           2           3           4           7          10          19          33          36          38          52         150 
0.001920662 0.005480186 0.000961198 0.000335883 0.006311521 0.004005570 0.000620979 0.001107773 0.013100210 0.003546136 0.007382468 0.011384935 
</code></pre>
<p>And I'm wondering if it's possible in R that I can reshape and directly get a 3 x 4 tables similar to this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">feature A / features B</th>
<th style=""text-align: center;"">bucket_in_90_to_100</th>
<th style=""text-align: right;"">bucket_in_50_to_90</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">bucket_in_0_to_30</td>
<td style=""text-align: center;"">...</td>
<td style=""text-align: right;"">...</td>
</tr>
<tr>
<td style=""text-align: left;"">bucket_in_0_to_30</td>
<td style=""text-align: center;"">...</td>
<td style=""text-align: right;"">...</td>
</tr>
</tbody>
</table>
</div>
<p>Thanks for the help!</p>
",19581556.0,-1.0,N/A,2022-07-19 18:37:14,Display result of predict in a 3 x 4 table,<r><data-science><data-analysis>,0,2,N/A,CC BY-SA 4.0
71890943,1,-1.0,2022-04-16 03:50:52,0,1074,"<p>I'm trying to load a machine learning model <code>.sav</code> file using <code>Pickle</code></p>
<p>but it always outputs the error  <code>ModuleNotFoundError: No module named 'wrappers'</code></p>
<p>I tried adding the import wrapper after a <code>pip intall wrapper</code> but it didn't solve my problem,
and I didn't seem to find stuff related to this anywhere else</p>
<p>this is my code :</p>
<pre class=""lang-py prettyprint-override""><code>import pandas
import os
import sys
import numpy as np
import pickle
import wrapper

with open('C:\\Users\\SBS\\IMPORTED_DATA\\Trend_Perfect_SVC\\machine_learning_model.sav', 'rb') as file:
    response = pickle.load(file)
</code></pre>
<p>This is the complete error output:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_5184/102863235.py in &lt;module&gt;
      1 with open('C:\\Users\\SBS\\IMPORTED_DATA\\Trend_Perfect_SVC\\machine_learning_model.sav', 'rb') as file:
----&gt; 2     response = pickle.load(file)

ModuleNotFoundError: No module named 'wrappers'
</code></pre>
<p>I'm running this code on my local machine using the <code>JupyterLab</code> environment</p>
<p>I can provide a link to the file I'm using if it helps
Any help would be appreciated</p>
",10956327.0,18321042.0,2022-04-17 06:41:22,2022-04-17 06:41:22,ModuleNotFoundError: No module named 'wrappers' when trying to load file using Pickle,<python><tensorflow><machine-learning><keras><data-science>,1,1,N/A,CC BY-SA 4.0
71893337,1,-1.0,2022-04-16 11:05:58,0,254,"<p>how can I actually proceed it in a single pipeline, is there any value missing or wrongly defined something.</p>
<pre><code> #instantiate
imputer = SimpleImputer()
ohe = OneHotEncoder(use_cat_names=True)

#fit
imputer.fit(X_train)
ohe.fit(X_train)

#transform
XT_train = imputer.transform(X_train[&quot;lat&quot;,&quot;lon&quot;])
XT_train = ohe.transform(X_train[&quot;neighborhood&quot;])



model = make_pipeline(
    SimpleImputer(),
    OneHotEncoder(use_cat_names=True),
    Ridge()
)
model.fit(X_train, y_train)
</code></pre>
<p><a href=""https://i.stack.imgur.com/Wt6F6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wt6F6.png"" alt=""enter image description here"" /></a></p>
<p>Error I found in the console like</p>
",13824261.0,-1.0,N/A,2022-08-25 22:39:36,Getting Error of using OneHotEncoder and SimpleImputer in a Single Pipeline,<python><machine-learning><data-science><one-hot-encoding><data-wrangling>,2,0,N/A,CC BY-SA 4.0
71894188,1,-1.0,2022-04-16 13:16:55,0,41,"<p>I want to add new column on basis of if column Local Channel Name contains HFS than new column value is MM and if Column Contains MR than value should be MR and if nothing matched it will be Small</p>
<p>Here is the dataframe</p>
<pre><code>    Site            Month               Local Channel Name                       Total
    Ahmedabad   FEBRUARY 2022   LARGE A PHARMACY [2002813449] HFS       2
    Ahmedabad   FEBRUARY 2022   LARGE A PHARMACY [2002813449]           7
    Ahmedabad   FEBRUARY 2022   LARGE A PHARMACY 
    Ahmedabad   FEBRUARY 2022   LARGE A PHARMACY [2002813449]   MR      1
    Ahmedabad   FEBRUARY 2022   LARGE B PHARMACY                        4
</code></pre>
<p>The actual logic which I am trying to code in pandas is
<a href=""https://i.stack.imgur.com/JDJsx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JDJsx.jpg"" alt=""logic-part1"" /></a>
<a href=""https://i.stack.imgur.com/mA1wW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mA1wW.jpg"" alt=""logic-part2"" /></a></p>
<p>I have tried this</p>
<pre><code>df1['Channel Group'] = pd.np.where(df1['Local Channel Name'].str.contains(regex1),&quot;ECOM&quot;,
pd.np.where( df1['Cust Type Name'].str.contains(regex),&quot;MR&quot;,
pd.np.where(df1['Cust Type Name'].str.match(regex5),&quot;HFS MM&quot;,     
pd.np.where(df1['Local Channel Name'].str.match(regex2),&quot;HFS MM&quot;, 
pd.np.where(df1['Cust Type Name'].str.match(regex3),&quot;HFS MM&quot;,  
pd.np.where(df1['Cust Type Name'].str.match(regex6),&quot;HFS MM&quot;,
pd.np.where(df1['Local Channel Name'].str.contains(regex4),&quot;Large&quot;,  
pd.np.where(df1['Local Channel Name'].str.contains(regex7),&quot;Medium&quot;,  
pd.np.where(df1['Cust Type Name'].str.contains(regex8),&quot;WS&quot;,  
pd.np.where(df1['Cust Type Name'].str.contains(regex9),&quot;HFS Others&quot;,
pd.np.where(df1['Cust Type Name'].str.contains(regex10),&quot;SUBED&quot;, &quot;Small&quot;)))))))))))
</code></pre>
",16647912.0,-1.0,N/A,2022-04-16 13:21:52,Add new column in pandas df on basis str contains and equals,<python><pandas><dataframe><numpy><data-science>,1,0,N/A,CC BY-SA 4.0
74756633,1,-1.0,2022-12-10 21:24:29,0,32,"<pre><code>model = pm.auto_arima(df.outtemp, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=None,           # let model determine 'd'
                      seasonal=False,   # No Seasonality
                      start_P=0, 
                      D=0, 
                      trace=True,
                      error_action='ignore',  
                      suppress_warnings=True, 
                      stepwise=True)

print(model.summary())
</code></pre>
<p>I want to use this arima to draw a graph based on the df.outtemp, so instead of only predict after this series, I want to make it predict inside the outtemp index, for example the outtemp start with 1, 2, 3... I want to predict start 1, 2, 3 and draw two lines in one graph to compare them.</p>
<p>But <code>model.predict(n_periods=n_periods, return_conf_int=True)</code> seems can only predict after the last outtemp index.</p>
<p>something like this:</p>
<p><a href=""https://i.stack.imgur.com/FEXG2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FEXG2.png"" alt=""enter image description here"" /></a></p>
",20410560.0,20410560.0,2022-12-10 21:30:54,2022-12-10 21:30:54,How to make ARIMA model predict many times one by one?,<data-science><statsmodels><arima>,0,0,N/A,CC BY-SA 4.0
74765501,1,-1.0,2022-12-12 00:02:21,0,50,"<p>So i have a sentiment value (range -2&lt;x&lt;2) with an attached categorical data (0,1). i want to see if the sentiment value can predict the number of categorical data. How would I do this? which model am I to use? data is like this</p>
<pre><code>element id | sentiment | Delta
1          | -1.038    | 0
2          |  1.263    | 0
3          | -1.900    | 0
4          |  0.038    | 1
5          |  0.000    | 0
6          |  0.458    | 0
</code></pre>
<p>Additionally, the data is highly skewed, with 90%+ being delta value of 0</p>
<p>I've tried fitting a linear model but its not working</p>
",20751663.0,9696037.0,2022-12-12 00:09:12,2022-12-12 00:11:09,How do I do linear regression on count output with continuous predictor? (Sentiment Analysis,<r><statistics><regression><data-science><sentiment-analysis>,1,1,N/A,CC BY-SA 4.0
74760126,1,-1.0,2022-12-11 10:44:58,0,68,"<p>My fruit detection app shows fruit results even though the user uploads a picture that is not a fruit. Is it possible to display a &quot;no plant detected&quot; message?</p>
<p>My code:</p>
<pre><code>private void classifyImage(Bitmap image) {
    try {
        FruitDisease model = FruitDisease.newInstance(getApplicationContext());

        TensorBuffer inputFeature = TensorBuffer.createFixedSize(new int[]{1, 224, 224, 3}, DataType.FLOAT32);
        ByteBuffer byteBuffer = ByteBuffer.allocateDirect(4 * imageSize * imageSize * 3);
        byteBuffer.order(ByteOrder.nativeOrder());

        int[] intValue = new int[imageSize * imageSize];
        image.getPixels(intValue, 0, image.getWidth(), 0, 0, image.getWidth(), image.getHeight());

        int pixel = 0;
        for (int i = 0; i &lt; imageSize; i++) {
            for (int j = 0; j &lt; imageSize; j++) {
                int val = intValue[pixel++];
                byteBuffer.putFloat(((val &gt;&gt; 16) &amp; 0xFF) * (1.f / 255.f));
                byteBuffer.putFloat(((val &gt;&gt; 8) &amp; 0xFF) * (1.f / 255.f));
                byteBuffer.putFloat((val &amp; 0xFF) * (1.f / 255.f));
            }
        }
        inputFeature.loadBuffer(byteBuffer);

        FruitDisease.Outputs outputs = model.process(inputFeature);
        TensorBuffer outputFeature0 = outputs.getOutputFeature0AsTensorBuffer();

        float[] confidences = outputFeature0.getFloatArray();
        int maxPos = 0;
        float maxConfidence = 0;
        for (int i = 0; i &lt; confidences.length; i++) {
            if (confidences[i] &gt; maxConfidence) {
                maxConfidence = confidences[i];
                maxPos = i;
            }
        }

        String[] classes = {&quot;Watermelon Healthy&quot;, &quot;Watermelon Blossom End Rot&quot;, &quot;Watermelon Anthracnose&quot;,
                &quot;Mango Healthy&quot;, &quot;Mango Bacterial Canker&quot;, &quot;Mango Anthracnose&quot;,
                &quot;Orange Scab&quot;, &quot;Orange Healthy&quot;,
                &quot;Orange Bacterial Citrus Canker&quot;, &quot;Banana Healthy&quot;, &quot;Banana Crown Rot&quot;,
                &quot;Banana Anthracnose&quot;, &quot;Apple Scab&quot;, &quot;Apple Healthy&quot;, &quot;Apple Black Rot Canker&quot;};

        result.setText(classes[maxPos]);
        String s = &quot;&quot;;
        int i;
        for (i = 0; i &lt; classes.length; i++) {
            s += String.format(&quot;%s: %.1f%%\n&quot;, classes[i], confidences[i] * 100);
            confidence.setText(s);
            confidence.setVisibility(View.VISIBLE);
        }
</code></pre>
",15776263.0,2347649.0,2022-12-12 14:16:04,2022-12-12 14:16:04,Custom objects classifier always returns one of classes even if the picture belongs to none of them,<java><machine-learning><data-science><image-classification>,1,2,N/A,CC BY-SA 4.0
74761900,1,-1.0,2022-12-11 15:05:54,0,31,"<pre><code>Q&lt;-patients_with_stroke &lt;- data[data$CVDSTRK3==1,]
max_numwomen&lt;-max(patients_with_stroke$NUMWOMEN, na.rm = TRUE)
max_numwomen
</code></pre>
<p>Please help. I tried to find the highest value for number of adult women in the household where someone has ever had a stroke and then summarise the value in a variable called max_numwomen   thank you</p>
",20602328.0,14992857.0,2023-05-15 12:24:25,2023-05-15 12:24:25,Error message: Error in data$CVDSTRK3 : object of type 'closure' is not subsettable,<r><data-science><data-analysis>,0,2,N/A,CC BY-SA 4.0
74776305,1,-1.0,2022-12-12 19:23:58,0,26,"<p>I have a dataset of close to 1million boards(chess) in a dataframe. I want to apply a custom function to get an evaluation score for each board using stockfish. Is there a way to speed up this process? Here are the applicable code</p>
<pre><code>def getScoreFromBoard(board):
  return engine.analyse(board,chess.engine.Limit(time=1))['score'].pov(chess.WHITE)

extendedBoardsArray = [] # THIS IS WHERE THE AROUND 1 MILLION BOARDS ARE KEPT
boardsWithEval = pd.DataFrame([x,getScoreFromBoard(x)]for x in extendedBoardsArray)
</code></pre>
<p>Will using pandas.apply be faster? Using the swifter package? I am open to any other alternative methods. I am using google colab if that matters.</p>
",17292959.0,17292959.0,2022-12-12 19:40:43,2022-12-12 19:40:43,How to improve efficiency of applying custom function for dataset of 1million rows,<pandas><data-science><google-colaboratory><apply><stockfish>,0,1,N/A,CC BY-SA 4.0
74771579,1,74771883.0,2022-12-12 13:08:04,0,267,"<p>I have a pandas dataframe like this.</p>
<pre><code>import pandas as pd
student_id = ['001', '002', '003', '004']
names = ['Jane', 'Mary', 'Andrew', 
'Paul']
address = ['7 karumu st Ikeja Lagos', '8 
logo street Umuahia Abia', 
       '10 jege close PH Rivers', '9 
Lekki gate Lagos']

test_1 = {'Student_ID': student_id, 
      'Name': names, 
      'Address': address}
df = pd.DataFrame(test_1)
df`
</code></pre>
<p><a href=""https://i.stack.imgur.com/tcxAi.png"" rel=""nofollow noreferrer"">Output</a></p>
<p>and a list like this:</p>
<pre><code>List = [Imo, Lagos, Abia, Ebonyi, Rivers]
</code></pre>
<p>So i am trying to iterate through the Address column and estract the states in the address which is also in the list. If a state in the list is spotted I would like to extract it and append to a new column called state.</p>
<p>I tried to use the iterrows() method but I am a bit lost</p>
",8446216.0,8446216.0,2022-12-12 13:30:42,2022-12-12 13:53:48,How can I create a new column in a pandas data frame by extracting words from sentences in another column?,<python><pandas><dataframe><loops><data-science>,3,2,N/A,CC BY-SA 4.0
74779033,1,74780673.0,2022-12-13 01:21:12,0,204,"<p>I'm trying to create a radiobutton in the style of the checkboxGroupButton() in R shiny.</p>
<p>I want to recreate this example with the same button aesthetics, but only allow the user to select one input at a time.</p>
<pre><code>library(shiny)
library(shinyWidgets)

ui &lt;- fluidPage(
  tags$h1(&quot;checkboxGroupButtons examples&quot;),
  
  checkboxGroupButtons(
    inputId = &quot;somevalue1&quot;,
    label = &quot;Make a choice: &quot;,
    choices = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)
  ),
  verbatimTextOutput(&quot;value1&quot;)
)
  
 

server &lt;- function(input, output) {
  
  output$value1 &lt;- renderPrint({ input$somevalue1 })

  
}

if (interactive())
  shinyApp(ui, server)
</code></pre>
<p>Thanks!</p>
",20154154.0,2372064.0,2022-12-13 02:57:14,2022-12-13 06:15:33,checkboxGroupButton - Select one at a time,<r><shiny><data-science><shinywidgets>,2,3,N/A,CC BY-SA 4.0
74786186,1,-1.0,2022-12-13 14:11:54,0,23,"<p>Is there a concept in the AI, DS space where in I have a number I'm trying to optimize/maximize. The way this number would be optimize/maximize would be to add value to my input run my calculation and see how that compares to my goal, but I want to see how different permuations/combinations of adding/remove values optimizes my goal. For example:</p>
<p>Agent A,Agent B,Agent C,Agent D ..... N</p>
<p>I want to add their respective inputs into my calculation and see if adding all, none , some combinations, etc of them effects my goal result. If I just A how does that affect my goal results or A and B how does that affect my goal results  or A,B,C  how does that affect my goal results  or B,C or just B, etc. I believe this is a monte carlo excercise but its unclear to me how to do these different permutation test to arrive at check them again my goal result.</p>
<p>For reference I'm trying to do all of this in python. Thanks!</p>
<p>I've looked around to see concepts of scipy optimize but its unclear how to simulate different permautions to arrive at checking them again my goal result.</p>
",20503094.0,-1.0,N/A,2022-12-13 14:11:54,Is there a concept/term for permutation simulations and compare to goal,<python><machine-learning><data-science><artificial-intelligence>,0,0,N/A,CC BY-SA 4.0
74786686,1,-1.0,2022-12-13 14:51:42,0,71,"<p>Recently I'm working on jupyter notebook of ibm watson and ibm db2 and I want to connect them. But it's showing error. And I am new to ibm db2 and ibm watson. Here is my code:</p>
<pre><code>!pip install --force-reinstall ibm_db==3.1.0 ibm_db_sa==0.3.3
!pip uninstall sqlalchemy==1.4 -y &amp;&amp; pip install sqlalchemy==1.3.24
!pip install ipython-sql


%load_ext sql
</code></pre>
<blockquote>
<p>The sql extension is already loaded. To reload it, use:   %reload_ext
sql</p>
</blockquote>
<pre><code>%sql ibm_db_sa://username:password@6667d8e9-9d4d-4ccb-ba32-21da3bb5aafc.c1ogj3sd0tgtu0lqde00.databases.appdomain.cloud:30376/bludb?security=SSL
</code></pre>
<p>Ans it showing this error:</p>
<blockquote>
<p>Connection info needed in SQLAlchemy format, example:
postgresql://username:password@hostname/dbname
or an existing connection: dict_keys([]) Can't load plugin: sqlalchemy.dialects:ibm_db_sa Connection info needed in
SQLAlchemy format, example:
postgresql://username:password@hostname/dbname
or an existing connection: dict_keys([])</p>
</blockquote>
<p>Any suggestion how to connect IBM DB2 on jupyter notebook of IBM watson.</p>
",12904389.0,-1.0,N/A,2022-12-13 14:51:42,DB2 database not connecting with notebooks on IBM Watson,<python><jupyter-notebook><data-science><jupyter><websphere>,0,0,N/A,CC BY-SA 4.0
71895885,1,-1.0,2022-04-16 17:06:33,0,430,"<p>I'm using Weka to implement a unsupervised algorithms like LOF to detect outliers within a given dataset but of course the dataset has to contain binary attributes as LOF is dependent on this (according Weka compatibility information) and it does (0,0,1,0,1). However, when I try to import the dataset from the preprocessing section on Weka I get back an error outling this:</p>
<p>File: C:\Users\JohnDoe\Downloads\labelled_Dataset.csv</p>
<p>Reason: index 13 out of bounds for length 13 Problem encountered on line: 293</p>
<p>Does anyone here have any ideas on how to overcome this, I've never encountered this error and a google search doesn't particularly help in this context too as it comes back with different results. I've been at this for 4 days, any advice will be taken into consideration.</p>
",14580096.0,-1.0,N/A,2022-04-16 17:06:33,Weka- Importing .csv file error into Weka,<algorithm><machine-learning><data-science><weka><unsupervised-learning>,0,9,N/A,CC BY-SA 4.0
74773713,1,-1.0,2022-12-12 15:49:09,0,293,"<p>I want to find the average of my column'Preheat_To_Pour_Time' based on the values of the column <code>Rampmelt_Active</code>. Column <code>Rampmelt_Active</code> values are either a 1 or a 0 based on if it's active. I can't figure out how to use the values I get from <code>.value_counts()</code> if I even need them.</p>
<p>I have tried using <code>.value_counts()</code> on <code>Rampmelt_Active</code> to get me the count I need to use in my division. As well as the .mean() method. However, this only gives me one value instead of the average of the 0's and 1's.</p>
",16947520.0,4398242.0,2022-12-16 04:45:27,2022-12-16 04:45:27,How to find the average based on the value of another column. Python/Pandas,<pandas><group-by><data-science>,1,0,N/A,CC BY-SA 4.0
74775126,1,-1.0,2022-12-12 17:38:28,0,303,"<p>I have quite some categorical variable in my dataset, These variables have more than two levels each. Now i want an R code function (or loop) that can calculate the entropy and information gain for each levels in each categorical variable and return the lowest entropy and highest information gain.</p>
<pre><code>data &lt;- list(buys = c(&quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;),credit = c(&quot;fair&quot;, &quot;excellent&quot;, &quot;fair&quot;, &quot;fair&quot;, &quot;fair&quot;, &quot;excellent&quot;, &quot;excellent&quot;, &quot;fair&quot;, &quot;fair&quot;, &quot;fair&quot;, &quot;excellent&quot;, &quot;excellent&quot;, &quot;fair&quot;, &quot;excellent&quot;),student = c(&quot;no&quot;, &quot;no&quot;, &quot;no&quot;,&quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;),income = c(&quot;high&quot;, &quot;high&quot;, &quot;high&quot;, &quot;medium&quot;, &quot;low&quot;, &quot;low&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;medium&quot;),age = c(25, 27, 35, 41, 48, 42, 36, 29, 26, 45, 23, 33, 37, 44))
data&lt;- as.data.frame(data)
</code></pre>
<p>Above is a sample dataframe</p>
<pre><code>entropy_tab &lt;- function(x) { tabfun2 &lt;- prop.table(table(data[,x],training_credit_Risk[,13]) + 1e-6, margin = 1)sum(prop.table(table(data[,x]))*rowSums(-tabfun2*log2(tabfun2)))}
</code></pre>
<p>Above function calculates entropy for each variable, i want a fuction to calculate the contribution to the entropy for each level? i.e the contribution of &quot;excellent&quot; and &quot;fair&quot; to the entropy of &quot;Credit&quot;</p>
",6686671.0,6686671.0,2022-12-12 18:36:40,2022-12-13 19:01:27,what R Code to calculate the entropy for each level in a categorical variable,<r><dataframe><data-science><entropy><information-gain>,2,7,N/A,CC BY-SA 4.0
74791314,1,-1.0,2022-12-13 21:55:59,-1,83,"<p>I have the following dataset and I am using the dbscan algorithm to try and perform clustering.
<a href=""https://i.stack.imgur.com/yIyTh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yIyTh.png"" alt=""enter image description here"" /></a></p>
<p>However no matter what values for min points and eps I choose I get very few and very bad clusters. Isn't dbscan supposed to perform well on clusters of similar density that are separated by some outliers?</p>
",12607149.0,-1.0,N/A,2022-12-13 22:24:02,Why does dbscan perform poorly on this dataset,<data-science><cluster-analysis>,1,0,N/A,CC BY-SA 4.0
74786810,1,74787590.0,2022-12-13 15:00:19,1,80,"<p>Can someone help me?</p>
<p>I have a huge dataframe to work with (90 thousand rows) and I need to apply this logic, but I can only think of solving the problem using a 'for' loop, and this is taking almost 2 hours to run... Can someone give me some light on how I can optimize the code?</p>
<p>The logic is as follows:
For each 'Customer', I need to check if his 'FinalKey' exists in the 'Key' column. If it exists, the 'Final Name' of this customer will be the same as the most repeated name in 'Customer' for that same 'FinalKey' in 'Key'.
Here is an example below:</p>
<pre><code>
</code></pre>
<h1>Creates the dataframe df</h1>
<pre><code>data = [['Tom','123', '123'], ['Tom', '54', '123'], \
    ['Tom', '21', '123'], ['Tom2', '123', '123'], \
    ['Tom3', '123', '123'], ['Tom3', '123', '123'], \
    ['John', '45', '45'], ['Mary', '23', '41']]

df = pd.DataFrame(data, columns=['Customer', 'Key', 'FinalKey'])
df['Final Name']=''
</code></pre>
<h1>Print dataframe</h1>
<p>df</p>
<pre><code>
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Customer</th>
<th>Key</th>
<th>FinalKey</th>
<th>Final Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tom</td>
<td>123</td>
<td>123</td>
<td></td>
</tr>
<tr>
<td>Tom</td>
<td>54</td>
<td>123</td>
<td></td>
</tr>
<tr>
<td>Tom</td>
<td>21</td>
<td>123</td>
<td></td>
</tr>
<tr>
<td>Tom2</td>
<td>123</td>
<td>123</td>
<td></td>
</tr>
<tr>
<td>Tom3</td>
<td>123</td>
<td>123</td>
<td></td>
</tr>
<tr>
<td>Tom3</td>
<td>123</td>
<td>123</td>
<td></td>
</tr>
<tr>
<td>John</td>
<td>45</td>
<td>45</td>
<td></td>
</tr>
<tr>
<td>Mary</td>
<td>41</td>
<td>41</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Here is the logic:</p>
<pre><code>
</code></pre>
<h1>Logic</h1>
<pre><code>for i in range(0, len(df['Customer'])):  

if str(df.loc[i, 'FinalKey']) in list(df['Key']):    
    df.loc[i, 'Final Name'] = df[df['Key']==df.loc[i, 'FinalKey']]['Customer'].value_counts().idxmax()

else:
    df.loc[i, 'Final Name'] = &quot;&quot;
</code></pre>
<p>df</p>
<pre><code>

| Customer |   Key   | FinalKey | Final Name | 
| -------- | ------- | -------- | ---------- |
|   Tom    |   123   |    123   |    Tom3    |
|   Tom    |    54   |    123   |    Tom3    |
|   Tom    |    21   |    123   |    Tom3    |
|   Tom2   |   123   |    123   |    Tom3    |
|   Tom3   |   123   |    123   |    Tom3    |
|   Tom3   |   123   |    123   |    Tom3    |
|   John   |    45   |     45   |    John    |
|   Mary   |    23   |     41   |            |
</code></pre>
",20767083.0,10836309.0,2022-12-13 15:02:24,2022-12-13 15:55:44,"Is there a better way to replace the ""for"" loop in python?",<python><pandas><dataframe><data-science>,1,2,N/A,CC BY-SA 4.0
74786916,1,-1.0,2022-12-13 15:07:13,0,49,"<p>I just did mean imputation on a pandas dataframe, but the column after imputation in converted to object type and is not converting back to float. I tried astype(float), which says need a string not a method, to_numeric() also donot work.</p>
<p><a href=""https://i.stack.imgur.com/jI1Iq.png"" rel=""nofollow noreferrer"">this is it is converted to object</a></p>
<p><a href=""https://i.stack.imgur.com/u8j0G.png"" rel=""nofollow noreferrer"">Applying astype() function</a></p>
<p><a href=""https://i.stack.imgur.com/7OCTs.png"" rel=""nofollow noreferrer"">This is the error that comes</a></p>
<pre><code># mean imputation
mean_age = df['Age'].mean
df['Mean_Age'] = df['Age'].fillna(mean_age)

# This makes the column to object and astype() and to_numeric() even donot work to convert it back to float 
</code></pre>
",14985270.0,14985270.0,2022-12-13 15:11:34,2022-12-13 15:11:34,Why Imputation converts float64 column to object and not converting again?,<pandas><machine-learning><scikit-learn><statistics><data-science>,0,2,N/A,CC BY-SA 4.0
74793086,1,74793115.0,2022-12-14 02:51:14,-1,56,"<p>I have a csv of size 12500 X 3. The first two columns (A and B) are inputs and the the final column  (C) is the sum of the two columns.</p>
<p>I wanted to build a prediction model to get the value of C for a given A and B. This is just a basic model to imporve my understanding of machine learning.</p>
<p>The accuracy score is almost zero <strong>(0.00032)</strong> and the model is way to simple to get the predictions wrong. The code is below:</p>
<pre><code>import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

data = pd.read_csv('Dataset.csv') #importing dataset
X = data.drop(columns=['C'])
y = data['C']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = DecisionTreeClassifier()
model.fit(X_train,y_train)
predictions = model.predict(X_test)
score = accuracy_score(y_test, predictions)
score
</code></pre>
<p>I did not even include outlier into the data and I create the csv using excel formulae. I used jupyter notebook to build this prediction model. Can someone please point out if/what I'm doing wrong?</p>
",10693689.0,-1.0,N/A,2022-12-19 07:22:56,Building a basic prediction model with the output being the sum of the two inputs but accuracy score is significantly low,<python><machine-learning><data-science>,1,0,N/A,CC BY-SA 4.0
74788481,1,-1.0,2022-12-13 17:08:06,0,30,"<p>I am doing some Kaggle competition called <strong>TitanicSpaceship</strong> (<a href=""https://www.kaggle.com/competitions/spaceship-titanic"" rel=""nofollow noreferrer"">https://www.kaggle.com/competitions/spaceship-titanic</a>) and I had an error:</p>
<pre><code>train['CryoSleep'] = train['CryoSleep'].map({'False':0, 'True':1}).astype(int)
IntCastingNaNError: Cannot convert non-finite values (NA or inf) to integer
</code></pre>
<p>I don't know what to do.</p>
<p>I tried to see if there was a problem with Pandas function called dropna() but it did not change a thing.</p>
",20628635.0,4398242.0,2022-12-15 13:09:47,2022-12-15 13:09:47,Why can't I change the dtype of a variable?,<python><pandas><integer><data-science><imputation>,1,1,N/A,CC BY-SA 4.0
74788520,1,74796561.0,2022-12-13 17:10:54,0,22,"<p>I'm having a problem with the definition of <strong>order</strong> of AR, MA and ARMA time series forecasting processes. Imagine we have a time series with data from January to December, and we're in July, trying to predict August. When we say AR(2), are we using lags relating to July and June, or can those two months be any month between January and June?</p>
<p>I tried checking multiple sources but they define order as different things.</p>
",10631087.0,-1.0,N/A,2022-12-14 10:08:49,"Does the order of an ARMA(p,0) model mean it uses the last p lags or any previous p lags?",<math><time-series><data-science><forecasting><arima>,1,0,N/A,CC BY-SA 4.0
74788905,1,74788950.0,2022-12-13 17:42:52,2,166,"<p>I have various categorical variable with more than 5 levels each, I want a function that can collapse them into just two levels</p>
<pre><code>column1&lt;- c(&quot;bad&quot;,&quot;good&quot;,&quot;nice&quot;,&quot;fair&quot;,&quot;great&quot;,&quot;bad&quot;,&quot;bad&quot;,&quot;good&quot;,&quot;nice&quot;,
            &quot;fair&quot;,&quot;great&quot;,&quot;bad&quot;)
column2&lt;- c(&quot;john&quot;,&quot;ben&quot;,&quot;cook&quot;,&quot;seth&quot;,&quot;brian&quot;,&quot;deph&quot;,&quot;omar&quot;,&quot;mary&quot;,
            &quot;frank&quot;,&quot;boss&quot;,&quot;kate&quot;,&quot;sall&quot;)

df&lt;- data.frame(column1,column2)
</code></pre>
<p>So for the data frame above, in the column1, I want to convert all &quot;bad&quot; to &quot;bad&quot; and other levels to &quot;others&quot; with a function. I have no idea how to do that.
Thanks</p>
",6686671.0,7727429.0,2022-12-13 17:44:52,2022-12-13 17:51:57,How to collapse levels in a categorical variable in R,<r><dataframe><data-science>,4,1,N/A,CC BY-SA 4.0
74794770,1,-1.0,2022-12-14 07:19:28,0,21,"<p>my question is related to my original wanting to train 70% of my data with only the cross-validation k-fold method, then use a &quot;modified model&quot; to test/predict the last 30%. I read some of the documentation, and I just want to check if cross-validation is only a scoring function with no power of altering parameters, and for it to get any altering ability it has to be paired with hyperparameter tuning.</p>
",19007427.0,-1.0,N/A,2022-12-14 07:19:28,dose cross-validation alter any parameter or hyperparameters in the algorithm or just give an average scoring?,<python><machine-learning><data-science><cross-validation><hyperparameters>,0,0,N/A,CC BY-SA 4.0
74795298,1,-1.0,2022-12-14 08:18:05,0,53,"<p>I have two dictionaries as input.
The first one contains pin numbers as keys and as values - the probability of a certain code.</p>
<pre><code>`dict_1={
8063: {'code15': 99.61, 'code17': 96.14}, 9621: {'15': 88.59}, 1583: {'code17': 99.37, 'code14':87.37}, 
7631: {'code17': 99.88}, 4345: {'code11': 99.97, 'code12': 99.93},  1799: {'code11': 99.8,   'code12': 99.18},
3604: {'code17': 98.77}, 1098: {'code12': 99.96}, 3752: {'code11': 99.95}}`
</code></pre>
<p>The second one has clusters as keys with list of pin numbers and names in a dictionary as values.</p>
<pre><code>`dict_2={ 0: {
'count': 4,
'id': [{'pin': 8063,'name': 'John'},
            {'pin': 9621,'name': 'Maria'},
            {'pin': 1583,'name': 'Peter'},
           {'pin': 7631,'name': 'Jess'}]},
3: {
'count': 5,
'id': [
        {'pin': 4345,'name': 'George'},
        {'pin': 1799,'name': 'Kevin'},
           {'pin': 3604,'name': 'Sarah'},
            {'pin': 1098,'name': 'Stewie'},
            {'pin': 3752, 'name': 'Jan'}]}
}`
</code></pre>
<p>I want to create a dictionary that has the clusters from dict_2 as keys and as values - a dictionary with the codes from dict_1 as keys and the probability, pins and names in a list as values. This should be the output:</p>
<pre><code>`merged_dict =    {
0: {'code15': [99.61, 8063, 'John', 88.59, 9621,'Maria'],
'code17':[99.37, 1583, 'Peter', 96.14, 8063, 'John',99.88, 7631, 'Jess', 98.77, 3604, 'Sarah'],
'code14':[87.37, 1583, 'Peter'] },
3: {'code11': [99.97, 4345, 'George', 99.8,  1799, 'Kevin', 99.95, 3752, 'Jan'],
'code12':[99.93, 4345, 'George', 99.18,  1799, 'Kevin', 99.96,  1098, 'Stewie']}
}`
</code></pre>
<p>I've tried to append merge the dictionaries by pin as a common element, but no success.</p>
",20731694.0,20731694.0,2022-12-14 08:19:30,2022-12-14 09:05:20,How to create a new dictionary from two other dictionaries taking different keys and values with iteration?,<python><python-3.x><data-structures><jupyter-notebook><data-science>,1,0,N/A,CC BY-SA 4.0
74791338,1,-1.0,2022-12-13 21:59:45,0,54,"<p>What I am looking to do is for every email address that is the same, take the corresponding rows with that same email and create new dataframes and then send an email with the row information to the email address in col 1.</p>
<pre><code>| email             | Acct # | Acct Status |
| ------------------|--------|-------------|
| janedoe@gmail.com | 1230   | Closed      |
| janedoe@gmail.com | 2546   | Closed      |
| janedoe@gmail.com | 2468   | Closed      |
| janedoe@gmail.com | 7896   | Closed      |
| michaeldoe@aol.com| 4565   | Closed      |
| michaeldoe@aol.com| 9686   | Closed      |
|jackdoe@aol.com    | 4656   | Closed      |
</code></pre>
<p>I tried something along the lines of converting the dataframe into a list by using groupby but I am stuck:</p>
<pre><code>    df_list = [x for _, x in df.groupby(['email'])
</code></pre>
",19209961.0,3986395.0,2022-12-14 01:33:11,2022-12-15 20:34:50,If the same string is in the first column of a sorted dataframe take the rows associated with the unique value and create new dataframes,<python><dataframe><data-science><columnsorting>,1,0,N/A,CC BY-SA 4.0
74792191,1,-1.0,2022-12-13 23:55:43,0,27,"<p>I'm trying to create some probabilistic bounds for my machine learning regression predictions. What I'm currently doing is taking the output of the regression algorithm (I'm predicting team points scored), passing it in as the mean to a normal distribution, using the team's historical standard deviation of points scored as the standard deviation for the normal distribution, and then using the CDF to get the probability of them scoring any given range of points.</p>
<p>The problem I'm facing is that depending on the regression prediction and the standard deviation, I could be accidentally giving 10% probability that the team will score negative points.</p>
<p>Does anyone have any suggestions for how I can take the output of a regression algorithm, use it as a parameter to a distribution (along with some sort of variance parameter), and get a probability that a team scores over N points, while keeping it to strictly positive values? AKA the probability of scoring less than 0 points should always be 0.</p>
<p>Everything else about my current approach works well - I'm able to combine machine learning with traditional statistics for probabilistic bounds. The problem is these negative values.</p>
<p>Here's a concrete example:</p>
<pre><code>from scipy.stats import norm

#model predicts team will score 5 points
model_points_prediction = 5

#teams historical standard deviation of points scored is 3
team_points_standard_deviation = 3

#pass it parameters to normal distribution
dist = norm(model_points_prediction, team_points_standard_deviation)

#probability of scoring less than 0 points
print(dist.cdf(0))

</code></pre>
<p>And it print's out 4.7% chance of points being less than zero. What's a better solution to this that keeps it strictly positive?</p>
",10901843.0,-1.0,N/A,2022-12-13 23:55:43,Probabilistic Bounds for a Strictly Positive Machine Learning Point Estimate?,<machine-learning><scipy><statistics><data-science><probability>,0,2,N/A,CC BY-SA 4.0
74799089,1,-1.0,2022-12-14 13:34:41,0,144,"<p>I used the k-means algorithm to clustering set of documents which are textual data only.</p>
<p>The number of documents are 2lack records</p>
<p>Surprisingly the result for the clustering is</p>
<p>95% of records is storing in 1 cluster, remaining records are going to another clusters.</p>
<p>This is not a problem if it chooses correct cluster while predicting the data. I thought this might be the issue behind my problem.</p>
<p>Why this is happening?</p>
<p>Parameters : number of clusters = 5 (used elbow method to know this), random state = 0 or 42 (both used but no use), { init=' kmeans++ ' } also used to but no difference</p>
<p>Here is my code showing how I created vectors</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import _pickle as cPickle

def build_tfidf_vect(series,save_model = True ,) :
    vectorizer = TfidfVectorizer(stop_words=&quot;english&quot;)
    vectors = vectorizer.fit_transform(series)
    print(&quot;Shape of tfidf matrix: {}&quot;.format(vectors.shape))
    if save_model:
        data_struct = {'vectors': vectors, 'vectorizer': vectorizer}
        with open('data_2l.bin', 'wb') as f:
            cPickle.dump(data_struct, f)
    return vectorizer, vectors

import pandas as pd

feed = pd.read_csv('2l_data.csv', encoding='latin')

build_tfidf_vect(feed['Column_name'])
</code></pre>
<pre><code>from sklearn.cluster import MiniBatchKMeans
import scipy

def load_tf_idfvectors():
    import time
    import _pickle as cPickle
    with open(r'C:\Users\data_2l.bin', 'rb') as f:
        data_struct = cPickle.load(f)
        vectors,vectorizer = data_struct['vectors'], data_struct['vectorizer']
    return vectorizer,vectors

def dump(cluster_0,cluster_1,cluster_2,cluster_3,cluster_4):
    save_model=True
    if save_model:
        data_struct = {'cluster0': cluster_0, 'cluster1': cluster_1,'cluster2': cluster_2, 'cluster3': cluster_3,'cluster4': cluster_4}
        with open(r'C:\Users\totalclusters_2l.bin', 'wb') as f:
            cPickle.dump(data_struct, f)

import pandas as pd
import pickle
data=pd.read_csv(r&quot;C:\Users\New_vectors_data_2l.csv&quot;, encoding=&quot;latin&quot;)
tfidf_vectorizer,tfidf=load_tf_idfvectors()

kmeans = MiniBatchKMeans(n_clusters=5, init= 'k-means++',random_state=0).fit(tfidf)
labels = kmeans.fit_predict(tfidf)
X_n=pd.DataFrame(tfidf,columns=['tf-idf'])
labels_n=pd.DataFrame(labels,columns=['cl_n'])
result = pd.concat([data['Column_name'],X_n,labels_n])
.
.
.
.
</code></pre>
",20646248.0,2275490.0,2022-12-19 18:48:18,2022-12-19 18:48:18,What can be the reasons for 95% of samples belong to one cluster when there are 5 clusters?,<python><machine-learning><scikit-learn><data-science><k-means>,1,4,N/A,CC BY-SA 4.0
74795623,1,-1.0,2022-12-14 08:50:09,2,76,"<p>I am working on my data science homework. I am trying to download the weekly table data by using pyython from : <a href=""https://www.nordpoolgroup.com/en/Market-data1/Dayahead/Area-Prices/ALL1/Hourly12/?view=table"" rel=""nofollow noreferrer"">https://www.nordpoolgroup.com/en/Market-data1/Dayahead/Area-Prices/ALL1/Hourly12/?view=table</a> for my exam.</p>
<p>I have the following code :</p>
<pre><code>import pandas as pd
urls = &quot;https://www.nordpoolgroup.com/en/Market-data1/Dayahead/Area-Prices/ALL1/Hourly12/?view=table&quot;
df = pd.read_html('urls')
df
</code></pre>
<p>I got the following repose:</p>
<pre><code>raise ValueError(&quot;No tables found&quot;)
</code></pre>
<p>I tried to inspect web console by using beautiful soup. This page have table (html) but i get empty data as a result. So, i tried to check with div then i get a result. That's mean the code is correct but the issue is why it not read table?</p>
<p>Here is my code :</p>
<pre><code>page = requests.get(url)
soup = BeautifulSoup(page.text, 'lxml')

for table in soup.find_all('div'):
    print(table.get('class'))

# then i tried to export the data in csv file.
open('export1.csv', 'wb').write(soup.content)

</code></pre>
<p>Photo of inspection</p>
<p><a href=""https://i.stack.imgur.com/UeH8K.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I tried many different solutions that recommend online but still couldn't solve the issue. Appreciate if you can help me download the table, thanks</p>
",17568688.0,17568688.0,2022-12-14 08:52:38,2022-12-14 09:48:03,couldn't download table dataset from https://www.nordpoolgroup.com/en/Market-data1/#/nordic/table,<python><web><beautifulsoup><data-science><screen-scraping>,1,0,N/A,CC BY-SA 4.0
74796361,1,-1.0,2022-12-14 09:53:02,-1,33,"<p>I am very new to data science, so I have a (basic?) question:</p>
<p>I have a set of materials (let's say plastics, glass, concrete…). I have a bunch of characteristics of each material (e.g. toughness, translucency) and for each of these materials I also have a score how they perform in a certain test.
Now I want to find out if there is some kind of correlation between the characteristics and the performance score. There is no linear correlation, I assume that it some combination of some (but not all) of the characteristics.</p>
<p>How do I go about finding out how they are &quot;connected&quot; ? What are the best methods? I was thinking of training a neural network but I don't have that much data and also, it seems like a bit of an overkill.</p>
<p>As I said I am very new to this so I am grateful for any hint or term I need to search for (I work with Python, btw).</p>
",19399312.0,-1.0,N/A,2022-12-15 10:50:10,Data Science: What is the best way to figure out the correlation between multiple characteristics and a performance score?,<python><statistics><data-science>,1,7,N/A,CC BY-SA 4.0
74803010,1,74803101.0,2022-12-14 18:54:09,2,82,"<p>I'm simulating a physical problem under different initial conditions using Python. As these realizations are completely independent of each other, I wanted to use the <code>multiprocessing</code> package.</p>
<p>Now, the result of one realization can easily be a few 100 Mb large (eg. 20 frames with 2500px by 2500px), so I quickly run out of memory. Luckily, I don't care about the individual results, but only the sum (or the average) of the arrays (also of the form 20 x 2500 x 2500, or <code>nsteps</code> x <code>nres</code> x <code>nres</code>).</p>
<p>However, I'm struggling to figure out how to find out when which process is done / how to access the data, add it to a results-array, and free the memory, while other realizations are still running. There must certainly be a more elegant solution than iterating over all non-finished processes with a &quot;try&quot;?</p>
<pre><code>def simulate():     # simulates one realization
    ...
    return pictures # np.array of shape (20x2500x2500)


if __name__ == '__main__':

    result = np.zeros([nsteps, nres, nres])
    
    pool = mp.Pool(6)
    processes = [pool.apply_async(simulate) for i in range(iterations)]

    result = np.sum([p.get() for p in processes], axis = 0)
</code></pre>
<p>I have the feeling that for the last line of my code there already exists a solution that performs exactly this, without waiting for all elements in <code>processes</code> to be finished.</p>
<p>Please note that even though my function does not take any arguments, the return value is always unique because there is random noise added to the result in the body of the function.</p>
",20778190.0,11659881.0,2022-12-15 22:41:52,2022-12-15 22:41:52,Multiprocessing in python - saving memory by adding results,<python><multiprocessing><data-science><python-multiprocessing>,1,1,2022-12-14 20:49:59,CC BY-SA 4.0
74803225,1,-1.0,2022-12-14 19:16:33,0,67,"<p>I have written a function to find the values for alpha, beta and gamma for ExponentialSmoothening. When I run this code without the gamma values, it works fine but when i give values for gamma, it gives me an error.
The function is outputting NaN values but for only a specific combination of the three values.</p>
<p>Code:</p>
<pre><code>import numpy as np
from sklearn.metrics import mean_absolute_error
from statsmodels.tsa.holtwinters import ExponentialSmoothing


# In[2]:


def auto_hwm(timeseries, val_split_date, alpha=[None], beta=[None], gamma=[None], phi=[None], 
              trend=None, seasonal=None, periods=None, verbose=False):

    '''The auto_hwm (short for auto holt winters model) function to search for the best possible parameter
        combination for the Exponential Smoothing model i.e. smoothing level, smoothing slope, 
        smoothing seasonal and damping slope based on mean absolute error.

        ****Paramters****

        timeseries: array-like

                  Time-Series

        val_split_date: str

                  The datetime to split the time-series for validation

        alpha: list of floats (optional)

                  The list of alpha values for the simple exponential smoothing parameter

        beta: list of floats (optional)

                  The list of beta values for the Holt’s trend method parameter

        gamma: list of floats (optional)

                  The list of gamma values for the holt winters seasonal method parameter

        phi: list of floats (optional)

                  The list of phi values for the damped method parameter

        trend: {“add”, “mul”, “additive”, “multiplicative”, None} (optional)

                  Type of trend component.

        seasonal: {“add”, “mul”, “additive”, “multiplicative”, None} (optional)
                  
                  Type of seasonal component.

        periods: int (optional)
                  
                  The number of periods in a complete seasonal cycle

        ****Returns****

        best_params: dict

                  The values of alpha, beta, gamma and phi for which the 
                  validation data (val_split_date) gives the least mean absolute error
    '''

    best_params = []
    actual = timeseries[val_split_date:]

    print('Evaluating Exponential Smoothing model for', len(alpha) * len(beta) * len(gamma) * len(phi), 'fits\n')

    for a in alpha:
        for b in beta:
            for g in gamma:
                for p in phi:

                    if(verbose == True):
                        print('Checking for', {'alpha': a, 'beta': b, 'gamma': g, 'phi': p})

                    model = ExponentialSmoothing(timeseries, trend=trend, seasonal=seasonal, seasonal_periods=periods)
                    model.fit(smoothing_level=a, smoothing_slope=b, smoothing_seasonal=g, damping_slope=p)
                    f_cast = model.predict(model.params, start=actual.index[0])
                    score = np.where(np.float64(mean_absolute_error(actual, f_cast)/actual).mean()&gt;0,np.float64(mean_absolute_error(actual, f_cast)/actual).mean(),0)

                    best_params.append({'alpha': a, 'beta': b, 'gamma': g, 'phi': p, 'mae': score})

    return min(best_params, key=lambda x: x['mae'])

</code></pre>
<p>auto_hwm(ts, val_split_date = '2018-10-01', alpha = [0.1, 0.2, 0.3,0.4, 0.5, 0.6, 0.8, 0.9], beta = [0.1, 0.2, 0.3,0.4, 0.5, 0.6, 0.8, 0.9], gamma = [0.1, 0.2, 0.3,0.4, 0.5, 0.6, 0.8, 0.9], trend='mul', seasonal='mul', periods=12, verbose=True)</p>
<p>It is giving NaN values for a specific combination of three values. There are no missing values in the series</p>
",19207823.0,-1.0,N/A,2022-12-14 19:16:33,Exponential smoothening giving NaN values,<dataframe><time-series><data-science>,0,0,N/A,CC BY-SA 4.0
74803563,1,-1.0,2022-12-14 19:56:08,0,115,"<p>I am trying to perform feature selection and to use RFECV from yellowbrick.model_selection. I have <strong>48 features</strong> in my train set. But when I run the code in below, the visualizer finds <strong>number of features as 49</strong>.</p>
<pre><code>xgboost_base_model = xgb.XGBClassifier(tree_method='hist')
visualizer_xgb = RFECV(xgboost_base_model, step=3, cv=3, scoring='roc_auc')
visualizer_xgb.fit(X_train, y_train)
visualizer_xgb.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/RLG3v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLG3v.png"" alt=""enter image description here"" /></a></p>
",20778625.0,2347649.0,2022-12-14 20:40:04,2022-12-14 20:40:04,Why RFECV visualizer from yellowbrick finds number of optimal features more than number of features in train set?,<scikit-learn><data-science><feature-selection><yellowbrick>,0,3,N/A,CC BY-SA 4.0
74789876,1,-1.0,2022-12-13 19:15:37,1,177,"<p>I'm trying to convert columns of latitude and longitude and create a new column for decimal degrees.</p>
<p>Both code examples work great for a single coordinate (also from Stack Overflow), but I need to iterate over multiple columns.</p>
<pre><code>latitude = &quot;20-18-6.65N&quot;
N = 'N' in latitude
d, m, s = map(float, latitude[:-1].split('-'))
latitude = (d + m / 60. + s / 3600.) * (1 if N else -1)
longitude = &quot;40-17-12.13W&quot;
W = 'W' in longitude
d, m, s = map(float, longitude[:-1].split('-'))
longitude = (d + m / 60. + s / 3600.) * (-1 if W else 1)

print(latitude)
print(longitude)


latitude = (&quot;20-18-6.65N&quot;)
longitude = (&quot;40-17-12.13W&quot;)

latitude = sum(float(x) / 60 ** n for n, x in enumerate(latitude[:-1].split('-')))  * (1 if 'N' in     latitude[-1] else -1)
longitude = sum(float(x) / 60 ** n for n, x in enumerate(longitude[:-1].split('-'))) * (1 if 'E' in longitude[-1] else -1)

print(latitude, longitude)


latlong = df.iloc[:, 0:3] # Subset for columns needed.
latlong_df = pd.DataFrame(data = latlong) # transform into pandas df.
lats_df = latlong_df.iloc[:, 1] # subset for latitude column.
longs_df = latlong_df.iloc[:, 2] # subset for longitude column.
for i in latlong_df: 
    lats_df = sum(float(x) / 60 ** n for n, x in enumerate(lats_df[:-1].split('-')))  * (1 if 'N' in lats_df[-1] else -1)
    longs_df = sum(float(x) / 60 ** n for n, x in enumerate(longs_df[:-1].split('-'))) * (1 if 'E' in longs_df[-1] else -1)
    print(lats_df, longs_df)
</code></pre>
<p>The last code block results in this error message: AttributeError: 'Series' object has no attribute 'split'</p>
<p>I've tried using a for loop, but I get this error message: 'DataFrame' object has no attribute 'split'. I've also tried using other data types other than a data frame and get the same error message. Any feedback is greatly appreciated!</p>
",20769516.0,20769516.0,2022-12-13 20:07:20,2022-12-13 20:07:20,Converting latitude/longitude to decimal degrees for entire data frame,<python><data-science><latitude-longitude>,0,3,N/A,CC BY-SA 4.0
74790294,1,-1.0,2022-12-13 20:02:26,0,169,"<p>I am currently trying to use the pycaret library in order to identify the best model for my data, however, I am getting this error message &quot;AttributeError: 'OneHotEncoder' object has no attribute 'get_feature_names'&quot;</p>
<p>This is the code that I am currently using</p>
<pre><code>from pycaret.regression import * 
df_caret = pd.concat([data_math, data_por], ignore_index = True) #This corresponds to my df
df_caret.columns
var_cat = ['school', 'sex', 'address', 'famsize', 'Pstatus',
       'Mjob', 'Fjob', 'reason', 'guardian','schoolsup', 'famsup', 'paid', 'activities', 'nursery',
       'higher', 'internet', 'romantic']
experimento = setup(df_caret,target='G3',categorical_features=var_cat,fold_shuffle=True)
</code></pre>
",13520364.0,-1.0,N/A,2022-12-13 20:02:26,Error while trying to use Pycaret.regression AttributeError: 'OneHotEncoder' object has no attribute 'get_feature_names',<python><machine-learning><data-science><pycaret>,0,0,N/A,CC BY-SA 4.0
74812287,1,-1.0,2022-12-15 13:27:42,0,101,"<p>I am using Jupyter Notedbook. I have concatenated multiple tables. When I run the head() command I am not able to see the values in age and gender columns in a table rather than it's showing me NaN values against each user_id.</p>
<p>The following image_1 shows us the output when I concatenated the different two tables
<a href=""https://i.stack.imgur.com/QZrcX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QZrcX.png"" alt=""In this Image_1 you can see I have the values in age and gender column but later on when I try to access the output of the tables by using head() command I am not able to see those values. I have attached another image_2. Where you can see it"" /></a></p>
<p><a href=""https://i.stack.imgur.com/kmUHP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kmUHP.png"" alt=""Image_2"" /></a></p>
<p>How can I sort it out this issue or suggest me another way to conatenate tables where I can see all of the table values?
or do I need to access tables separeately and apply operations on different tables?</p>
<p>I am expecting to get the values in age and gender table rather than NaN values.</p>
<p>When I use these tables separately. It shows correct results but I have a big data problem so I need to concatenate the tables to access each of the feature column. In the end, I can apply operations on concatenated table features.</p>
",5677485.0,-1.0,N/A,2022-12-17 11:43:48,How to fix NaN values in concatenated multiple tables in Jupyter Notebook,<python-3.x><machine-learning><jupyter-notebook><data-science><data-analysis>,1,2,N/A,CC BY-SA 4.0
74803720,1,-1.0,2022-12-14 20:13:28,0,144,"<p>I have Speed Vs Time data I want to count the number of increasing peaks which peaks lower end is less than 13 and peaks upper end is in between 20 and 25</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
x=np.array([ 0,  8,  8, 14, 14, 14, 14, 11, 11,  8, 11, 12, 14,  9,  8, 13, 15,
        7, 11, 13, 11,  1,  1,  0,  0, 13,  9, 14,  8,  9, 10, 19,  8, 11,
       11, 13, 16,  8,  0,  0,  7, 14, 15, 12,  8,  0, 10,  9,  8,  0,  0,
        9,  9,  7,  7, 11, 13, 12, 11,  7, 12, 16, 16, 15,  0,  0,  8, 13,
       12, 10, 10, 10, 11, 13, 14,  7, 11, 13, 17,  8,  8,  9,  9, 10,  7,
        7,  9, 10,  8,  9, 10,  7,  8,  7, 10, 10, 12, 13,  9,  8, 12,  9,
        0,  0,  0,  0,  0, 11, 11, 14,  9, 16, 26, 23,  9, 16, 19,  7,  0,
        2,  0, 12, 16, 15, 16, 17, 15, 12, 12, 15, 21, 25, 27, 26, 26, 27,
       27, 28,  7, 10, 12, 14, 17,  0,  0,  0, 10, 10, 12,  7, 12, 16, 20,
       18,  7, 18,  1,  0, 13, 20, 21, 22, 23, 19, 10,  8,  9, 14, 14, 16,
       10,  8, 10, 17, 20, 18,  0, 11, 17, 18, 10,  7,  9, 13,  9, 11, 10,
        0,  1,  1,  1,  1,  8, 12, 16, 16, 17, 15,  7, 13, 14,  8, 13, 14,
        0,  0,  8, 13, 16, 14, 18,  0, 27, 24, 14, 22, 22,  8, 24, 18, 21,
       22, 19, 26, 11,  0,  0, 11,  8,  7, 10,  7,  9,  9, 15, 18, 16, 13,
       17, 16,  8, 15, 17, 25, 22, 18, 22, 16, 12, 16, 14,  9, 10, 18, 17,
       13, 19,  7,  8,  9,  8, 13, 10, 12, 10,  7,  7,  0,  0,  0,  0,  0,
        7,  9, 12, 10,  9, 13, 10,  9, 13,  7,  7,  7, 10,  9,  8,  0,  0])

zero_locs = np.where(x&lt;=13) # find zeros in x
search_lims = np.append(zero_locs, len(x)) # limits for search area

diff_x = np.diff(x) # find the derivative of x
diff_x_mapped = diff_x &gt; 0 
peak_locs = []
for i in range(len(search_lims)-1):
    peak_locs.append(search_lims[i] + 
np.where(diff_x_mapped[search_lims[i]:search_lims[i+1]]==0)[0][0])
for i in range(len(search_lims)-2):
    peak_loc = search_lims[i] + np.where(diff_x_mapped[search_lims[i]:search_lims[i+1]]==0)[0][0]
    if x[peak_loc] &gt;18 and x[peak_loc] &lt;26 :
        peak_locs.append(peak_loc)
fig= plt.figure(figsize=(19,5))
plt.plot(x)
plt.plot(np.array(peak_locs), x[np.array(peak_locs)], &quot;x&quot;, color = 'r')
</code></pre>
<p>with the above code starting of the line when i take np.where(x==0) works completly fine when i take the the condition x&lt;=13 it throws an index error at peak_loc for search limits part iam getting error</p>
<p>so far i used the cycle count method</p>
<pre><code>sub_lists = np.split(x, np.where(np.diff(x) &lt; 0)[0] + 1)
id_count = 0
id_list = []
for unit in sub_lists:
    if min(unit)&lt;=13 and max(unit) &gt;=18 and max(unit)&lt;=26 and len(set(unit)) &gt; 1:
        id_count += 1
        id_list.append(unit)
id_count
</code></pre>
<p>above code works fine and gives the output but here iam unable to see where the peaks occuring with the output array iam not able to find the peaks values i want to compare both algorithams if both algoritam produce same output then my algoritham is correct any helps gratly appreciated</p>
",-1.0,-1.0,N/A,2022-12-14 20:13:28,how to detect the increasing ramp signal using peak finding algoritham,<python><numpy><scipy><data-science><cycle>,0,0,N/A,CC BY-SA 4.0
74811497,1,-1.0,2022-12-15 12:26:05,-2,37,"<p>I am new to programming and this seems like an easy one but I do not have experience so I need help. Thank you</p>
<p>So, I am doing an NBA themed task in which I need to make a column (feature) called TOTAL_GAME_TIME from PERIOD and GAME_CLOCK. I converted GAME_CLOCK to seconds so I now need to somehow multiplicate every period with 12 and add game clock to it so I can get total game time left in the game.</p>
<p>We are using a .csv file and the code is in Python. Head looks like this:
<a href=""https://i.stack.imgur.com/wjMRp.png"" rel=""nofollow noreferrer"">Top 5 rows</a></p>
<p>Thank you</p>
",20784273.0,20784273.0,2022-12-15 12:27:06,2022-12-15 12:57:49,How to combine two columns (a simple multiplication and addition) from a .csv file to a new one using Python?,<python><python-3.x><pandas><csv><data-science>,1,1,N/A,CC BY-SA 4.0
74793282,1,-1.0,2022-12-14 03:30:05,0,31,"<p>I am trying to teach myself Python for data analysis, and I am working with a dataset that contains both strings and integers.</p>
<p><a href=""https://i.stack.imgur.com/KxYOe.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I was able to fix the reviews_avg column by using :</p>
<p>df['reviews_avg_num'] = df['reviews_avg'].str[8:11]</p>
<p>That took out all text and left me with just numbers.</p>
<p>However, when I get to the reviews_count, it ranges from single digits all the way up to hundreds of thousands.</p>
<p>Is there a way I can simply remove all text and just leave the numbers?</p>
<p>Thank you for any help with this.</p>
",20771829.0,-1.0,N/A,2022-12-14 03:58:27,Removing All Text From a Column,<python><python-3.x><dataframe><data-science><data-analysis>,1,0,N/A,CC BY-SA 4.0
74801775,1,74801956.0,2022-12-14 16:59:42,-2,42,"<p>When I am trying to scrap website over multiple pages <code>BeautifulSoup</code> returning the 1st page content for all the page range.. It is getting repeated again and again..</p>
<pre><code>data=pd.DataFrame()
for i in range(1,10):
  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}
  url=&quot;https://www.collegesearch.in/engineering-colleges-india&quot;.format(i)

  r = requests.get(url, headers=headers)
  soup = BeautifulSoup(r.content, 'html5lib') 
    
  #clg url and name
  clg=soup.find_all('h2', class_='media-heading mg-0')

   #other details
  details=soup.find_all('dl', class_='dl-horizontal mg-0')

  _dict={'clg':clg,'details':details}

  df=pd.DataFrame(_dict)

  data=data.append(df,ignore_index=True)
</code></pre>
",20777506.0,14460824.0,2022-12-14 17:17:08,2022-12-14 17:17:44,Why BeautifulSoup returning same information over and over again,<python><python-3.x><web-scraping><beautifulsoup><data-science>,1,2,N/A,CC BY-SA 4.0
74829309,1,74834199.0,2022-12-16 20:09:20,1,51,"<p>I am generating a boxplot in Shiny app in R based on user input,
The user selects one factor variable and one numeric variable then a box blot of these variables is shown. My problem is that; some of the factor variables have more than 10 levels, the x-axis labels overlap, so I would like the boxplot to flip if the variable chosen has more than a certain number of levels so the x-axis label names can be horizontal.</p>
<p>I have tried using the If statement in the server section, I used the <code>nlevels(dataset[[input$varX]])</code>but it's not getting the number of levels for the factor variables. The plot area is blank. The <code>nlevels(dataset[[input$varX]])</code> fetches the number of levels in the category,</p>
<pre><code>    column1&lt;- c(&quot;box&quot;,&quot;box&quot;,&quot;box&quot;,&quot;box&quot;,&quot;rec&quot;,&quot;rec&quot;,&quot;circle&quot;,&quot;circle&quot;,&quot;circle&quot;,&quot;circle&quot;,&quot;circle&quot;,&quot;circle&quot;,&quot;circle&quot;)
    column2&lt;- c(1,2,3,6,8,9,10,12,15,18,11,19,20)
    column3&lt;- c(&quot;blue&quot;,&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;,&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;,&quot;black&quot;,&quot;red&quot;,&quot;black&quot;,&quot;yellow&quot;)
    dataset&lt;- data.frame(column1,column2,column3)
</code></pre>
<pre><code>ui &lt;- fluidPage(theme = shinytheme(&quot;flatly&quot;),
                navbarPage(tabPanel(&quot;Boxplot&quot;, 
                            
                           sidebarLayout(
                             sidebarPanel(
                             
                               varSelectInput(&quot;varX&quot;, &quot;Var1:&quot;, dataset),
                               varSelectInput(&quot;varY&quot;, &quot;Var2:&quot;, dataset)
                             ),
                             
                             # Show plot 
                             mainPanel(
                               br(),
                               plotOutput(&quot;boxPlot&quot;)
                             )
                           )
                           ),


server &lt;- function(input, output) {
    output$boxPlot &lt;- renderPlot({
      if(nlevels(dataset[[input$varX]]) &lt; 4){
        plot_a&lt;- ggplot(dataset,aes_string(x=input$varX, y=input$varY))+
          geom_boxplot()+
          ggtitle(paste(&quot;boxplot of &quot;, input$varX, &quot;vs&quot;, input$varY))+
          
        plot_a
      }

      if(nlevels(dataset[[input$varX]]) &gt;= 4){
        plot_box + coord_flip()
      }
      
})
}

</code></pre>
",6686671.0,6686671.0,2022-12-17 11:56:11,2022-12-17 14:05:39,flip boxplot with conditional statement in Shiny app R,<r><shiny><data-science><shinyapps>,1,1,N/A,CC BY-SA 4.0
74817271,1,-1.0,2022-12-15 20:35:30,0,23,"<p>I'm having a problem, I try some stuff but i still Can't make it.</p>
<p>This is my DF.</p>
<p><a href=""https://i.stack.imgur.com/2O1Za.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2O1Za.png"" alt=""enter image description here"" /></a></p>
<p>But my problem is some steps ahead. Here its my code and then I explain:</p>
<pre><code>df1 = df1.iloc[6:]
del df1['Unnamed: 0']
del df1['Unnamed: 3']
df1.rename(columns={'Unnamed: 1': 'Fecha', 'Unnamed: 2': 'Numero de Envio','Unnamed: 4': 'Zona', 'Unnamed: 5': 'Tarifa' }, inplace=True)
df1 = df1.replace('-', np.nan)
df1['Fecha'] = df1['Fecha'].ffill()
df1 = df1[df1['Numero de Envio'] != 143.97]
df1 = df1[df1['Numero de Envio'] != 113.97]
df1 = df1[df1['Numero de Envio'] != 243.37]
df2= pd.concat([df, df1], axis=1)
dfn=df2[[&quot;Numero de Envio&quot;,&quot;Unnamed: 13&quot;, 'Fecha']]
pd.options.display.float_format = '{:.0f}'.format
dfn.dropna()
dfn = dfn.astype('str')
diff = dfn['Numero de Envio'][~dfn['Numero de Envio'].isin(dfn['Unnamed: 13'])].dropna().tolist() 
print(f&quot;\n Hay ventas que podrían no ser nuestras: \U0001F611 - Revisar :&quot; '\n')
diff 
</code></pre>
<p>The output is:</p>
<pre><code> Hay ventas que podrían no ser nuestras: 😑 - Revisar :

['Ciudad vieja',
 'Jacinto vera ',
 'Bella Italia',
 'Colon',
 '41782431545',
 'Malvin',
 'Punta riels',
 'Nuevo París',
 'Punta carretas',
 'Aeropuerto',
 '41815769960',
 'Punta carretas',
 '41826206208',
 'Aguada',
 'Cordón']
</code></pre>
<p>The problem in question is that I need the info from the output and also the column called ''Fecha'' in the original DF I show above.</p>
<p>I would like a result like:</p>
<pre><code>'Ciudad vieja', 2022-11-03 
 'Jacinto vera ',2022-11-04
</code></pre>
<p>and so on.</p>
<p>Hope you can help me!
Best Wishes!
PD: sorry for bad english!</p>
",20533290.0,-1.0,N/A,2022-12-15 20:47:32,How can I solve this Isin problem? I cant make it,<pandas><dataframe><date><data-science><isin>,1,0,N/A,CC BY-SA 4.0
74824266,1,-1.0,2022-12-16 12:09:01,1,345,"<p>So I have been trying to implement a CNN-based solution for classification as proposed in this paper (<a href=""https://arxiv.org/pdf/1810.08923.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1810.08923.pdf</a>). Here is my code for reproduction. It's a fairly simple implementation but I don't understand why would the torchsummary would yield such a result. I have gone through their GitHub Q&amp;A as well but there is no such issue raised so far as well.</p>
<pre class=""lang-py prettyprint-override""><code>class CNN_Pred2D(nn.Module):
    def __init__(self, n_filters=[8,8,8], debug=True):
        super().__init__()
        self.debug = debug
        
        self.model = nn.Sequential(
            nn.Conv2d(1, n_filters[0], kernel_size=(1,82)),
            nn.ReLU(),
            nn.Conv2d(n_filters[0], n_filters[0], kernel_size=(3,1)),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,1)),
            
            nn.Conv2d(n_filters[0], n_filters[1], kernel_size=(3,1)),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,1)),
            
            nn.Flatten(),
            nn.Linear(104,1),
            nn.Sigmoid()
        )

        
    def forward(self, X):
        out = self.model(X)
#         print(out.shape)
        return out

model = CNN_Pred2D().to(device)

summary(model, [(1, 60,82)])
</code></pre>
<p>Here is its output:</p>
<p><a href=""https://i.stack.imgur.com/LJtaQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LJtaQ.png"" alt=""enter image description here"" /></a></p>
",1591392.0,20671531.0,2022-12-17 00:06:14,2023-10-27 12:47:03,summary function from torchsummary printing its output twice,<python><pytorch><data-science>,2,1,N/A,CC BY-SA 4.0
74824860,1,-1.0,2022-12-16 13:05:55,0,58,"<p>A client gave me a file in <em>CSV</em> format and called that <em>PFA</em>. I want to open a <em>PFA</em> file, but I got something like the one below.
I would appreciate it if I get help from you.</p>
<pre class=""lang-py prettyprint-override""><code>df=pd.read_csv( '/home/mikaeil/Downloads/ccd4b28d-4340-4e4f-a3b4-e2de3a2de142.csv')
</code></pre>
<p>The result is like below:</p>
<pre><code>0   f71069a5840386c6ece104de3f2bafc3ecb1ff37f1bc64...   99406be6f8f8044366aef3271ac4109d0538df39276e9e...   cb3e421b1d1c2c3448f323268f972ee3b2a54cc1021bf4...   3a14097e562c77b201d23978b706f07e52133506bf5902...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   8ce86a6ae65d3692e7305e2c58ac62eebd97d3d943e093...   8ce86a6ae65d3692e7305e2c58ac62eebd97d3d943e093...   e15274dcfe2d0cb541e8aa23fef7aaaa116bfcbc937d35...   ...     8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   c22cd1ea7268eed0a17f08b05559b6f7939040cf707d5a...   a416ea84421fa7e1351582da48235bac88380a337ec5cb...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   52ebab80b30e4922e932f30f2dd0de3a66dfccdc96c921...   1.0
1   f7e645ca1739c318ae12efa6767cf1e7f38accd55c238c...   9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6...   9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6...   9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   8ce86a6ae65d3692e7305e2c58ac62eebd97d3d943e093...   8ce86a6ae65d3692e7305e2c58ac62eebd97d3d943e093...   b690285b6e60662df771bc7a6b6ee1cb13f799e3d902ec...   ...     8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   26c9a96ce053a14dd88a71a4830c9cbed7e1fed7e3f3f8...   1.0
2   7a2987f600f343f47c4bd9c1bb704db28517da1e814f95...   ca9f6563c7614c5ef5f033c9aaa1aa21611412da771a91...   27e1c95e64999f594001f3b8fada27406bb96ae8adfded...   78df16cdd1a174c6372267773cf96fc1e7bbcc40a0e935...   9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6...   9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6...   eb1e33e8a81b697b75855af6bfcdbcbf7cbbde9f94962c...   8ce86a6ae65d3692e7305e2c58ac62eebd97d3d943e093...   8ce86a6ae65d3692e7305e2c58ac62eebd97d3d943e093...   8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   ...     8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   8ab31b5afaea56114427e1f01b81d001b079a0f59539f6...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   648e5bca018ffe4b6db96361d9b1a5245ee72d27e49269...   1.0
3   e1eb1c05e9cd661690a5eca2566f93ff0a6146bd91025b...   9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6...   4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...   be683e560cde64bff281f929cab1c6588dc9b7057d3cbf...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758...   8ce86a6ae65d3692e7305e2c58ac62eebd97d3d943e093...   8ce86a6ae65d3692e7305e2c58ac62eebd97d3d943e093...   3883138a87732955cea4ec542b22ef82c5342262ca1dab...   ...     8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   8aed642bf5118b9d3c859bd4be35ecac75b6e873cce34e...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   c72ae9676454d7cc0014583a9d0a2e28553d5c62f087c7...   a416ea84421fa7e1351582da48235bac88380a337ec5cb...   d0ff5974b6aa52cf562bea5921840c032a860a91a3512f...   1f4d86b1e0415f97659988ec13c7753c4bffc5f95d373c...   1.0
4   8e741abe8c2cdee489ce0b8c7fc3a87e726e66c858bed5...   ca9f6563c7614c5ef5f033c9aaa1aa21611412da771a91...
</code></pre>
",20014920.0,11833435.0,2022-12-18 08:10:21,2022-12-18 08:10:21,How to open a PFA csv file,<python><csv><jupyter-notebook><data-science><opencsv>,0,5,N/A,CC BY-SA 4.0
74825075,1,-1.0,2022-12-16 13:25:47,0,50,"<p>I wanted to decode a string into its original data value, I'm not sure if I took the right steps, or if had used the wrong encoder, is there anyway to know which encoder to use when receiving decoded data?</p>
<pre><code>import base64

x = 'f71069a5840386c6ece104de3f2bafc3ecb1ff37f1bc64d20a75a98715b17f17'
x = base64.b64decode(x)
print(x)

And I get the following: 
b'\x7f\xbdt\xeb\xd6\xb9\xf3\x8d7\xf3\xa7:y\xc7\xb5\xd3\x87^\xdd\xfd\x9bi\xf77y\xc6\xf5}\xfd\xfb\x7fV\xdc\xeb\x87v\xd1\xae\xf9k\xdf;\xd7\x96\xf5\xed\xfd{'

Where do I go from here?
</code></pre>
",20571159.0,-1.0,N/A,2022-12-16 18:40:31,"As a learning data scientist, how do I understand which decoder to use?",<python-3.x><data-science>,1,2,N/A,CC BY-SA 4.0
74814114,1,74814210.0,2022-12-15 15:44:35,-4,53,"<p>I wonderer how can I create an int variable that contains an huge amount of 0's and and big amount of 1's to do a Data Science models.</p>
<p>I know how crated a variable but not how to create a variable that contains lots of ints rapidly, I tried using fillna() to complete a variable, but I dont know how to add an especific amounts of an int to a variable on Python.</p>
",20628635.0,-1.0,N/A,2022-12-15 15:51:12,How can I create an int variable?,<python><pandas><jupyter-notebook><data-science><fillna>,1,2,2022-12-15 18:05:28,CC BY-SA 4.0
74827956,1,-1.0,2022-12-16 17:37:23,0,25,"<p>I just discovered the dtale python module and am extremely impressed. One issue that I'm having trouble with is fully displaying long column names in output (correlation heatmap for example). Does anyone have any suggestions to remedy this?</p>
<p>Thanks!</p>
",20726674.0,-1.0,N/A,2022-12-16 17:37:23,Displaying Full Column Names in DTale output,<python><pandas><data-science><pypi>,0,1,N/A,CC BY-SA 4.0
74828103,1,-1.0,2022-12-16 17:50:59,0,408,"<p>I'd like to create a Pipeline where I can call <code>fit_transform()</code> just <em>one</em> time on my train dataset (train_df) and receive a fully preprocessed dataset. I don't think I can currently do that, however, because I have to call PCA() on the output of a ColumnTransformer and then concatenate <em>that</em> output with the result of a separate ColumnTransformer called on train_df. Basically, I think I'm going too high up the abstraction ladder, with one too many pipelines/ct's embedded within each other. There's no way to streamline the entire preprocessing process by passing train_df to a single Pipeline or ColumnTransformer - unless I'm missing something and you have any insight? I've spent hours wracking myself around this problem and have finally faced the reality I'm just spinning my wheels. Any help or solutions would be greatly appreciated.</p>
<p>Thank you!</p>
<pre><code>num_ct = ColumnTransformer([
                        ('non_skewed_num', non_skewed_num_pipe, non_skewed_vars),
                        ('skewed_num', skewed_num_pipe, skewed_vars)
                        ], remainder='drop')

total_num_pipe = Pipeline([('num_ct', num_ct), 
                           ('dim_reduc', PCA(n_components=5))])


cat_ct = ColumnTransformer([
                        ('cat_pipe1', cat_pipe1, cat_vars1),
                        ('cat_pipe2', cat_pipe2, cat_vars2)
                        ], remainder='drop')


final_num = total_num_pipe.fit_transform(train_df)
final_cat = cat_ct.fit_transform(train_df)
final_X_train = np.c_[final_num, final_cat]
</code></pre>
",12577026.0,-1.0,N/A,2022-12-16 19:28:20,Is there a way to combine these sklearn Pipelines/ColumnTransformers so I don't have to make multiple fit_transform() calls?,<python><machine-learning><scikit-learn><data-science><sklearn-pandas>,1,2,N/A,CC BY-SA 4.0
74828255,1,-1.0,2022-12-16 18:07:17,-1,151,"<p>I am trying to preprocess a bengali dataset using BLTK. But, it gives me an error. I dont know why. I have sci-kit learn installed and updated it.
This is my code:</p>
<pre><code>from bltk.langtools import remove_stopwords
from bltk.langtools import Tokenizer
from bltk.langtools import UgraStemmer
from bltk.langtools import PosTagger
import re

tokenizer = Tokenizer()
stemmer = UgraStemmer()


corpus = []
y_val = []
for i in range(0, len(messages)):
    review = messages['text'][i]
    y_val_temp = messages['Class'][i]
    review = &quot;&quot;.join(i for i in review if i in [&quot;।&quot;] or 2432 &lt;= ord(i) &lt;= 2559 or ord(i)== 32)
    review =&quot; &quot;.join(review.split())
    
    review = tokenizer.word_tokenizer(review)
    #print(review)
    while(&quot;&quot; in review) : 
        review.remove(&quot;&quot;) 
    review = remove_stopwords(review, level='hard')
    #print(review)
    review = stemmer.stem(review)
    #print(review)
    if(review==None):
        continue
    review = ' '.join(review)
    
    corpus.append(review)
    y_val.append(y_val_temp)
    
</code></pre>
<p>The error shows:</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_11864/3574551415.py in &lt;module&gt;
      6 
      7 tokenizer = Tokenizer()
----&gt; 8 stemmer = UgraStemmer()
      9 
     10 

D:\Anaconda\lib\site-packages\bltk\langtools\stemmer.py in __init__(self)
      5 class UgraStemmer:
      6     def __init__(self):
----&gt; 7         self.pos_tagger = PosTagger()
      8         self.pronoun_values = list(pronouns.values())
      9         self.pronoun_keys = list(pronouns.keys())

D:\Anaconda\lib\site-packages\bltk\langtools\pos_tagger.py in __init__(self)
     10 class PosTagger:
     11     def __init__(self):
---&gt; 12         self.data = PosTagger.get_data()
     13 
     14     @staticmethod

D:\Anaconda\lib\site-packages\bltk\langtools\pos_tagger.py in get_data()
     15     def get_data():
     16         with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), &quot;..//data//pos_tagger.pkl&quot;), &quot;rb&quot;) as tagger:
---&gt; 17             pos_tagger = pickle.load(tagger)
     18             return pos_tagger
     19 

ModuleNotFoundError: No module named 'sklearn.feature_extraction.dict_vectorizer'
</code></pre>
<p>I dont know why is this error occuring. I started doing ml few days back. so, I will appreciate if you anyone answer it.</p>
",20796018.0,-1.0,N/A,2022-12-16 20:00:06,ModuleNotFoundError: No module named 'sklearn.feature_extraction.dict_vectorizer',<python><machine-learning><scikit-learn><jupyter-notebook><data-science>,1,2,N/A,CC BY-SA 4.0
74828686,1,-1.0,2022-12-16 18:55:22,0,34,"<p>Well, this is burning my brain and I decided to ask it here.<br />
I have two existing Dataframes, the first one contains a small description of a physical object, and the other Dataframe usually has a detailed description of that same object.</p>
<p>The problem is that this description don't always have the same amount of words so it <strong>may</strong> exist different descriptions for the same object.</p>
<p>Since the dataframes don't have any other columns that make possible to match the correct information, my idea is to match these descriptions by a percentage of words contained in the second dataframe.</p>
<p>Dataframe 1:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Item</th>
</tr>
</thead>
<tbody>
<tr>
<td>Livro didatico -  Ciência da Computação, Física, Sistemas de Informação, Química e Matemática</td>
</tr>
<tr>
<td>Livro didatico - Eng. Civil, Eng. Elétrica, Eng. Mecânica, Eng. de Produção e Sistemas, Tecnologia Mecânica – Produção Industrial de Móveis, Tecnologia de Sistemas de Informação, Eng. do Petróleo, Eng. de Pesca, Eng. Sanitária e Eng. de Software.</td>
</tr>
<tr>
<td>Livro didatico -  Ciências da Saúde Enfermagem, Fisioterapia, Educação Física</td>
</tr>
</tbody>
</table>
</div>
<p>Dataframe 2 (Detailed object description):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Item</th>
</tr>
</thead>
<tbody>
<tr>
<td>Livro didatico - pedagogico Diversos ( para aplicacao direta) Livros nacionais na área de Ciências Exatas e da Terra Ciência da Computação, Física, Sistemas de Informação, Química e Matemática</td>
</tr>
<tr>
<td>Livro didatico - pedagogico Diversos ( para aplicacao direta) Livros nacionais na área de Engenharias Eng. Civil, Eng. Elétrica, Eng. Mecânica, Eng. de Produção e Sistemas, Tecnologia Mecânica – Produção Industrial de Móveis, Tecnologia de Sistemas de Informação, Eng. do Petróleo, Eng. de Pesca, Eng. Sanitária e Eng. de Software.</td>
</tr>
<tr>
<td>Livro didatico - pedagogico Diversos ( para aplicacao direta) Livros nacionais na área de Ciências da Saúde Enfermagem, Fisioterapia, Educação Física</td>
</tr>
</tbody>
</table>
</div>
<p>I already managed to remove any special characters and letter accents so it can be easier to search.</p>
<p>This is the method i used to calculate the percentage between two different strings, but i wanted to do this between these two dataframes. Is it possible in a pythonic way, or i will have to iterate in every row to match the desired string?</p>
<pre><code>@staticmethod
def valida_descricao(string_contida: str, string_completa: str) -&gt; int:
    porcentagem_str_contida = 100 / len(string_contida.split())
    soma_porcentagem = 0

    for token in string_contida.split(' '):
        if token in string_completa:
            soma_porcentagem += porcentagem_str_contida

    return soma_porcentagem
</code></pre>
",14391327.0,-1.0,N/A,2022-12-16 19:24:57,Retrieve Dataframe value(s) if the percentage of a string existing in another Dataframe row is >= 75%,<python><python-3.x><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
74835341,1,-1.0,2022-12-17 15:40:21,0,730,"<p>Here is the code:</p>
<pre><code>import pandas as pd
import pandas_datareader.data as web
import matplotlib.pyplot as plt

start_date = &quot;2020-01-01&quot;
end_date = &quot;2020-12-31&quot;

data = web.DataReader(name=&quot;TSLA&quot;, data_source='yahoo', start=start_date, end=end_date)
print(data)

close = data['Close']
ax = close.plot(title='Tesla')
ax.set_xlabel('Date')
ax.set_ylabel('Close')
ax.grid()
plt.show()
</code></pre>
<p>And this is the error that it returns:</p>
<pre><code>File &quot;F:\anaconda\lib\site-packages\pandas\core\tools\datetimes.py&quot;, line 403, in _convert_listlike_datetimes
    arg, _ = maybe_convert_dtype(arg, copy=False, tz=timezones.maybe_get_tz(tz))

TypeError: maybe_convert_dtype() got an unexpected keyword argument 'tz'
</code></pre>
<p>It should display a graph of the stock.</p>
",20802151.0,4306257.0,2022-12-18 15:36:04,2022-12-18 15:36:04,pandas datareader should get stock market data but returns error,<python><pandas><data-science><pandas-datareader>,1,1,N/A,CC BY-SA 4.0
74813673,1,74815039.0,2022-12-15 15:12:33,0,254,"<p>I want to create some unit tests to make sure the correct <code>NotImplemented</code> exceptions are thrown for a module I am developing. Is there a way that I could use some dummy data, create and fit multiple <code>sklearn</code> models to feed into the unit tests?</p>
<p>I'm looking for a parametric solution, that in some way I could go through many models instead of manually defining each different model. I do not need well-defined models, or models that make sense, just models that can be constructed with their default parameters.</p>
",17986681.0,-1.0,N/A,2022-12-15 17:15:01,How to parametrise unit tests for sklearn models,<python><unit-testing><machine-learning><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
74821265,1,-1.0,2022-12-16 07:17:32,-2,308,"<p>Is there any code to see or view the data in a cluster after doing k-means clustering in python,
so that i can know which type of data clustered into which cluster and why.</p>
<p>help me with this ?</p>
<p>The cluster file is in .File extension, so I am unable to open it.</p>
",20646248.0,-1.0,N/A,2022-12-19 17:41:59,Can anyone know how to see the data in a cluster after doing k-means clustering?,<python><machine-learning><scikit-learn><data-science><k-means>,2,2,N/A,CC BY-SA 4.0
74833767,1,-1.0,2022-12-17 11:39:53,0,67,"<p>I encountered an error while building a linear regression model. Judging by their shape, it seems appropriate.</p>
<pre class=""lang-py prettyprint-override""><code>X = df_one_hot[features]
y = df_one_hot[&quot;price&quot;]

X_train, y_train, X_test, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)
model = LinearRegression().fit(X_train,y_train)
y_pred = model.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
print(&quot;RMSE: &quot;, RMSE)
</code></pre>
<p>X.shape</p>
<pre class=""lang-py prettyprint-override""><code>(205, 46)
</code></pre>
<p>y.shape</p>
<pre class=""lang-py prettyprint-override""><code>(205,)
</code></pre>
<p>X_train.shape</p>
<pre class=""lang-py prettyprint-override""><code>(137, 46)
</code></pre>
<p>y_train.shape</p>
<pre class=""lang-py prettyprint-override""><code>(68, 46)
</code></pre>
<p>This is the error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_7740\1016843113.py in &lt;module&gt;
      1 X_train, y_train, X_test, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)
----&gt; 2 model = LinearRegression().fit(X_train,y_train)
      3 y_pred = model.predict(X_test)
      4 RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
      5 print(&quot;RMSE: &quot;, RMSE)

~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py in fit(self, X, y, sample_weight)
    660         accept_sparse = False if self.positive else [&quot;csr&quot;, &quot;csc&quot;, &quot;coo&quot;]
    661 
--&gt; 662         X, y = self._validate_data(
    663             X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True
    664         )

~\anaconda3\lib\site-packages\sklearn\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
    579                 y = check_array(y, **check_y_params)
    580             else:
--&gt; 581                 X, y = check_X_y(X, y, **check_params)
    582             out = X, y
    583 

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
    979     y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric)
    980 
--&gt; 981     check_consistent_length(X, y)
    982 
    983     return X, y

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_consistent_length(*arrays)
    330     uniques = np.unique(lengths)
    331     if len(uniques) &gt; 1:
--&gt; 332         raise ValueError(
    333             &quot;Found input variables with inconsistent numbers of samples: %r&quot;
    334             % [int(l) for l in lengths]

ValueError: Found input variables with inconsistent numbers of samples: [137, 68]
</code></pre>
<p>I can't build a linear regression model despite all I've tried</p>
",20800975.0,16775594.0,2022-12-17 23:32:00,2022-12-17 23:32:00,"ValueError: Found input variables with inconsistent numbers of samples: [137, 68]",<python><machine-learning><data-science><data-cleaning>,0,2,N/A,CC BY-SA 4.0
74831491,1,-1.0,2022-12-17 02:55:15,0,265,"<p>I have a 100x100 DataFrame (representing a Gaussian KDE) that represents a small sample of data so there are many holes. I have another 100x100 df that represents a larger sample of data with no holes. I would like to combine these dfs into a single 100x100 df but with a preference for the smaller df. In other words, I would like to fill in all the missing gaps in the smaller df with data from the larger df.</p>
<p>I can think of two ways of doing this:</p>
<ol>
<li>Start with the smaller df and replace all null values with the equivalent value from the larger</li>
<li>Start with the larger df and overlay all non-null values of the smaller df</li>
</ol>
<p>Is there an efficient way to do this that doesn't involve going cell-by-cell?</p>
",10616197.0,-1.0,N/A,2022-12-17 17:46:45,Multiple polars DataFrames with same shape: update one will non-null values of the other,<python><dataframe><data-science><python-polars>,2,0,N/A,CC BY-SA 4.0
74837471,1,-1.0,2022-12-17 21:06:05,0,66,"<p>When i train a simple NN architecture with cross entropy i get same loss when using the built-in <code>keras crossentropy</code> loss and <code>user defined crossentropy</code> while the <strong>gradients are different</strong> for the different implementation ...</p>
<p>Important to mention that this issue is comes up when my &quot;target&quot; is not categorical variable (As it happening in policy gradients). When i define the target to be 0 or 1 the gradient and losses are the same.</p>
<p>I made some public google colab notebook so you can see and run the example <a href=""https://colab.research.google.com/drive/1yjOXQUYwFFjBjW4Frj6o6JFIqwFru87Z?authuser=1#scrollTo=VGgO-L3VzAUU"" rel=""nofollow noreferrer"">link</a></p>
<p>Also attaching the code and the results here.
I run the the code twice for the built-in and the manual cross entropy. once with categorical target , in order to show that the results are the same. and once with non categorical target.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow import constant ,GradientTape, math
import numpy as np 

def get_loss_and_grads(function, 
                       y_true,
                       y_pred):
  
  with GradientTape() as gr:
    gr.watch(y_pred)

    loss = function(y_true = y_true,
                    y_pred = y_pred)
    
  dy_dx = gr.gradient(loss, y_pred)
  return loss, dy_dx

def udf_loss(y_true, y_pred):
  loss = - math.log(y_pred) * y_true
  return loss


# Define predicted and actual target variables 
y_true = constant(np.array([1.]))
y_pred = constant(np.array([0.5]))

# Run for built in keras cross entropy 
loss, dy_dx = get_loss_and_grads(function = BinaryCrossentropy(from_logits=False) , 
                                 y_true = y_true,
                                 y_pred = y_pred)

print(f""\n\n-- ==  built in cross entropy -  y_true = {y_true} , y_pred = {y_pred} == --\n"")
print(""loss "", loss)
print(""dy_dx "", dy_dx)


# Run for user defined cross entropy 
loss, dy_dx = get_loss_and_grads(function = udf_loss , 
                                 y_true = y_true,
                                 y_pred = y_pred)

print(f""\n-- ==  user defined cross entropy - y_true = {y_true} , y_pred = {y_pred} == --\n"")
print(""loss "", loss)
print(""dy_dx "", dy_dx)



# Define predicted and actual target variables 
y_true = constant(np.array([5.]))
y_pred = constant(np.array([0.5]))

# Run for built in keras cross entropy 
loss, dy_dx = get_loss_and_grads(function = BinaryCrossentropy(from_logits=False) , 
                                 y_true = y_true,
                                 y_pred = y_pred)

print(f""\n\n-- ==  built in cross entropy -  y_true = {y_true} , y_pred = {y_pred} == --\n"")
print(""loss "", loss)
print(""dy_dx "", dy_dx)


# Run for user defined cross entropy 
loss, dy_dx = get_loss_and_grads(function = udf_loss , 
                                 y_true = y_true,
                                 y_pred = y_pred)

print(f""\n-- ==  user defined cross entropy - y_true = {y_true} , y_pred = {y_pred} == --\n"")
print(""loss "", loss)
print(""dy_dx "", dy_dx)


-- ==  built in cross entropy -  y_true = [1.] , y_pred = [0.5] == --

loss  tf.Tensor(0.6931469805599654, shape=(), dtype=float64)
dy_dx  tf.Tensor([-1.9999996], shape=(1,), dtype=float64)

-- ==  user defined cross entropy - y_true = [1.] , y_pred = [0.5] == --

loss  tf.Tensor([0.69314718], shape=(1,), dtype=float64)
dy_dx  tf.Tensor([-2.], shape=(1,), dtype=float64)


-- ==  built in cross entropy -  y_true = [5.] , y_pred = [0.5] == --

loss  tf.Tensor(0.6931469805599653, shape=(), dtype=float64)
dy_dx  tf.Tensor([-17.9999964], shape=(1,), dtype=float64)

-- ==  user defined cross entropy - y_true = [5.] , y_pred = [0.5] == --

loss  tf.Tensor([3.4657359], shape=(1,), dtype=float64)
dy_dx  tf.Tensor([-10.], shape=(1,), dtype=float64)</code></pre>
</div>
</div>
</p>
",20804046.0,20804046.0,2022-12-20 08:06:04,2022-12-20 08:06:04,User defined cross entropy and built-in cross entropy of KERAS gives same loss but different gradients,<deep-learning><data-science><tf.keras><loss-function><cross-entropy>,0,0,N/A,CC BY-SA 4.0
74844395,1,-1.0,2022-12-18 21:01:20,1,85,"<p>To test the matplotlib's interactive mode, I used the following two code snippets:</p>
<p><strong>Snippet 1:</strong> Has <code>plt.ion()</code> with no call to <code>fig.canvas.draw()</code>:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt

x = np.array([])
y_1 = np.array([])
y_2 = np.array([])

plt.ion()

fig = plt.figure(figsize=(9,4))
ax1 = plt.subplot(1,2,1)
ax2 = plt.subplot(1,2,2)

fig.show()

for i in range(0, 100000):
    x = np.append(x, i)
    y_1 = np.append(y_1, i**2)
    y_2 = np.append(y_2, i**3)

    ax1.clear()
    ax2.clear()
    ax1.scatter(x, y_1)
    ax2.scatter(x, y_2)

    #fig.canvas.draw()
    plt.pause(0.02)
</code></pre>
<p><strong>Snippet 2:</strong> Has <code>plt.ion()</code> with call to <code>fig.canvas.draw()</code>:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt

x = np.array([])
y_1 = np.array([])
y_2 = np.array([])

plt.ion()

fig = plt.figure(figsize=(9,4))
ax1 = plt.subplot(1,2,1)
ax2 = plt.subplot(1,2,2)

fig.show()

for i in range(0, 100000):
    x = np.append(x, i)
    y_1 = np.append(y_1, i**2)
    y_2 = np.append(y_2, i**3)

    ax1.clear()
    ax2.clear()
    ax1.scatter(x, y_1)
    ax2.scatter(x, y_2)

    fig.canvas.draw()
    plt.pause(0.02)
</code></pre>
<p>I could not see any difference between the two plotted figures. Both the figures were updated/redrawn automatically.</p>
<p>Further, I set the interactive mode off using <code>plt.ioff()</code> with no call to <code>fig.canvas.draw()</code>:</p>
<p><strong>Snippet 3:</strong></p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt

x = np.array([])
y_1 = np.array([])
y_2 = np.array([])

plt.ioff()

fig = plt.figure(figsize=(9,4))
ax1 = plt.subplot(1,2,1)
ax2 = plt.subplot(1,2,2)

fig.show()

for i in range(0, 100000):
    x = np.append(x, i)
    y_1 = np.append(y_1, i**2)
    y_2 = np.append(y_2, i**3)

    ax1.clear()
    ax2.clear()
    ax1.scatter(x, y_1)
    ax2.scatter(x, y_2)

    #fig.canvas.draw()
    plt.pause(0.02)
</code></pre>
<p>Same behaviour was observed across all the 3 snippets.</p>
<p>According to what is mentioned on <a href=""https://www.mail-archive.com/matplotlib-users@lists.sourceforge.net/msg21139.html"" rel=""nofollow noreferrer"">https://www.mail-archive.com/matplotlib-users@lists.sourceforge.net/msg21139.html</a> about <code>ion()</code>:</p>
<blockquote>
<p>PS: the documentation I was referring to reads: &quot;The interactive property of
the pyplot interface controls whether a figure canvas is drawn on every
pyplot command. If interactive is False, then the figure state is updated on
every plot command, but will only be drawn on explicit calls to draw(). When
interactive is True, then every pyplot command triggers a draw.&quot;</p>
</blockquote>
<p>the expected behaviour of <strong>Snippet 3</strong> should be different than that of <strong>Snippet 1</strong> &amp; <strong>Snippet 2.</strong> For <strong>Snippet 3</strong>, the figure should not get updated/redrawn automatically.</p>
<p>Is my understanding correct or am I missing something?</p>
",7422352.0,7422352.0,2022-12-19 07:58:46,2022-12-19 07:58:46,matplotlib's ion() does not make any difference,<python><matplotlib><data-science><matplotlib-animation><matplotlib-ion>,0,0,N/A,CC BY-SA 4.0
74829836,1,-1.0,2022-12-16 21:15:11,0,19,"<p>here is my data:</p>
<pre><code>|Object |  HJD 24... |    Filter | Magnitude |
|-------| ---------- |-----------|-----------|
|SU_Hor | 55896.30476|      B    |  14.877   |
|SU_Hor | 55896.27438|     Ic    |  13.885   |
|SU_Hor | 55896.27349|      B    |  14.809   |
|SU_Hor | 55896.27397|      V    |  14.434   |
|SU_Hor | 55896.40882|     Ic    |  14.033   |
|SU_Hor | 55896.40829|      V    |  14.540   |
|SU_Hor | 55896.40770|      B    |  14.941   |
|SU_Hor | 55896.34973|     Ic    |  13.958   |
|SU_Hor | 55896.34943|      V    |  14.494   |
|SU_Hor | 55896.34906|      B    |  14.861   |
|SU_Hor | 55896.30542|     Ic    |  13.912   |
|SU_Hor | 55896.30512|      v    |  14.440   |
|SU_Hor | 55897.38547|      V    |  14.536   |
|SU_Hor | 55897.28281|      B    |  14.882   |
|SU_Hor | 55897.28317|      V    |  14.428   |
|SU_Hor | 55897.28347|     Ic    |  13.927   |
|RZ_Lyr | 27359.3030 |      V    |  10.630   |
|RZ_Lyr | 27684.4510 |      V    |  10.610   |
|RZ_Lyr | 27685.4780 |      V    |  10.580   |
|RZ_Lyr | 27701.3150 |      V    |  10.700   |
|RZ Lyr | 27934.4560 |      V    |  10.660   |
|RZ Lyr | 27955.4100 |      V    |  10.570   |
|rzlyr  | 30604.2000 |      V    |  11.030   |
|RZ_Lyr | 55314.5695 |      B    |  12.047   |
|RZ_Lyr | 55314.5724 |      B    |  12.036   |
|RZ_Lyr | 55314.5900 |      B    |  12.042   |
|RZ_Lyr | 55314.6105 |      B    |  12.045   |
|RZ_Lyr | 55314.6163 |      B    |  12.027   |
|RZ_Lyr | 55342.3509 |      B    |  12.057   |
|RZLyr  | 55342.3557 |      B    |  12.058   |
|RZ_Lyr | 55342.3606 |      B    |  12.052   |
|RZ_Lyr | 55342.3654 |      B    |  12.058   |
</code></pre>
<p>my code:</p>
<pre><code>mport csv


def searchByObject():
    object = input('Enter Object name\n')
    csv_file = csv.reader(open(&quot;D:/Астрономия Курс 2/PycharmProjects/Python_2ndLab.csv&quot;, &quot;r&quot;))

    for column in csv_file:
        if object == column[0]:
            print(column)


def searchByFilter():
    filter = str(input('Enter filter to show data'))
    csv_file = csv.reader(open(&quot;D:/Астрономия Курс 2/PycharmProjects/Python_2ndLab.csv&quot;, &quot;r&quot;))

    for column in csv_file:
        if filter in column[2]:
            print(column)


print('Enter 1 to search by object name')
print('Enter 2 to search by filter')

src = (input('Enter here: '))

if src == 1:
    searchByObject()
elif src == 2:
    searchByFilter()
else:
    print('Sorry, invalid input')

</code></pre>
<p>the Output:</p>
<ul>
<li>Name: Filter, dtype: object</li>
<li>Enter 1 to search by object name</li>
<li>Enter 2 to search by filter</li>
<li>Enter here: 1
Sorry, invalid input</li>
</ul>
<p><strong>What I expected the Output will be like:</strong></p>
<ul>
<li>Name: Filter, dtype: object</li>
<li>Enter 1 to search by object name</li>
<li>Enter 2 to search by filter</li>
<li>Enter here: 1</li>
<li>Enter the object name to show data: SU_Hor
(let's imagine all of the data from SU_Hor appear)</li>
</ul>
",20794630.0,20794630.0,2022-12-16 21:21:46,2022-12-16 21:21:46,"I ask user to enter name of""object"" for the data they want to receive. But when I enter ""1"" the output says ""Sorry, invalid input"" how can I fix it?",<python><search><data-science>,0,2,2022-12-16 21:18:29,CC BY-SA 4.0
74839139,1,-1.0,2022-12-18 04:32:08,0,40,"<p>I am working on MMM, I have a small doubt on how to optimize decay rate lambda in adstock using Grid search? Please share a sample code.</p>
<p>I am confused about this, How can we define a model for it.</p>
",20203276.0,-1.0,N/A,2022-12-18 04:32:08,Optimization of decay rate for adstock using Grid Search,<dataframe><deep-learning><data-science><linear-regression><grid-search>,0,1,N/A,CC BY-SA 4.0
74846389,1,-1.0,2022-12-19 04:51:13,0,159,"<p>The Seaborn code does not work.
I use jupyterlite to execute seaborn python code. first, i import seaborn in <a href=""https://stackoverflow.com/a/74048613/8508004"">the following way</a> --</p>
<pre><code>import piplite
await piplite.install('seaborn')
import matplotlib.pyplot as plt
import seaborn as sn
%matplotlib inline
</code></pre>
<p>But when I insert seaborn code like the following one then it shows many errors that i do not understand yet --
<a href=""https://seaborn.pydata.org/examples/anscombes_quartet.html"" rel=""nofollow noreferrer"">link of the code</a></p>
<p><a href=""https://i.stack.imgur.com/LAGqg.png"" rel=""nofollow noreferrer"">the problem that I face</a></p>
<p>But I insert this code in the google colab it works nicely
<a href=""https://i.stack.imgur.com/0VdlE.png"" rel=""nofollow noreferrer"">google colab</a></p>
",20812193.0,8508004.0,2022-12-19 18:22:43,2022-12-19 18:43:59,Seaborn code Anscombe’s quartet does not work,<jupyter-notebook><data-science><google-colaboratory><jupyter-lab><graph-data-science>,1,7,N/A,CC BY-SA 4.0
74835683,1,74836252.0,2022-12-17 16:30:28,0,51,"<p>for instance the column i want to split is duration here, it has data points like - 110 or 2 seasons, i want to make a differerent column for seasons and in place of seasons in my current column it should say null as this would make the type of column int from string
<a href=""https://i.stack.imgur.com/0jYQi.png"" rel=""nofollow noreferrer"">screenshot of my data</a></p>
<p>i tried the split function but that's for splliting in between data points, unlike splitting different other data points</p>
",20802678.0,-1.0,N/A,2022-12-17 17:59:38,how do i split a column into two in python on the basis of data in it,<python><csv><split><data-science>,2,0,N/A,CC BY-SA 4.0
74843126,1,74843149.0,2022-12-18 17:09:51,0,106,"<p>Using dataprep API and I am getting a recursion error when I use the dataprep functions in Google Colab. Oddly it works fine on 144 features of uncleaned data. But once reduced to 20 features and clean the missing values, I get a recursion error</p>
<p>Code:</p>
<pre><code>df.isna().sum()
Output:
rade                      0
sub_grade                 0
emp_length                0
home_ownership            0
annual_inc                0
verification_status       0
loan_status               0
purpose                   0
dti                       0
delinq_2yrs               0
inq_last_6mths            0
mths_since_last_delinq    0
open_acc                  0
pub_rec                   0
revol_bal                 0
revol_util                0
total_acc                 0
recoveries                0
pub_rec_bankruptcies      0
tax_liens                 0
dtype: int64

sys.setrecursionlimit(15000)
    
from dataprep.eda import create_report, plot, plot_correlation

create_report(df)
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
RecursionError                            Traceback (most recent call last)
&lt;ipython-input-55-463fb2fdfb17&gt; in &lt;module&gt;
----&gt; 1 create_report(df)

33 frames
... last 10 frames repeated, from the frame below ...

/usr/local/lib/python3.8/dist-packages/pandas/core/series.py in __repr__(self)
   1463         show_dimensions = get_option(&quot;display.show_dimensions&quot;)
   1464 
-&gt; 1465         self.to_string(
   1466             buf=buf,
   1467             name=self.name,

RecursionError: maximum recursion depth exceeded
</code></pre>
<p>Following the advice of the first answer, I was able to go through one series at a time and it looks like this code is causing the issue. How can this be written better?</p>
<pre><code># these columns will take the median value for fillna
median_fill = ['emp_length','annual_inc','open_acc','pub_rec','open_acc','revol_util','total_acc']
for med in median_fill:
  df[med].fillna(df[med].median,inplace=True)
</code></pre>
",16366260.0,16366260.0,2022-12-18 17:36:45,2022-12-18 17:42:09,Recursion error: Dataprep function not working post cleaning data,<python><jupyter-notebook><data-science><data-preprocessing>,1,0,N/A,CC BY-SA 4.0
74843702,1,-1.0,2022-12-18 19:06:46,0,124,"<p>I'm a beginner in Data Science and I'm currently working on building a model for the IBM Employee Attrition Dataset. How do I get around this error?</p>
<pre><code># LogisticRegression
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.model_selection import train_test_split

#Copy the DataFrame
df1 = df.copy()

#Convert categorical variables to numeric 
dummy_df = pd.get_dummies(df1, columns=[&quot;Attrition&quot;, &quot;BusinessTravel&quot;, &quot;Department&quot;, &quot;EducationField&quot;, 
                                        &quot;Gender&quot;, &quot;JobRole&quot;, &quot;OverTime&quot;, &quot;MaritalStatus&quot;], drop_first = True)
dummy_df = pd.concat([df1, dummy_df], axis=1)

dummy_df = dummy_df.drop([&quot;Attrition&quot;, &quot;BusinessTravel&quot;, &quot;Department&quot;, &quot;EducationField&quot;, 
                                        &quot;Gender&quot;, &quot;JobRole&quot;, &quot;OverTime&quot;, &quot;MaritalStatus&quot;], axis=1)

dummy_df.rename({&quot;Attrition_Yes&quot;:&quot;Attrition&quot;, &quot;OverTime_Yes&quot;:&quot;OverTime&quot;}, axis=1, inplace=True)

#Drop duplicate columns
dummy_df = dummy_df.loc[:,~dummy_df.columns.duplicated()]

X = dummy_df.drop(&quot;Attrition&quot;, axis=1).values

y = dummy_df[&quot;Attrition&quot;].values


X_train, X_test,  y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

logreg.score(y_pred, y_test)

ValueError: Expected 2D array, got 1D array instead:
</code></pre>
",18402119.0,18402119.0,2022-12-18 19:14:25,2022-12-18 19:14:25,"ValueError: Expected 2D array, got 1D array instead",<python><model><regression><data-science><logistic-regression>,1,0,N/A,CC BY-SA 4.0
74861755,1,-1.0,2022-12-20 10:38:30,0,1608,"<p><img src=""https://i.stack.imgur.com/Z1w4h.png"" alt=""enter image description here"" /> Below is my code, where I am converting pdf to xml format.
But this gives me xml formated file only if I used XFA-PDF(pdf form) formated pdf.
I need to convert any type of pdf to xml format and xml contain information about text value, tables, images, objects/drawings and their x,y co-ordinates.
Is there any way to get this type of xml from pdf?</p>
<p>Thank you!</p>
<pre><code>import PyPDF2
import re
def findInDict(needle, haystack):
    for key in haystack.keys():
        try:
            value=haystack[key]
        except:
            continue
        if key==needle:
            return value
        if isinstance(value,dict):          
            x=findInDict(needle,value)            
            if x is not None:
                return x

 

def create_xml_PDFform(xfa):
    for i in range(0,len(xfa)):
        try:
            xml = xfa[i].getObject().getData()
            f = open('C:\\Users\\tanvi_karekar\\'+str(pdf_file)+'.xml', 'ab')
            f.write(xml)
            f.close()
        except:
            continue

 

if __name__ == '__main__':
    pdf_file = 'sampleDoc3'
    pdf_file_path = 'C:\\Users\\tanvi_karekar\\'+str(pdf_file)+'.pdf'
    pdfobject = open(pdf_file_path,'rb')
    pdf = PyPDF2.PdfFileReader(pdfobject)
    xfa = findInDict('/XFA',pdf.resolved_objects) 
    create_xml_PDFform(xfa)
</code></pre>
<p>Is there any way to get this type of xml from pdf? or any library to get pdf structure?</p>
<p>This is my pdf looks like.</p>
",20822227.0,1729265.0,2022-12-26 23:59:35,2022-12-26 23:59:35,How to get PDF structure in xml format,<python><python-3.x><xml><pdf><data-science>,1,1,N/A,CC BY-SA 4.0
74861827,1,-1.0,2022-12-20 10:43:13,0,34,"<p>The dataset is from a survey about 91 art pieces from 300 people. The first 91 columns are the preference ratings for each painting scored 1-7. The 91 columns after that are their ratings for the “energy” of the painting. Additionally, there are more columns that represent demographic information of each person. I am supposed to create a regression model to predict preference rating from energy rating and demographic. I was able to do a single regression with this code but multiple isnt working. What's wrong?</p>
<pre><code>rate = data[:,0:91] #isolate preference ratings
energy = data[:,91:182] #isolate energy ratings
demo = data[:,215:221] #isolate demographic ratings
demo[np.isnan(demo)] = 4.68
comb = np.concatenate((energy,demo),axis=1)
x = comb.reshape(-1,1)
y = rate.reshape(-1,1)
ourModel = LinearRegression().fit(x, y)
rSq = ourModel.score(x,y)
slope = ourModel.coef_
intercept = ourModel.intercept_
yHat = slope * x + intercept
print(rSq)

plt.plot(x,y,'o',)
plt.xlabel('Energy + Demographic') 
plt.ylabel('Preference')  
plt.plot(x,yHat,color='orange',linewidth=3)
plt.title('Predicting Art Preference from Energy and Demographic') 
plt.show()
</code></pre>
<p>It is an issue with the reshaping but I've tried a lot of things and to no avail.</p>
<pre><code>  File &quot;/Users/pwigglybie/opt/anaconda3/lib/python3.9/site-packages/spyder_kernels/py3compat.py&quot;, line 356, in compat_exec
    exec(code, globals, locals)

  File &quot;/Users/pwigglybie/Downloads/CapstoneProject.py&quot;, line 132, in &lt;module&gt;
    ourModel = LinearRegression().fit(x, y)

  File &quot;/Users/pwigglybie/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py&quot;, line 684, in fit
    X, y = self._validate_data(

  File &quot;/Users/pwigglybie/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py&quot;, line 596, in _validate_data
    X, y = check_X_y(X, y, **check_params)

  File &quot;/Users/pwigglybie/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py&quot;, line 1092, in check_X_y
    check_consistent_length(X, y)

  File &quot;/Users/pwigglybie/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py&quot;, line 387, in check_consistent_length
    raise ValueError(

ValueError: Found input variables with inconsistent numbers of samples: [29100, 27300]
</code></pre>
",20822326.0,20822326.0,2022-12-20 19:34:03,2022-12-20 19:34:03,"ValueError: Found input variables with inconsistent numbers of samples: [29100, 27300]",<python><regression><data-science><linear-regression>,0,2,N/A,CC BY-SA 4.0
74841523,1,-1.0,2022-12-18 12:58:07,0,194,"<p>I have to make a &quot;behavioural model&quot; upon a huge transactions dataset, able to return a &quot;behavioral score&quot; given to every new transaction of a user, comparing to the historical data of that user.</p>
<p>I am looking for advices on how I can run the DBSCAN with every user without manually setting every hyperparameter for each one (there are thousands).</p>
<p>My idea is:</p>
<ol>
<li>extrapolate the transactions from a single user</li>
<li>since the datas are in 15 dimensions, I am putting MinPts = 30</li>
<li>i want to plot the distance of the K (= MinPts) neighbours curve and set eps = the point of maximum curvature of the curve .</li>
</ol>
<p>The automation of the third point is giving me some difficulties. I have tried to use a library called KneeLocator, but did not give results.</p>
<p>I am looking for any suggestion</p>
",20735531.0,20735531.0,2022-12-18 13:00:59,2022-12-18 13:00:59,Auto-Tuning of DBSCAN hyperparameters for clustering users' behaviours,<machine-learning><data-science><cluster-analysis><unsupervised-learning><dbscan>,0,0,N/A,CC BY-SA 4.0
74856431,1,-1.0,2022-12-19 21:54:36,0,20,"<p>I have dataset of customers from 2019-2022 . My goal is to predict customer Churn at a specific point in time , say <code>exactly 3 months</code> from the observation point using Logistic Regression.</p>
<p>So if I look at my customer base at <code>Jan-2022(Say Month0)</code> , i can tag churners as Customers who <code>churned exactly at month3(April)</code> and non churners as Customers who <code>stayed Active at Month3(April)</code>.</p>
<p>The issue that I was thinking of was there could be a group of customers that churned at Month-1 or Month2 .</p>
<p>I  wouldn't be able to include them in the  training dataset because technically they did not churn at Month-3 but before(Feb or March) . Is excluding these customers the right approach to model this problem?</p>
<p>There are enough articles on modelling churn within a specific window(say within 3 months) using logistic Regression , but since I would be modelling churn at a specific point in time(Exactly at 3 months) , any guidance on the query is helpful. Thanks</p>
",4517807.0,-1.0,N/A,2022-12-19 21:54:36,Training Dataset preparation for predicting customer Churn at a specific Month,<machine-learning><statistics><data-science><classification><modeling>,0,0,N/A,CC BY-SA 4.0
74863870,1,-1.0,2022-12-20 13:30:52,1,90,"<p>I have observed that there are differences in the implementation of the <code>decision_function</code> and the <code>predict</code> methods the one versus one multi class implementation of <code>SVC</code> in the sci-kit learn package. Is there a way to align this issue or is this indeed a bug in the implementation?</p>
<p>I was solving a multi classification problem with 11 classes with around 36 initial feature variables (160 features after some transformation e.g. scaling, missing value indicators etc.) and over 6,000 observations using the OvO linear SVC implemented in sci-kit learn. In cases where there are ties in votes for 2 or more of the 55 classifiers generated by the OvO SVC, the decision_function method uses confidences to break the ties in this case (see the source code from line 479 -521 <a href=""https://github.com/scikit-learn/scikit-learn/blob/2e481f114169396660f0051eee1bcf6bcddfd556/sklearn/utils/multiclass.py#L479"" rel=""nofollow noreferrer"">here</a> ). However, the <code>predict</code> method implements LIBSVM and a close look at the C++ code reveals that in the case of ties, the first runtime index in the loop is kept as the decision for the OvO linear SVC (see the source code from line 2865 to 2899 <a href=""https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/svm/src/libsvm/svm.cpp#L2836"" rel=""nofollow noreferrer"">here</a>).</p>
<p>For example, if there is a tie between class 5 and 7 with 17 votes each, class 5 will be taken to be the decision of the predict function since 5 is taken as the index before 7 in the for loop. I am not allowed to share the actual examples i have, but it is clear from the implementation that these cases my arise simply because this confidences are not implemented in LIBSVM.</p>
<p>I expected the decisions/predictions of both the decision_function method and the predict method to give the same results for the same parameters of the OvO SVC. In this particular multi classification case, whenever there are ties, there are some cases where the decision_function method give different results to those of the predict method. In my, actually the implementation of the decision function is more correct since it involves some quantified justification for making the decision in case of ties.</p>
<p>Is this really how the implementation should be or is this a bug in the implementation? Would it be possible to implement this confidence in LIBSVM package as part of the standard implementation ? What would these custom modifications look like? I mean somehow I don't think I am the first person to see this issue so I am quite sure there is something I am missing, but I have not found this case discussed anywhere in this forum or any other mains stream python forum. I would appreciate any help and clarification from the experts here.</p>
",20057979.0,12439119.0,2022-12-24 18:39:31,2022-12-24 18:39:31,Inconsistency between decision_function and predict methods in OvO SVC in Scikit learn package,<python><scikit-learn><data-science><svm>,0,2,N/A,CC BY-SA 4.0
74853435,1,-1.0,2022-12-19 16:42:26,0,79,"<p>What is the range of RMSE during a evaluation model? I know that close to 0 is better, but tis there a maximum value for this? I can't find the answer... If I have a 52 RMSE how can I say if it's a good result?</p>
",19610821.0,-1.0,N/A,2022-12-19 16:42:26,What's the range of RMSE? Is there a maximum value?,<machine-learning><model><regression><data-science><metrics>,0,2,N/A,CC BY-SA 4.0
74860609,1,74861477.0,2022-12-20 08:59:14,3,1137,"<p>I am using CNN to classify apple type. I achieved high accuracy on train data but really low accuracy on test data. Data is split into 80:20. I am not sure if my data is overfitting or not.</p>
<p>I have 2 folders that contain <code>TraningData</code> and <code>TestData</code>, and each folder has <code>4</code> subfolders <code>braeburn, red_apple, red_delicious, rotten</code> (containing corresponding pictures).</p>
<pre><code>TRAIN_DIR = 'apple_fruit'
TEST_DIR = 'apple_fruit'
classes = ['braeburn','red_apples','red_delicious','rotten'] train_datagen = ImageDataGenerator(rescale = 1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')
 
test_datagen = ImageDataGenerator(rescale = 1./255) 

training_set = train_datagen.flow_from_directory(TRAIN_DIR,
shuffle=True,
target_size = (100,100),
batch_size = 25,
classes =['braeburn','red_apples','red_delicious','rotten'])

test_set= test_datagen.flow_from_directory(TEST_DIR,
target_size = (100, 100),
shuffle=True,
 batch_size = 25,classes = classes)

model =Sequential()
model.add(Conv2D(filters=128, kernel_size=(3,3),input_shape=(100,100,3), activation='relu', padding
= 'same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding = 'same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.6))
model.add(Dense(4,activation='softmax'))
model.compile(optimizer ='adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

history = model.fit(x=training_set,#y=training_set.labels,
steps_per_epoch=len(training_set),
epochs =10)

model.save('Ripe2_model6.h5')  # creates a HDF5 file 'my_model.h5'

model_path = &quot;Ripe2_model6.h5&quot;
loaded_model = keras.models.load_model(model_path)
classes = ['braeburn','red_apples','red_delicious','rotten']
predictions = model.predict(x=test_set, steps=len(test_set), verbose=True)
pred = np.round(predictions)

y_true=test_set.classes
y_pred=np.argmax(pred, axis=-1)
    &gt; cm = confusion_matrix(y_true=test_set.classes, y_pred=np.argmax(pred, axis=-1))
test_set.classes
np.argmax(pred, axis=-1)
def plot_confusion_matrix(cm, classes,
normalize=False,
title='Confusion matrix',
cmap=plt.cm.Blues):

accuracy = np.trace(cm) / float(np.sum(cm))
misclass = 1 - accuracy

  &quot;&quot;&quot;
This function prints and plots the confusion matrix.
Normalization can be applied by setting `normalize=True`.
    &quot;&quot;&quot;
plt.imshow(cm, interpolation='nearest', cmap=cmap)
plt.title(title,color = 'white')
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45,color = 'white')
plt.yticks(tick_marks, classes,color = 'white')
target_names = ['braeburn','red_apples','red_delicious','rotten']

if target_names is not None:
 tick_marks = np.arange(len(target_names))
 plt.xticks(tick_marks, target_names, rotation=45)
 plt.yticks(tick_marks, target_names)

if normalize:
 cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
 thresh = cm.max() / 1.5 if normalize else cm.max() / 2
 for i, j in itertools.product(range(cm.shape[0]), 
  range(cm.shape[1])):
  if normalize:
   plt.text(j, i, &quot;{:0.4f}&quot;.format(cm[i, j]),
   horizontalalignment=&quot;center&quot;,
   color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;)
  else:
   plt.text(j, i, &quot;{:,}&quot;.format(cm[i, j]),
   horizontalalignment=&quot;center&quot;,
   color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;)

plt.tight_layout()
plt.ylabel('True label',color = 'white')
plt.xlabel('Predicted label',color = 'white')

cm_plot_labels = ['braeburn','red_apples','red_delicious','rotten']
plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')

print(accuracy_score(y_true, y_pred))
print(recall_score(y_true, y_pred, average=None))
print(precision_score(y_true, y_pred, average=None))
</code></pre>
<p>The confusion matrix:</p>
<ul>
<li>accuracy - 0.2909090909090909</li>
<li>recall - [0.23484848 0.32319392 0.15151515 0.36213992]</li>
<li>precision - [0.23308271 0.32319392 0.15151515 0.36363636]</li>
</ul>
<p>I have tried changing many features but still no progress.</p>
",20800233.0,2347649.0,2023-01-03 09:54:55,2023-01-03 09:54:55,High accuracy training but low accuracy test/prediction,<python><validation><machine-learning><data-science><classification>,2,1,N/A,CC BY-SA 4.0
74860662,1,-1.0,2022-12-20 09:04:29,0,383,"<p>I need to provide AADHAAR card photo and need to extract the Data on AADHAAR and display in JSON with masking partial Unique AADHAAR Number</p>
<pre><code>from PIL import Image
from pytesseract import pytesseract

#Define path to tessaract.exe
path_to_tesseract = r'C:/Program Files (x86)/Tesseract-OCR/tesseract.exe'

#Define path to image
path_to_image = 'C:/Users/PJ185101/Documents/images/sample2.jpg'

#Point tessaract_cmd to tessaract.exe
pytesseract.tesseract_cmd = path_to_tesseract

#Open image with PIL
img = Image.open(path_to_image)

#Extract text from image
text = pytesseract.image_to_string(img)

print(text)
</code></pre>
",20821609.0,-1.0,N/A,2022-12-20 09:04:29,is there a way to extract the AADHAAR card data from image and display in JSON with masking partial AADHAAR numbers?,<python><nlp><data-science><ocr><aadhaar>,0,0,N/A,CC BY-SA 4.0
74867521,1,-1.0,2022-12-20 18:38:35,0,158,"<p>I'm running an ANOVA in R using aov, so I can analyze a lot of data at once. I've picked out a few samples to ensure accuracy, and my prism p-values are different than the ones I'm getting in R. My ANOVA in prism is not repeated measures.</p>
<p>I saw this question:</p>
<p><a href=""https://stackoverflow.com/questions/70284098/multitest-p-values-are-different-compared-to-graphpad-prism"">Multitest p-values are different compared to graphpad prism?</a></p>
<p>But I'm not using statsmodel/python.</p>
<p>In R in doing:</p>
<pre><code>analy$genotype &lt;- as.character(for.analy$genotype)
analy$diet &lt;- as.character(for.analy$diet)
analy$gene &lt;- as.numeric(for.analy$gene)

interaction &lt;- aov(gene ~ diet*genotype, data = analy)
summary(interaction)
</code></pre>
<p>This gives me:</p>
<pre><code>              Df    Sum Sq  Mean Sq F value Pr(&gt;F)  
diet           1     34164    34164   0.012 0.9121  
genotype       1       261      261   0.000 0.9923  
diet:genotype  1  12808573 12808573   4.611 0.0361 *
Residuals     56 155556948  2777803   
</code></pre>
<p>While in graphpad prism I get:</p>
<pre><code>ANOVA table SS (Type III)   DF  MS      F (DFn, DFd)        P value
    Interaction 2852813     1   2852813 F (1, 56) = 1.043   P=0.3115
    Diet        12752869    1   12752869F (1, 56) = 4.663   P=0.0351
    Genotype    60058       1   60058   F (1, 56) = 0.02196 P=0.8827
    Residual    153146125   56  2734752     
</code></pre>
<p>Thanks!</p>
",15945265.0,-1.0,N/A,2022-12-20 18:38:35,R AOV giving different p-values compared to graphpad prism?,<r><statistics><data-science><anova>,0,4,N/A,CC BY-SA 4.0
74867704,1,74868751.0,2022-12-20 18:58:22,1,113,"<p>I have an IterativeImputer that uses DecisionTreeRegressor as estimator and I want to print it's tree with export_text method:</p>
<pre><code>import pandas as pd
from sklearn import tree
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer
from sklearn.tree import DecisionTreeRegressor

regressor = DecisionTreeRegressor(criterion=&quot;squared_error&quot;, 
                                  max_depth=None, 
                                  min_samples_split=2,
                                  min_samples_leaf=1, 
                                  random_state=0)
iterative_imputer = IterativeImputer(
    estimator=regressor,
    sample_posterior=False,
    max_iter=10,
    initial_strategy='mean',
    imputation_order='roman',
    verbose=2,
    random_state=0)
iterative_imputer.fit(df)
print(tree.export_text(iterative_imputer.estimator))
</code></pre>
<p>But I'm getting an error:</p>
<blockquote>
<p>sklearn.exceptions.NotFittedError: This DecisionTreeRegressor
instance is not fitted yet. Call 'fit' with appropriate arguments
before using this estimator.</p>
</blockquote>
<p>What am I doing wrong?</p>
",5449454.0,5449454.0,2022-12-20 20:04:19,2022-12-22 00:28:12,sklearn: print DecisionTreeRegressor's tree from IterativeImputer,<python><machine-learning><scikit-learn><data-science>,1,0,N/A,CC BY-SA 4.0
74862462,1,74866553.0,2022-12-20 11:33:56,0,112,"<p>I have a dataset that I'm trying to cluster into. Although I set min_df and max_df in the Tfidf, the output MiniBatchKmeans returns to me contains words that according to the documentation Vectorizer should eliminate because they are present in at least one other document (max_df=1.).</p>
<p>The tfidf settings:</p>
<pre><code>min_df = 5            
max_df = 1.         
vectorizer = TfidfVectorizer(stop_words='english',min_df=min_df, 
max_df=max_df,  max_features=100000) ## Corpus is in English
c_vectorizer = CountVectorizer(stop_words='english',min_df=min_df,   
max_df=max_df, max_features=100000) ## Corpus is in English
X = vectorizer.fit_transform(dataset)
C_X = c_vectorizer.fit_transform(dataset)
</code></pre>
<p>The output of MiniBatchKMeans:</p>
<pre><code>Topic0: information book history read good great lot author write    
useful use recommend need time make know provide like easy   
excellent just learn look work want help reference buy guide 
interested
Topic1: book read good great use make write buy time work like   
just recommend know look year need author want think help new life 
way love people really excellent easy say
Topic2: story novel character book life read love time write make   
like reader great end woman world good man work plot way people  
just family know come young author think year
</code></pre>
<p>As you can see &quot;book&quot; is in all the 3 topic, but with max_df=1. Shouldn't it be deleted?</p>
",20822622.0,20822622.0,2022-12-20 11:39:44,2022-12-20 17:26:55,TfidfVectorizer it does not eliminate words that occur more than once,<scikit-learn><data-science><k-means><tfidfvectorizer>,1,0,N/A,CC BY-SA 4.0
74877847,1,-1.0,2022-12-21 14:52:21,1,39,"<p>In this Amazon dataset I've Product_Description , Product_Type &amp; Sentiment column where I want to build classification model. keeping Product_Description &amp; Product_Type as X and Sentiment as Y. but i receive few error still not able to find the solution. I want the sentence itself to be tokenize for tfidf not different words.</p>
<pre><code>&gt; amazon.head()
</code></pre>
<p><a href=""https://i.stack.imgur.com/Czg9l.png"" rel=""nofollow noreferrer"">Link to data example</a></p>
<pre><code>&gt; `Z = amazon[&quot;Product_Description&quot;]
&gt; Y = amazon[&quot;Sentiment&quot;]
&gt; tfidf = TfidfVectorizer()
&gt; tf = pd.DataFrame(tfidf.fit_transform(Z),columns = [&quot;Product_Description&quot;])
&gt; X = pd.concat((tf,amazon[&quot;Product_Type&quot;]),axis = 1)
&gt; X.drop(X[X[&quot;Product_Description&quot;].isnull()].index, inplace = True)
&gt; test_size = 0.2
&gt; seed = 45
&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, random_state = seed)
&gt; X_train.shape,X_test.shape # [((5081, 2), (1271, 2))] is the output
&gt; model = LogisticRegression(max_iter = 500)
&gt; 
&gt; rfe = RFE(model, n_features_to_select = 2)
&gt; fit = rfe.fit(X, Y)
&gt; 
&gt; fit.n_features_
&gt; fit.support_
&gt; fit.ranking_`
</code></pre>
<pre><code>TypeError                                 Traceback (most recent call last)
TypeError: float() argument must be a string or a number, not 'csr_matrix'

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
Input In [57], in &lt;cell line: 7&gt;()
      4 model = LogisticRegression(max_iter = 500)
      6 rfe = RFE(model, n_features_to_select = 2)
----&gt; 7 fit = rfe.fit(X, Y)
      9 fit.n_features_
     10 fit.support_
```ValueError: setting an array element with a sequence.
</code></pre>
<p>​</p>
",19539308.0,891919.0,2022-12-26 05:29:55,2022-12-26 05:29:55,Tokenize Review wise for sentiment analysis,<nlp><data-science><sentiment-analysis><text-classification><tfidfvectorizer>,0,1,N/A,CC BY-SA 4.0
74885479,1,74885519.0,2022-12-22 07:46:03,1,50,"<p>I have three excel files that was read to pandas DFs, the tables are below :
DF1:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column A</th>
<th>Column B</th>
</tr>
</thead>
<tbody>
<tr>
<td>F</td>
<td>G</td>
</tr>
<tr>
<td>M</td>
<td>K</td>
</tr>
</tbody>
</table>
</div>
<p>DF2:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column A</th>
<th>Column B</th>
</tr>
</thead>
<tbody>
<tr>
<td>J</td>
<td>D</td>
</tr>
<tr>
<td>N</td>
<td>L</td>
</tr>
</tbody>
</table>
</div>
<p>DF3:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column A</th>
<th>Column B</th>
<th>Column C</th>
</tr>
</thead>
<tbody>
<tr>
<td>J</td>
<td>E</td>
<td>Y</td>
</tr>
<tr>
<td>N</td>
<td>Q</td>
<td>V</td>
</tr>
<tr>
<td>J</td>
<td>D</td>
<td>B</td>
</tr>
<tr>
<td>E</td>
<td>B</td>
<td>F</td>
</tr>
</tbody>
</table>
</div>
<p>What I want to do is creating a fourth DF containing joined certain columns from the dataframes (Based on the column index) from the dataframes.
for DF4, what I want it to be :
1)(Column A from DF1) with (Column C from Df3) with (Column A from DF2)<br />
2)(Column B from DF1) with (Column A from Df3) with (Column B from DF2)</p>
<p>The expected result of DF4 :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Char 1</th>
<th>CHAR2</th>
</tr>
</thead>
<tbody>
<tr>
<td>F</td>
<td>G</td>
</tr>
<tr>
<td>M</td>
<td>K</td>
</tr>
<tr>
<td>Y</td>
<td>J</td>
</tr>
<tr>
<td>V</td>
<td>N</td>
</tr>
<tr>
<td>B</td>
<td>J</td>
</tr>
<tr>
<td>F</td>
<td>E</td>
</tr>
<tr>
<td>J</td>
<td>D</td>
</tr>
<tr>
<td>N</td>
<td>L</td>
</tr>
</tbody>
</table>
</div>
<p>what I wrote for now :
`</p>
<pre><code>import pandas as pd
NUR = pd.read_excel(r'C:\Users\jalal.hasain\Desktop\Copy of NUR Data 20-12.xlsx', 
                   sheet_name=['2G','3G','4G'])

DF1=NUR.get('2G')
DF2=NUR.get('3G')
DF3=NUR.get('4G')
</code></pre>
<p>`</p>
<p>Thanks in advance.</p>
",13766202.0,-1.0,N/A,2022-12-22 08:17:43,Combining certain columns from three excel files in one pandas DF based on the column number-Using python pandas,<python><pandas><dataframe><data-science>,1,0,N/A,CC BY-SA 4.0
74872071,1,-1.0,2022-12-21 06:29:00,-2,96,"<p>Creating a ML model for predictive maintenance .</p>
<p>table A contains log information with start time , end time and event status.
<strong>table A</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>startTime</th>
<th>endTime</th>
<th>event</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>7/1/2021 1:05:04</td>
<td>7/1/2021 1:06:04</td>
<td>OS</td>
</tr>
</tbody>
</table>
</div>
<p>Table B contains multiple records of what are the temperature (for each seconds) present for the event  in table A .</p>
<p><strong>Table B</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>startTime</th>
<th>endTime</th>
<th>time(seconds)</th>
<th>temp</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>7/1/2021 1:05:04</td>
<td>7/1/2021 1:06:04</td>
<td>1</td>
<td>0.23</td>
</tr>
<tr>
<td>100</td>
<td>7/1/2021 1:05:04</td>
<td>7/1/2021 1:06:04</td>
<td>2</td>
<td>0.32</td>
</tr>
<tr>
<td>100</td>
<td>7/1/2021 1:05:04</td>
<td>7/1/2021 1:06:04</td>
<td>55</td>
<td>0.41</td>
</tr>
<tr>
<td>100</td>
<td>7/1/2021 1:05:04</td>
<td>7/1/2021 1:06:04</td>
<td>56</td>
<td>0.36</td>
</tr>
<tr>
<td>100</td>
<td>7/1/2021 1:05:04</td>
<td>7/1/2021 1:06:04</td>
<td>57</td>
<td>0.32</td>
</tr>
<tr>
<td>100</td>
<td>7/1/2021 1:05:04</td>
<td>7/1/2021 1:06:04</td>
<td>58</td>
<td>0.41</td>
</tr>
<tr>
<td>100</td>
<td>7/1/2021 1:05:04</td>
<td>7/1/2021 1:06:04</td>
<td>59</td>
<td>0.35</td>
</tr>
</tbody>
</table>
</div>
<p>What is the best way to aggregate table B, so that i can merge with table A for creating ML models.</p>
<p>Used Mean () and standard deviation for the aggregation, but feel like there is chances of losing actual data information.</p>
<p>Is there any great way to handle this kind of data or any other statistical method will be useful in this case as the temperature in each second is more critical  when dealing with error analysis</p>
",20260341.0,20260341.0,2022-12-21 07:21:38,2022-12-21 07:21:38,Best Approaches to summarize or aggregate multiple rows to single row for Machine learning model creation,<machine-learning><data-science><aggregate><data-analysis>,1,0,N/A,CC BY-SA 4.0
74886400,1,-1.0,2022-12-22 09:16:33,-2,765,"<p>KEY POINT: the Dataset is so large that I am barely able to store it in hardware. (PetaBytes)</p>
<p>Say I have trillions and trillion of rows in a dataset. This dataset is too large to be stored in memory. I want to train a machine learning model, say logisitc regression, on this dataset. How do I go about this?</p>
<p>Now, I know Amazon/Google does machine learning on huge amounts of data. How do they go about it? For example, click dataset, where globally each smart devices' inputs are stored in a dataset.</p>
<p>Desperately looking for new ideas and open to corrections.</p>
<p>My train of thoughts:</p>
<ol>
<li>load a part of data into the memory</li>
<li>Perform gradient descent</li>
</ol>
<p>This way the optimization is mini batch descent.</p>
<p>Now the problem is, in the optimization, be it SGD or mini batch, it stops when it has gone through ALL the data in the worst case. Traversing the whole dataset is not possible.</p>
<p>So I had the idea of early stopping. Early stopping reserves a validation set and will stop optimization when the error stops going down/converges on the validation set. But again this might not be feasible due to the size of the dataset.</p>
<p>Now I am thinking of simply random sampling a training set and a test set, with workable sizes to train the model.</p>
",16454711.0,-1.0,N/A,2022-12-22 13:39:55,How to train a machine learning model on huge amount of data?,<database><dataframe><machine-learning><deep-learning><data-science>,3,1,N/A,CC BY-SA 4.0
74887601,1,-1.0,2022-12-22 11:06:21,0,74,"<p>I have to plot this by subplots, I have 4 features and i want to have 2 rows and 2 columns, how can i do that?</p>
<pre><code>cat_features = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']

f, axs = plt.subplots(2, len(cat_features[:-1]), figsize=(10, 4))
for ax, col in zip(axs, cat_features[:-1]):
    sns.countplot(data=df_train, x=col, hue=&quot;satisfaction&quot;,palette='Blues',ax=ax.flatten()[i])
f.tight_layout()
</code></pre>
<p>This is the result but it's so bad
<a href=""https://i.stack.imgur.com/NKo7q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NKo7q.png"" alt=""enter image description here"" /></a></p>
",19610821.0,-1.0,N/A,2022-12-22 15:06:51,How to transform this plot in a right subplot?,<python><matplotlib><seaborn><data-science><visualization>,1,1,N/A,CC BY-SA 4.0
74893906,1,-1.0,2022-12-22 21:13:06,0,23,"<p>I have a dict:</p>
<pre><code>d = {12 : [8,2,3,4]
     15 : [6,1,9,2]}
</code></pre>
<p>I want to transform it to a flat dataframe:</p>
<pre><code>df = val  0 1 2 3 
     12   8 2 3 4
     15   6 1 9 2
</code></pre>
<p>How can I do this?</p>
",6057371.0,-1.0,N/A,2022-12-22 21:13:06,pandas nested dict to flat dataframe,<python><pandas><dataframe><data-science>,0,2,2022-12-22 21:18:02,CC BY-SA 4.0
74894560,1,74896528.0,2022-12-22 22:44:16,0,37,"<p>I read somewhere suggesting that in case there are multiple features(multi linear model) no feature scaling is needed because co-efficient takes care of that.</p>
<p>But for single feature(simple linear model); feature scaling is needed.</p>
<p>Is this how python scikilt learn works or I read something wrong?</p>
<p>Need answer from someone who has tested both with and without feature scaling in simple linear regression</p>
",20827830.0,-1.0,N/A,2022-12-23 05:49:15,Simple linear regressions vs multiple linear regression model scaling,<python-3.x><machine-learning><data-science>,1,1,N/A,CC BY-SA 4.0
74901614,1,74901740.0,2022-12-23 15:59:20,0,133,"<p>first of all, in case I comment on any mistakes while writing this, sorry, English is not my first language.</p>
<p>So I started to study data science and data visualization in sports with Python just for hobby, I'm a really begginer on this. I want to calculate the percentile of each columns based on the highest value, I will put a image below, for example, in the column ''xg'', the highest value is 1.03, I want to transform this value in a new column with the value 100%. And so on in the other columns</p>
<p><img src=""https://i.stack.imgur.com/zUR9d.png"" alt=""enter image description here"" /></p>
<p>I want to do something like this:</p>
<p>[The stat/the percentile of stat compared to all rows]</p>
<p><img src=""https://i.stack.imgur.com/L1FTH.png"" alt=""1"" />I</p>
",19409446.0,1367454.0,2022-12-23 16:10:55,2022-12-23 16:21:17,Calculate percentile with column values,<python><pandas><math><statistics><data-science>,2,0,N/A,CC BY-SA 4.0
74902506,1,-1.0,2022-12-23 17:39:26,0,150,"<p>I have been trying to fine tune a BERT model to give response sentences like a character based on input sentences but I am getting a rather odd error every time . the code is
`</p>
<p>Here source<em>texts is a list of sentences that give the context and target_text is a list of sentences that give response to context statments</em></p>
<pre><code>
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained(&quot;bert-base-cased&quot;).to(device)
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

input_ids = \[\]
output_ids = \[\]
for i in range (0 , len(source_text):
input_ids.append(tokenizer.encode(source_texts\[i\], return_tensors=&quot;pt&quot;))
output_ids.append(tokenizer.encode(target_texts\[i\], return_tensors=&quot;pt&quot;))

import torch
device = torch.device(&quot;cuda&quot;)

from transformers import BertForMaskedLM, AdamW

model = BertForMaskedLM.from_pretrained(&quot;bert-base-cased&quot;)
optimizer = AdamW(model.parameters(), lr=1e-5)
loss_fn = torch.nn.CrossEntropyLoss()

def train(input_id, output_id):
input_id = input_id.to(device)
output_id = output_id.to(device)

    model.zero_grad()
    
    logits, _ = model(input_id, labels=output_id)
    
    # Compute the loss
    loss = loss_fn(logits.view(-1, logits.size(-1)), output_id.view(-1))
    
    loss.backward()
    optimizer.step()
    return loss.item()

for epoch in range(50):
\# Train the model on the training dataset
train_loss = 0.0
for input_sequences, output_sequences in zip(input_ids, output_ids):
input_sequences = input_sequences.to(device)
output_sequences = output_sequences.to(device)
train_loss += train(input_sequences, output_sequences)
</code></pre>
<p>This is the <a href=""https://i.stack.imgur.com/MK2sF.png"" rel=""nofollow noreferrer"">Error</a> that I am getting</p>
<p>Any help would be really appreciated .</p>
<p>Pls help!!</p>
",20848918.0,-1.0,N/A,2022-12-24 09:36:16,Fine tuning a BERT Model as a chatbot giving error while training,<machine-learning><deep-learning><nlp><data-science><fine-tune>,1,1,N/A,CC BY-SA 4.0
74892132,1,-1.0,2022-12-22 17:49:29,0,18,"<p>I am trying to convert my dataset from long to wide format only using <strong>base R packages and dplyr <strong>only</strong></strong> due to university's project requirements. I have tried to use reshape function, but I have encountered problems with converting them using the reshape() function.
My data looks like this <em>please note that in original dataset in geo column there are 27 countries and dates that go from 2000-02-01 to 2022-09-01</em>:</p>
<pre><code># A tibble: 8 × 3
  geo     time       values
  &lt;chr&gt;   &lt;date&gt;      &lt;dbl&gt;
1 Poland  2022-09-01   1.3 
2 Germany 2022-09-01   2.1 
3 Denmark 2022-09-01   3.2 
4 Greece  2022-09-01   4.1 
5 Poland  2022-08-01   1.1 
6 Germany 2022-08-01   2.7 
7 Denmark 2022-08-01   3.8 
8 Greece  2022-08-01   4.12
</code></pre>
<p>I want to convert that data into this format :</p>
<pre><code># A tibble: 4 × 5
  time       Poland Germany Denmark Greece
  &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
1 2022-09-01    1       2       3     4   
2 2022-08-01    2.1     3.3     4.5   5.7 
3 2022-07-01    7       8       5     8   
4 2022-06-01   21.2     2.3     1.2   2.13
</code></pre>
<p>You can reproduce these two datasets with this code</p>
<p>Long data:</p>
<pre><code>library(dplyr)

datasample &lt;- matrix(c(&quot;Poland&quot;,&quot;Germany&quot;,&quot;Denmark&quot;,&quot;Greece&quot;,&quot;Poland&quot;,&quot;Germany&quot;,&quot;Denmark&quot;,&quot;Greece&quot;,'2022-09-01','2022-09-01','2022-09-01','2022-09-01','2022-08-01','2022-08-01','2022-08-01','2022-08-01',1.3,2.1,3.2,4.1,1.1,2.7,3.8,4.12),
                     ncol = 3,
                     byrow = FALSE)
colnames(datasample) &lt;- c(&quot;geo&quot;,&quot;time&quot;,&quot;values&quot;)
data_tibble &lt;- as_tibble(datasample)
data_tibble &lt;- data_tibble %&gt;% mutate(geo = as.character(geo),
                                      time = as.Date(time),
                                      values = as.double(values))
</code></pre>
<p>Wide data:</p>
<pre><code>data_wide &lt;- matrix(c(&quot;2022-09-01&quot;,1,2,3,4,&quot;2022-08-01&quot;,2.1,3.3,4.5,5.7,&quot;2022-07-01&quot;,7,8,5,8,&quot;2022-06-01&quot;,21.2,2.3,1.2,2.13),ncol=5, byrow=TRUE)
colnames(data_wide) &lt;- c(&quot;time&quot;, &quot;Poland&quot;,&quot;Germany&quot;,&quot;Denmark&quot;,&quot;Greece&quot;)
data_wide &lt;- as_tibble(data_wide)
data_wide &lt;- data_wide %&gt;%
  mutate(time = as.Date(time),
         Poland = as.double(Poland),
         Germany = as.double(Germany),
         Denmark = as.double(Denmark),
         Greece = as.double(Greece))
</code></pre>
<p>I tried to use the simple reshape function from base R library:</p>
<pre><code>data_wide &lt;- reshape(original_data, idvar = &quot;time&quot;, timevar = &quot;geo&quot;, direction = &quot;wide&quot;)
</code></pre>
<p>and I get something like this:</p>
<pre><code># A tibble: 272 × 2
   time       values.c(&quot;Austria&quot;, &quot;Belgium&quot;, &quot;Bulgaria&quot;, &quot;Cyprus&quot;, &quot;Czech Republi…¹
   &lt;date&gt;                                                                     &lt;dbl&gt;
 1 2022-09-01                                                                    NA
 2 2022-08-01                                                                    NA
 3 2022-07-01                                                                    NA
 4 2022-06-01                                                                    NA
 5 2022-05-01                                                                    NA
 6 2022-04-01                                                                    NA
 7 2022-03-01                                                                    NA
 8 2022-02-01                                                                    NA
 9 2022-01-01                                                                    NA
10 2021-12-01                                                                    NA
</code></pre>
<p>This is something close to the result that I want but as you can see it is now divided into 2 columns
there should be time and seperate columns for each country but it is now a singular vector with country names.</p>
<p>How can i form that vector into seperate columns and insert the values to appropriate records?</p>
",20811243.0,-1.0,N/A,2022-12-22 17:49:29,How to convert tibble data with various classes from long to wide format problem using ONLY base R package?,<r><data-science><reshape><tibble>,0,2,2022-12-22 18:18:29,CC BY-SA 4.0
74893218,1,74893831.0,2022-12-22 19:48:24,0,89,"<p><strong>This website <a href=""https://aviation-safety.net/wikibase/"" rel=""nofollow noreferrer"">https://aviation-safety.net/wikibase/</a> DB begins from year 1902 to 2022.
I am trying to scrape the table, narrative, probable cause and classification for every accidents in the year 2015 and 2016: <a href=""https://aviation-safety.net/database/dblist.php?Year=2015"" rel=""nofollow noreferrer"">https://aviation-safety.net/database/dblist.php?Year=2015</a>. With the below code I am able to scrape the table only:</strong></p>
<pre><code>import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import re
import concurrent.futures
import itertools
from random import randint
from time import sleep

def scraping(year):


    headers =   {
        'accept':'*/*',
        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
        }

    url = f'https://aviation-safety.net/database/dblist.php?Year={year}&amp;sorteer=datekey&amp;page=1'
    #sleep(randint(1,3))
    req = requests.get(url, headers=headers)

    soup = BeautifulSoup(req.text,'html.parser')

    page_container = soup.find('div',{'class':'pagenumbers'})

    pages = max([int(page['href'].split('=')[-1]) for page in  page_container.find_all('a')])
        

    #info = []
    tl = []
    for page in range(1,pages+1):

        new_url = f'https://aviation-safety.net/database/dblist.php?Year={year}&amp;lang=&amp;page={page}'
        print(new_url)
        
        #sleep(randint(1,3))
        data = requests.get(new_url,headers=headers)
        soup = BeautifulSoup(data.text,'html.parser')


        table = soup.find('table')
   
    
        for index,row in enumerate(table.find_all('tr')):
            if index == 0:
                continue

            link_ = 'https://aviation-safety.net/'+row.find('a')['href']
            
            #sleep(randint(1,3))
            new_page = requests.get(link_, headers=headers)
            new_soup = BeautifulSoup(new_page.text, 'lxml')
            table1 = new_soup.find('table')
            
           
            for i in table1.find_all('tr'):
                title = i.text
                tl.append(title)
                
                
    df= pd.DataFrame(tl)
    df.columns = ['status'] 
    df.to_csv(f'{year}_aviation-safety_new.csv', encoding='utf-8-sig', index=False)    
          

if __name__ == &quot;__main__&quot;:

    START = 2015
    STOP = 2016

    years = [year for year in range(START,STOP+1)]

    print(f'Scraping {len(years)} years of data')

    with concurrent.futures.ThreadPoolExecutor(max_workers=60) as executor:
        final_list = executor.map(scraping,years)

</code></pre>
<p><strong>But the data is not organized. The dataframe looks like this:</strong></p>
<p><a href=""https://i.stack.imgur.com/5vWq0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5vWq0.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>The outcome should be like this:</strong></p>
<p><a href=""https://i.stack.imgur.com/NqxM9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NqxM9.jpg"" alt=""enter image description here"" /></a></p>
",13829846.0,13829846.0,2022-12-31 00:17:59,2022-12-31 00:17:59,How to scrape a table from a page and create a multi-column dataframe with python?,<python><pandas><dataframe><web-scraping><data-science>,2,2,N/A,CC BY-SA 4.0
74908086,1,74908440.0,2022-12-24 13:52:20,1,154,"<p>I have a csv file called purchases.csv and I am trying to find how many purchases of each item in a month and every month seperately. I found every month seperately and how many purchases of each item. But if a item is never got purchased that month I need it to be shown as 0. This is my code so far;</p>
<pre><code> #Reading the dataset
data = pd.read_csv('purchases.csv')
df = pd.DataFrame(data)


#Filtering the data
df['date']=pd.to_datetime(df['date'])
dataset=df[df['date'].dt.year == 2020]

#january
# Select DataFrame rows between two dates
januaryFilter = (dataset['date'] &gt; '2020-01-01') &amp; (dataset['date'] &lt;= '2020-01-31')
january = dataset.loc[januaryFilter]
jan = pd.crosstab(january['item_id'], januaryFilter)
print(jan)

#february
# Select DataFrame rows between two dates
februaryFilter = (dataset['date'] &gt; '2020-02-01') &amp; (dataset['date'] &lt;= '2020-02-28')
february = dataset.loc[februaryFilter]
feb = pd.crosstab(february['item_id'], februaryFilter)
print(feb)
</code></pre>
<p>This is my dataset.</p>
<pre><code>session_id,item_id,date
3,15085,2020-12-18 21:26:47.986
13,18626,2020-03-13 19:36:15.507
18,24911,2020-08-26 19:20:32.049
19,12534,2020-11-02 17:16:45.92
24,13226,2020-02-26 18:27:44.114
28,26394,2020-05-18 12:52:09.764
31,8345,2021-04-20 19:46:42.594
36,14532,2020-06-21 10:33:22.535
42,11784,2021-03-01 15:17:04.264
44,4028,2020-11-27 20:46:08.951
48,24022,2020-04-15 17:29:15.414
49,2011,2020-05-01 12:34:29.86
52,12556,2020-03-21 11:49:07.324
75,28057,2020-05-24 17:27:54.288
77,4243,2020-09-20 21:37:20.838
107,4016,2020-01-15 06:07:23.177
108,18532,2020-06-06 17:25:15.508
113,21107,2021-05-05 14:15:07.278
115,25976,2021-05-27 10:24:05.043
119,434,2020-10-11 06:32:22.085
124,3732,2020-05-18 11:04:15.42
127,25117,2020-01-15 15:17:43.659
140,23502,2021-04-28 13:45:31.202
</code></pre>
<p>This is my output right now.</p>
<p><a href=""https://i.stack.imgur.com/2nly3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2nly3.png"" alt=""january data"" /></a>
<a href=""https://i.stack.imgur.com/Orpno.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Orpno.png"" alt=""february data"" /></a></p>
<p>I know that there are item_id's (purchases) which are 0. But I can't see any. Can you help me with this problem ?</p>
",10964038.0,10964038.0,2022-12-28 17:32:31,2022-12-28 17:32:31,How to fill null values in a an aggregated table with Pandas?,<python><python-3.x><pandas><dataset><data-science>,2,0,N/A,CC BY-SA 4.0
74866878,1,-1.0,2022-12-20 17:39:43,0,328,"<p>So, the order is &quot;Ask the user to enter the name of the object for which he/she wants to receive data and the names of the filters,
data in which you need (it is possible to introduce several filters).&quot;</p>
<p>this is my data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Object</th>
<th>HJD 24...</th>
<th>Filter</th>
<th>Magnitude</th>
</tr>
</thead>
<tbody>
<tr>
<td>SU_Hor</td>
<td>55896.30476</td>
<td>B</td>
<td>14.877</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.27438</td>
<td>Ic</td>
<td>13.885</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.27349</td>
<td>B</td>
<td>14.809</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.27397</td>
<td>V</td>
<td>14.434</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.40882</td>
<td>Ic</td>
<td>14.033</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.40829</td>
<td>V</td>
<td>14.540</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.40770</td>
<td>B</td>
<td>14.941</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.34973</td>
<td>Ic</td>
<td>13.958</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.34943</td>
<td>V</td>
<td>14.494</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.34906</td>
<td>B</td>
<td>14.861</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.30542</td>
<td>Ic</td>
<td>13.912</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55896.30512</td>
<td>v</td>
<td>14.440</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55897.38547</td>
<td>V</td>
<td>14.536</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55897.28281</td>
<td>B</td>
<td>14.882</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55897.28317</td>
<td>V</td>
<td>14.428</td>
</tr>
<tr>
<td>SU_Hor</td>
<td>55897.28347</td>
<td>Ic</td>
<td>13.927</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>27359.3030</td>
<td>V</td>
<td>10.630</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>27684.4510</td>
<td>V</td>
<td>10.610</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>27685.4780</td>
<td>V</td>
<td>10.580</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>27701.3150</td>
<td>V</td>
<td>10.700</td>
</tr>
<tr>
<td>RZ Lyr</td>
<td>27934.4560</td>
<td>V</td>
<td>10.660</td>
</tr>
<tr>
<td>RZ Lyr</td>
<td>27955.4100</td>
<td>V</td>
<td>10.570</td>
</tr>
<tr>
<td>rzlyr</td>
<td>30604.2000</td>
<td>V</td>
<td>11.030</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>55314.5695</td>
<td>B</td>
<td>12.047</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>55314.5724</td>
<td>B</td>
<td>12.036</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>55314.5900</td>
<td>B</td>
<td>12.042</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>55314.6105</td>
<td>B</td>
<td>12.045</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>55314.6163</td>
<td>B</td>
<td>12.027</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>55342.3509</td>
<td>B</td>
<td>12.057</td>
</tr>
<tr>
<td>RZLyr</td>
<td>55342.3557</td>
<td>B</td>
<td>12.058</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>55342.3606</td>
<td>B</td>
<td>12.052</td>
</tr>
<tr>
<td>RZ_Lyr</td>
<td>55342.3654</td>
<td>B</td>
<td>12.058</td>
</tr>
</tbody>
</table>
</div>
<p>here is my code:</p>
<pre><code>def searchByFilter():
    filter = input('Enter filter to show data \n')
    df = pd.read_csv('Python_2ndLab.csv')
    print(df.loc[df['Filter'] == filter, :])


print('Enter 1 to search by object name')
print('Enter 2 to search by filter')

src = (input('Enter here: '))

if src == '1':
    searchByObject()
elif src == '2':
    searchByFilter()
else:
    print('Sorry, invalid input')
</code></pre>
<p>So in the output, I enter &quot;2&quot; to search by the filter, then the output ask to enter the filter name then I enter &quot;B&quot; but instead of the data from the B filter, I receive the error.</p>
<pre><code>Name: Filter, dtype: object
Enter 1 to search by object name
Enter 2 to search by filter
Enter here: 2
Enter filter to show data 
B
</code></pre>
<p>this is the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\Acer\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\indexes\base.py&quot;, line 3803, in get_loc
    return self._engine.get_loc(casted_key)
  File &quot;pandas\_libs\index.pyx&quot;, line 138, in pandas._libs.index.IndexEngine.get_loc
  File &quot;pandas\_libs\index.pyx&quot;, line 165, in pandas._libs.index.IndexEngine.get_loc
  File &quot;pandas\_libs\hashtable_class_helper.pxi&quot;, line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File &quot;pandas\_libs\hashtable_class_helper.pxi&quot;, line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'Filter'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\Acer\PycharmProjects\2nd Lab\main.py&quot;, line 78, in &lt;module&gt;
    searchByFilter()
  File &quot;C:\Users\Acer\PycharmProjects\2nd Lab\main.py&quot;, line 64, in searchByFilter
    print(df.loc[df['Filter'] == filter, :])
  File &quot;C:\Users\Acer\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\frame.py&quot;, line 3805, in __getitem__
    indexer = self.columns.get_loc(key)
  File &quot;C:\Users\Acer\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\indexes\base.py&quot;, line 3805, in get_loc
    raise KeyError(key) from err
KeyError: 'Filter'
</code></pre>
<p>Could anyone help me to debug it? I really have no idea where is the error.</p>
",20794630.0,20794630.0,2022-12-23 14:33:21,2022-12-23 14:33:21,KeyError: 'Filter' (receiving specific data from dataframe),<python><pandas><dataset><data-science><src>,1,3,N/A,CC BY-SA 4.0
74867083,1,-1.0,2022-12-20 17:57:55,1,52,"<p>I'm getting started on my ML journey. I'm using the data science handbook to learn about PCA. The following code snippet is used to generate a random 2D data set with a linear relationship. Could someone explain the intuition of how multiplying a 2x2 and 2x200 random matrices results in a linear relationship as shown in the plot?</p>
<p><a href=""https://i.stack.imgur.com/97mn9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/97mn9.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/ym6ru.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ym6ru.png"" alt=""enter image description here"" /></a></p>
",978524.0,-1.0,N/A,2022-12-20 17:57:55,How to generate 2D random dataset for PCA,<machine-learning><data-science>,0,2,N/A,CC BY-SA 4.0
74888969,1,-1.0,2022-12-22 13:08:21,1,484,"<p>I am trying to convert an ONNX model with a dynamic input shape to TensorFlow format using the <strong>onnx_tf</strong> package. I am using <strong>TensorFlow 2.11.0</strong>, <strong>ONNX 1.13.0</strong>, and <strong>onnx_tf 1.10.0</strong>. The input to the model contains 3 arrays: the data with dynamic shape (1, None) and 2 arrays with fixed shapes (2,1,64). When I run the following code, I get on the last line a &quot;ValueError: Cannot take the length of shape with unknown rank&quot; :</p>
<pre><code># Imports
import onnx
from onnx import version_converter
import os
from onnx_tf.backend import prepare

# load onnx model
model_dir = r'model\vad'
model_name = 'vad.onnx'
onnx_model = onnx.load(os.path.join(model_dir,'onnx', model_name))  # load onnx model

# Export to Tensorflow
os.makedirs(os.path.join(model_dir ,'tensorflow'), exist_ok=True)
tf_rep = prepare(onnx_model)  # prepare tf representation
tf_rep.export_graph(os.path.join(model_dir,'tensorflow','vad'))  # export the model
</code></pre>
<p>What could be causing this error and how can I fix it?</p>
",17636499.0,-1.0,N/A,2022-12-22 13:08:21,How to convert an ONNX model with dynamic input shape to Tensorflow model?,<python><tensorflow><data-science><onnx>,0,0,N/A,CC BY-SA 4.0
74896493,1,74897052.0,2022-12-23 05:42:46,1,61,"<p>I have an original array in python : <em>t_array</em></p>
<pre><code>t_array=(['META' , 'AAPL' , 'AMZN' , 'NFLX' , 'GOOG' ])

sample = random.sample (t_array, 3)

print=( 'sample')
</code></pre>
<p>and got this :</p>
<p><code>['AAPL', 'AMZN', 'META']</code></p>
<p>Now when I run the <code>(1)</code> code again the sample refreshes to a new triple element array like the following :</p>
<p><code>['AMZN', 'META', 'NFLX']</code> or so</p>
<p>I wish to <strong>get all possible combinations</strong> without replacement from the original array <strong>t_array</strong> <strong>and print them</strong> at once in a <strong>dataframe</strong> format or <strong>series</strong> format so I can refer to them by index in my further code</p>
<p><strong>How do I code that in python?</strong></p>
<p><strong>machine</strong>: currently using jupyter notebook on mac osx</p>
",15491880.0,20174226.0,2022-12-23 07:09:28,2022-12-23 07:14:24,Python - Printing multiple arrays of randomly sampled elements from an original array,<python><pandas><data-science>,1,3,N/A,CC BY-SA 4.0
74903336,1,-1.0,2022-12-23 19:24:12,0,161,"<p>i have this error below from my code for a K-Fold cross validation</p>
<pre><code>&quot;None of [Int64Index([      0,       1,       3,       4,       5,       6,       7,\n                  8,       9,      10,\n            ...\n            1048565, 1048566, 1048567, 1048568, 1048569, 1048570, 1048571,\n            1048572, 1048573, 1048574],\n           dtype='int64', length=943717)] are in the [columns]&quot;
</code></pre>
<p>the model is logistic regression model and i need to evaluate it using K-fold
however if you have another code in python i would be thankful</p>
<p>this is my code</p>
<pre><code>y3_data = data['DEATH']
    #dependant variable y_data:
x3_data = df14
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=0)
x3_resampled,y3_resampled = rus.fit_resample(x3_data,y3_data)




    #training and test sample :
x3_training_data, x3_test_data, y3_training_data, y3_test_data = train_test_split(x3_data, y3_data, test_size = 0.3)

    # Estimation result:
logit_model=sm.Logit(y3_training_data,x3_training_data)
result3=logit_model.fit()
print(result3.summary2())

    # Model Evaluation  :
logreg=LogisticRegression()
logreg.fit(x3_training_data,y3_training_data)
y_pred=logreg.predict(x3_test_data)
print('Logistic regression model accuracy:{:.2f}'.format(logreg.score(x3_test_data,y3_test_data)))
print(&quot;Logistic Regression F1 Score :&quot;,f1_score(y3_test_data,logreg.predict(x3_test_data),average=None))
sns.heatmap(confusion_matrix(y3_test_data, logreg.predict(x3_test_data)), annot=True, fmt=&quot;.0f&quot;)
plt.title(&quot;Logistic Regression Confusion Matrix&quot;,fontsize=18, color=&quot;red&quot;);

num_splits = 10 
kfold = StratifiedKFold(num_splits, shuffle= True, random_state = 1)
train_accs, test_accs = [], []  #create empty lists to store accurcy values
for train_index, test_index in kfold.split(x3_data, y3_data):  #Generate indices to split data into training and test set.
    x3_train, x3_test = x3_data[train_index], x3_data[test_index]
    y3_train, y3_test = y3_data[train_index], y3_data[test_index]
  
    logreg.fit(x3_train, y3_train)
    y3_pred_train = logreg.predict(x3_train)
    y3_pred_test = logreg.predict(x3_test)
    
    train_accs.append(metrics.accuracy_score(y3_train, y3_pred_train) * 100)
    test_accs.append(metrics.accuracy_score(y3_test, y3_pred_test) * 100)

ave_train_acc = 0
ave_test_acc = 0

print(&quot;\t&quot;,&quot;Training_Acc&quot;,&quot;\t&quot;,&quot;\t&quot;, &quot;Testing_Acc&quot;)

for i in range(num_splits):
    print(i,&quot;\t&quot;, train_accs[i],&quot;\t&quot;, test_accs[i])
    
    ave_train_acc+= train_accs[i]/num_splits
    ave_test_acc+= test_accs[i]/num_splits
    
print(&quot;Av&quot;, &quot;\t&quot;, ave_train_acc,&quot;\t&quot;, ave_test_acc)
</code></pre>
<p>the error is only on the k fold part</p>
<p>and i want the output to be like this one below</p>
<pre><code>     Training_Acc        Testing_Acc
0    76.0586319218241    75.32467532467533
1    75.2442996742671    74.67532467532467
2    73.9413680781759    81.16883116883116
3    77.72357723577235   71.89542483660131
4    76.7479674796748    73.20261437908496
Av   75.94316887794285   75.25337407690348

</code></pre>
<p>PLEASE help</p>
<p>THIS IS the full error trace track</p>
<pre><code>RF Regression F1 Score : [0.96507625 0.47731195]

---------------------------------------------------------------------------

KeyError                                  Traceback (most recent call last)

&lt;ipython-input-19-955883a0e9f9&gt; in &lt;module&gt;
     62 train_accs, test_accs = [], []  #create empty lists to store accurcy values
     63 for train_index, test_index in kfold.split(x_data, y_data):  #Generate indices to split data into training and test set.
---&gt; 64     x_train_data, x_test_data = x_data[train_index], x_data[test_index]
     65     y_train, y_test = y_data[train_index], y_data[test_index]
     66 

    2 frames
    
    /usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis)
       1372                 if use_interval_msg:
       1373                     key = list(key)
    -&gt; 1374                 raise KeyError(f&quot;None of [{key}] are in the [{axis_name}]&quot;)
       1375 
       1376             not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())
    
    KeyError: &quot;None of [Int64Index([      0,       1,       3,       4,       5,       6,       7,\n                  8,       9,      10,\n            ...\n            1048564, 1048565, 1048566, 1048567, 1048568, 1048569, 1048570,\n            1048572, 1048573, 1048574],\n           dtype='int64', length=838860)] are in the [columns]&quot;
</code></pre>
<p>new error</p>
<pre><code>&gt; ---------------------------------------------------------------------------

KeyError                                  Traceback (most recent call last)

/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3360             try:
-&gt; 3361                 return self._engine.get_loc(casted_key)
   3362             except KeyError as err:

4 frames

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 0


The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)

/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3361                 return self._engine.get_loc(casted_key)
   3362             except KeyError as err:
-&gt; 3363                 raise KeyError(key) from err
   3364 
   3365         if is_scalar(key) and isna(key) and not self.hasnans:
</code></pre>
",20822865.0,20822865.0,2022-12-24 17:53:44,2022-12-24 17:53:44,i have a simple Error from my k-fold cross validation code for a logistic regression model Python,<python><machine-learning><statistics><data-science><google-colaboratory>,0,16,N/A,CC BY-SA 4.0
74904822,1,-1.0,2022-12-23 23:51:24,0,47,"<p>I do not know how I can merge my two .csv files. The second file should be added to the first one based on a foreign key which should extend the first table.</p>
<p>So I did my research but whenever I tried something the second table added to the first one based on their primary key and not the numerical foreign key. But I have a list of people and need to add their sales of this year to the list from the second list.</p>
<p>Do I need to add another column to the first table named after the foreign key although it's based on the numbers of the primary key?</p>
",20850259.0,-1.0,N/A,2022-12-23 23:51:24,How can I merge my two files - one has a foreign key and the rows should be added to the other table,<python><csv><merge><foreign-keys><data-science>,0,1,N/A,CC BY-SA 4.0
74875388,1,74882928.0,2022-12-21 11:29:16,0,600,"<p>I have a document library which consists of several hundred PDF Documents. I am attempting to export the first page of each PDF document. Below is my script which extracts the page. It saves each page as an individual PDF. However, the files which are exported seem to be exporting in unreadable or damaged format.</p>
<p>Is there something missing from my script?</p>
<pre class=""lang-py prettyprint-override""><code>import os
from PyPDF2 import PdfReader, PdfWriter

# get the file names in the directory
input_directory = &quot;Fund_Docs_Sample&quot;
entries = os.listdir(input_directory)
output_directory = &quot;First Pages&quot;
outputs = os.listdir(output_directory)

for output_file_name in entries:
    reader = PdfReader(input_directory + &quot;/&quot; + output_file_name)
    page = reader.pages[0]
    first_page = &quot;\n&quot; + page.extract_text() + &quot;\n&quot;

    with open(output_file_name, &quot;wb&quot;) as outputStream:
        pdf_writer = PdfWriter(output_file_name + first_page)
</code></pre>
<p><a href=""https://i.stack.imgur.com/i13ix.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i13ix.png"" alt=""enter image description here"" /></a></p>
",18723865.0,562769.0,2022-12-23 11:20:24,2022-12-23 11:20:24,How do I extract the text of a single page with PyPDF2?,<python><data-science><text-mining><pypdf>,2,3,N/A,CC BY-SA 4.0
74891311,1,-1.0,2022-12-22 16:30:16,0,241,"<p>i just start learning Data Analytics, and trying to understand step by step cleaning the data, I have a example data from freeCodeCamp
<a href=""https://github.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example"" rel=""nofollow noreferrer"">https://github.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example</a>
what I confused is in the &quot;data/sales_data.csv&quot; I try to read data in jupyter, but why jupyter cannot read my data properly(head of column renamed accidentally) , this is what jupyter shown on lab
<a href=""https://i.stack.imgur.com/sJijh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sJijh.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/5CmWo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5CmWo.png"" alt=""enter image description here"" /></a></p>
<p>and this is the data should look like before I upload it in jupyter
<a href=""https://i.stack.imgur.com/Wh94x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wh94x.png"" alt=""enter image description here"" /></a></p>
<p>anyone can help me with this problem, I try other csv file and doesn't see this problem</p>
",13589824.0,-1.0,N/A,2023-03-06 21:21:07,jupyter Notebook cannot read head of csv file correctly,<python-3.x><jupyter-notebook><data-science>,1,7,N/A,CC BY-SA 4.0
74906232,1,74908961.0,2022-12-24 07:39:24,1,316,"<p>This error keeps coming when I try to find MI values. My code is as follows</p>
<pre><code>X_new = X.copy()
X_new = X_new.fillna(0)
y = data.SalePrice


def make_mi_scores(X, y):
    X = X.copy()
    for colname in X.select_dtypes([&quot;object&quot;, &quot;category&quot;]):
        X[colname], _ = X[colname].factorize()       
    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]   
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)
    mi_scores = pd.Series(mi_scores, name=&quot;MI Scores&quot;, index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title(&quot;Mutual Information Scores&quot;)
    
plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(make_mi_scores(X_new,y))

</code></pre>
<p>if you want the full notebook here is a link   <a href=""https://www.kaggle.com/code/snigdhkarki/house-price-competition"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/snigdhkarki/house-price-competition</a></p>
<p>The error is as follows</p>
<pre><code>    ValueError                                Traceback (most recent call last)
/tmp/ipykernel_19/1575243112.py in &lt;module&gt;
     42 
     43 plt.figure(dpi=100, figsize=(8, 5))
---&gt; 44 plot_mi_scores(make_mi_scores(X_new,y))

/tmp/ipykernel_19/1575243112.py in make_mi_scores(X, y)
     28     print(X.isnull().any().any())
     29     print(y.isnull().any().any())
---&gt; 30     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)
     31     mi_scores = pd.Series(mi_scores, name=&quot;MI Scores&quot;, index=X.columns)
     32     mi_scores = mi_scores.sort_values(ascending=False)

/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_mutual_info.py in mutual_info_regression(X, y, discrete_features, n_neighbors, copy, random_state)
    382            of a Random Vector&quot;, Probl. Peredachi Inf., 23:2 (1987), 9-16
    383     &quot;&quot;&quot;
--&gt; 384     return _estimate_mi(X, y, discrete_features, False, n_neighbors, copy, random_state)
    385 
    386 

/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_mutual_info.py in _estimate_mi(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)
    300     mi = [
    301         _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)
--&gt; 302         for x, discrete_feature in zip(_iterate_columns(X), discrete_mask)
    303     ]
    304 

/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_mutual_info.py in &lt;listcomp&gt;(.0)
    300     mi = [
    301         _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)
--&gt; 302         for x, discrete_feature in zip(_iterate_columns(X), discrete_mask)
    303     ]
    304 

/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_mutual_info.py in _compute_mi(x, y, x_discrete, y_discrete, n_neighbors)
    160         return mutual_info_score(x, y)
    161     elif x_discrete and not y_discrete:
--&gt; 162         return _compute_mi_cd(y, x, n_neighbors)
    163     elif not x_discrete and y_discrete:
    164         return _compute_mi_cd(x, y, n_neighbors)

/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_mutual_info.py in _compute_mi_cd(c, d, n_neighbors)
    137     radius = radius[mask]
    138 
--&gt; 139     kd = KDTree(c)
    140     m_all = kd.query_radius(c, radius, count_only=True, return_distance=False)
    141     m_all = np.array(m_all) - 1.0

sklearn/neighbors/_binary_tree.pxi in sklearn.neighbors._kd_tree.BinaryTree.__init__()

/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    806                 &quot;Found array with %d sample(s) (shape=%s) while a&quot;
    807                 &quot; minimum of %d is required%s.&quot;
--&gt; 808                 % (n_samples, array.shape, ensure_min_samples, context)
    809             )
    810 

ValueError: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.
</code></pre>
<p>There were only a few places where this question was asked and even in those places I was unable to find any answers to my question</p>
",19785329.0,19785329.0,2022-12-24 16:33:16,2022-12-24 16:33:16,"Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required",<python><machine-learning><scikit-learn><data-science><kaggle>,1,2,N/A,CC BY-SA 4.0
74906851,1,-1.0,2022-12-24 10:00:07,0,15,"<p>I have made this custom kernel of svm using gram matrix. I saved it in pickle file. But this pickle file don't run in streamlit. I got attribute error. Can anyone give any solutions that how i can save and run it?</p>
<pre><code># sinh kernel with sigma multiplied and C added afterwards
def Kernel_sinh(U,V,sigma,C):
    return np.sinh((sigma*np.dot(U,V))+C)
def sinh_kernel(U,V,sigma=0.1,C=0):
    G = np.zeros((U.shape[0], V.shape[0]))
    for i in range(0,U.shape[0]):
        print(i)
        for j in range(0,V.shape[0]):
            G[i][j] = Kernel_sinh(U[i],V[j],sigma,C)
    return G

sinh_regressor=SVR(kernel = &quot;precomputed&quot;)
y_train1=y_train1.ravel()
sigma=float(input(&quot;paramter for sigma: &quot;))
C=float(input(&quot;paramter for C: &quot;))
sinh_regressor.fit(sinh_kernel(x_train1,x_train1,sigma,C),y_train1)
y_pred=sinh_regressor.predict(sinh_kernel(x_test1,x_train1,sigma,C))
y_pred=np.reshape(y_pred,(y_pred.shape[0],1))
y_pred=sc_y.inverse_transform(y_pred)
</code></pre>
<p>Here is the pickle file</p>
<pre><code>with open('sinh_kernel.pkl','wb') as f:
    pickle.dump(sinh_kernel,f)
with open('sinh_regressor.pkl','wb') as f:
    pickle.dump(sinh_regressor,f)
</code></pre>
<p>I tried to open it in streamlit with this :</p>
<pre><code>sinh_kernel=pickle.load(open('C:\system project\Deployment\model\sinh_kernel.pkl','rb'))
sinh_regressor=pickle.load(open('C:\system project\Deployment\model\sinh_regressor.pkl','rb'))
</code></pre>
<p>and i got that error :</p>
<p>Can't get attribute 'sinh_kernel' on &lt;module '_<em>main</em>_' from 'C:\\system project\\Deployment\\final.py'&gt;</p>
<p>please give me a solution of this problem.</p>
<p>I got error. I want to load the pickle file</p>
",11496684.0,-1.0,N/A,2022-12-24 10:00:07,saving the custom kernel model using gram matrix related probelm,<python><machine-learning><data-science><pickle><streamlit>,0,0,N/A,CC BY-SA 4.0
74921189,1,-1.0,2022-12-26 14:19:27,0,183,"<p>I try to read CSV from URL of the pandas dataframe, but it shows an error &quot;SSLCertVerificationError &quot;, how to deal with it? My laptop is M1 macOS, I'm not sure why.
Here is my code:</p>
<pre><code>import pandas as pd
path = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00523/Exasens.csv&quot;
df = pd.read_csv(path, skiprows=3)
</code></pre>
<p>Here is error detail:</p>
<pre><code>SSLCertVerificationError                  Traceback (most recent call last)
File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:1348, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)
   1347 try:
-&gt; 1348     h.request(req.get_method(), req.selector, req.data, headers,
   1349               encode_chunked=req.has_header('Transfer-encoding'))
   1350 except OSError as err: # timeout error

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1282, in HTTPConnection.request(self, method, url, body, headers, encode_chunked)
   1281 &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;
-&gt; 1282 self._send_request(method, url, body, headers, encode_chunked)

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1328, in HTTPConnection._send_request(self, method, url, body, headers, encode_chunked)
   1327     body = _encode(body, 'body')
-&gt; 1328 self.endheaders(body, encode_chunked=encode_chunked)

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1277, in HTTPConnection.endheaders(self, message_body, encode_chunked)
   1276     raise CannotSendHeader()
-&gt; 1277 self._send_output(message_body, encode_chunked=encode_chunked)

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1037, in HTTPConnection._send_output(self, message_body, encode_chunked)
   1036 del self._buffer[:]
-&gt; 1037 self.send(msg)
   1039 if message_body is not None:
   1040 
   1041     # create a consistent interface to message_body

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:975, in HTTPConnection.send(self, data)
    974 if self.auto_open:
--&gt; 975     self.connect()
    976 else:

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1454, in HTTPSConnection.connect(self)
   1452     server_hostname = self.host
-&gt; 1454 self.sock = self._context.wrap_socket(self.sock,
   1455                                       server_hostname=server_hostname)

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:513, in SSLContext.wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)
    507 def wrap_socket(self, sock, server_side=False,
    508                 do_handshake_on_connect=True,
    509                 suppress_ragged_eofs=True,
    510                 server_hostname=None, session=None):
    511     # SSLSocket class handles server_hostname encoding before it calls
    512     # ctx._wrap_socket()
--&gt; 513     return self.sslsocket_class._create(
    514         sock=sock,
    515         server_side=server_side,
    516         do_handshake_on_connect=do_handshake_on_connect,
    517         suppress_ragged_eofs=suppress_ragged_eofs,
    518         server_hostname=server_hostname,
    519         context=self,
    520         session=session
    521     )

</code></pre>
<p>don't know how to solve it.</p>
",20248425.0,-1.0,N/A,2022-12-26 14:19:27,"SSLCertVerificationError in python pandas dataframe , M1 macos",<python><pandas><dataframe><data-science><python-3.10>,0,5,N/A,CC BY-SA 4.0
74921884,1,-1.0,2022-12-26 15:48:30,0,56,"<p>My problem is that I'm trying to visualize some data of the cars.csv containing the variables for age and mileage (kilometers). I want to visualize the relationship between both variables with regression lines. When I'm adding the bin parameter to set the classes for age and kilometer, it is not possible to show the regression line anymore. Maybe someone got a better idea for better visualisation.</p>
<pre class=""lang-py prettyprint-override""><code>import altair as alt

scatter_plot = alt.Chart(sample).mark_bar().encode(
    alt.X('age', stack='zero', bin=True, sort='-y', axis=alt.Axis(labels=True, title='Age')),
    alt.Y('kilometer', bin=True),
    color=alt.Color('count():Q', scale=alt.Scale(scheme='category10'), legend=alt.Legend(title='Count')),
    tooltip=[&quot;count():Q&quot;]
)

scatter_plot + scatter_plot.transform_regression('age', 'kilometer').mark_line()
</code></pre>
<p>The output of code:</p>
<p><img src=""https://i.stack.imgur.com/V5HIY.png"" alt=""output of code"" /></p>
<p>I deleted the bin parameter; it works well for regression but is a complete mess with all the different data points.</p>
",12396787.0,10452700.0,2023-03-11 21:31:37,2023-03-17 01:12:47,Python Altair transform_regression with binned data,<python><data-science><visualization><altair>,1,0,N/A,CC BY-SA 4.0
74908952,1,74909470.0,2022-12-24 16:31:32,1,154,"<p>I have a months time series data that I am trying calculate total hours, minutes, seconds in the dataset as well as for a unique Boolean column for when the column is True or a 1. And for some reason I am doing something wrong where the total time calculations don't appear correct. The code (runs) below goes through calculating the time delta between each index time stamp:</p>
<pre><code>import pandas as pd

df = pd.read_csv('https://raw.githubusercontent.com/bbartling/Data/master/hvac_random_fake_data/testdf2_fc5.csv',
             index_col='Date',
             parse_dates=True)

print(df)

df[&quot;timedelta_alldata&quot;] = df.index.to_series().diff()
seconds_alldata = df.timedelta_alldata.sum().seconds
print('SECONDS ALL DATA: ',seconds_alldata)

days_alldata = df.timedelta_alldata.sum().days
print('DAYS ALL DATA: ',days_alldata)

hours_alldata = round(seconds_alldata/3600, 2)
print('HOURS ALL DATA: ',hours_alldata)

minutes_alldata = round((seconds_alldata/60) % 60, 2)
total_hours_calc = days_alldata * 24.0 + hours_alldata
print('TOTAL HOURS CALC: ',total_hours_calc)

# fault flag 5 true time delta calc
df[&quot;timedelta_fddflag_fc5&quot;] = df.index.to_series(
).diff().where(df[&quot;fc5_flag&quot;] == 1)

seconds_fc5_mode = df.timedelta_fddflag_fc5.sum().seconds
print('FALT FLAG TRUE TOTAL SECONDS: ',seconds_fc5_mode)

hours_fc5_mode = round(seconds_fc5_mode/3600, 2)
print('FALT FLAG TRUE TOTAL HOURS: ',hours_fc5_mode)

percent_true_fc5 = round(df.fc5_flag.mean() * 100, 2)
print('PERCENT TIME WHEN FLAG 5 TRUE: ',percent_true_fc5,'%')

percent_false_fc5 = round((100 - percent_true_fc5), 2)
print('PERCENT TIME WHEN FLAG 5 FALSE: ',percent_false_fc5,'%')
</code></pre>
<p>returns:</p>
<pre><code>SECONDS ALL DATA:  85500   &lt;--- I think NOT correct
DAYS ALL DATA:  30
HOURS ALL DATA:  23.75   &lt;--- I think NOT correct
TOTAL HOURS CALC:  743.75
FALT FLAG TRUE TOTAL SECONDS:  1800   &lt;--- I think NOT correct
FALT FLAG TRUE TOTAL HOURS:  0.5   &lt;--- I think NOT correct
PERCENT TIME WHEN FLAG 5 TRUE:  74.29 %
PERCENT TIME WHEN FLAG 5 FALSE:  25.71 %
</code></pre>
<p>30 days is correct (<code>DAYS ALL DATA:  30</code>) and the percent of time when a Boolean column (<code>fc5_flag</code>) is True or False but the total seconds and hours seems way off...? Would anyone have any tips to write this better?</p>
",8372455.0,8372455.0,2022-12-24 17:30:47,2022-12-24 18:06:37,Pandas calculating time deltas from index,<python><pandas><data-science><data-wrangling><data-scrubbing>,1,4,N/A,CC BY-SA 4.0
74910517,1,-1.0,2022-12-24 21:55:46,-1,67,"<p>I was working on a task where I'm required to find if there is increase in price while increase in number of rooms. I've used <code>ggplot2</code> and <code>geom_point</code>.<a href=""https://i.stack.imgur.com/8Bl5M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Bl5M.png"" alt=""enter image description here"" /></a></p>
<p>But I'm unable to understand is there any increment. Could any one help to make me understand this graph please. Or is there any other way to draw graph so that I can understand easily.</p>
<p>The following line is my code.</p>
<pre><code>ggplot(df, aes(x = rooms, y = price)) + geom_point()
</code></pre>
",13573872.0,-1.0,N/A,2022-12-25 12:09:03,Unable to understand graph data in R language,<r><ggplot2><graph><data-science><geom-point>,2,5,N/A,CC BY-SA 4.0
74924942,1,-1.0,2022-12-27 00:40:31,1,40,"<p><a href=""https://i.stack.imgur.com/gzFur.png"" rel=""nofollow noreferrer"">Radiology Report</a></p>
<p>I am trying to extract the data by the subject ('findings', 'impression') and trying to put it on pandas dataframe</p>
",-1.0,-1.0,N/A,2022-12-27 12:40:13,"How to extract data from this .txt files and put the data into pandas dataframe by columns (like 'Report', 'Findings', 'Impression', 'Recomendation')",<machine-learning><nlp><data-science>,1,2,N/A,CC BY-SA 4.0
74933606,1,-1.0,2022-12-27 19:54:04,0,591,"<p>I am new to Glue and PySpark. I'm trying to create a job that crawls a large volume of small files and consolidates them into larger files. I understand this is a non-issue using pushdown predicates or partition keys. However, the only available partition key in this data is a date column. But, the files are already separated based on that date. Some of the dates (and their subsequent files) are very small (15kb-5MB usually). And the files are organized into directories in S3 of their subsequent tables. Some of these directories are large (40GB+) and are made of thousands of these small files. The file format is parquet. As a requirement, all columns need to be cast to string (don't worry about why. Just know that it is a hard and fast requirement). Some of the S3 top-level keys (table directories) process just fine. But, only the smaller ones. The larger ones error out every time. I'm assuming it's because the executor runs out of memory. Mainly because the large top-level keys do manage to output some consolidated files. But, only 13.8GB worth regardless of the files processed. I'm running the job on a G.1x (16GB of RAM) and can only assume this is because it ran out of memory though I've seen no specific OOM error in the Cloudwatch Logs. G.2x jobs process more files but eventually error out as well. Furthering my thinking this is a RAM issue. I have tried the G1.x job with anywhere from 10 to 150 DPUs. Same result.</p>
<p>The job script is in python and I'm having a heck of a time trying to keep memory down. I've tried reading all files into one pyspark dataframe with the schema I need, repartitioning it to contain roughly 128MB partitions (my desired end file size), reading that dataframe into a DynamicFrame, then writing the result to the Glue Catalog and writing the files out. That fails for what I feel are obvious reasons.</p>
<p>My latest iteration reads the list of file objects from a top-level S3 directory, gets their sizes, and creates a list of lists. The interior lists contain file keys of S3 objects that add up to roughly 1.28GB in size. Then, we read this into a pyspark frame, repartition this frame to have 10 partitions (for 10 ~128MB files), read into dynamic frame, and write those partitions out. I've used this chunking method with traditional functions, which failed. I then created generators that would create both the pyspark frame and dynamic frame. For whatever reason, python won't release the memory even when generators are involved. That led me to even try using the ol' &quot;run the function in a separate process&quot; trick. But, that fails because the Glue workers are not picklable. My goal is to be able to run this one job to iterate over all top-level S3 keys. But, as you can see in the sample code, at this point I'm just trying one of the larger ones.</p>
<p>I'm nearly certain I'm just so green with Glue that I'm not even approaching this the way Glue wants me to. I've been searching SO, YouTube, and random internet guides (including AWS documentation on Glue) but nothing has helped. I am fully aware this code contains a number of techniques that are less than ideal. At this point, I'm just trying to make some headway, so please try to keep your feedback to the question at hand. And that question is: <strong>How do I merge a number of small files into a series of larger files of a fixed size in a Glue job if the total size of the data being processed is greater than the amount of memory in a Glue job?</strong> Below is the code I am trying now and the error I am getting. Additional details available on request. Thanks so much for any help you can provide!</p>
<p><strong>Code</strong></p>
<pre><code>import sys
import math
import boto3
from concurrent.futures import ProcessPoolExecutor
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from pyspark.sql.types import StringType, StructType, StructField

args = getResolvedOptions(sys.argv, [&quot;JOB_NAME&quot;])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args[&quot;JOB_NAME&quot;], args)
s3_resource = boto3.resource('s3')
client = boto3.client('s3')

input_bucket = &quot;test-datastore&quot;
output_bucket = &quot;test-unified-datastore&quot;
num_of_partitions = 10

def get_s3_dirs(input_bucket):
    paginator = client.get_paginator('list_objects')
    result = paginator.paginate(Bucket=input_bucket, Delimiter='/')
    dirs = [prefix.get('Prefix').strip('/') for prefix in result.search('CommonPrefixes')]
    return dirs

def get_file_meta(input_bucket, input_key):
    bucket_obj = s3_resource.Bucket(input_bucket)
    for obj in bucket_obj.objects.filter(Prefix=input_key):
        file_meta = {'path': f's3://{input_bucket}/{obj.key}', 'size': obj.size}
        yield file_meta

def get_chunk_size():
    return math.ceil(num_of_partitions * 128000000)

def create_chunks(input_key):
    sized_chunks = []
    target_chunk_size = get_chunk_size()
    chunks_sized = False
    file_meta = get_file_meta(input_bucket, input_key)
    while not chunks_sized:
        current_chunk = []
        current_chunk_size = 0
        while current_chunk_size &lt; target_chunk_size:
            try:
                meta = next(file_meta)
                file_path = meta['path']
                file_size = meta['size']
                current_chunk_size += file_size
                current_chunk.append(file_path)
            except StopIteration:
                if current_chunk:
                    # if the current chunk is not the only chunk and its smaller than
                    # 100MB, merge it with the last chunk in sized_chunks
                    if sized_chunks and current_chunk_size &lt; (1048576 * 100):
                        current_chunk.extend(sized_chunks.pop(-1))
                chunks_sized = True
                break
        sized_chunks.append(current_chunk)
    return sized_chunks

def gen_create_spark_df(schema, chunk, num_of_partitions):
    spark_df = spark.read.schema(schema).parquet(*chunk).repartition(num_of_partitions)
    yield spark_df

def gen_process_chunks(chunks):
    for chunk in chunks:
        sample_file = chunk[0]
        sample_df = spark.read.parquet(sample_file)
        schema = sample_df.schema
        struct_fields = []
        for field in schema.fields:
            schema_field = StructField(
                field.name,
                StringType(),
                field.nullable
            )
            struct_fields.append(schema_field)
        schema = StructType(struct_fields)
        output_dynamic = DynamicFrame.fromDF(
            next(gen_create_spark_df(schema, chunk, num_of_partitions)), 
            glueContext, &quot;amazonS3_consolidator_3&quot;
        )
        yield output_dynamic

def merge_table(input_key):
    chunks = create_chunks(input_key)
    dfs = gen_process_chunks(chunks)
    for df in dfs:
        AmazonS3_node1670429555763 = glueContext.getSink(
            path=f&quot;s3://{output_bucket}/{input_key}/&quot;,
            connection_type=&quot;s3&quot;,
            updateBehavior=&quot;UPDATE_IN_DATABASE&quot;,
            partitionKeys=[],
            enableUpdateCatalog=True,
            transformation_ctx=&quot;AmazonS3_node1670429555763&quot;,
        )
        AmazonS3_node1670429555763.setCatalogInfo(
            catalogDatabase=&quot;test_db&quot;, catalogTableName=input_key.lower()
        )
        AmazonS3_node1670429555763.setFormat(&quot;glueparquet&quot;)
        AmazonS3_node1670429555763.writeFrame(df)
        

s3_dirs = get_s3_dirs(input_bucket)[7:8]
for input_key in s3_dirs:
    merge_table(input_key)

</code></pre>
<p><strong>Error</strong></p>
<pre><code>Py4JJavaError: An error occurred while calling o200.pyWriteDynamicFrame.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 26.0 failed 4 times, most recent failure: Lost task 24.3 in stage 26.0 (TID 1928, 172.35.39.79, executor 19): ExecutorLostFailure (executor 19 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
    at com.amazonaws.services.glue.sinks.GlueParquetHadoopWriter.doParquetWrite(GlueParquetHadoopWriter.scala:178)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.amazonaws.services.glue.sinks.HadoopDataSink$$anonfun$writeDynamicFrame$1$$anonfun$4.apply$mcV$sp(HadoopDataSink.scala:243)
    at com.amazonaws.services.glue.sinks.HadoopDataSink$$anonfun$writeDynamicFrame$1$$anonfun$4.apply(HadoopDataSink.scala:235)
    at com.amazonaws.services.glue.sinks.HadoopDataSink$$anonfun$writeDynamicFrame$1$$anonfun$4.apply(HadoopDataSink.scala:235)
    at scala.util.Try$.apply(Try.scala:192)
    at com.amazonaws.services.glue.sinks.HadoopDataSink$$anonfun$writeDynamicFrame$1.apply(HadoopDataSink.scala:235)
    at com.amazonaws.services.glue.sinks.HadoopDataSink$$anonfun$writeDynamicFrame$1.apply(HadoopDataSink.scala:149)
    at com.amazonaws.services.glue.util.FileSchemeWrapper$$anonfun$executeWithQualifiedScheme$1.apply(FileSchemeWrapper.scala:89)
    at com.amazonaws.services.glue.util.FileSchemeWrapper$$anonfun$executeWithQualifiedScheme$1.apply(FileSchemeWrapper.scala:89)
    at com.amazonaws.services.glue.util.FileSchemeWrapper.executeWith(FileSchemeWrapper.scala:82)
    at com.amazonaws.services.glue.util.FileSchemeWrapper.executeWithQualifiedScheme(FileSchemeWrapper.scala:89)
    at com.amazonaws.services.glue.sinks.HadoopDataSink.writeDynamicFrame(HadoopDataSink.scala:148)
    at com.amazonaws.services.glue.DataSink.pyWriteDynamicFrame(DataSink.scala:65)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:750)
</code></pre>
<p>I've tried reading the whole top-level key into a dynamic frame, reading the files in the top-level key into chunks and processing each chunk as its own dynamic frame, chunking the files into a dynamic frame created by a generator function, and chunking the files into a dynamic frame that was created in a separate python Process. I've followed a number of AWS Glue guides from AWS and third parties in both blog and YouTube video format.</p>
<p>I've followed this guide from AWS: <a href=""https://aws.amazon.com/blogs/big-data/best-practices-to-scale-apache-spark-jobs-and-partition-data-with-aws-glue/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/big-data/best-practices-to-scale-apache-spark-jobs-and-partition-data-with-aws-glue/</a></p>
<p>And reviewed this SO question: <a href=""https://stackoverflow.com/questions/68886337/aws-glue-job-running-out-of-memory"">AWS Glue job running out of memory</a></p>
<p>And found no relevant help.</p>
<p>I expected the glue jobs to consolidate the large array of small files into a smaller subset of larger files.</p>
<p>Regardless of the technique used, I received an error telling me that the RPC client disassociated due to containers exceeding thresholds or network issues.</p>
",15368705.0,-1.0,N/A,2023-01-04 00:19:02,AWS Glue Job Failing When Merging Small Files,<python><dataframe><pyspark><data-science><aws-glue>,1,1,N/A,CC BY-SA 4.0
74908128,1,-1.0,2022-12-24 13:59:38,0,421,"<p>Can anyone tell me when do we use line='r'??</p>
<p><code>sm.qqplot(df['Delivery_time'], line='r')</code></p>
<p>and when do we use line=45?</p>
<p><code>sm.qqplot(df['Delivery_time'], line='45')</code></p>
",13929274.0,13929274.0,2022-12-24 16:50:20,2022-12-24 18:36:20,When to use line=r and line=45 in qqplot,<python><data-science><statsmodels>,1,1,N/A,CC BY-SA 4.0
74923958,1,74924039.0,2022-12-26 21:03:12,-2,374,"<p>I got this error</p>
<pre><code>The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>
<p>Let be x an array of continuous numerical values between 0 and 10</p>
<pre><code>dts1['NewCol'] = np.apply_along_axis(getbool,axis=0, arr=x)
</code></pre>
<p>Let be getbool a function which returns 1 if the numerical value is more than 5 and 0 otherwise</p>
<pre><code>def getbool(x):
    if x &gt; 5: 
        return 1
    else: 
        return 0
</code></pre>
",20817497.0,20817497.0,2022-12-26 21:10:32,2022-12-26 21:26:08,The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(),<pandas><data-science><data-analysis><exploratory-data-analysis>,1,1,2022-12-26 21:53:13,CC BY-SA 4.0
74924763,1,-1.0,2022-12-26 23:52:20,0,69,"<p>I would like to build a tensorflow model to categorize loans for credit risk.
Unfortunately, I have a problem cause my model puts all cases into one class.
I don't know if it comes from the definition of metadata in specify_feature_usages function.
Maybe this part should be done in another way? But how to do that?
Does this come from imbalanced training data? 1.113 for risk cases and 2.220 cases for others.
Data source used in the project <a href=""https://github.com/kriss024/Python-for-Data-Science/blob/main/credit_data.h5"" rel=""nofollow noreferrer"">credit_data.h5</a>.</p>
<p><a href=""https://i.stack.imgur.com/BMxII.png"" rel=""nofollow noreferrer"">confusion matrix</a></p>
<pre class=""lang-py prettyprint-override""><code>import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 

import pprint as pp
import math
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import Sequential
from tensorflow.keras.layers import DenseFeatures, Dense, Activation
import tensorflow as tf
from tensorflow import keras
from IPython.display import display
import matplotlib.pyplot as plt
sns.set(style='darkgrid')
print('TensorFlow version: ' + tf.__version__)

# credit_data = pd.read_csv('german_credit_data.csv')
# credit_data.to_hdf('credit_data.h5', key='df', mode='w')
credit_data = pd.read_hdf('credit_data.h5', 'df')  
display(credit_data)

data = credit_data.copy()
# Name of the label column.
label = 'Risk'
id_column = 'CustomerID'
data[label].value_counts().to_frame().T

data.info()

train = 2/3
test = 1 - train
label_lst = [label]
train_data, test_data = train_test_split(data, test_size=test, random_state=0, stratify=data[label_lst])
print(f&quot;Train data shape: {train_data.shape}&quot;)
print(f&quot;Test data shape: {test_data.shape}&quot;)
train_data[label].value_counts().to_frame().T

label_new = label + '_target'
train_data[label_new] = train_data[label].apply(lambda x: 1 if x == 'Risk' else 0)
train_data.drop(label, axis=1, inplace=True)

test_data[label_new] = test_data[label].apply(lambda x: 1 if x == 'Risk' else 0)
test_data.drop(label, axis=1, inplace=True)

print(&quot;Training dataset&quot;)
display(train_data.agg({label_new : ['sum', 'count']}))
print(&quot;Test dataset&quot;)
display(test_data.agg({label_new : ['sum', 'count']}))

label = label_new
id_column = 'CustomerID'

### Define continuous list
numericFeatures  = ['LoanDuration','LoanAmount','InstallmentPercent','CurrentResidenceDuration','Age','ExistingCreditsCount','Dependents']
### Define the categorical list
objectFeatures = ['CheckingStatus','CreditHistory','LoanPurpose','ExistingSavings','EmploymentDuration','Sex',
'OthersOnLoan','OwnsProperty','InstallmentPlans','Housing','Job','Telephone','ForeignWorker']

# Target column name.
TARGET_COLUMN_NAME = label
# Numeric feature names.
NUMERIC_FEATURE_NAMES = numericFeatures
# Categorical features and their vocabulary lists.
CATEGORICAL_FEATURE_NAMES = objectFeatures

print(TARGET_COLUMN_NAME)
print(NUMERIC_FEATURE_NAMES)
print(CATEGORICAL_FEATURE_NAMES)

def specify_feature_usages(df, label):
    feature_usages = list()
    feature_names = list()

    for feature_name in NUMERIC_FEATURE_NAMES:
        
        mean = df[feature_name].mean()
        std = df[feature_name].std()
        
        def zscore(x):
            x = tf.dtypes.cast(x, tf.float32)
            return (x - mean)/std
        
        feature_usage = tf.feature_column.numeric_column(key=feature_name, normalizer_fn=zscore)
        feature_usages.append(feature_usage)
        feature_names.append(feature_name)

    for feature_name in CATEGORICAL_FEATURE_NAMES:
        
        aggregate = df.groupby(feature_name)[label].agg(['sum','count'])
        aggregate['share'] = aggregate['sum'] / aggregate['count']
        aggregate.sort_values('share', ascending=False, inplace=True)
        vocabulary = aggregate.index.values.tolist()
        
        feature_usage = tf.feature_column.indicator_column(
            tf.feature_column.categorical_column_with_vocabulary_list(
            key=feature_name, vocabulary_list=vocabulary, default_value=0
        ))
        feature_usages.append(feature_usage)
        feature_names.append(feature_name)

    return feature_usages, feature_names


feature_columns, feature_names = specify_feature_usages(train_data, label)
l_inputs = len(feature_names)
l_outputs = len([label])
print('inputs={inputs}, output={output}'.format(inputs=l_inputs,output=l_outputs))
pp.pprint(feature_columns)

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, target_cols, shuffle=True):
    dataframe = dataframe.copy()
    total_rows = dataframe.shape[0]
    batch_size = int(total_rows/10)
    labels = dataframe[target_cols]
    features = dataframe.drop(target_cols, axis=1)
    ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(features))
        ds = ds.batch(batch_size)
    return ds


# Convert the dataset into a TensorFlow dataset.
train_dataset = df_to_dataset(train_data, label, True)
test_dataset = df_to_dataset(test_data, label, True)

model = Sequential()
model.add(DenseFeatures(feature_columns))
model.add(Dense(24, activation=tf.nn.sigmoid))
model.add(Dense(12, activation=tf.nn.sigmoid))
model.add(Dense(6, activation=tf.nn.sigmoid))
model.add(Dense(l_outputs, activation=tf.nn.softmax))

METRICS = [
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc')
]

model.compile(optimizer=keras.optimizers.Adam(),
              loss=keras.losses.BinaryCrossentropy(),
              metrics=METRICS)



model.fit(train_dataset, verbose=0)

# evaluate the model
print(&quot;Training dataset&quot;)
scores = model.evaluate(train_dataset, verbose=0)
scores_metrics = zip(model.metrics_names, scores)
for m, s in list(scores_metrics):
    print(m, s)
    
print(&quot;Test dataset&quot;)
scores = model.evaluate(test_dataset, verbose=0)
scores_metrics = zip(model.metrics_names, scores)
for m, s in list(scores_metrics):
    print(m, s)


y_predicted = model.predict(test_dataset)

df_hist = pd.DataFrame(y_predicted)
df_hist.columns = [&quot;Risk_prediction&quot;]
sns.histplot(data=df_hist, x=&quot;Risk_prediction&quot;, bins=10)
plt.show()

y_predicted = np.where(y_predicted &gt; 0.5, 1, 0)
y_actual = test_data[label]
matrix = confusion_matrix(y_actual, y_predicted, labels=[1,0])
report = classification_report(y_actual, y_predicted)
print(matrix)
print(report)
</code></pre>
<p>I would appreciate for all advices how to fix model classification.</p>
",8491121.0,8491121.0,2022-12-27 17:10:30,2022-12-27 17:10:30,Tensorflow misclassification problem. All cases are assigned to one class,<python><tensorflow><machine-learning><data-science><tensorflow2.0>,0,2,N/A,CC BY-SA 4.0
74931141,1,-1.0,2022-12-27 15:25:22,0,31,"<p>I have the following setting using matplotlib:
I got particles that have x and y coordinates, and for every particle, I plot the way it moved over some time.</p>
<p>So I have multiple arrays containing x and y coordinates from particles like:</p>
<pre><code>vals_x = [[] for x in x_vals[0]]
vals_y = [[] for x in x_vals[0]]
for i, x in enumerate(x_vals):
    for j, x1 in enumerate(x):
        vals_x[j].append(x1)
        vals_y[j].append(y_vals[i][j])

fig, ax = plt.subplots(figsize=(15, 8))

for i, xs in enumerate(vals_x):
    ax.plot(vals_y[i], xs)
</code></pre>
<p>The problem I am having is that it allowed particles to move from one side of the canvas to the other side ([-100,100] for both axis). So, for example, if a particle leaves the plot &quot;left,&quot; it comes in &quot;right&quot; again.
However, if I plot it like above, matplot logically plots over the whole canvas (sticking to the example from: a line from &quot;left&quot; to &quot;right&quot;). But I want that the line wrapps around the edges of the canvas.</p>
<p>For example, the orange line going from one side of the canvas to the other should not be here. It should go to the left edge of the canvas and start at the right edge instead of plotting the line through the whole canvas.
<a href=""https://i.stack.imgur.com/GLhVA.png"" rel=""nofollow noreferrer"">example of the problem</a></p>
<p>I hope my problem is clear. Is there any easy solution to this?</p>
",20519169.0,19939086.0,2022-12-27 15:53:44,2022-12-27 15:53:44,"Matplotlib avoid plotting ""overshooting"" lines through canvas",<python><matplotlib><data-science>,0,0,N/A,CC BY-SA 4.0
74932736,1,-1.0,2022-12-27 18:08:10,0,65,"<p>Ok so I'm trying to solve a challenge that says I have to add weight to a graph's edges.
I am open to choosing what ever I want as my weight and I choose to add 1 each time I found a duplicated row in the Data frame.
The problem is as in this data set: <a href=""https://www.kaggle.com/datasets/csanhueza/the-marvel-universe-social-network?select=hero-network.csv"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/csanhueza/the-marvel-universe-social-network?select=hero-network.csv</a> (hero network)
we have several rows with 2 columns, I have to find a way to compare each row with another one, and if I found a value more than once add a 1 as its weight. As for .duplicated() function, I know as a fact that there are more rows with the same 2 heroes connected to each other
So my problem is I don't really know how to do it.
any help would be great!</p>
",10945297.0,-1.0,N/A,2022-12-27 19:46:03,Comparing row values in a dataframe,<python><pandas><dataframe><data-science>,2,0,N/A,CC BY-SA 4.0
74929470,1,-1.0,2022-12-27 12:49:27,0,25,"<p>hi i want my results to be presented as i single number like the output below
for example</p>
<p>Precision   0.94  Recall      0.93  F1 score    0.93</p>
<p>these are my data</p>
<blockquote>
<p>data.head()
<a href=""https://i.stack.imgur.com/hqbvv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hqbvv.png"" alt=""enter image description here"" /></a></p>
</blockquote>
<p>the variables with series (dummy variables the data has gender 1 for female and 2 for male so i add separated them into female 0 and 1 , male 0 and 1 that explains the series)</p>
<p>this is my logistic regression code</p>
<pre><code>
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=0)
x_resampled,y_resampled = rus.fit_resample(x1_data,y1_data)
train_x, test_x, train_y, test_y = train_test_split(x_resampled,y_resampled, test_size=0.2, random_state=42)
logreg_under = LogisticRegression()
logreg_under.fit(train_x,train_y)
print(&quot;Logistic Regression Accuracy after undersampling :&quot;,logreg_under.score(test_x, test_y))
print(&quot;Logistic Regression F1 Score after undersampling :&quot;,f1_score(test_y,logreg_under.predict(test_x),average=None))

ymc1_pred=logreg.predict(test_x)

Mc1 = metrics.confusion_matrix(test_y,ymc1_pred)

sns.heatmap(Mc1,annot = True, fmt ='0.3f', linewidth=0.5 ,
            square = True, cbar = False)

plt.ylabel('actual values')
plt.xlabel('predicted Values')
plt.show()
</code></pre>
<p>and this is my out put</p>
<pre><code> Logistic regression model accuracy:0.94
 Logistic Regression F1 Score : [0.9655461  0.47566509] Logistic
 Regression Accuracy after undersampling : 0.902297169964584 Logistic
 Regression F1 Score after undersampling : [0.90023556 0.9042753 ]
</code></pre>
<p>this is my confusion matrix
<a href=""https://i.stack.imgur.com/ydSGY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ydSGY.png"" alt=""enter image description here"" /></a></p>
<p>please i am a beginner and i looked for code to find the preciion and the recall and the avergae F1 score like the below but i could not find any</p>
<pre><code>Precision   0.94  
Recall      0.93 
F1 score    0.93 
</code></pre>
<p>Help me
Thank you</p>
",20822865.0,7117003.0,2022-12-27 13:01:27,2022-12-27 13:01:27,i have a simple error when presenting precision recall and F1score of a logestic regression model,<python><machine-learning><statistics><data-science><precision-recall>,0,0,N/A,CC BY-SA 4.0
74930368,1,-1.0,2022-12-27 14:12:30,0,26,"<p>I have four different datasets. I have merged three of the dataframes correctly. I have same name column in 3rd and 4th dataset. When I merge it with 4th dataset. I am not getting the same name column values in well mannerd way. The user_id is repeating when I merge. I don't want to repeat the user_id. I want to see the value in the del_keys column where it's showing me NaN value rather than it's showing me the value in the last of table. Moreover, I want to merge values of same name column on the basis of their user_id.</p>
<p><a href=""https://i.stack.imgur.com/RYsHk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RYsHk.png"" alt=""enter image description here"" /></a></p>
<p>In the above image you can see what kind of problem I am getting.</p>
<p>My expected output will look like. There should not be repeated user_id.</p>
<p><a href=""https://i.stack.imgur.com/S5JfL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S5JfL.png"" alt=""enter image description here"" /></a></p>
",5677485.0,-1.0,N/A,2022-12-27 15:54:34,How to merge same name column from two different dataframes?,<dataframe><machine-learning><jupyter-notebook><data-science><data-analysis>,2,0,N/A,CC BY-SA 4.0
74937376,1,-1.0,2022-12-28 07:23:15,-1,112,"<p>I have dataset with data from 2019 to 2022, and I want to predict values for the years 2023 to 2025</p>
<pre><code>2019-01-31      11286

2019-02-29      11182
        
2022-07-31      11286
2022-08-31      11182
2022-09-30      12023
2022-10-31      33200
2022-11-30      31228
</code></pre>
<p>Calling <code>df.info()</code> outputs the following:</p>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1430 entries, 0 to 1429
 #   Column       Non-Null Count  Dtype 
---  ------       --------------  ----- 

 0   Date         1430 non-null   object
 1  Sales        1430 non-null   int64 
 
</code></pre>
<p>I have tried with this code :</p>
<pre><code>from pandas.tseries.offsets import DateOffset
future_dates=[df.index[-1]+ DateOffset(months=x)for x in range(0,24)]

error: TypeError
Traceback (most recent call last)
Input In [44], in &lt;cell line: 2&gt;()
      1 from pandas.tseries.offsets import DateOffset
----&gt; 2 future_dates=[df.index[-1]+ DateOffset(months=x)for x in range(0,24)]
TypeError: 'int' object is not callable****

</code></pre>
<p>output should be <a href=""https://i.stack.imgur.com/sjvpr.png"" rel=""nofollow noreferrer"">like this</a></p>
<p>Also tried  this code:</p>
<pre><code>from pandas.tseries.offsets import DateOffset
future_dates=[df.index[-1] + DateOffset(months=x,days = y ,hours = z)for x in range(0,12) for y in range(1,32) for z in range(0,24)] 
future_datest_df=pd.DataFrame(index=future_dates[1:],columns=df.columns)
</code></pre>
<p>still same error</p>
",19624654.0,15673412.0,2022-12-28 08:27:12,2022-12-28 08:27:12,Predict Future values Using LSTM,<python><data-science><predict>,1,2,N/A,CC BY-SA 4.0
74920203,1,-1.0,2022-12-26 12:17:54,-1,174,"<pre><code>Invalid classes inferred from unique values of `y`.  Expected: [0 1], got ['N' 'Y']
</code></pre>
<p>anyone can help me out with this problem it is Xgboost problem i solve this with label encoding but for further steps i face problems</p>
",20864545.0,-1.0,N/A,2022-12-26 12:22:30,"Invalid classes inferred from unique values of `y`. Expected: [0 1], got ['N' 'Y']",<data-science>,1,0,N/A,CC BY-SA 4.0
74927978,1,-1.0,2022-12-27 10:06:11,0,73,"<p>I want to train the AWS xgBoost model for classification into two categories based on my dataset which contains 1000 Emails and the value to predict. Ex data -</p>
<ul>
<li>abc@gmail -- high</li>
<li>123@yahoo -- low</li>
<li>xyz@gmail -- low</li>
<li>jkl@yahoo -- high</li>
</ul>
<p>For training the model on 1000 rows of such data, as xgboost only expects numerical data, I had to use pd.getDummies() which converts this into 1000 columns by 1000 rows of data. Converts the result column into two - isHigh, isLow with values 0 and 1.</p>
<p>Now when I want predictions from my model, it expects an input of 1000 parameters for the REST API(which I created for predictions), ideally, I would like to provide an email as the parameter and get a result of high/low or isHigh value between 0 and 1.</p>
<p>Is the choice of Algorithm wrong?
Is there any other way so that when I ask the model to provide predictions it does not want 100 parameters?</p>
",17925080.0,-1.0,N/A,2022-12-27 10:06:11,Reduce the number of parameters in ML model,<python><amazon-web-services><machine-learning><data-science>,0,2,N/A,CC BY-SA 4.0
74934950,1,-1.0,2022-12-27 23:27:10,0,15,"<p>I built m model as:</p>
<pre><code>X = alldata.drop(columns= ['S&amp;P 500 Price','Date'])
y = alldata['S&amp;P 500 Price']
X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.25,random_state=0)
LR_model = LinearRegression()
LR_model.fit(X_train,y_train)
y_pred = LR_model.predict(X_test)
</code></pre>
<p>When I plot my predict and actual data as below:</p>
<pre><code>plt.plot(y_pred,color='red', alpha=0.6, label =' Predicted Data')
plt.plot(y_test,color='green', alpha=0.6, label='Actual Data')
plt.title('Actual and Predicted Data for S&amp;P 500 Prediction')
plt.show()
</code></pre>
<p>My output come like <a href=""https://i.stack.imgur.com/72goZ.png"" rel=""nofollow noreferrer"">this</a>. As you can see my test actual data test values goes up to like 4000 ish in reality it is 1055 just like predicted data I don't understand that my y_test values seems like over 4000 values in it when I plot it in reality it is not. And I can't udnerstand anything with this plot picture but my performance metrics is really good fo example r2 score 0.97 and validation is good also. Is there any way I can fix this plot?</p>
",19017614.0,-1.0,N/A,2022-12-27 23:27:10,Multivaraible Regression Performca Metrics are Good but Plot is Broken,<python><matplotlib><regression><data-science><linear-regression>,0,0,N/A,CC BY-SA 4.0
74935467,1,-1.0,2022-12-28 01:30:23,0,346,"<p>I'm trying to follow along with <a href=""https://towardsdatascience.com/clustering-geospatial-data-f0584f0b04ec"" rel=""nofollow noreferrer"">this post</a> but when I get to the &quot;Self Organizing Maps (SOMs)&quot; section I run the code and get the following error:</p>
<pre><code>AttributeError: 'MiniSom' object has no attribute 'cluster_centers_'
</code></pre>
<p>It's pretty clear that MiniSom doesn't have the attribute 'cluster_centers_' unlike the Kmeans model which is referenced further above in the post, but my question is... what could be changed to make this code run as intended?</p>
<p>After some research it looks like MiniSom has a win_map attribute which I'm struggling to implement with my dataset.</p>
",6197054.0,-1.0,N/A,2023-02-22 22:24:50,"""AttributeError: 'MiniSom' object has no attribute 'cluster_centers_""",<python><data-science><geospatial>,1,0,N/A,CC BY-SA 4.0
74944314,1,74945513.0,2022-12-28 19:35:53,2,78,"<p>I have text data that looks like the following after extracting from a file and cleaning. I want to put the data into a <code>pandas</code> dataframe where the columns are <code>('EXAMINATION', 'TECHNIQUE', 'COMPARISON', 'FINDINGS', 'IMPRESSION')</code>, and each cell in each row contains the extracted data related to the column name (i.e. the keyword).</p>
<blockquote>
<p>'FINAL REPORT EXAMINATION: CHEST PA AND LAT INDICATION: F with new onset ascites eval for infection TECHNIQUE: Chest PA and lateral COMPARISON: None FINDINGS: There is no focal consolidation pleural effusion or pneumothorax Bilateral nodular opacities that most likely represent nipple shadows The cardiomediastinal silhouette is normal Clips project over the left lung potentially within the breast The imaged upper abdomen is unremarkable Chronic deformity of the posterior left sixth and seventh ribs are noted IMPRESSION: No acute cardiopulmonary process'</p>
</blockquote>
<p>For example, under the column <code>TECHNIQUE</code> there should be a cell containing &quot;Chest PA and lateral&quot;, and under the column <code>IMPRESSION</code>, there should be a cell containing &quot;No acute cardiopulmonary process&quot;.</p>
",10019553.0,20087266.0,2022-12-29 21:18:36,2022-12-29 21:18:36,Text data extraction between keywords in a string,<dataframe><machine-learning><nlp><data-science><data-cleaning>,2,0,N/A,CC BY-SA 4.0
74937913,1,-1.0,2022-12-28 08:25:40,0,205,"<p>I am currently working in a project for my MSc and I am having this issue with that dataset. I don't have previous experience in machine learning and this is my first exposure.</p>
<p>In my dataset I started doing my EDA (Exploratory Data Analysis) and I have a categorical feature with missing data which is <em>Province_State</em>. This column has <code>52360</code> missing values and as a percentage that is a <code>5.40%</code>. I guess that is not too bad and according to what I learnt, I should impute these missing values or delete the column if I have reasonable reasonings.</p>
<p>My logical reasoning is that, not every country has provinces. Therefore that is pretty normal that there are missing values. I clearly don't see a point in imputing these missing values with a random value because that is not logically and it will also lead inaccuracy within the model because we cannot come up with a value which does not practically exist for that particular country.</p>
<p>I think I should do one of the following:</p>
<ul>
<li>Impute all the missing values to a constant value such as -1 or &quot;NotApplicable&quot;</li>
<li>Remove the feature from the dataset</li>
</ul>
<p>Please help me with a solution and thank you very much in advance.</p>
<p>(This dataset can be accessed from this <a href=""https://www.kaggle.com/c/covid19-global-forecasting-week-5/data?select=test.csv"" rel=""nofollow noreferrer"">link</a>)</p>
",7218881.0,-1.0,N/A,2022-12-29 06:37:32,Machine learning with handling features which are suppose to have missing data,<machine-learning><statistics><data-science><bigdata><exploratory-data-analysis>,1,0,N/A,CC BY-SA 4.0
74938099,1,-1.0,2022-12-28 08:46:48,0,33,"<p>I am struggling with a simple commercial objective: let's say we want to boost a sale of a toy in a given country, through an action (sending email to customers). I want to select the customers for sending, and for a personalized email on the toy.</p>
<p>Approach 1: clustering of clients (unsupervised), to understand what are the profiles of our clients. This will give me groups of clients sharing the same characteristics.</p>
<p>Approach 2: for each client, I predict the probability of buying a toy (supervised, using the purchase bahaviour of my existing clients) and based on that I select clients to send the communication to.</p>
<p>Can both approaches work?</p>
<p>What worries me in the clustering is that the groups will be created based on the minimization of the distance between all variables, while I want to have homogeneous groups in terms of the purchase behaviour of the toy.
I expect to see for example that being a parent is an important feature. In the clustering, it will be &quot;one feature between others&quot;, while in approach 2 &quot;buying a toy&quot; will be the target variable, so probably &quot;being a parent&quot; will be triggered important.
So maybe I could choose some variables for the clustering, with help of a decision tree for example. This means combining both supervised and unsupervised.</p>
<p>Is my reasoning correct or did I miss something in the theory? Many thanks for your help!</p>
",20877863.0,2347649.0,2022-12-28 19:43:54,2022-12-28 19:43:54,Boosting product sales: confusion supervised vs unsupervised,<machine-learning><data-science><cluster-analysis><unsupervised-learning><supervised-learning>,0,0,N/A,CC BY-SA 4.0
74946132,1,-1.0,2022-12-29 00:15:01,0,38,"<p>I have two tables, <code>x</code> and <code>y</code>. <code>x</code> has <code>customer_id</code> and <code>reporting_date</code> and <code>y</code> contains <code>customer_id</code> and <code>trigger_date</code>.</p>
<p>For each row in <code>x</code> I want to join the row in <code>y</code> whose <code>trigger_date</code> is the nearest to the <code>reporting_date</code> in <code>x</code>. I have hard-coded my desired result as <code>z</code> in the example below, which should run in the terminal out-of-the-box.</p>
<pre><code>import pandas as pd
import numpy as np

x = pd.DataFrame({
    'customer_id': [1, 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4],
    'reporting_date': [
        '2021-09-01',
        '2021-10-01',
        '2021-11-01',
        '2019-01-01',
        '2019-02-01',
        '2021-12-01',
        '2022-01-01',
        '2022-02-01',
        '2022-03-01',
        '2022-04-01',
        '2022-07-01',
        '2022-08-01',
        '2022-09-01' 
    ],
    'payment_status': [0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2]
})

y = pd.DataFrame({
    'customer_id': [1, 1, 3, 3, 3, 4],
    'trigger_date': [
        '2019-08-21',
        '2021-09-17',
        '2020-01-01',
        '2022-01-03',
        '2022-03-08',
        '2022-10-01']})

# DESIRED RESULT IS z:
z = x.__deepcopy__()
z['nearest_trigger_date'] = [
    '2019-09-17',
    '2019-09-17',
    '2019-09-17',
    np.nan,
    np.nan,
    '2022-01-03',
    '2022-01-03',
    '2022-01-03',
    '2022-03-08',
    '2022-03-08',
    '2022-10-01',
    '2022-10-01',
    '2022-10-01' ]

x['reporting_date'] = x['reporting_date'].apply(pd.to_datetime)
y['trigger_date'] = y['trigger_date'].apply(pd.to_datetime)
z['nearest_trigger_date'] = z['nearest_trigger_date'].apply(pd.to_datetime)

</code></pre>
<p>EDIT This is <strong>not</strong> a duplicate of <a href=""https://stackoverflow.com/questions/66938440/merging-series-of-pandas-dataframe-into-single-dataframe"">Merging series of pandas dataframe into single dataframe</a>. That is a concatenation, not a &quot;merge&quot; or &quot;join&quot;. And pd.merge_asof only supports merging on one column.</p>
",4726737.0,4726737.0,2022-12-29 13:06:27,2022-12-30 01:16:45,How to join on an id and the closest date in Pandas?,<python><pandas><join><filter><data-science>,0,1,N/A,CC BY-SA 4.0
74961042,1,-1.0,2022-12-30 11:31:00,2,90,"<p>I am new to NLP and Transformers library. Perhaps my doubt is naive but I am not finding a good solution for it.</p>
<p>I have documents whose content in sensitive and it is a requirements of mine not to publish it clearly on cloud. However my model is running on a Cloud Virtual Machine.</p>
<p>My idea would be to perform OCR and Tokenization on premise and then uploading the results.</p>
<p>However, tokenization with PreTrainedTokenizer by Transformers library returns the ids of the token from its vocabulary, and everyone can decode it having the same pretrained model.</p>
<p>So here is the question: it is possible to fine-tune or just change the vocabulary index so that the tokenization can't be easily decoded?</p>
",19609320.0,-1.0,N/A,2022-12-30 11:31:00,I need to make a Pre Trained Tokenizer (Hugging Face) safer for privacy,<nlp><data-science><ocr><tokenize><huggingface-tokenizers>,0,0,N/A,CC BY-SA 4.0
74966373,1,-1.0,2022-12-30 22:45:14,0,287,"<p>Don't know how to do it.</p>
<p>Write a function <code>C = make_means(k, radius)</code> that outputs a <code>k x 2</code> data matrix <code>C</code> containing the 2D coordinates of <code>k</code> means (of the data to be generated later). The means of the data generator must lie on the vertices of a regular polygon (if <code>k=3</code> the polygon is a equilateral triangle, if <code>k=6</code> it is a regular hexagon, etc). Write your own code to determine the position of the vertices of a regular polygon given a radius value (<code>input radius</code>) of the circle centered in the origin and inscribing the regular polygon. The first point of the polygon on the x-axis.</p>
<p>For example <code>make_means(3, radius=1)</code> would yield:</p>
<blockquote>
<p>[[ 1. ,  0.       ],</p>
</blockquote>
<blockquote>
<p>[-0.5 ,  0.8660254],</p>
</blockquote>
<blockquote>
<p>[-0.5 , -0.8660254]]</p>
</blockquote>
<p>here is my code:</p>
<pre><code>for i in range(0, 360, 72):
    theta = np.radians(i)
    mean = np.array([1., 0.])
    c, s = np.cos(theta), np.sin(theta)
    cov = np.array(((c, -s), (s, c)))
    C = np.random.multivariate_normal(mean, cov, size= 1000)
    #C
    plt.scatter(C[:, 0], C[:, 1])
</code></pre>
<p>but it seems does not appear to rotate.</p>
",20248425.0,-1.0,N/A,2023-01-02 23:18:45,data generator of a regular polygon,<python><data-science><data-generation>,1,0,N/A,CC BY-SA 4.0
74966513,1,74966878.0,2022-12-30 23:17:46,1,107,"<p>I was trying to extract text data from files in different sub-directories and put the extracted data into pandas dataframes.</p>
<p>An example of the text data is given below:</p>
<blockquote>
<p>&quot;EXAMINATION: CHEST PA AND LAT INDICATION: History: F with shortness of breath TECHNIQUE: Chest PA and lateral COMPARISON: FINDINGS: The cardiac mediastinal and hilar contours are normal. Pulmonary vasculature is normal. Lungs are clear. No pleural effusion or pneumothorax is present. Multiple clips are again seen projecting over the left breast. Remote leftsided rib fractures are also re demonstrated. IMPRESSION: No acute cardiopulmonary abnormality.&quot;</p>
</blockquote>
<p>However, when attempting to execute the code given below, it produced the following error, how do I resolve this?</p>
<h3>Error</h3>
<pre class=""lang-none prettyprint-override""><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-108-bbeeb452bdef&gt; in &lt;module&gt;
     48         df = pd.DataFrame(columns=keywords)
     49         # Extract text
---&gt; 50         result = extract_text_using_keywords(text, keywords)
     51         # Append list of extracted text to the end of the pandas df
     52         df.loc[len(df)] = result

&lt;ipython-input-108-bbeeb452bdef&gt; in extract_text_using_keywords(clean_text, keyword_list)
     39             for prev_kw, current_kw in zip(keyword_list, keyword_list[1:]):
     40                 prev_kw_index = clean_text.index(prev_kw)
---&gt; 41                 current_kw_index = clean_text.index(current_kw)
     42                 extracted_texts.append(clean_text[prev_kw_index + len(prev_kw) + 2:current_kw_index])
     43                 if current_kw == keyword_list[-1]:

ValueError: substring not found
</code></pre>
<h3>Code</h3>
<pre class=""lang-py prettyprint-override""><code>out = []
result = {}

for filename in glob.iglob('/content/sample_data/**/*.txt', recursive = True):
    
    out.append(filename)

print('File names: ',out)

for file in out:
      
        with open(file) as f:
          data = f.read()
          
    
        import re
        text = re.sub(r&quot;[-_()\n\&quot;#//@;&lt;&gt;{}=~|?,]*&quot;, &quot;&quot;, data)
        text = re.sub(r'FINAL REPORT', '', text)
        text = re.sub(r'\s+', ' ', text)
        print(text)

        keywords = [&quot;INDICATION&quot;, &quot;TECHNIQUE&quot;, &quot;COMPARISON&quot;, &quot;FINDINGS&quot;, &quot;IMPRESSION&quot;]

        # Create function to extract text between each of the keywords
        # Assumption
        def extract_text_using_keywords(clean_text, keyword_list):
            extracted_texts = []
            for prev_kw, current_kw in zip(keyword_list, keyword_list[1:]):
                prev_kw_index = clean_text.index(prev_kw)
                current_kw_index = clean_text.index(current_kw)
                extracted_texts.append(clean_text[prev_kw_index + len(prev_kw) + 2:current_kw_index])
                if current_kw == keyword_list[-1]:
                    extracted_texts.append(clean_text[current_kw_index + len(current_kw) + 2:len(clean_text)])
            return extracted_texts

        # Create empty pandas df with keywords as column names
        df = pd.DataFrame(columns=keywords)
        # Extract text
        result = extract_text_using_keywords(text, keywords)
        # Append list of extracted text to the end of the pandas df
        df.loc[len(df)] = result

        #print(df)

        with pd.option_context('display.max_colwidth', None): # For diplaying full columns
          display(df)
</code></pre>
",10019553.0,20087266.0,2022-12-31 13:58:56,2023-01-03 22:48:26,"Extracting text data from files in different sub-directories raises ""ValueError: substring not found""",<python><machine-learning><nlp><data-science>,1,0,N/A,CC BY-SA 4.0
74947287,1,74947366.0,2022-12-29 04:36:20,0,41,"<p>I have two data frames like the samples given below:</p>
<pre><code>df1 = pd.DataFrame({&quot;items&quot;:[&quot;i1&quot;, &quot;i1&quot;, &quot;i1&quot;, &quot;i2&quot;,&quot;i2&quot;, &quot;i2&quot;], &quot;dates&quot;:[&quot;09-Nov-2022&quot;, &quot;10-Aug-2022&quot;, &quot;27-May-2022&quot;, &quot;20-Oct-2022&quot;, &quot;01-Nov-2022&quot;,&quot;27-Jul-2022&quot;]})
df2 = pd.DataFrame({&quot;items&quot;: [&quot;i1&quot;, &quot;i1&quot;, &quot;i1&quot;, &quot;i1&quot;, &quot;i2&quot;, &quot;i2&quot;], &quot;prod_mmmyyyy&quot;: [&quot;Sep 2022&quot;, &quot;Jun 2022&quot;, &quot;Mar 2022&quot;, &quot;Dec 2021&quot;, &quot;Sep 2022&quot;, &quot;Jun 2022&quot;]})
</code></pre>
<p>Here I wanted to map df1 into df2 with the next closest date of df2.prod_mmmyyyy from df1.dates column for each category on items.
The expected result would be like this below:</p>
<pre><code>result = pd.DataFrame({&quot;items&quot;:[&quot;i1&quot;, &quot;i1&quot;, &quot;i1&quot;, &quot;i1&quot;, &quot;i2&quot;, &quot;i2&quot;], 
                        &quot;prod_mmmyyyy&quot;: [&quot;Sep 2022&quot;, &quot;Jun 2022&quot;, &quot;Mar 2022&quot;, &quot;Dec 2021&quot;, &quot;Sep 2022&quot;, &quot;Jun 2022&quot;],
                        &quot;mapped_date&quot;: [&quot;09-Nov-2022&quot;, &quot;10-Aug-2022&quot;, &quot;27-May-2022&quot;, &quot;27-May-2022&quot;, &quot;20-Oct-2022&quot;, &quot;27-Jul-2022&quot;]})
</code></pre>
<p>I have tried to convert the prod_mmmyyyy to month end date then group by on the items but after that what should be done is being very difficult for me.</p>
<p>Thanks in advance for any help provided.</p>
",6291574.0,6291574.0,2023-01-18 06:42:45,2023-01-18 06:42:45,how to merge next nearest data points using a common datetime stamp value for a group?,<python><pandas><dataframe><group-by><data-science>,1,1,2022-12-29 04:45:30,CC BY-SA 4.0
74948199,1,74948674.0,2022-12-29 07:08:53,0,149,"<p>I had 2 different dataset and a merge them with:</p>
<pre><code>aaa = pd.merge(time_aragonit, floridaco2, how='inner', on='Time')
</code></pre>
<p>This is my dataset after the merge:</p>
<p><a href=""https://i.stack.imgur.com/PaTdd.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I have datetime type index and when i want to create a correlation matrix, the output look like this:
<a href=""https://i.stack.imgur.com/zu1so.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Why is the correlation between the values not shown? Where am i doing wrong could you please help me?</p>
<p>i also tried join but got the same result</p>
<pre><code>aaa=time_aragonit.join(floridaco2)
</code></pre>
",20786733.0,20786733.0,2022-12-29 07:24:49,2022-12-29 08:10:25,create a correlation matrix with a datetime type index dataset in python,<python><pandas><dataframe><seaborn><data-science>,1,1,N/A,CC BY-SA 4.0
74955875,1,-1.0,2022-12-29 20:40:24,0,84,"<p>For example if I have a large dataframe of all individuals in a zoo and two columns are Animal_Common_Name and Animal_Scientific_Name. I suspect one of those is redundant as one characteristic is totally determined by the other and viceversa. Basically are the same charasteristic but renamed.</p>
<p>Is there any fuction that selected two different columns tell you so?</p>
",20883512.0,-1.0,N/A,2022-12-30 07:09:49,Is there any way in a Python dataframe to see if two columns are the same but with renamed values?,<python><pandas><dataframe><numpy><data-science>,4,2,N/A,CC BY-SA 4.0
74956039,1,-1.0,2022-12-29 21:03:12,0,43,"<p>I'm calculating the cumulative sum of one specific Clothes store stock over time (grouped by Family, Groups, Year and months). To be able to re-establish the stock levels of the past based on three values: Number of purchased items , Number of sold items and the current stock i have today.</p>
<p><a href=""https://i.stack.imgur.com/wcc67.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wcc67.png"" alt=""Initialy my dataframe looks like these"" /></a></p>
<p>I have already solved the calculation problem by: Merging the stock table with the movement table and calculating the <em>mov_itens['novo_estoque']</em> with the formula bellow:</p>
<pre><code>mov_itens['novo_estoque'] = mov_itens['vendas'] - mov_itens['compras'] + mov_itens['estoque']

</code></pre>
<p>Then i have transformed it on a multi-index dataframe. Where indexes are respecticelly: codfamily, codgroup, year and month. By doing:</p>
<pre><code>gruposemindice = mov_itens.groupby(['codfamilia','codgrupo','ano','mes']).sum()

</code></pre>
<p>And calculated the <code>CUMSUM()</code> on the 'estoque' column. Where i could not use it after a map or something like that, because i wasn't able to return (to my new dataframe) the other columns that shouldn't receive the cumulative sum.</p>
<pre><code> gruposemindice_ord = gruposemindice.sort_index(ascending=False)

</code></pre>
<pre><code>for i in gruposemindice_ord.index:
    if(f == i[0]):#codfamilia
        if(g == i[1]):#codgrupo
                gruposemindice_ord.loc[i[:-2]]['estoque'] = (gruposemindice_ord.loc[i[:-2]]['novo_estoque']).cumsum()
                #calcular giro de estoque nessa linha
                print(gruposemindice_ord.loc[i[:-2]])

        else:
            g = i[1]
    else:
        f = i[0]
</code></pre>
<p>The problem is that I'm doing it iteratively and the dataframe is sorted DESCENDING by index, which makes the query last like order(n) for each index that I access. In fact, it should be Order (1) [direct access], to be fast enough and do not cause a bottleneck and these errors that are appearing....</p>
<pre><code>gruposemindice_ord.loc[i[:-2]]['estoque'] = (gruposemindice_ord.loc[i[:-2]]['novo_estoque']).cumsum()
C:\Users\Diego\AppData\Local\Temp/ipykernel_20264/1416332248.py:10: **PerformanceWarning: indexing past lexsort depth may impact performance.**
  print(gruposemindice_ord.loc[i[:-2]])
C:\Users\Diego\AppData\Local\Temp/ipykernel_20264/1416332248.py:9: PerformanceWarning: indexing past lexsort depth may impact performance.
  gruposemindice_ord.loc[i[:-2]]['estoque'] = (gruposemindice_ord.loc[i[:-2]]['novo_estoque']).cumsum()
C:\Users\Diego\AppData\Local\Temp/ipykernel_20264/1416332248.py:9: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
</code></pre>
<p>According to python,I can correct it just ASCENDING sorting the dataframe before doing the manipulation, but I need a DESCENDING sorting, because stock info is only available for the current month, which is the last month of the table, and then I calculate the previous months starting for it.... If I sort the other way the calculation will not work.</p>
<p><strong>Some Observations</strong></p>
<ul>
<li><p>The dataframe Multi-indexes are the numbers of the Families, Groups and Brands of the store, so I can't re-index and loose these numbers,</p>
</li>
<li><p>Also i cannot do this by account on an ascending order, as my first stock is on the last month</p>
</li>
</ul>
<p>I am already checking if the sort and transaction are correct (As pointed on another stack answer).</p>
<pre><code>gruposemindice_ord = gruposemindice.sort_index(ascending=False)
gruposemindice_ord.index.is_lexsorted()
</code></pre>
<p>Hope someone can help me!
Best Regards, Diego Mello</p>
",3954572.0,-1.0,N/A,2022-12-29 21:03:12,Optimize Past stock calculation using cumulative sum in python and pandas without iterating through dataframe (Performance warning),<python-3.x><pandas><python-2.7><data-science><cumulative-sum>,0,1,N/A,CC BY-SA 4.0
74967680,1,-1.0,2022-12-31 05:23:56,0,68,"<p>I have a highly skewed and heavy tailed data. For this reason, instead of using the adjusted boxplot in R (which accounts only for the skewness of the data and not the underlying heaviness of the tail), I wanted to use the generalized boxplot since it accounts for both skewness and heaviness of the tail.</p>
<p>I am unable to try the generalized boxplot since I could not find its R package (if any).</p>
<p><a href=""https://sci-hub.hkvisa.net/10.1016/j.spl.2014.08.016"" rel=""nofollow noreferrer"">https://sci-hub.hkvisa.net/10.1016/j.spl.2014.08.016</a></p>
",20898222.0,5221626.0,2022-12-31 07:04:42,2022-12-31 07:04:42,"Is there any R package to create the generalized box plot as mentioned in the research paper of (Bruffaerts, et al., 2014)",<r><data-science><exploratory-data-analysis>,0,5,N/A,CC BY-SA 4.0
74959530,1,-1.0,2022-12-30 08:23:59,0,474,"<p>i need my  precision,recall and f1 score results to be like the output below</p>
<pre><code>precision  0.98
recall     0.98 
f1 score   0.93
</code></pre>
<p>the numbers are just an example</p>
<p>here is my data head
<a href=""https://i.stack.imgur.com/jtdUi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jtdUi.png"" alt=""enter image description here"" /></a></p>
<p>here is my code</p>
<pre><code>    #training and test sample :
x1_training_data, x1_test_data, y1_training_data, y1_test_data = train_test_split(x1_data, y1_data, test_size = 0.3)

    # Estimation result:
logit_model=sm.Logit(y1_training_data,x1_training_data)
result1=logit_model.fit()
print(result1.summary2())

    # Model Evaluation:
logreg=LogisticRegression()
logreg.fit(x1_training_data,y1_training_data)
y1_pred=logreg.predict(x1_test_data)
print('Logistic regression model accuracy:{:.2f}'.format(logreg.score(x1_test_data,y1_test_data)))
print(&quot;Logistic Regression F1 Score :&quot;,f1_score(y1_test_data,logreg.predict(x1_test_data),average=None))
</code></pre>
<p>here is my results of the code</p>
<pre><code>logistic Regression Accuracy after undersampling : 0.902297169964584
Logistic Regression F1 Score after undersampling : [0.90023556 0.9042753 ]
</code></pre>
<p>i had two numbers for the F1 score i wanted to be just one number and i do not know how
and i tried to find a code to find out the precision or the recall and i could not find any</p>
<p>please help me at least with the F1 score output
Thank you</p>
",20822865.0,-1.0,N/A,2022-12-30 10:26:44,"How to compute precision,recall and f1 score of an balanced logistic regression model in python",<python><machine-learning><statistics><data-science><precision-recall>,1,0,N/A,CC BY-SA 4.0
74968842,1,-1.0,2022-12-31 10:09:49,0,63,"<p>To simplify think about I have data of only two columns:</p>
<p>MonthlyIncome: numerical</p>
<p>Attrition: Yes or No</p>
<p>When I draw a histplot, it gave me a picture like this:</p>
<p><a href=""https://i.stack.imgur.com/1WfX1.png"" rel=""nofollow noreferrer"">histplot</a>
<a href=""https://i.stack.imgur.com/SKrNk.png"" rel=""nofollow noreferrer"">lineplot</a></p>
<p>I can see that in 2500 MonthlyIncome area, Attrition=Yes counts is high, but also Attrition=No is high.
Same in 10000 Income bin area. Attrition=Yes is high but also Attrition=No is high.</p>
<p>So I can not decide if this 2500 (or 10000) is important, is it enough to make that conclusion ''2500 Income People tend to leave job''. Maybe it is not because in the all bins attrition rate might be %20. Maybe machine learning models can understand but I can not understand</p>
<p>Could you give me some information and code sample to determine if this areas important or not?
How can I convert them to percentage format?</p>
",13938389.0,13938389.0,2022-12-31 10:22:57,2022-12-31 10:22:57,How to understand percentages of hue and significance of x value from histplot,<python><pandas><statistics><data-science>,0,0,N/A,CC BY-SA 4.0
74954392,1,-1.0,2022-12-29 17:42:16,0,31,"<p>I've got an excel file which contains text data that I have to clean.</p>
<p>Example of the data I have</p>
<p><img src=""https://i.stack.imgur.com/TZmab.png"" alt=""1"" /></p>
<p>Each sentence contains many informations that are messed up. I have to extract dimension, colors, size, type of the product... from each sentence.</p>
<p>I really don't know where to start. I've thought of using NLTK or some NLP techniques but I'm still not sure what to do exactly.</p>
",15502482.0,2395282.0,2022-12-29 18:55:51,2022-12-29 18:55:51,Categorizing/classifying unlabeled text data,<python><text><nlp><data-science><data-cleaning>,0,1,N/A,CC BY-SA 4.0
74954492,1,-1.0,2022-12-29 17:53:24,-1,78,"<p>Let's say that you have an insanely large dataframe with several observations (rows) and labels/characteristics (columns) and the first thing you want to do is to exclude all the columns who has irrelevant informantion. For that, you need to first of all, glance over the different values the columns, but you can't truly do that with head or tail.</p>
<p>Is there a fuction who returns all the non-repeated values of the columns of a dataframe instead of doing column by column? Thank in advance</p>
<p>I'm able to do it with single columns through the fuction unique. For example using df.color.unique(), it gives me the list of the different colors that there are but I want to do it directly for all of the 100 colums of my dataframe</p>
",20883512.0,-1.0,N/A,2022-12-29 23:01:52,Is there any way to show the different values of all the columns of a dataframe in python?,<python><pandas><dataframe><numpy><data-science>,1,3,2022-12-30 12:01:44,CC BY-SA 4.0
74955186,1,-1.0,2022-12-29 19:12:25,1,32,"<p>I tried to create a CNN but I got an error. Could you figure out what I did wrong?</p>
<pre class=""lang-py prettyprint-override""><code>C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=&quot;float32&quot;)

def score(y_true, y_pred):
    tf.dtypes.cast(y_true, tf.float32)
    tf.dtypes.cast(y_pred, tf.float32)
    sigma = y_pred[:, 2] - y_pred[:, 0]
    fvc_pred = y_pred[:, 1]

    # sigma_clip = sigma + C1
    sigma_clip = tf.maximum(sigma, C1)
    delta = tf.abs(y_true[:, 0] - fvc_pred)
    delta = tf.minimum(delta, C2)
    sq2 = tf.sqrt(tf.dtypes.cast(2, dtype=tf.float32))
    metric = (delta / sigma_clip) * sq2 + tf.math.log(sigma_clip * sq2)

    return K.mean(metric)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def mloss(_lambda):
    def loss(y_true, y_pred):
        return _lambda * qloss(y_true, y_pred) + (1 - _lambda) * score(y_true, y_pred)

    return loss
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def make_model():
    z = L.Input((9,), name=&quot;Patient&quot;)
    x = L.Dense(100, activation=&quot;relu&quot;, name=&quot;d1&quot;)(z)
    x = L.Dense(100, activation=&quot;relu&quot;, name=&quot;d2&quot;)(x)
    
    p1 = L.Dense(3, activation=&quot;linear&quot;, name=&quot;p1&quot;)(x)
    p2 = L.Dense(3, activation=&quot;relu&quot;, name=&quot;p2&quot;)(x)
    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), name=&quot;preds&quot;)([p1, p2])

    model = M.Model(z, preds, name=&quot;CNN&quot;)

    model.compile(loss=mloss(0.8), optimizer=&quot;adam&quot;, metrics=[score])
    return model
</code></pre>
<pre class=""lang-py prettyprint-override""><code>net = make_model()

net.fit(z[tr_idx], y[tr_idx], batch_size=200, epochs=1000,
        validation_data=(z[val_idx], y[val_idx]), verbose=0)
</code></pre>
<blockquote>
<p>Error here: ValueError: Tensor conversion requested dtype int64 for
Tensor with dtype float32: &lt;tf.Tensor 'CNN/preds/add:0' shape=(None,
3) dtype=float32&gt;</p>
</blockquote>
<p>type cast but didn't solve the problem</p>
",14777212.0,2347649.0,2022-12-30 10:31:14,2022-12-30 10:31:14,Tensor conversion requested dtype int64 for Tensor with dtype float32 when creating CNN model,<deep-learning><conv-neural-network><data-science>,0,0,N/A,CC BY-SA 4.0
